---
title: 35、MyBatis性能优化
---
## 📚 目录

1. [N+1查询问题](#1-N+1查询问题)
2. [批量操作优化](#2-批量操作优化)
3. [延迟加载配置](#3-延迟加载配置)
4. [连接池调优](#4-连接池调优)
5. [SQL语句优化](#5-SQL语句优化)
6. [索引使用优化](#6-索引使用优化)
7. [缓存策略优化](#7-缓存策略优化)
8. [大数据量处理](#8-大数据量处理)
9. [内存使用优化](#9-内存使用优化)
10. [并发性能优化](#10-并发性能优化)
11. [核心要点总结](#11-核心要点总结)

---

## 1. 🚨 N+1查询问题


### 1.1 什么是N+1查询问题


**通俗理解**：就像你去超市买东西，本来可以一次性把所有商品拿齐，但你却先拿一个商品，再回头拿第二个，再回头拿第三个...这样来回跑N次，效率特别低。

```
现实场景对比：

❌ N+1查询（低效）：
第1次：查询所有用户（1条SQL）
第2次：查询用户1的订单（1条SQL）
第3次：查询用户2的订单（1条SQL）
第4次：查询用户3的订单（1条SQL）
...
总共：1 + N条SQL

✅ 一次性查询（高效）：
第1次：一次性查询所有用户和订单（1条SQL）
总共：1条SQL
```

### 1.2 N+1问题的典型场景


**场景演示**：查询用户及其订单信息

```java
// ❌ 错误示范：会产生N+1查询
List<User> users = userMapper.selectAll(); // 1条SQL：查询所有用户

for(User user : users) {
    // 每个用户都执行一次查询订单的SQL，如果有100个用户就是100条SQL
    List<Order> orders = orderMapper.selectByUserId(user.getId());
    user.setOrders(orders);
}
// 总共：1 + 100 = 101条SQL 😱
```

**为什么会这样**：
- 第一次查询得到所有用户（1条SQL）
- 循环中每个用户都要单独查订单（N条SQL）
- 数据库执行了太多次查询，性能极差

### 1.3 解决方案一：联表查询


**原理**：一次性把相关数据都查出来，就像一次性把购物车的东西全拿走。

```xml
<!-- ✅ 使用JOIN一次性查询 -->
<select id="selectUsersWithOrders" resultMap="UserOrderMap">
    SELECT 
        u.id as user_id,
        u.name as user_name,
        o.id as order_id,
        o.order_no,
        o.amount
    FROM users u
    LEFT JOIN orders o ON u.id = o.user_id
</select>

<resultMap id="UserOrderMap" type="User">
    <id property="id" column="user_id"/>
    <result property="name" column="user_name"/>
    <collection property="orders" ofType="Order">
        <id property="id" column="order_id"/>
        <result property="orderNo" column="order_no"/>
        <result property="amount" column="amount"/>
    </collection>
</resultMap>
```

**优势对比**：

| 方式 | SQL执行次数 | 性能 | 适用场景 |
|------|------------|------|---------|
| ❌ **N+1查询** | `1 + N条` | 极差 | 数据量小时勉强可用 |
| ✅ **联表查询** | `1条` | 优秀 | 关联数据不是特别多时推荐 |

### 1.4 解决方案二：分步查询+延迟加载


**原理**：先不急着查关联数据，等真正用到的时候再查（懒加载思想）。

```xml
<!-- 第一步：查询用户 -->
<select id="selectUsers" resultMap="UserLazyMap">
    SELECT * FROM users
</select>

<resultMap id="UserLazyMap" type="User">
    <id property="id" column="id"/>
    <result property="name" column="name"/>
    <!-- fetchType="lazy" 表示延迟加载 -->
    <collection property="orders" 
                select="selectOrdersByUserId" 
                column="id"
                fetchType="lazy"/>
</resultMap>

<!-- 第二步：按需查询订单（只有访问orders属性时才执行） -->
<select id="selectOrdersByUserId" resultType="Order">
    SELECT * FROM orders WHERE user_id = #{userId}
</select>
```

**使用示例**：

```java
List<User> users = userMapper.selectUsers(); // 只查用户，不查订单

// 如果只需要用户名，不访问orders，就不会触发订单查询
for(User user : users) {
    System.out.println(user.getName()); // ✅ 不触发订单查询
}

// 只有访问orders时才会查询
User user = users.get(0);
List<Order> orders = user.getOrders(); // 🔄 此时才执行订单查询SQL
```

---

## 2. ⚡ 批量操作优化


### 2.1 为什么需要批量操作


**通俗理解**：就像快递员送包裹，一次送一个包裹要跑100趟，一次送100个包裹只需要1趟。

```
场景对比：

❌ 逐条插入（慢）：
insert into users values(1, 'Tom');   -- 执行1次
insert into users values(2, 'Jerry'); -- 执行1次
insert into users values(3, 'Bob');   -- 执行1次
...执行100次数据库操作

✅ 批量插入（快）：
insert into users values
(1, 'Tom'),
(2, 'Jerry'),
(3, 'Bob'),
...
(100, 'Alice');  -- 只执行1次
```

### 2.2 批量插入实现


**方式一：foreach标签批量插入**

```xml
<insert id="batchInsert">
    INSERT INTO users (name, age, email) VALUES
    <foreach collection="list" item="user" separator=",">
        (#{user.name}, #{user.age}, #{user.email})
    </foreach>
</insert>
```

```java
// 使用示例
List<User> users = new ArrayList<>();
users.add(new User("Tom", 20, "tom@example.com"));
users.add(new User("Jerry", 22, "jerry@example.com"));
// ... 添加更多用户

userMapper.batchInsert(users); // ✅ 一次性插入所有数据
```

**性能对比**：

```
插入1000条数据：
逐条插入：约3-5秒 ❌
批量插入：约0.3-0.5秒 ✅

性能提升：约10倍
```

### 2.3 批量更新实现


```xml
<update id="batchUpdate">
    <foreach collection="list" item="user" separator=";">
        UPDATE users 
        SET name = #{user.name}, age = #{user.age}
        WHERE id = #{user.id}
    </foreach>
</update>
```

> ⚠️ **注意**：批量更新需要在数据库连接URL中添加 `allowMultiQueries=true`
>
> jdbc:mysql://localhost:3306/test?**allowMultiQueries=true**

### 2.4 批量操作最佳实践


**数量控制**：

```java
// ✅ 推荐：分批处理，每批500-1000条
List<User> allUsers = ...; // 假设有10000条数据
int batchSize = 500;

for (int i = 0; i < allUsers.size(); i += batchSize) {
    int end = Math.min(i + batchSize, allUsers.size());
    List<User> batch = allUsers.subList(i, end);
    userMapper.batchInsert(batch); // 分批插入
}
```

**为什么要分批**：
- 一次性太多数据会导致SQL语句过长
- 可能超过数据库的 `max_allowed_packet` 限制
- 占用过多内存

---

## 3. 🔄 延迟加载配置


### 3.1 什么是延迟加载


**通俗理解**：就像你点外卖，不是所有菜都一起上，而是先上主食，想吃甜点时再单独点。

```
延迟加载流程：

用户查询 → 立即加载用户基本信息
         ↓
    需要订单数据? 
         ↙     ↘
      是          否
      ↓           ↓
  加载订单      不加载订单
  
这样避免了加载用不到的数据
```

### 3.2 全局延迟加载配置


```xml
<!-- mybatis-config.xml -->
<settings>
    <!-- 开启延迟加载 -->
    <setting name="lazyLoadingEnabled" value="true"/>
    
    <!-- 关闭积极加载（防止触碰对象就加载全部关联） -->
    <setting name="aggressiveLazyLoading" value="false"/>
</settings>
```

**配置说明**：

| 配置项 | 作用 | 推荐值 |
|--------|------|--------|
| `lazyLoadingEnabled` | 是否开启延迟加载 | `true` |
| `aggressiveLazyLoading` | 是否积极加载（访问对象任何方法都加载关联） | `false` |

### 3.3 局部延迟加载配置


```xml
<!-- 方式一：在association/collection上配置 -->
<resultMap id="UserMap" type="User">
    <id property="id" column="id"/>
    <result property="name" column="name"/>
    
    <!-- fetchType="lazy" 延迟加载 -->
    <collection property="orders" 
                select="selectOrders"
                column="id"
                fetchType="lazy"/>
</resultMap>

<!-- 方式二：fetchType="eager" 立即加载 -->
<collection property="orders" 
            select="selectOrders"
            fetchType="eager"/>
```

**fetchType取值**：
- `lazy`：延迟加载，用到时才查 ✅ 推荐
- `eager`：立即加载，马上查询

### 3.4 延迟加载使用场景


**✅ 适合延迟加载的场景**：
- 关联数据不是每次都需要
- 关联数据量较大
- 提高首次查询速度

**❌ 不适合延迟加载的场景**：
- 关联数据必定要用
- 会造成多次数据库访问（N+1问题）
- 查询结果要序列化传输（延迟加载的数据可能丢失）

---

## 4. 🔌 连接池调优


### 4.1 什么是数据库连接池


**通俗理解**：就像共享单车，不用每次骑车都去买一辆新的，而是从池子里取一辆，用完还回去。

```
传统方式（无连接池）：
请求1 → 创建连接 → 查询 → 关闭连接
请求2 → 创建连接 → 查询 → 关闭连接
请求3 → 创建连接 → 查询 → 关闭连接
❌ 每次都要创建和销毁，很耗时

连接池方式：
初始化 → 创建10个连接放入池中
请求1 → 从池中取连接 → 查询 → 归还到池
请求2 → 从池中取连接 → 查询 → 归还到池
请求3 → 从池中取连接 → 查询 → 归还到池
✅ 重复使用连接，性能高
```

### 4.2 常用连接池：Druid配置


```yaml
spring:
  datasource:
    type: com.alibaba.druid.pool.DruidDataSource
    druid:
      # 初始连接数
      initial-size: 5
      
      # 最小空闲连接数
      min-idle: 5
      
      # 最大活动连接数
      max-active: 20
      
      # 获取连接的最大等待时间（毫秒）
      max-wait: 60000
      
      # 连接验证查询
      validation-query: SELECT 1
      
      # 空闲连接检测时间（毫秒）
      time-between-eviction-runs-millis: 60000
      
      # 连接最小空闲时间（毫秒）
      min-evictable-idle-time-millis: 300000
```

**参数说明**：

| 参数 | 含义 | 推荐值 | 说明 |
|------|------|--------|------|
| **initial-size** | 初始连接数 | `5-10` | 应用启动时创建的连接数 |
| **min-idle** | 最小空闲连接 | `5-10` | 池中保持的最少连接数 |
| **max-active** | 最大连接数 | `20-50` | 池中最多能有多少连接 |
| **max-wait** | 最大等待时间 | `60000ms` | 获取连接的超时时间 |

### 4.3 连接池调优策略


**场景一：高并发Web应用**

```yaml
druid:
  initial-size: 10
  min-idle: 10
  max-active: 50      # ✅ 较大，应对高并发
  max-wait: 30000     # ✅ 较短，快速失败
```

**场景二：定时任务/批处理**

```yaml
druid:
  initial-size: 5
  min-idle: 5
  max-active: 20      # ✅ 较小，不需要太多连接
  max-wait: 60000     # ✅ 较长，允许等待
```

**调优口诀**：
```
🔸 并发高 → max-active大一些
🔸 响应快 → max-wait小一些  
🔸 资源省 → min-idle小一些
🔸 启动快 → initial-size小一些
```

---

## 5. 📝 SQL语句优化


### 5.1 只查询需要的字段


**通俗理解**：去自助餐厅，只拿你要吃的菜，不要全部拿一遍。

```xml
<!-- ❌ 查询所有字段（浪费） -->
<select id="selectUser" resultType="User">
    SELECT * FROM users WHERE id = #{id}
</select>

<!-- ✅ 只查需要的字段 -->
<select id="selectUserName" resultType="String">
    SELECT name FROM users WHERE id = #{id}
</select>
```

**为什么要这样**：
- 减少网络传输数据量
- 降低数据库IO开销
- 提高查询速度

### 5.2 避免在WHERE中使用函数


```xml
<!-- ❌ 在WHERE中使用函数（无法使用索引） -->
<select id="selectByDate" resultType="User">
    SELECT * FROM users 
    WHERE DATE_FORMAT(create_time, '%Y-%m-%d') = #{date}
</select>

<!-- ✅ 使用范围查询（可以使用索引） -->
<select id="selectByDateRange" resultType="User">
    SELECT * FROM users 
    WHERE create_time >= #{startTime} 
      AND create_time < #{endTime}
</select>
```

**原理**：
- 函数会导致索引失效
- 数据库需要逐行计算函数结果
- 范围查询可以利用索引快速定位

### 5.3 合理使用LIMIT


```xml
<!-- ✅ 分页查询，避免一次取太多数据 -->
<select id="selectPage" resultType="User">
    SELECT * FROM users 
    LIMIT #{offset}, #{pageSize}
</select>
```

**分页示例**：

```
第1页（每页10条）：LIMIT 0, 10   → 取第1-10条
第2页（每页10条）：LIMIT 10, 10  → 取第11-20条
第3页（每页10条）：LIMIT 20, 10  → 取第21-30条

offset = (页码 - 1) × 每页条数
```

---

## 6. 🔍 索引使用优化


### 6.1 什么是索引


**通俗理解**：索引就像书的目录，通过目录能快速找到内容，而不用一页一页翻。

```
没有索引的查询：
WHERE name = 'Tom'
→ 从第1行开始
→ 逐行检查name是否等于Tom
→ 扫描全表 ❌ 慢

有索引的查询：
WHERE name = 'Tom'  
→ 查索引树
→ 直接定位到Tom的数据
→ 快速返回 ✅ 快
```

### 6.2 哪些字段需要加索引


**✅ 应该加索引的字段**：
- WHERE条件中频繁使用的字段
- JOIN连接字段
- ORDER BY排序字段
- GROUP BY分组字段

**❌ 不应该加索引的字段**：
- 数据重复度高的字段（如性别）
- 很少查询的字段
- 频繁更新的字段

### 6.3 索引失效的常见情况


**情况一：使用函数**

```sql
-- ❌ 索引失效
WHERE UPPER(name) = 'TOM'

-- ✅ 索引有效
WHERE name = 'Tom'
```

**情况二：隐式类型转换**

```sql
-- ❌ phone是字符串类型，但传入数字，索引失效
WHERE phone = 13800138000

-- ✅ 使用字符串，索引有效  
WHERE phone = '13800138000'
```

**情况三：前导模糊查询**

```sql
-- ❌ 以通配符开头，索引失效
WHERE name LIKE '%Tom'

-- ✅ 以通配符结尾，索引有效
WHERE name LIKE 'Tom%'
```

### 6.4 使用EXPLAIN分析索引


```sql
EXPLAIN SELECT * FROM users WHERE name = 'Tom';
```

**关键字段说明**：

| 字段 | 含义 | 好的值 |
|------|------|--------|
| **type** | 访问类型 | `const`、`ref` 好<br>`ALL` 差（全表扫描） |
| **key** | 使用的索引 | 有值表示用了索引<br>`NULL`表示没用索引 |
| **rows** | 扫描行数 | 越小越好 |

---

## 7. 💾 缓存策略优化


### 7.1 MyBatis的两级缓存


**通俗理解**：
- **一级缓存**：就像你的口袋，放刚查过的数据，下次马上能拿到
- **二级缓存**：就像你家冰箱，全家人都能用，但要手动开启

```
查询流程：

查询请求 → 一级缓存（SqlSession级别）
              ↓ 没命中
          → 二级缓存（Mapper级别）
              ↓ 没命中
          → 数据库
```

### 7.2 一级缓存（默认开启）


**作用范围**：同一个SqlSession内

```java
// 同一个SqlSession内的查询
SqlSession session = sqlSessionFactory.openSession();
UserMapper mapper = session.getMapper(UserMapper.class);

User user1 = mapper.selectById(1); // 查数据库
User user2 = mapper.selectById(1); // ✅ 走缓存，不查数据库

System.out.println(user1 == user2); // true，同一个对象
```

**缓存失效情况**：
- SqlSession关闭
- 执行增删改操作
- 手动清除缓存

### 7.3 二级缓存（需手动开启）


**步骤一：开启全局二级缓存**

```xml
<!-- mybatis-config.xml -->
<settings>
    <setting name="cacheEnabled" value="true"/>
</settings>
```

**步骤二：在Mapper中启用**

```xml
<!-- UserMapper.xml -->
<mapper namespace="com.example.mapper.UserMapper">
    <!-- 开启二级缓存 -->
    <cache/>
    
    <select id="selectById" resultType="User">
        SELECT * FROM users WHERE id = #{id}
    </select>
</mapper>
```

**步骤三：实体类实现序列化**

```java
public class User implements Serializable {
    private static final long serialVersionUID = 1L;
    // ...
}
```

**使用效果**：

```java
// 不同SqlSession之间共享缓存
SqlSession session1 = sqlSessionFactory.openSession();
UserMapper mapper1 = session1.getMapper(UserMapper.class);
User user1 = mapper1.selectById(1); // 查数据库
session1.close(); // 关闭后数据会进入二级缓存

SqlSession session2 = sqlSessionFactory.openSession();
UserMapper mapper2 = session2.getMapper(UserMapper.class);
User user2 = mapper2.selectById(1); // ✅ 走二级缓存，不查数据库
```

### 7.4 缓存的使用建议


**✅ 适合使用缓存的场景**：
- 查询频繁、变化少的数据（如字典表）
- 读多写少的数据

**❌ 不适合使用缓存的场景**：
- 实时性要求高的数据
- 频繁更新的数据
- 多表关联的复杂查询

**替代方案：集成Redis**

```java
// 使用Redis作为二级缓存
@CacheNamespace(implementation = RedisCache.class)
public interface UserMapper {
    User selectById(Integer id);
}
```

---

## 8. 📊 大数据量处理


### 8.1 分页查询优化


**传统分页问题**：

```sql
-- ❌ 深度分页慢（LIMIT 1000000, 10）
SELECT * FROM users 
ORDER BY id 
LIMIT 1000000, 10;

-- 需要扫描前1000010行，丢弃前1000000行 😱
```

**优化方案：使用ID范围**

```xml
<!-- ✅ 基于上次最大ID查询 -->
<select id="selectByIdRange" resultType="User">
    SELECT * FROM users 
    WHERE id > #{lastId}
    ORDER BY id
    LIMIT #{pageSize}
</select>
```

```java
// 使用示例
int lastId = 0;
int pageSize = 100;

while(true) {
    List<User> users = userMapper.selectByIdRange(lastId, pageSize);
    if(users.isEmpty()) break;
    
    // 处理数据
    for(User user : users) {
        // ...
    }
    
    // 更新lastId
    lastId = users.get(users.size() - 1).getId();
}
```

### 8.2 流式查询


**传统查询问题**：一次性加载所有数据到内存，可能导致内存溢出。

```java
// ❌ 100万条数据全部加载到内存
List<User> users = userMapper.selectAll(); // 💥 内存爆了
```

**流式查询**：边读边处理，不占用大量内存

```java
// ✅ 使用流式查询
@Options(resultSetType = ResultSetType.FORWARD_ONLY, fetchSize = 1000)
@Select("SELECT * FROM users")
void selectAllStream(ResultHandler<User> handler);

// 使用方式
userMapper.selectAllStream(resultContext -> {
    User user = resultContext.getResultObject();
    // 逐条处理，不会占用大量内存
    processUser(user);
});
```

**fetchSize说明**：
- 每次从数据库取多少条数据
- 不是一次性全取，而是分批取
- 推荐值：500-1000

### 8.3 游标查询（Cursor）


```java
// 使用游标
@Select("SELECT * FROM users")
Cursor<User> selectAllCursor();

// 使用方式
try (Cursor<User> cursor = userMapper.selectAllCursor()) {
    for (User user : cursor) {
        // 逐条处理
        processUser(user);
    }
}
```

**三种方式对比**：

| 方式 | 内存占用 | 性能 | 适用场景 |
|------|---------|------|---------|
| **普通查询** | 高（全部加载） | 快但可能OOM | 数据量小 |
| **流式查询** | 低（分批读取） | 中等 | 大数据量读取 |
| **游标查询** | 低（逐条读取） | 较慢 | 超大数据量 |

---

## 9. 🧠 内存使用优化


### 9.1 避免大对象查询


```xml
<!-- ❌ 查询包含大字段的完整对象 -->
<select id="selectUser" resultType="User">
    SELECT id, name, avatar, description, big_data 
    FROM users WHERE id = #{id}
</select>

<!-- ✅ 按需查询，大字段单独处理 -->
<select id="selectUserBasic" resultType="User">
    SELECT id, name FROM users WHERE id = #{id}
</select>

<select id="selectUserAvatar" resultType="byte[]">
    SELECT avatar FROM users WHERE id = #{id}
</select>
```

### 9.2 ResultMap字段映射优化


```xml
<!-- ❌ 映射过多不需要的字段 -->
<resultMap id="UserFullMap" type="User">
    <id property="id" column="id"/>
    <result property="name" column="name"/>
    <result property="age" column="age"/>
    <result property="email" column="email"/>
    <!-- ...20个字段 -->
</resultMap>

<!-- ✅ 只映射需要的字段 -->
<resultMap id="UserSimpleMap" type="UserVO">
    <id property="id" column="id"/>
    <result property="name" column="name"/>
</resultMap>
```

### 9.3 及时关闭资源


```java
// ✅ 使用try-with-resources自动关闭
try (SqlSession session = sqlSessionFactory.openSession()) {
    UserMapper mapper = session.getMapper(UserMapper.class);
    List<User> users = mapper.selectAll();
    // ...
} // 自动关闭session

// ❌ 手动关闭容易遗漏
SqlSession session = sqlSessionFactory.openSession();
try {
    // ...
} finally {
    session.close(); // 可能忘记关闭
}
```

---

## 10. 🚀 并发性能优化


### 10.1 使用批量执行器（Batch Executor）


**适用场景**：批量插入、更新时提升性能

```java
// 方式一：通过配置开启批量执行器
SqlSession session = sqlSessionFactory.openSession(ExecutorType.BATCH);
UserMapper mapper = session.getMapper(UserMapper.class);

for (User user : userList) {
    mapper.insert(user); // 批量暂存
}

session.commit(); // 一次性提交
session.close();
```

**性能对比**：

```
插入10000条数据：
普通模式：约10秒 ❌
批量模式：约2秒  ✅

性能提升：5倍
```

### 10.2 合理使用事务


```java
// ❌ 每条数据一个事务（慢）
for (User user : userList) {
    SqlSession session = sqlSessionFactory.openSession();
    UserMapper mapper = session.getMapper(UserMapper.class);
    mapper.insert(user);
    session.commit(); // 每次都提交
    session.close();
}

// ✅ 批量数据一个事务（快）
SqlSession session = sqlSessionFactory.openSession();
UserMapper mapper = session.getMapper(UserMapper.class);

for (User user : userList) {
    mapper.insert(user);
}

session.commit(); // 一次性提交
session.close();
```

### 10.3 数据库连接池与并发


**连接数计算公式**：

```
最大连接数 = ((核心数 * 2) + 磁盘数)

示例：
4核CPU + 1块硬盘
最大连接数 = (4 * 2) + 1 = 9

实际建议：10-20个连接
```

**并发场景配置**：

```yaml
# 高并发场景
druid:
  max-active: 50           # 最大连接数增大
  max-wait: 10000          # 等待时间缩短
  min-idle: 20             # 保持足够空闲连接
```

---

## 11. 📋 核心要点总结


### 11.1 性能优化核心原则


> 💡 **优化三原则**：减少查询次数 + 减少数据量 + 合理使用缓存

```
🔸 N+1问题 → 使用JOIN或延迟加载解决
🔸 批量操作 → 用foreach批量插入/更新
🔸 延迟加载 → 按需加载关联数据
🔸 连接池 → 合理配置连接数
🔸 SQL优化 → 只查需要的字段，善用索引
🔸 索引使用 → 避免索引失效情况
🔸 缓存策略 → 合理使用一二级缓存
🔸 大数据量 → 使用分页、流式查询
🔸 内存优化 → 避免大对象，及时释放
🔸 并发优化 → 批量执行器+合理事务
```

### 11.2 常见性能问题速查表


| 问题现象 | 可能原因 | 解决方案 |
|---------|---------|---------|
| **查询慢** | N+1查询 | 使用JOIN或延迟加载 |
| **查询慢** | 没用索引 | 添加索引，检查索引失效 |
| **插入慢** | 逐条插入 | 使用批量插入 |
| **内存溢出** | 一次加载太多数据 | 分页或流式查询 |
| **连接超时** | 连接池太小 | 增加max-active |
| **缓存不生效** | 未开启或配置错误 | 检查cache配置 |

### 11.3 实战优化检查清单


**📝 上线前检查**：
- [ ] 是否存在N+1查询
- [ ] 批量操作是否使用了foreach
- [ ] 是否只查询了需要的字段
- [ ] WHERE条件字段是否有索引
- [ ] 连接池配置是否合理
- [ ] 是否使用了合适的缓存策略
- [ ] 大数据量是否使用了分页

**🔍 性能排查步骤**：
```
1️⃣ 开启SQL日志，查看执行的SQL
2️⃣ 使用EXPLAIN分析SQL执行计划
3️⃣ 检查是否有N+1查询
4️⃣ 查看连接池监控指标
5️⃣ 分析慢查询日志
6️⃣ 使用性能分析工具（如Druid监控）
```

**核心记忆口诀**：
```
查询优化记心间，N+1问题要避免
批量操作效率高，索引使用莫忘掉
缓存合理能提速，连接池要配置好
大数据量分页查，内存占用要关照
```