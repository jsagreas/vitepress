---
title: 23、批量操作优化
---
## 📚 目录

1. [批量操作基础认知](#1-批量操作基础认知)
2. [批量保存操作详解](#2-批量保存操作详解)
3. [批量更新操作详解](#3-批量更新操作详解)
4. [批量删除操作详解](#4-批量删除操作详解)
5. [批量操作性能优化](#5-批量操作性能优化)
6. [内存控制与风险防范](#6-内存控制与风险防范)
7. [核心要点总结](#7-核心要点总结)

---

## 1. 🎯 批量操作基础认知


### 1.1 为什么需要批量操作


**生活场景类比**：
想象你去超市购物，有两种结账方式：
- 🛒 **方式一**：买一件结一次账（循环单条操作）
- 🛍️ **方式二**：把所有商品放购物车，一次性结账（批量操作）

显然方式二更高效！数据库操作也是同样的道理。

```
传统循环插入的问题：
┌─────────────────────────────┐
│  for (User user : userList) │
│    userMapper.insert(user); │ ← 每次都要：建立连接→执行SQL→关闭连接
│  }                           │   1000条数据 = 1000次数据库交互
└─────────────────────────────┘

批量插入的优势：
┌─────────────────────────────┐
│  userService.saveBatch(list)│ ← 一次连接，执行多条SQL
│                              │   1000条数据 = 1次数据库交互
└─────────────────────────────┘

性能对比：
循环插入1000条：可能需要 10秒+
批量插入1000条：可能只需要 1秒
```

### 1.2 MyBatis-Plus批量操作全景


**🔧 Service层提供的批量方法**：

| 方法名 | **功能说明** | **适用场景** | **返回值** |
|--------|------------|------------|----------|
| `saveBatch` | `批量插入数据` | `新增多条记录` | `boolean` |
| `saveOrUpdateBatch` | `批量保存或更新` | `不确定是新增还是更新` | `boolean` |
| `updateBatchById` | `批量更新数据` | `修改多条已存在记录` | `boolean` |
| `removeBatchByIds` | `批量删除数据` | `删除多条记录` | `boolean` |

**💡 核心理念**：
```
批量操作 = 减少网络交互 + 优化SQL执行 + 提升整体性能

本质上是：
1️⃣ 把多次数据库操作合并成一次或少数几次
2️⃣ 利用数据库的批处理能力
3️⃣ 减少网络往返时间(RTT)
```

### 1.3 Service层继承结构


要使用批量操作，你的Service需要继承MyBatis-Plus提供的基类：

```java
// 第一步：让你的Service接口继承IService
public interface UserService extends IService<User> {
    // 自动拥有批量操作方法
}

// 第二步：让实现类继承ServiceImpl
@Service
public class UserServiceImpl 
    extends ServiceImpl<UserMapper, User> 
    implements UserService {
    // 批量方法已经内置，直接调用即可
}
```

**🎯 这样设计的好处**：
```
继承体系图：
         IService<T>
              ↑
              │ 继承（获得批量方法）
              │
        UserService
              ↑
              │ 实现
              │
      ServiceImpl<M,T> ← 提供批量方法的实现
              ↑
              │ 继承
              │
      UserServiceImpl ← 你的业务类，直接使用批量方法

好处：
✅ 不用自己写批量操作代码
✅ 统一的方法命名和使用方式
✅ 内置了性能优化逻辑
```

---

## 2. 💾 批量保存操作详解


### 2.1 saveBatch - 批量插入新数据


**基本使用**：
当你有一批全新的数据需要插入数据库时，用这个方法最合适。

```java
@Service
public class UserServiceImpl extends ServiceImpl<UserMapper, User> 
    implements UserService {
    
    /**
     * 批量保存用户数据
     * 场景：导入Excel数据、批量注册用户
     */
    public boolean batchSaveUsers(List<User> userList) {
        // 最简单的用法
        return this.saveBatch(userList);
    }
}
```

**🔍 实际应用场景**：

```java
// 场景1：Excel批量导入用户
public void importUsersFromExcel(MultipartFile file) {
    // 1. 解析Excel得到用户列表
    List<User> userList = parseExcel(file);
    
    // 2. 数据校验
    userList.forEach(user -> {
        if (StringUtils.isEmpty(user.getUsername())) {
            throw new BusinessException("用户名不能为空");
        }
    });
    
    // 3. 批量保存 - 关键步骤
    boolean success = this.saveBatch(userList);
    
    if (!success) {
        throw new BusinessException("批量导入失败");
    }
}

// 场景2：批量创建测试数据
public void createTestUsers(int count) {
    List<User> userList = new ArrayList<>();
    
    for (int i = 0; i < count; i++) {
        User user = new User();
        user.setUsername("test_" + i);
        user.setEmail("test" + i + "@example.com");
        user.setAge(20 + i % 30);
        userList.add(user);
    }
    
    // 批量插入
    this.saveBatch(userList);
}
```

**⚙️ 指定批次大小**：

当数据量特别大时，一次性插入所有数据可能会：
- 超出数据库的最大SQL长度限制
- 占用过多内存
- 事务时间过长

这时需要分批处理：

```java
/**
 * 分批保存大量数据
 * @param userList 用户列表
 * @param batchSize 每批数量（默认1000）
 */
public boolean saveLargeData(List<User> userList) {
    // 每500条数据执行一次插入
    return this.saveBatch(userList, 500);
}
```

**📊 批次大小选择建议**：
```
数据量级          建议批次大小         说明
─────────────────────────────────────────
< 1000条          不用分批            一次性插入即可
1000 - 10000      500-1000           平衡性能和安全
10000 - 100000    500-1000           防止内存溢出
> 100000          500               小批次更稳定

实际调优需要考虑：
• 单条记录大小（字段多少、内容长度）
• 数据库配置（max_allowed_packet等）
• 服务器内存情况
• 网络带宽
```

### 2.2 saveOrUpdateBatch - 智能保存或更新


**这个方法解决什么问题？**
想象这个场景：你从外部系统同步数据，有些记录可能是新的，有些可能已经存在需要更新。如果用传统方式：

```java
// ❌ 传统方式：需要先查询判断
for (User user : userList) {
    User existUser = this.getById(user.getId());
    if (existUser == null) {
        this.save(user);      // 新增
    } else {
        this.updateById(user); // 更新
    }
}
// 问题：大量的查询操作，性能差
```

**✅ 使用saveOrUpdateBatch**：
```java
// MyBatis-Plus智能判断
public void syncUsersFromThirdParty(List<User> userList) {
    // 自动判断：有ID且数据库存在则更新，否则插入
    this.saveOrUpdateBatch(userList);
}
```

**🔍 工作原理解析**：
```
saveOrUpdateBatch执行逻辑：
┌────────────────────────────┐
│ 1. 检查每条记录的主键      │
├────────────────────────────┤
│ 2. 有主键？                │
│    └─> 查数据库是否存在    │
│         └─> 存在：加入更新列表│
│         └─> 不存在：加入插入列表│
├────────────────────────────┤
│ 3. 无主键？直接加入插入列表│
├────────────────────────────┤
│ 4. 批量执行INSERT和UPDATE  │
└────────────────────────────┘

性能优化点：
• 只查询主键是否存在，不查完整记录
• 插入和更新分别批量执行
• 减少了循环中的单条操作
```

**💼 实际业务示例**：
```java
/**
 * 同步商品数据
 * 场景：定时从供应商系统同步商品信息
 */
@Scheduled(cron = "0 0 2 * * ?") // 每天凌晨2点执行
public void syncProducts() {
    // 1. 从第三方API获取商品数据
    List<Product> productList = thirdPartyApi.fetchProducts();
    
    // 2. 数据转换和清洗
    productList.forEach(product -> {
        product.setUpdateTime(LocalDateTime.now());
        // 其他业务处理...
    });
    
    // 3. 批量保存或更新
    // 已存在的商品会更新，新商品会插入
    boolean success = this.saveOrUpdateBatch(productList, 500);
    
    log.info("同步商品完成，处理数量：{}", productList.size());
}
```

**⚠️ 使用注意事项**：
```
注意点1：主键策略要明确
// 实体类必须有明确的主键
@TableId(type = IdType.AUTO)  // 或其他主键策略
private Long id;

注意点2：判断依据是主键
// 判断是否存在，看的是主键值
// 不是根据其他字段（如username）

注意点3：性能考虑
// 会先执行SELECT查询判断是否存在
// 数据量大时，仍然有大量查询开销
// 如果明确知道是全部新增或全部更新，用专门方法更好
```

---

## 3. 🔄 批量更新操作详解


### 3.1 updateBatchById - 根据ID批量更新


**基本用法**：
当你需要修改一批已存在的记录时使用。

```java
/**
 * 批量更新用户状态
 */
public boolean batchUpdateUserStatus(List<Long> userIds, Integer status) {
    // 1. 构建要更新的用户列表
    List<User> updateList = userIds.stream()
        .map(id -> {
            User user = new User();
            user.setId(id);
            user.setStatus(status);
            user.setUpdateTime(LocalDateTime.now());
            return user;
        })
        .collect(Collectors.toList());
    
    // 2. 批量更新
    return this.updateBatchById(updateList);
}
```

**🎯 实际应用场景**：

```java
// 场景1：批量审核通过
public void batchApprove(List<Long> orderIds) {
    List<Order> orders = orderIds.stream()
        .map(id -> {
            Order order = new Order();
            order.setId(id);
            order.setStatus(OrderStatus.APPROVED);
            order.setApproveTime(LocalDateTime.now());
            return order;
        })
        .collect(Collectors.toList());
    
    this.updateBatchById(orders, 500);
}

// 场景2：批量修改价格
public void batchUpdatePrice(List<PriceUpdateDTO> priceList) {
    List<Product> products = priceList.stream()
        .map(dto -> {
            Product product = new Product();
            product.setId(dto.getProductId());
            product.setPrice(dto.getNewPrice());
            product.setUpdateTime(LocalDateTime.now());
            return product;
        })
        .collect(Collectors.toList());
    
    this.updateBatchById(products);
}
```

**⚙️ 底层执行原理**：
```
updateBatchById执行过程：
┌──────────────────────────────────┐
│ 输入：List<User> updateList      │
│ [{id:1,name:"张三"},             │
│  {id:2,name:"李四"},             │
│  {id:3,name:"王五"}]             │
├──────────────────────────────────┤
│ MyBatis-Plus处理：               │
│ 1. 分批（默认1000条）            │
│ 2. 生成批量UPDATE语句            │
│ 3. 执行SQL                       │
├──────────────────────────────────┤
│ 执行的SQL（简化）：              │
│ UPDATE user SET name=? WHERE id=?│
│ UPDATE user SET name=? WHERE id=?│
│ UPDATE user SET name=? WHERE id=?│
│ ...（批量执行，一次提交）        │
└──────────────────────────────────┘

注意：每条记录仍是单独的UPDATE语句
但是批量提交到数据库，减少网络交互
```

**🔍 与循环更新的性能对比**：
```
场景：更新1000条用户记录

❌ 循环单条更新：
for (User user : userList) {
    userMapper.updateById(user);
}
执行时间：约 8-10秒
原因：
• 1000次网络往返
• 1000次SQL执行
• 每次都要创建预处理语句

✅ 批量更新：
userService.updateBatchById(userList, 500);
执行时间：约 1-2秒
原因：
• 2次网络往返（分2批）
• SQL批量执行
• 复用预处理语句

性能提升：5-10倍
```

### 3.2 批量更新的高级技巧


**技巧1：只更新有变化的字段**
```java
/**
 * 智能批量更新：只更新非空字段
 */
public boolean smartBatchUpdate(List<User> userList) {
    // 使用UpdateWrapper可以实现动态字段更新
    // 但批量操作中，建议在构建对象时就明确要更新的字段
    
    List<User> updateList = userList.stream()
        .filter(user -> user.getId() != null) // 确保有ID
        .peek(user -> {
            // 统一设置更新时间
            user.setUpdateTime(LocalDateTime.now());
            // 其他需要统一处理的逻辑
        })
        .collect(Collectors.toList());
    
    return this.updateBatchById(updateList);
}
```

**技巧2：结合Lambda进行条件更新**
```java
/**
 * 批量更新指定用户的指定字段
 */
public void batchUpdateByCondition(List<Long> userIds, String newCity) {
    // 使用Lambda更新
    LambdaUpdateChainWrapper<User> wrapper = this.lambdaUpdate()
        .set(User::getCity, newCity)
        .set(User::getUpdateTime, LocalDateTime.now())
        .in(User::getId, userIds);
    
    wrapper.update();
    
    // 这种方式的SQL：
    // UPDATE user SET city=?, update_time=? WHERE id IN (1,2,3...)
    // 是真正的一条SQL，比updateBatchById更高效
}
```

---

## 4. 🗑️ 批量删除操作详解


### 4.1 removeBatchByIds - 批量物理删除


**基本用法**：
根据ID列表批量删除记录。

```java
/**
 * 批量删除用户
 */
public boolean batchDeleteUsers(List<Long> userIds) {
    // 简单直接的批量删除
    return this.removeBatchByIds(userIds);
}
```

**💼 实际应用场景**：
```java
// 场景1：批量删除过期数据
@Scheduled(cron = "0 0 3 * * ?") // 每天凌晨3点
public void cleanExpiredData() {
    // 1. 查询30天前的过期记录
    LocalDateTime expireTime = LocalDateTime.now().minusDays(30);
    
    List<Long> expiredIds = this.lambdaQuery()
        .select(Log::getId)
        .lt(Log::getCreateTime, expireTime)
        .list()
        .stream()
        .map(Log::getId)
        .collect(Collectors.toList());
    
    // 2. 批量删除
    if (!expiredIds.isEmpty()) {
        this.removeBatchByIds(expiredIds);
        log.info("清理过期数据：{} 条", expiredIds.size());
    }
}

// 场景2：购物车批量删除
public void clearCartItems(Long userId, List<Long> itemIds) {
    // 先验证权限：确保这些商品属于该用户
    long count = this.lambdaQuery()
        .eq(CartItem::getUserId, userId)
        .in(CartItem::getId, itemIds)
        .count();
    
    if (count != itemIds.size()) {
        throw new BusinessException("存在无权删除的商品");
    }
    
    // 批量删除
    this.removeBatchByIds(itemIds);
}
```

**🔍 执行过程分析**：
```
removeBatchByIds执行流程：
┌─────────────────────────────────┐
│ 输入：List<Long> ids            │
│ [1, 2, 3, 4, 5, ..., 1000]     │
├─────────────────────────────────┤
│ MyBatis-Plus处理：              │
│ 1. 检查列表是否为空             │
│ 2. 分批处理（默认1000条）       │
│ 3. 生成DELETE语句               │
├─────────────────────────────────┤
│ 生成的SQL：                     │
│ DELETE FROM user                │
│ WHERE id IN (1,2,3,...,1000)    │
│                                 │
│ 优势：一条SQL删除多条记录       │
└─────────────────────────────────┘

性能对比：
循环删除1000条：8-10秒
批量删除1000条：0.5-1秒
```

### 4.2 逻辑删除的批量处理


很多业务场景不建议物理删除数据，而是使用逻辑删除：

```java
/**
 * 配置逻辑删除
 */
@TableLogic
private Integer deleted; // 0-未删除，1-已删除

/**
 * 批量逻辑删除
 */
public boolean logicBatchDelete(List<Long> userIds) {
    // removeBatchByIds在配置了@TableLogic后
    // 自动变成逻辑删除（UPDATE语句）
    return this.removeBatchByIds(userIds);
    
    // 实际执行的SQL：
    // UPDATE user SET deleted=1 WHERE id IN (...)
}
```

**🔐 批量删除的安全性考虑**：
```java
/**
 * 安全的批量删除实现
 */
public void safeBatchDelete(List<Long> ids, Long currentUserId) {
    // 1. 参数校验
    if (CollectionUtils.isEmpty(ids)) {
        throw new BusinessException("删除列表不能为空");
    }
    
    if (ids.size() > 1000) {
        throw new BusinessException("单次删除不能超过1000条");
    }
    
    // 2. 权限校验：确保只能删除自己的数据
    long ownedCount = this.lambdaQuery()
        .eq(Order::getUserId, currentUserId)
        .in(Order::getId, ids)
        .count();
    
    if (ownedCount != ids.size()) {
        throw new BusinessException("存在无权删除的数据");
    }
    
    // 3. 状态检查：某些状态的数据不允许删除
    long processingCount = this.lambdaQuery()
        .in(Order::getId, ids)
        .eq(Order::getStatus, OrderStatus.PROCESSING)
        .count();
    
    if (processingCount > 0) {
        throw new BusinessException("处理中的订单不能删除");
    }
    
    // 4. 执行删除
    boolean success = this.removeBatchByIds(ids);
    
    // 5. 记录日志
    if (success) {
        log.info("用户[{}]批量删除订单：{}", currentUserId, ids);
    }
}
```

---

## 5. ⚡ 批量操作性能优化


### 5.1 批次大小配置原则


**全局配置方式**：
```yaml
# application.yml
mybatis-plus:
  global-config:
    db-config:
      # 默认批次大小
      batch-size: 1000
```

**动态调整策略**：
```java
/**
 * 根据数据量动态选择批次大小
 */
public boolean smartBatchSave(List<User> userList) {
    int size = userList.size();
    int batchSize;
    
    if (size <= 500) {
        batchSize = size; // 小量数据一次性插入
    } else if (size <= 5000) {
        batchSize = 500;  // 中量数据500一批
    } else {
        batchSize = 1000; // 大量数据1000一批
    }
    
    return this.saveBatch(userList, batchSize);
}
```

**📊 批次大小影响因素**：
```
影响批次大小的关键因素：

1. 单条记录大小
   ┌─────────────────────────────┐
   │ 小记录（几个字段）→ 可大批次│
   │ 大记录（很多字段）→ 要小批次│
   └─────────────────────────────┘

2. 数据库配置
   • max_allowed_packet：MySQL单次传输数据大小限制
   • innodb_buffer_pool_size：影响批量写入性能

3. 网络状况
   • 局域网环境：可以大批次
   • 公网环境：建议小批次

4. 内存情况
   • 内存充足：可以大批次
   • 内存紧张：必须小批次

实践建议：
• 普通场景：500-1000条
• 字段多/数据大：200-500条
• 网络不好：100-300条
• 内存紧张：100以内
```

### 5.2 数据库层面优化


**MySQL批量插入优化**：
```sql
-- 1. 关闭自动提交（在代码中已处理）
SET autocommit = 0;

-- 2. 调整批量插入参数
SET $$session.bulk_insert_buffer_size = 128 * 1024 * 1024; -- 128MB

-- 3. 临时关闭索引（谨慎使用）
ALTER TABLE user DISABLE KEYS;
-- 批量插入数据...
ALTER TABLE user ENABLE KEYS;
```

**在代码中的体现**：
```java
/**
 * 大数据量插入优化
 */
@Transactional(rollbackFor = Exception.class)
public void optimizedBatchInsert(List<User> largeUserList) {
    // 1. 分批处理
    int batchSize = 500;
    List<List<User>> batches = Lists.partition(largeUserList, batchSize);
    
    // 2. 批量执行
    for (List<User> batch : batches) {
        this.saveBatch(batch);
        
        // 3. 每批之后清理一级缓存
        this.baseMapper.getConfiguration().getLocalCacheScope();
    }
    
    log.info("优化批量插入完成，总计：{} 条", largeUserList.size());
}
```

### 5.3 并发批量操作


当数据量特别大时，可以考虑并发处理：

```java
/**
 * 并发批量插入（适用于百万级数据）
 */
public void concurrentBatchSave(List<User> hugeUserList) {
    // 1. 数据分片
    int threadCount = 4; // 4个线程并发
    int pageSize = hugeUserList.size() / threadCount;
    
    List<List<User>> partitions = Lists.partition(hugeUserList, pageSize);
    
    // 2. 创建线程池
    ExecutorService executor = Executors.newFixedThreadPool(threadCount);
    CountDownLatch latch = new CountDownLatch(partitions.size());
    
    // 3. 并发执行
    for (List<User> partition : partitions) {
        executor.submit(() -> {
            try {
                this.saveBatch(partition, 500);
            } finally {
                latch.countDown();
            }
        });
    }
    
    // 4. 等待完成
    try {
        latch.await();
    } catch (InterruptedException e) {
        Thread.currentThread().interrupt();
        throw new BusinessException("并发插入被中断");
    } finally {
        executor.shutdown();
    }
}
```

**⚠️ 并发批量注意事项**：
```
并发批量操作的风险：
1. 事务问题
   • 多线程导致事务失效
   • 需要编程式事务控制
   
2. 死锁风险
   • 多线程同时写入可能死锁
   • 需要合理的数据分区策略

3. 连接池耗尽
   • 并发线程过多占满连接池
   • 需要调整连接池配置

安全实践：
✅ 控制并发线程数（不超过CPU核心数的2倍）
✅ 使用独立的数据分片（避免锁冲突）
✅ 充分的异常处理和回滚机制
✅ 监控连接池和数据库负载
```

---

## 6. 🛡️ 内存控制与风险防范


### 6.1 内存溢出风险识别


**危险场景分析**：
```java
// ❌ 危险操作：一次性加载百万数据
public void dangerousOperation() {
    // 查询100万条数据
    List<User> allUsers = this.list(); // 可能内存溢出！
    
    // 批量更新
    this.updateBatchById(allUsers); // 雪上加霜！
}

// 为什么危险？
┌────────────────────────────────────┐
│ 100万条User记录                    │
│ 假设每条1KB                        │
│ 总内存占用：1GB                    │
│                                    │
│ + JVM其他对象                      │
│ + 批量操作临时对象                 │
│ ≈ 需要2-3GB堆内存                 │
│                                    │
│ 如果JVM配置-Xmx2g                 │
│ → OutOfMemoryError！              │
└────────────────────────────────────┘
```

### 6.2 分页批量处理方案


**✅ 正确的大数据量处理方式**：
```java
/**
 * 安全的大数据量批量更新
 */
public void safeBatchUpdate() {
    int pageSize = 1000; // 每页1000条
    int currentPage = 1;
    
    while (true) {
        // 1. 分页查询
        Page<User> page = this.page(
            new Page<>(currentPage, pageSize),
            Wrappers.<User>lambdaQuery()
                .eq(User::getStatus, 0) // 需要更新的条件
        );
        
        List<User> records = page.getRecords();
        
        // 2. 没有数据了，退出循环
        if (records.isEmpty()) {
            break;
        }
        
        // 3. 批量更新当前页数据
        records.forEach(user -> {
            user.setStatus(1);
            user.setUpdateTime(LocalDateTime.now());
        });
        this.updateBatchById(records);
        
        // 4. 处理下一页
        currentPage++;
        
        // 5. 日志记录进度
        log.info("已处理 {} 条数据", (currentPage - 1) * pageSize);
    }
}
```

**🔄 流式处理优化**（MyBatis-Plus 3.5.0+）：
```java
/**
 * 使用游标流式处理大数据
 */
public void streamBatchProcess() {
    // 使用流式查询，逐条处理
    this.lambdaQuery()
        .eq(User::getStatus, 0)
        .last("limit 100000") // 限制总数
        .stream() // 游标方式，不会一次性加载全部到内存
        .forEach(user -> {
            // 处理单条数据
            user.setStatus(1);
            this.updateById(user);
        });
}
```

### 6.3 内存监控与预警


**内存使用监控**：
```java
/**
 * 带内存监控的批量操作
 */
public void monitoredBatchSave(List<User> userList) {
    // 1. 获取JVM内存信息
    Runtime runtime = Runtime.getRuntime();
    long maxMemory = runtime.maxMemory(); // 最大内存
    long totalMemory = runtime.totalMemory(); // 已分配内存
    long freeMemory = runtime.freeMemory(); // 空闲内存
    long usedMemory = totalMemory - freeMemory; // 已使用内存
    
    // 2. 计算内存使用率
    double memoryUsageRate = (double) usedMemory / maxMemory * 100;
    
    log.info("批量操作前内存使用率：{}%", String.format("%.2f", memoryUsageRate));
    
    // 3. 内存预警
    if (memoryUsageRate > 80) {
        log.warn("内存使用率超过80%，执行GC");
        System.gc();
        
        // 重新计算
        usedMemory = runtime.totalMemory() - runtime.freeMemory();
        memoryUsageRate = (double) usedMemory / maxMemory * 100;
        
        if (memoryUsageRate > 85) {
            throw new BusinessException("内存不足，请稍后再试");
        }
    }
    
    // 4. 执行批量操作
    this.saveBatch(userList, 500);
    
    log.info("批量操作后内存使用率：{}%", 
        String.format("%.2f", (double)(runtime.totalMemory() - runtime.freeMemory()) / maxMemory * 100));
}
```

### 6.4 异常处理与事务回滚


**完善的异常处理机制**：
```java
/**
 * 健壮的批量操作实现
 */
@Transactional(rollbackFor = Exception.class)
public BatchResult robustBatchSave(List<User> userList) {
    BatchResult result = new BatchResult();
    
    try {
        // 1. 前置校验
        if (CollectionUtils.isEmpty(userList)) {
            result.setSuccess(false);
            result.setMessage("数据列表为空");
            return result;
        }
        
        if (userList.size() > 10000) {
            result.setSuccess(false);
            result.setMessage("单次操作不能超过10000条");
            return result;
        }
        
        // 2. 数据去重
        List<User> distinctList = userList.stream()
            .collect(Collectors.collectingAndThen(
                Collectors.toCollection(() -> 
                    new TreeSet<>(Comparator.comparing(User::getUsername))),
                ArrayList::new
            ));
        
        // 3. 执行批量保存
        boolean success = this.saveBatch(distinctList, 500);
        
        // 4. 返回结果
        result.setSuccess(success);
        result.setTotalCount(userList.size());
        result.setProcessedCount(distinctList.size());
        result.setMessage("批量保存成功");
        
        return result;
        
    } catch (Exception e) {
        log.error("批量保存失败", e);
        result.setSuccess(false);
        result.setMessage("批量保存失败：" + e.getMessage());
        throw e; // 触发事务回滚
    }
}

/**
 * 批量操作结果对象
 */
@Data
public class BatchResult {
    private boolean success;
    private String message;
    private int totalCount;     // 总数量
    private int processedCount; // 实际处理数量
    private int failedCount;    // 失败数量
}
```

---

## 7. 📋 核心要点总结


### 7.1 批量操作核心方法速查


```
📦 批量操作方法一览：

插入类：
• saveBatch(list)          → 批量插入
• saveBatch(list, size)    → 指定批次大小插入
• saveOrUpdateBatch(list)  → 智能保存或更新

更新类：
• updateBatchById(list)         → 根据ID批量更新
• updateBatchById(list, size)   → 指定批次大小更新

删除类：
• removeBatchByIds(ids)    → 批量删除（支持逻辑删除）
```

### 7.2 性能优化关键要点


```
⚡ 性能优化核心策略：

1️⃣ 合理设置批次大小
   • 普通场景：500-1000
   • 大字段/复杂数据：200-500
   • 网络较差：100-300

2️⃣ 避免内存溢出
   • 使用分页处理大数据
   • 及时清理不用的对象
   • 监控内存使用情况

3️⃣ 善用事务
   • 批量操作放在同一事务
   • 失败自动回滚
   • 注意事务超时

4️⃣ 数据库优化
   • 合理的索引设计
   • 调整数据库参数
   • 考虑临时关闭约束检查

5️⃣ 并发处理
   • 大数据可考虑并发
   • 注意连接池配置
   • 防止死锁问题
```

### 7.3 常见问题与解决方案


```
❓ 批量操作常见问题：

问题1：批量插入慢
原因：
• 索引过多影响插入速度
• 批次大小设置不合理
• 数据库参数未优化
解决：
✅ 临时禁用非必要索引
✅ 调整批次大小
✅ 优化数据库配置

问题2：内存溢出
原因：
• 一次性加载数据过多
• JVM堆内存设置过小
解决：
✅ 使用分页批量处理
✅ 调整JVM内存参数
✅ 及时释放大对象

问题3：事务超时
原因：
• 批量数据量过大
• 单个事务时间过长
解决：
✅ 减少批次大小
✅ 调整事务超时时间
✅ 考虑拆分成多个事务

问题4：主从延迟
原因：
• 批量写入主库
• 从库同步跟不上
解决：
✅ 批量操作后等待同步
✅ 关键查询强制走主库
✅ 使用中间件处理延迟
```

### 7.4 最佳实践总结


```
🎯 批量操作最佳实践：

1. 使用前提
   ✅ Service继承IService
   ✅ ServiceImpl继承ServiceImpl
   ✅ 了解各方法特点

2. 数据校验
   ✅ 批量前统一校验
   ✅ 去重处理
   ✅ 异常数据过滤

3. 批次控制
   ✅ 根据场景选择批次大小
   ✅ 大数据必须分批
   ✅ 监控处理进度

4. 异常处理
   ✅ 完善的try-catch
   ✅ 事务回滚机制
   ✅ 详细的日志记录

5. 性能监控
   ✅ 记录执行时间
   ✅ 监控内存使用
   ✅ 数据库负载监控
```

### 7.5 学习建议


```
📚 学习路径：

第一步：基础使用
• 掌握saveBatch基本用法
• 理解批次大小的意义
• 学会简单的批量操作

第二步：进阶应用
• 了解各种批量方法的区别
• 学会选择合适的方法
• 掌握性能优化技巧

第三步：生产实践
• 处理大数据量场景
• 内存和性能优化
• 异常处理和监控

第四步：深入原理
• 研究底层实现机制
• 数据库批处理原理
• 自定义批量操作
```

**🧠 记忆口诀**：
```
批量操作提性能，
减少网络交互次数。
分批处理保稳定，
内存监控防溢出。
事务回滚保一致，
异常处理要完善。
```

**核心理念**：
批量操作不是简单的"批量执行"，而是一种系统化的性能优化策略。要在效率、安全、稳定之间找到平衡点，根据实际业务场景选择最合适的方案。记住：没有银弹，只有最适合的解决方案！