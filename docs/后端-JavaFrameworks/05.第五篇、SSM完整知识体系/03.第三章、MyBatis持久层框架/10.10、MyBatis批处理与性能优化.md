---
title: 10、MyBatis批处理与性能优化
---
## 📚 目录

1. [什么是批处理与性能优化](#1-什么是批处理与性能优化)
2. [BatchExecutor批量执行器](#2-BatchExecutor批量执行器)
3. [批量插入优化策略](#3-批量插入优化策略)
4. [批量更新优化方案](#4-批量更新优化方案)
5. [foreach批量处理](#5-foreach批量处理)
6. [分批处理大数据量](#6-分批处理大数据量)
7. [SQL执行器性能分析](#7-SQL执行器性能分析)
8. [PreparedStatement重用机制](#8-PreparedStatement重用机制)
9. [ResultSet处理优化](#9-ResultSet处理优化)
10. [连接获取优化](#10-连接获取优化)
11. [语句缓存机制](#11-语句缓存机制)
12. [核心要点总结](#12-核心要点总结)

---

## 1. 🎯 什么是批处理与性能优化


### 1.1 为什么需要批处理


**🤔 先看一个实际问题**

假设你需要向数据库插入1万条用户数据，如果一条一条插入会发生什么？

```
普通方式（逐条插入）：
循环1万次 {
  ① 创建SQL语句
  ② 发送到数据库
  ③ 数据库执行
  ④ 返回结果
}
总耗时：可能需要几分钟甚至更久！
```

**⚡ 批处理的核心思想**

批处理就是把多条操作"打包"一次性发给数据库，大大减少网络通信次数。

```
批处理方式：
准备1万条数据 → 一次性打包 → 发送到数据库 → 批量执行
总耗时：可能只需要几秒钟！
```

### 1.2 性能优化的三个层面


| 优化层面 | **优化内容** | **性能提升** |
|---------|------------|------------|
| 🔷 **网络层** | 减少数据库通信次数 | 减少50%-80%的网络开销 |
| 🔷 **执行层** | SQL预编译重用、批量执行 | 提升3-10倍执行效率 |
| 🔷 **资源层** | 连接复用、结果集优化 | 降低70%的资源消耗 |

### 1.3 批处理的适用场景


```
✅ 适合批处理的场景：
• 批量导入数据（如Excel导入用户信息）
• 定时任务批量更新（如每日订单状态更新）
• 数据迁移（如旧系统数据导入新系统）
• 批量删除过期数据

❌ 不适合批处理的场景：
• 实时性要求高的操作（如支付扣款）
• 需要立即返回结果的业务
• 数据量很小（少于10条）
```

---

## 2. ⚙️ BatchExecutor批量执行器


### 2.1 什么是Executor执行器


**📋 执行器是MyBatis的核心组件**

简单理解：执行器就是MyBatis中负责"执行SQL"的工具，就像不同的快递公司有不同的派送方式。

```
MyBatis的三种执行器：

📦 SimpleExecutor（简单执行器）
   - 工作方式：每次执行SQL都创建新的Statement
   - 就像：每发一个快递就派一辆车
   
📦 ReuseExecutor（重用执行器）  
   - 工作方式：重用相同SQL的Statement
   - 就像：同一条线路的快递共用一辆车
   
📦 BatchExecutor（批量执行器）
   - 工作方式：攒够一批SQL再统一执行
   - 就像：把同一区域的快递装满车再发
```

### 2.2 BatchExecutor的工作原理


**🔄 批量执行的流程**

```
普通执行器流程：
添加SQL1 → 立即执行 → 返回结果
添加SQL2 → 立即执行 → 返回结果
添加SQL3 → 立即执行 → 返回结果

BatchExecutor流程：
添加SQL1 → 暂存
添加SQL2 → 暂存  
添加SQL3 → 暂存
调用flushStatements() → 一次性执行所有SQL → 返回结果
```

### 2.3 如何使用BatchExecutor


**方式一：通过配置文件开启**

```xml
<!-- mybatis-config.xml -->
<settings>
    <!-- 设置默认执行器为BATCH -->
    <setting name="defaultExecutorType" value="BATCH"/>
</settings>
```

**方式二：手动创建批量会话**

```java
// 获取批量执行的SqlSession
SqlSession batchSession = sqlSessionFactory.openSession(ExecutorType.BATCH);

try {
    UserMapper mapper = batchSession.getMapper(UserMapper.class);
    
    // 循环执行插入（暂存到批量执行器）
    for (int i = 0; i < 10000; i++) {
        User user = new User("user" + i, 20 + i);
        mapper.insertUser(user);
    }
    
    // 🔑 关键：手动触发批量执行
    batchSession.flushStatements();
    batchSession.commit();
    
} finally {
    batchSession.close();
}
```

**💡 重要提示**

> ⚠️ **注意**：使用BatchExecutor时，必须手动调用`flushStatements()`或`commit()`才会真正执行SQL，否则数据不会保存到数据库！

---

## 3. 📥 批量插入优化策略


### 3.1 传统插入方式的问题


```java
// ❌ 性能最差的写法
for (User user : userList) {
    userMapper.insertUser(user);
    sqlSession.commit(); // 每次都提交事务
}

问题分析：
1. 每条数据都建立一次数据库连接
2. 每条SQL都独立执行
3. 每次都提交事务
效率：插入1万条可能需要5-10分钟
```

### 3.2 优化策略一：使用BatchExecutor


```java
// ✅ 推荐写法：批量执行器
SqlSession batchSession = sqlSessionFactory.openSession(ExecutorType.BATCH);
UserMapper mapper = batchSession.getMapper(UserMapper.class);

for (User user : userList) {
    mapper.insertUser(user);
}

batchSession.flushStatements(); // 触发批量执行
batchSession.commit();           // 一次性提交事务

效率提升：插入1万条约10-30秒
```

### 3.3 优化策略二：使用foreach批量插入


**Mapper XML配置：**

```xml
<insert id="batchInsert" parameterType="list">
    INSERT INTO user (username, age) VALUES
    <foreach collection="list" item="user" separator=",">
        (#{user.username}, #{user.age})
    </foreach>
</insert>
```

**Java调用代码：**

```java
// ✅ 一次性插入多条数据
List<User> userList = new ArrayList<>();
// ... 添加数据到列表
userMapper.batchInsert(userList);
```

**生成的SQL（假设有3条数据）：**

```sql
INSERT INTO user (username, age) VALUES
('张三', 20),
('李四', 21),
('王五', 22);
```

### 3.4 性能对比分析


| 插入方式 | **1000条耗时** | **10000条耗时** | **推荐指数** |
|---------|--------------|---------------|------------|
| 逐条插入+每次提交 | 约60秒 | 约10分钟 | ⭐☆☆☆☆ |
| 逐条插入+批量提交 | 约15秒 | 约2分钟 | ⭐⭐☆☆☆ |
| BatchExecutor | 约3秒 | 约30秒 | ⭐⭐⭐⭐☆ |
| foreach批量插入 | 约1秒 | 约10秒 | ⭐⭐⭐⭐⭐ |

**🎯 选择建议**

```
数据量 < 100条：
→ 普通插入即可，性能差异不大

数据量 100-1000条：
→ 使用BatchExecutor或foreach

数据量 > 1000条：
→ 优先使用foreach + 分批处理
```

---

## 4. 🔄 批量更新优化方案


### 4.1 批量更新的常见场景


```
实际业务场景：
• 批量修改订单状态（如批量发货）
• 批量更新商品库存
• 批量修改用户积分
• 批量标记消息已读
```

### 4.2 方案一：BatchExecutor批量更新


```java
SqlSession batchSession = sqlSessionFactory.openSession(ExecutorType.BATCH);
UserMapper mapper = batchSession.getMapper(UserMapper.class);

// 循环更新
for (User user : userList) {
    mapper.updateUser(user);
}

batchSession.flushStatements();
batchSession.commit();
```

**优点**：代码简单，适合更新逻辑复杂的场景  
**缺点**：仍然是多条UPDATE语句

### 4.3 方案二：使用CASE WHEN批量更新


**Mapper XML配置：**

```xml
<update id="batchUpdate" parameterType="list">
    UPDATE user
    <trim prefix="SET" suffixOverrides=",">
        <trim prefix="age = CASE" suffix="END,">
            <foreach collection="list" item="item">
                WHEN id = #{item.id} THEN #{item.age}
            </foreach>
        </trim>
        <trim prefix="username = CASE" suffix="END,">
            <foreach collection="list" item="item">
                WHEN id = #{item.id} THEN #{item.username}
            </foreach>
        </trim>
    </trim>
    WHERE id IN
    <foreach collection="list" item="item" open="(" separator="," close=")">
        #{item.id}
    </foreach>
</update>
```

**生成的SQL示例：**

```sql
UPDATE user
SET age = CASE
    WHEN id = 1 THEN 20
    WHEN id = 2 THEN 21
    WHEN id = 3 THEN 22
END,
username = CASE
    WHEN id = 1 THEN '张三'
    WHEN id = 2 THEN '李四'  
    WHEN id = 3 THEN '王五'
END
WHERE id IN (1, 2, 3);
```

**💡 理解要点**

> 这个方案把多条UPDATE合并成一条SQL，通过CASE WHEN判断每个ID应该更新成什么值。就像一次性告诉数据库："id=1的改成这样，id=2的改成那样"。

### 4.4 批量更新性能对比


```
场景：更新1000条用户数据

⏱️ 逐条UPDATE：约15秒
⏱️ BatchExecutor：约3秒  
⏱️ CASE WHEN方式：约0.5秒

性能提升：CASE WHEN > BatchExecutor > 逐条更新
```

---

## 5. 🔁 foreach批量处理


### 5.1 foreach标签详解


**📋 foreach是什么？**

foreach是MyBatis提供的循环标签，用于遍历集合生成SQL片段。就像Java的for循环，但用在SQL中。

**核心属性说明：**

```xml
<foreach 
    collection="list"          集合名称（list、array、map的key）
    item="user"               当前循环的元素变量名
    index="i"                 当前循环的索引（可选）
    open="("                  开始符号
    close=")"                 结束符号
    separator=","             元素之间的分隔符
>
    #{user.username}
</foreach>
```

### 5.2 foreach批量插入实战


**场景：批量添加用户**

```xml
<insert id="batchInsertUsers" parameterType="list">
    INSERT INTO user (username, age, email) VALUES
    <foreach collection="list" item="user" separator=",">
        (#{user.username}, #{user.age}, #{user.email})
    </foreach>
</insert>
```

**Java调用：**

```java
List<User> users = Arrays.asList(
    new User("张三", 20, "zhang@example.com"),
    new User("李四", 21, "li@example.com"),
    new User("王五", 22, "wang@example.com")
);

userMapper.batchInsertUsers(users);
```

**生成的SQL：**

```sql
INSERT INTO user (username, age, email) VALUES
('张三', 20, 'zhang@example.com'),
('李四', 21, 'li@example.com'),
('王五', 22, 'wang@example.com');
```

### 5.3 foreach批量删除实战


```xml
<delete id="batchDeleteByIds" parameterType="list">
    DELETE FROM user
    WHERE id IN
    <foreach collection="list" item="id" open="(" separator="," close=")">
        #{id}
    </foreach>
</delete>
```

**生成的SQL：**

```sql
DELETE FROM user WHERE id IN (1, 2, 3, 4, 5);
```

### 5.4 foreach批量查询实战


```xml
<select id="getUsersByIds" resultType="User">
    SELECT * FROM user
    WHERE id IN
    <foreach collection="ids" item="id" open="(" separator="," close=")">
        #{id}
    </foreach>
</select>
```

**💡 实用技巧**

```
✅ foreach适用场景：
• IN查询：WHERE id IN (1,2,3)
• 批量插入：VALUES (a),(b),(c)
• 批量更新：CASE WHEN语句

⚠️ 注意事项：
• 集合不能为空，否则SQL语法错误
• 数据量过大时要分批，避免SQL过长
• MySQL的max_allowed_packet限制SQL大小
```

---

## 6. 📦 分批处理大数据量


### 6.1 为什么要分批处理


**❌ 一次性处理大数据的问题**

```
问题1：SQL语句过长
• MySQL默认max_allowed_packet=4MB
• 超过限制会报错：Packet for query is too large

问题2：内存溢出
• 一次性加载10万条数据到内存
• 可能导致OOM（Out of Memory）

问题3：事务过大
• 长时间占用数据库连接
• 锁定大量数据，影响其他业务
```

### 6.2 分批处理策略


**🔑 核心思想**：把大任务拆成多个小任务，每次处理一批。

```
原始需求：插入10万条数据

分批策略：
第1批：插入1-1000条
第2批：插入1001-2000条
第3批：插入2001-3000条
...
第100批：插入99001-100000条

每批独立提交，降低风险
```

### 6.3 分批处理代码实现


**工具方法：分割集合**

```java
/**
 * 将大集合分割成多个小批次
 * @param list 原始集合
 * @param batchSize 每批大小
 */
public static <T> List<List<T>> splitList(List<T> list, int batchSize) {
    List<List<T>> batches = new ArrayList<>();
    
    for (int i = 0; i < list.size(); i += batchSize) {
        int end = Math.min(i + batchSize, list.size());
        batches.add(list.subList(i, end));
    }
    
    return batches;
}
```

**批量插入实战：**

```java
// 10万条数据
List<User> allUsers = generateUsers(100000);

// 每批1000条
int batchSize = 1000;
List<List<User>> batches = splitList(allUsers, batchSize);

// 分批执行
for (int i = 0; i < batches.size(); i++) {
    List<User> batch = batches.get(i);
    
    try {
        userMapper.batchInsertUsers(batch);
        sqlSession.commit();
        
        System.out.println("第" + (i+1) + "批完成，插入" + batch.size() + "条");
        
    } catch (Exception e) {
        sqlSession.rollback();
        System.err.println("第" + (i+1) + "批失败：" + e.getMessage());
    }
}
```

### 6.4 批量大小的选择


```
🎯 推荐的批量大小：

数据简单（3-5个字段）：
→ 每批1000-5000条

数据复杂（10+个字段）：
→ 每批500-1000条

大文本数据（含BLOB/TEXT）：
→ 每批100-500条

经验公式：
批量大小 = 4MB / 单条数据大小（字节）
```

**💡 性能测试建议**

> 在实际项目中，应该根据服务器配置和数据特点进行测试，找到最优的批量大小。可以从500开始测试，逐步调整到1000、2000，观察性能变化。

---

## 7. 🔍 SQL执行器性能分析


### 7.1 MyBatis的三种执行器对比


**📊 执行器性能特点**

| 执行器类型 | **工作方式** | **性能** | **使用场景** |
|-----------|------------|---------|------------|
| **SimpleExecutor** | 每次新建Statement | ⭐⭐☆☆☆ | 简单查询，无性能要求 |
| **ReuseExecutor** | 重用相同SQL的Statement | ⭐⭐⭐☆☆ | 相同SQL频繁执行 |
| **BatchExecutor** | 批量执行多条SQL | ⭐⭐⭐⭐⭐ | 批量插入/更新/删除 |

### 7.2 SimpleExecutor执行流程


```
SimpleExecutor每次执行SQL的过程：

步骤1：准备SQL → "SELECT * FROM user WHERE id = ?"
步骤2：创建PreparedStatement对象
步骤3：设置参数 → id = 1
步骤4：执行SQL
步骤5：处理结果
步骤6：关闭Statement

下次查询又重复步骤2-6
```

**缺点**：每次都创建新的Statement，资源浪费

### 7.3 ReuseExecutor执行流程


```
ReuseExecutor的优化：

第一次执行：
步骤1：准备SQL → "SELECT * FROM user WHERE id = ?"
步骤2：创建PreparedStatement对象（缓存起来）
步骤3：设置参数 → id = 1
步骤4：执行SQL
步骤5：处理结果

第二次执行相同SQL：
步骤1：从缓存取出PreparedStatement（跳过创建）
步骤2：设置参数 → id = 2
步骤3：执行SQL
步骤4：处理结果
```

**优点**：避免重复创建Statement，提升10%-30%性能

### 7.4 BatchExecutor执行流程


```
BatchExecutor的批量处理：

添加SQL1：INSERT INTO user VALUES (1, '张三')  → 暂存
添加SQL2：INSERT INTO user VALUES (2, '李四')  → 暂存
添加SQL3：INSERT INTO user VALUES (3, '王五')  → 暂存

调用flushStatements()：
→ 一次性发送3条SQL到数据库
→ 数据库批量执行
→ 返回结果

性能提升：减少网络通信次数
```

### 7.5 如何选择合适的执行器


```
场景分析：

🔹 查询操作为主
→ SimpleExecutor即可
→ 性能瓶颈不在执行器

🔹 重复执行相同SQL
→ ReuseExecutor
→ 示例：循环查询用户详情

🔹 批量写操作
→ BatchExecutor
→ 示例：批量导入数据

🔹 混合场景
→ 动态切换执行器
→ 查询用Simple，批量用Batch
```

---

## 8. 🔄 PreparedStatement重用机制


### 8.1 什么是PreparedStatement


**📋 基础概念**

PreparedStatement是Java JDBC中的预编译SQL语句对象。简单理解：就是一个"SQL模板"。

```
普通Statement：
每次执行都要重新解析SQL
"SELECT * FROM user WHERE id = 1"
"SELECT * FROM user WHERE id = 2"
→ 数据库解析2次

PreparedStatement：
先创建SQL模板，然后填参数
模板："SELECT * FROM user WHERE id = ?"
参数1：id = 1
参数2：id = 2
→ 数据库只解析1次模板
```

### 8.2 PreparedStatement的优势


**⚡ 性能优势**

```
1. SQL预编译
   • 数据库编译一次，多次执行
   • 执行计划缓存，提速20%-50%

2. 参数绑定
   • 避免SQL拼接
   • 减少字符串处理开销

3. 防止SQL注入
   • 参数自动转义
   • 安全性大幅提升
```

### 8.3 MyBatis中的重用机制


**ReuseExecutor的Statement缓存**

```java
// ReuseExecutor内部实现原理（简化版）
public class ReuseExecutor extends BaseExecutor {
    
    // 缓存Map：key是SQL，value是PreparedStatement
    private Map<String, Statement> statementMap = new HashMap<>();
    
    @Override
    public <E> List<E> doQuery(String sql, Object parameter) {
        // 检查缓存
        Statement stmt = statementMap.get(sql);
        
        if (stmt == null) {
            // 首次执行，创建并缓存
            stmt = connection.prepareStatement(sql);
            statementMap.put(sql, stmt);
        }
        
        // 设置参数并执行
        setParameters(stmt, parameter);
        return stmt.executeQuery();
    }
}
```

**💡 重用条件**

> ⚠️ **注意**：只有完全相同的SQL才能重用PreparedStatement。SQL中任何差异（包括空格、大小写）都会被认为是不同的SQL。

### 8.4 重用机制的性能测试


```
测试场景：循环查询1000次用户信息

SimpleExecutor（不重用）：
→ 创建1000个PreparedStatement
→ 耗时：约800ms

ReuseExecutor（重用）：
→ 创建1个PreparedStatement，重用1000次
→ 耗时：约500ms

性能提升：约40%
```

---

## 9. 📋 ResultSet处理优化


### 9.1 什么是ResultSet


**📋 基础概念**

ResultSet是数据库查询结果的集合，就像一个"数据表格"的游标。

```
数据库查询结果：
+----+----------+-----+
| id | username | age |
+----+----------+-----+
| 1  | 张三      | 20  |
| 2  | 李四      | 21  |
| 3  | 王五      | 22  |
+----+----------+-----+

ResultSet工作方式：
→ 游标初始指向第一行之前
→ 调用next()移动到下一行
→ 读取当前行的数据
→ 重复直到没有数据
```

### 9.2 ResultSet的性能问题


**❌ 常见性能陷阱**

```
问题1：全量加载大结果集
• 查询10万条数据一次性加载到内存
• 导致内存溢出（OOM）

问题2：频繁的类型转换
• 数据库INT → Java Integer
• 每次转换都有性能开销

问题3：重复的对象创建
• 每行都创建新的对象
• GC压力大
```

### 9.3 优化策略一：分页查询


```java
// ❌ 不推荐：一次性查询所有数据
List<User> allUsers = userMapper.selectAllUsers();

// ✅ 推荐：分页查询
int pageSize = 100;
int currentPage = 1;

while (true) {
    List<User> users = userMapper.selectUsersByPage(currentPage, pageSize);
    
    if (users.isEmpty()) {
        break; // 没有更多数据
    }
    
    // 处理当前页数据
    processUsers(users);
    
    currentPage++;
}
```

**对应的Mapper SQL：**

```xml
<select id="selectUsersByPage" resultType="User">
    SELECT * FROM user
    LIMIT #{offset}, #{pageSize}
</select>
```

### 9.4 优化策略二：ResultType优化


**选择合适的返回类型**

```xml
<!-- ❌ 返回完整对象（字段很多时浪费） -->
<select id="getUserList" resultType="User">
    SELECT * FROM user
</select>

<!-- ✅ 只查询需要的字段 -->
<select id="getUserBasicInfo" resultType="map">
    SELECT id, username FROM user
</select>
```

### 9.5 优化策略三：懒加载关联对象


**场景：用户关联订单**

```xml
<resultMap id="UserWithOrders" type="User">
    <id property="id" column="id"/>
    <result property="username" column="username"/>
    
    <!-- 懒加载：只有访问orders时才查询 -->
    <collection property="orders" 
                ofType="Order" 
                select="selectOrdersByUserId" 
                column="id"
                fetchType="lazy"/>
</resultMap>
```

**配置开启懒加载：**

```xml
<settings>
    <!-- 开启延迟加载 -->
    <setting name="lazyLoadingEnabled" value="true"/>
    <!-- 按需加载，不触发全部加载 -->
    <setting name="aggressiveLazyLoading" value="false"/>
</settings>
```

**💡 懒加载的好处**

```
不使用懒加载：
查询100个用户 → 自动关联查询100个用户的订单
→ 即使不需要订单数据也会查询
→ 性能浪费

使用懒加载：
查询100个用户 → 仅查询用户数据
→ 访问user.getOrders()时才查询订单
→ 按需加载，节省资源
```

---

## 10. 🔌 连接获取优化


### 10.1 数据库连接池概念


**🏊 连接池是什么？**

连接池就像一个"数据库连接的仓库"，预先创建好一些连接，需要时直接取用，用完归还。

```
不使用连接池：
请求1 → 创建连接 → 使用 → 关闭（耗时200ms）
请求2 → 创建连接 → 使用 → 关闭（耗时200ms）
请求3 → 创建连接 → 使用 → 关闭（耗时200ms）

使用连接池：
初始化 → 预创建10个连接（一次性耗时2s）
请求1 → 取出连接 → 使用 → 归还（耗时5ms）
请求2 → 取出连接 → 使用 → 归还（耗时5ms）
请求3 → 取出连接 → 使用 → 归还（耗时5ms）
```

### 10.2 常用连接池对比


| 连接池 | **性能** | **功能** | **推荐场景** |
|-------|---------|---------|------------|
| **Druid** | ⭐⭐⭐⭐⭐ | 监控、防SQL注入 | 生产环境首选 |
| **HikariCP** | ⭐⭐⭐⭐⭐ | 轻量、速度最快 | 高性能要求 |
| **C3P0** | ⭐⭐⭐☆☆ | 稳定、成熟 | 传统项目 |
| **DBCP** | ⭐⭐☆☆☆ | 基础功能 | 小型项目 |

### 10.3 Druid连接池配置


**application.yml配置：**

```yaml
spring:
  datasource:
    type: com.alibaba.druid.pool.DruidDataSource
    driver-class-name: com.mysql.cj.jdbc.Driver
    url: jdbc:mysql://localhost:3306/mydb
    username: root
    password: 123456
    
    # Druid连接池配置
    druid:
      initial-size: 5          # 初始连接数
      min-idle: 5              # 最小空闲连接
      max-active: 20           # 最大活跃连接
      max-wait: 60000          # 获取连接最大等待时间(ms)
      
      # 检测配置
      test-while-idle: true              # 空闲时检测连接
      test-on-borrow: false              # 获取时不检测
      test-on-return: false              # 归还时不检测
      validation-query: SELECT 1         # 检测SQL
      
      # 性能优化
      pool-prepared-statements: true     # 开启PSCache
      max-pool-prepared-statement-per-connection-size: 20
```

### 10.4 连接池参数调优


**🎯 参数设置原则**

```
初始连接数（initial-size）：
• 建议值：5-10
• 含义：应用启动时创建的连接数
• 原则：够用即可，避免过多占用资源

最大连接数（max-active）：
• 建议值：根据并发量计算
• 计算公式：并发量 × 单次处理时间 / 1000
• 示例：100并发，平均50ms处理 → 100×50/1000=5个连接

最小空闲连接（min-idle）：
• 建议值：与initial-size相同
• 含义：保持的最少空闲连接
• 原则：应对突发流量
```

**💡 实战建议**

> **小型应用**（日访问<1万）：max-active=10-20  
> **中型应用**（日访问1-10万）：max-active=20-50  
> **大型应用**（日访问>10万）：max-active=50-200

### 10.5 连接泄漏检测


**什么是连接泄漏？**

```
连接泄漏示例：
try {
    SqlSession session = sqlSessionFactory.openSession();
    // 执行业务
    // ❌ 忘记关闭session
} catch (Exception e) {
    // 异常处理
}

结果：连接没有归还到池中，最终耗尽连接池
```

**Druid的泄漏检测：**

```yaml
druid:
  # 开启连接泄漏检测
  remove-abandoned: true
  # 超过300秒未归还视为泄漏
  remove-abandoned-timeout: 300
  # 打印泄漏日志
  log-abandoned: true
```

---

## 11. 💾 语句缓存机制


### 11.1 MyBatis的两级缓存


**🗂️ 缓存架构**

```
一级缓存（SqlSession级别）
    ↓
查询user(id=1) → 从数据库查询 → 放入一级缓存
再次查询user(id=1) → 直接从缓存返回
    ↓
二级缓存（Mapper级别）
    ↓
跨SqlSession共享
多个会话可以共享查询结果
```

### 11.2 一级缓存详解


**📋 一级缓存特点**

```
生命周期：与SqlSession相同
作用范围：仅当前会话
默认开启：无需配置
清除时机：
• 执行commit()
• 执行rollback()
• 执行update/insert/delete
• 手动调用clearCache()
```

**一级缓存示例：**

```java
SqlSession session = sqlSessionFactory.openSession();
UserMapper mapper = session.getMapper(UserMapper.class);

// 第一次查询，从数据库获取
User user1 = mapper.getUserById(1);

// 第二次查询，从一级缓存获取（无需查库）
User user2 = mapper.getUserById(1);

System.out.println(user1 == user2); // true，同一个对象

session.close();
```

**💡 一级缓存的陷阱**

```
问题场景：
• 线程A查询user(id=1) → 缓存
• 线程B更新user(id=1)
• 线程A再次查询user(id=1) → 返回旧数据（脏读）

解决方案：
• 及时提交事务清空缓存
• 避免长时间持有SqlSession
```

### 11.3 二级缓存详解


**📋 二级缓存特点**

```
生命周期：与应用相同
作用范围：整个Mapper
默认关闭：需要手动开启
存储位置：内存或Redis等
```

**开启二级缓存步骤：**

**步骤1：全局配置开启**

```xml
<!-- mybatis-config.xml -->
<settings>
    <setting name="cacheEnabled" value="true"/>
</settings>
```

**步骤2：Mapper中配置**

```xml
<!-- UserMapper.xml -->
<mapper namespace="com.example.mapper.UserMapper">
    
    <!-- 开启二级缓存 -->
    <cache 
        eviction="LRU"           淘汰策略：LRU最近最少使用
        flushInterval="60000"    刷新间隔：60秒
        size="512"               缓存对象数量：512个
        readOnly="true"/>        只读缓存
    
    <select id="getUserById" resultType="User" useCache="true">
        SELECT * FROM user WHERE id = #{id}
    </select>
</mapper>
```

**步骤3：实体类实现序列化**

```java
public class User implements Serializable {
    private static final long serialVersionUID = 1L;
    
    private Long id;
    private String username;
    // ...
}
```

### 11.4 缓存淘汰策略


| 策略 | **说明** | **适用场景** |
|-----|---------|------------|
| **LRU** | 最近最少使用，移除最长时间不用的 | 默认推荐 |
| **FIFO** | 先进先出，移除最早进入的 | 访问模式均匀 |
| **SOFT** | 软引用，内存不足时清除 | 内存敏感 |
| **WEAK** | 弱引用，GC时清除 | 短期缓存 |

### 11.5 缓存使用建议


```
✅ 适合使用缓存的场景：
• 查询频繁
• 数据变化不频繁
• 对实时性要求不高
• 示例：字典表、配置表

❌ 不适合使用缓存的场景：
• 数据实时性要求高
• 数据变化频繁
• 多表关联复杂
• 示例：订单表、库存表

🎯 最佳实践：
• 一级缓存默认开启即可
• 二级缓存谨慎使用
• 分布式环境使用Redis缓存
• 定期监控缓存命中率
```

---

## 12. 📋 核心要点总结


### 12.1 批处理优化核心要点


```
🎯 批处理三大法宝：

1️⃣ BatchExecutor批量执行器
   • 适合：批量DML操作
   • 原理：攒够一批再执行
   • 用法：openSession(ExecutorType.BATCH)

2️⃣ foreach批量SQL
   • 适合：批量插入、IN查询
   • 原理：生成一条包含多值的SQL
   • 优势：性能最优，推荐首选

3️⃣ 分批处理大数据
   • 适合：10万+数据量
   • 原理：拆分成小批次
   • 批量大小：500-5000条/批
```

### 12.2 性能优化关键点


```
⚡ 性能优化四个维度：

🔹 执行器优化
   • SimpleExecutor：普通场景
   • ReuseExecutor：相同SQL频繁执行
   • BatchExecutor：批量写操作

🔹 SQL优化
   • PreparedStatement重用
   • 减少数据库交互次数
   • 合理使用批量操作

🔹 结果集优化
   • 分页查询避免一次性加载
   • 懒加载减少不必要的关联
   • 只查询需要的字段

🔹 连接优化
   • 使用连接池（Druid/HikariCP）
   • 合理配置连接数
   • 防止连接泄漏
```

### 12.3 性能提升对照表


| 优化手段 | **优化前** | **优化后** | **提升比例** |
|---------|-----------|-----------|------------|
| 批量插入1万条 | 5-10分钟 | 10-30秒 | 🚀 10-30倍 |
| 批量更新1000条 | 15秒 | 0.5秒 | 🚀 30倍 |
| Statement重用 | 800ms | 500ms | ⚡ 40% |
| 连接池优化 | 200ms/次 | 5ms/次 | ⚡ 97% |

### 12.4 实战开发建议


```
🛠️ 开发规范：

1. 批量操作优先级：
   foreach > BatchExecutor > 循环单条

2. 数据量阈值：
   • <100条：普通操作
   • 100-1000：使用BatchExecutor
   • >1000：foreach + 分批

3. 连接池配置：
   • 小项目：max-active=10-20
   • 中项目：max-active=20-50
   • 大项目：max-active=50-200

4. 缓存策略：
   • 一级缓存：默认开启
   • 二级缓存：谨慎使用
   • 分布式：用Redis
```

### 12.5 常见问题速查


```
❓ 批量操作没生效？
→ 检查是否调用flushStatements()和commit()

❓ 批量插入报错SQL too long？
→ 调整max_allowed_packet或减小批量大小

❓ 性能提升不明显？
→ 检查网络延迟、数据库配置、索引是否合理

❓ 内存溢出OOM？
→ 使用分批处理，减少单批数据量

❓ 连接池耗尽？
→ 检查连接泄漏，确保session正确关闭
```

**🎓 学习建议**

> 1. 先理解原理，再动手实践  
> 2. 小数据量验证，大数据量压测  
> 3. 监控性能指标，持续优化调整  
> 4. 根据实际业务选择合适方案