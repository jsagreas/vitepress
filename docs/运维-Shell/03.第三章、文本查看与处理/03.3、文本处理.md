---
title: 3、文本处理
---
## 📚 目录

1. [文本排序与去重](#1-文本排序与去重)
2. [文本提取与切割](#2-文本提取与切割)
3. [文件连接与合并](#3-文件连接与合并)
4. [字符处理与转换](#4-字符处理与转换)
5. [文件分割与比较](#5-文件分割与比较)
6. [编码转换与格式处理](#6-编码转换与格式处理)
7. [实际应用场景](#7-实际应用场景)
8. [核心要点总结](#8-核心要点总结)

---

## 1. 📊 文本排序与去重


### 1.1 sort命令 - 文本排序神器


**🔸 什么是sort**
```
sort就像一个智能的排序助手，能把杂乱无章的文本按规则整理得井井有条
想象你有一堆学生成绩单要按分数排序，sort就是帮你做这件事的工具
```

**⚡ 基本用法**

| 参数 | **含义** | **实际用途** | **记忆要点** |
|------|---------|-------------|-------------|
| `-n` | `数字排序` | `按数值大小排序，不是字符` | `n=number数字` |
| `-r` | `反向排序` | `从大到小排序` | `r=reverse反转` |
| `-k` | `指定排序列` | `按第几列排序` | `k=key关键列` |
| `-t` | `指定分隔符` | `设置列分隔符` | `t=tab制表符` |

**💡 实用示例**

```bash
# 示例数据：学生成绩表
cat > scores.txt << EOF
张三:85:数学
李四:92:英语
王五:78:数学  
赵六:96:英语
EOF

# 按姓名排序（默认字符排序）
sort scores.txt
# 张三:85:数学
# 李四:92:英语
# 王五:78:数学
# 赵六:96:英语

# 按分数排序（数字排序）
sort -n -t: -k2 scores.txt
# 王五:78:数学
# 张三:85:数学
# 李四:92:英语
# 赵六:96:英语

# 按分数从高到低排序
sort -nr -t: -k2 scores.txt
# 赵六:96:英语
# 李四:92:英语
# 张三:85:数学
# 王五:78:数学
```

**🎯 参数组合技巧**
```
-t: -k2,2n     指定:为分隔符，按第2列数字排序
-k1,1 -k2,2nr  先按第1列排序，再按第2列数字倒序
-u             排序的同时去重（相当于sort + uniq）
```

### 1.2 uniq命令 - 去重专家


**🔸 什么是uniq**
```
uniq专门处理重复行，但有个重要前提：只能处理相邻的重复行
所以通常要先用sort排序，再用uniq去重
就像整理重复的名片，要先摞在一起才能发现重复
```

**📋 核心参数**

| 参数 | **作用** | **输出结果** | **使用场景** |
|------|---------|-------------|-------------|
| `无参数` | `去除重复行` | `只保留唯一行` | `数据清洗` |
| `-c` | `统计重复次数` | `次数 + 内容` | `频率统计` |
| `-d` | `只显示重复行` | `重复的内容` | `找重复数据` |
| `-u` | `只显示唯一行` | `不重复的内容` | `找独特数据` |

**💡 实际操作示例**

```bash
# 创建测试数据
cat > names.txt << EOF
张三
李四
张三
王五
李四
李四
赵六
EOF

# 必须先排序再去重
sort names.txt | uniq
# 张三
# 李四
# 王五
# 赵六

# 统计每个名字出现次数
sort names.txt | uniq -c
#   2 张三
#   3 李四
#   1 王五
#   1 赵六

# 只显示重复的名字
sort names.txt | uniq -d  
# 张三
# 李四

# 只显示出现一次的名字
sort names.txt | uniq -u
# 王五
# 赵六
```

**⚠️ 重要提醒**
```
uniq只能处理相邻的重复行！
错误用法：uniq names.txt  ❌ （没有排序）
正确用法：sort names.txt | uniq  ✅
```

---

## 2. ✂️ 文本提取与切割


### 2.1 cut命令 - 精确提取工具


**🔸 cut的本质**
```
cut就像一把精准的剪刀，能从文本中"剪出"你要的部分
比如从"姓名:年龄:职业"中只要年龄部分，cut就能精确剪出来
```

**🔧 核心参数详解**

| 参数 | **含义** | **使用方法** | **实际场景** |
|------|---------|-------------|-------------|
| `-d` | `指定分隔符` | `-d:`或`-d,` | `处理CSV、配置文件` |
| `-f` | `提取字段` | `-f1,3`提取1和3列 | `提取特定列数据` |
| `-c` | `提取字符位置` | `-c1-5`提取1到5字符 | `固定格式文本` |

**💡 实用案例演示**

```bash
# 员工信息表
cat > employee.txt << EOF
001:张三:开发:8000:北京
002:李四:测试:6000:上海
003:王五:运维:7000:深圳
EOF

# 只提取姓名（第2列）
cut -d: -f2 employee.txt
# 张三
# 李四  
# 王五

# 提取姓名和薪资（第2和4列）
cut -d: -f2,4 employee.txt
# 张三:8000
# 李四:6000
# 王五:7000

# 提取第2列到第4列
cut -d: -f2-4 employee.txt
# 张三:开发:8000
# 李四:测试:6000
# 王五:运维:7000

# 按字符位置提取（提取前3个字符）
cut -c1-3 employee.txt
# 001
# 002
# 003
```

**🎯 实际应用场景**

```bash
# 从系统日志中提取IP地址（假设IP在第3列）
cut -d' ' -f3 /var/log/access.log

# 从CSV文件中提取特定列
cut -d, -f1,3,5 data.csv

# 提取文件的前10个字符
cut -c1-10 filename.txt
```

### 2.2 tr命令 - 字符转换大师


**🔸 tr的核心作用**
```
tr专门做字符级别的替换和删除
就像查找替换功能，但更强大，能批量处理字符集
比如把所有大写字母变小写，删除数字字符等
```

**⚡ 常用功能**

**字符替换**
```bash
# 大小写转换
echo "Hello World" | tr 'a-z' 'A-Z'
# HELLO WORLD

echo "LINUX SHELL" | tr 'A-Z' 'a-z'  
# linux shell

# 字符替换
echo "hello world" | tr 'l' 'L'
# heLLo worLd
```

**字符删除**
```bash
# 删除数字
echo "abc123def456" | tr -d '0-9'
# abcdef

# 删除空格
echo "a b c d" | tr -d ' '
# abcd

# 删除换行符（合并多行）
tr -d '\n' < multiline.txt
```

**特殊字符集**
```bash
# 删除所有标点符号
echo "Hello, World!" | tr -d '[:punct:]'
# Hello World

# 压缩重复空格为单个空格
echo "a    b        c" | tr -s ' '
# a b c
```

**💡 实际应用**

```bash
# 将文件中的Windows换行符转为Unix换行符
tr -d '\r' < windows.txt > unix.txt

# 将制表符替换为空格
tr '\t' ' ' < file.txt

# 生成密码（删除混淆字符）
tr -dc 'A-Za-z0-9' < /dev/urandom | head -c 12
```

---

## 3. 🔗 文件连接与合并


### 3.1 join命令 - 智能文件连接


**🔸 join的工作原理**
```
join像数据库的表连接，根据共同的"键"把两个文件合并
就像学生表和成绩表通过学号连接，得到完整信息
重要：两个文件都必须按连接字段排序！
```

**📊 基本用法示例**

```bash
# 学生基本信息
cat > students.txt << EOF
001 张三 男
002 李四 女  
003 王五 男
EOF

# 学生成绩信息
cat > scores.txt << EOF
001 85 数学
002 92 英语
003 78 物理
EOF

# 按第一列（学号）连接两个文件
join students.txt scores.txt
# 001 张三 男 85 数学
# 002 李四 女 92 英语  
# 003 王五 男 78 物理
```

**🔧 高级参数**

| 参数 | **作用** | **示例** |
|------|---------|----------|
| `-t` | `指定分隔符` | `join -t: file1 file2` |
| `-1 N` | `file1按第N列连接` | `join -1 2 file1 file2` |
| `-2 N` | `file2按第N列连接` | `join -2 3 file1 file2` |
| `-a 1` | `显示file1的所有行` | `左外连接效果` |
| `-v 1` | `只显示file1独有行` | `找差异数据` |

### 3.2 paste命令 - 简单文件合并


**🔸 paste vs join的区别**
```
paste: 简单粗暴，按行号对应合并，像拼积木
join:  智能连接，按内容匹配合并，像数据库连接

paste适合：两个文件行数相同，要按行合并
join适合：两个文件有共同字段，要按内容合并
```

**💡 实用示例**

```bash
# 姓名文件
cat > names.txt << EOF
张三
李四
王五
EOF

# 年龄文件  
cat > ages.txt << EOF
25
30
28
EOF

# 简单合并（默认用制表符分隔）
paste names.txt ages.txt
# 张三	25
# 李四	30
# 王五	28

# 指定分隔符
paste -d: names.txt ages.txt
# 张三:25
# 李四:30
# 王五:28

# 合并多个文件
paste names.txt ages.txt cities.txt
```

**🎯 实际应用场景**

```bash
# 合并多个单列数据文件
paste col1.txt col2.txt col3.txt > combined.txt

# 将多行变成一行（用指定分隔符）
paste -sd, list.txt
# item1,item2,item3

# 每两行合并成一行
paste - - < file.txt
```

---

## 4. 🔄 字符处理与转换


### 4.1 字段分隔符高级处理


**🔸 分隔符处理的重要性**
```
现实中的数据格式千奇百怪：
- CSV用逗号分隔
- TSV用制表符分隔  
- 日志用空格分隔
- 配置文件用冒号分隔

掌握分隔符处理，就能处理各种格式的数据
```

**⚡ 复杂分隔符处理技巧**

```bash
# 处理多种分隔符混合的数据
echo "name:age,city;country" | tr ':,;' '\t' | cut -f1,2
# name	age

# 处理引号包围的字段（CSV常见情况）
echo '"张三","25","北京"' | tr -d '"' | cut -d, -f2
# 25

# 处理可变长度的空白分隔
echo "张三   25    北京" | tr -s ' ' | cut -d' ' -f2
# 25
```

**🎯 处理不规范数据的策略**

```
数据清洗三步法：
1. 统一分隔符：tr命令转换
2. 去除多余字符：tr -d删除  
3. 标准化格式：cut提取需要的列
```

### 4.2 文本格式转换


**🔸 换行符转换的必要性**
```
Windows系统：每行结尾是\r\n（回车+换行）
Unix/Linux系统：每行结尾是\n（换行）
Mac系统：每行结尾是\r（回车）

不同系统间传输文件，换行符不匹配会导致显示异常
```

**💻 实用转换命令**

```bash
# Windows到Unix转换
dos2unix windows_file.txt

# Unix到Windows转换  
unix2dos unix_file.txt

# 手动转换（如果没有专用命令）
tr -d '\r' < windows.txt > unix.txt  # 删除回车符
sed 's/$/\r/' unix.txt > windows.txt  # 行尾添加回车符
```

**⚠️ 识别文件格式**

```bash
# 查看文件的换行符类型
file filename.txt
# filename.txt: ASCII text, with CRLF line terminators  # Windows格式
# filename.txt: ASCII text                              # Unix格式

# 显示不可见字符
cat -A filename.txt
# Windows文件会在行尾显示 ^M$
# Unix文件只在行尾显示 $
```

---

## 5. ✂️ 文件分割与比较


### 5.1 split命令 - 文件分割工具


**🔸 为什么需要分割文件**
```
实际场景：
- 大日志文件需要分批处理
- 上传文件有大小限制
- 数据备份需要分卷
- 并行处理需要分块

split就是解决这些问题的利器
```

**📊 分割方式对比**

| 分割方式 | **命令格式** | **适用场景** | **示例** |
|---------|-------------|-------------|----------|
| `按行数` | `split -l N` | `处理结构化数据` | `split -l 1000 big.txt` |
| `按大小` | `split -b N` | `文件传输分卷` | `split -b 1M video.mp4` |
| `按模式` | `csplit` | `按内容分割` | `csplit log.txt /ERROR/ {*}` |

**💡 实用分割示例**

```bash
# 按行数分割（每1000行一个文件）
split -l 1000 bigfile.txt part_
# 生成：part_aa, part_ab, part_ac...

# 按大小分割（每10MB一个文件）
split -b 10M largefile.dat chunk_
# 生成：chunk_aa, chunk_ab...

# 指定文件名前缀和后缀格式
split -l 500 -d --suffix-length=3 data.txt part_
# 生成：part_000, part_001, part_002...

# 重新合并分割的文件
cat part_* > original_file.txt
```

**🎯 高级分割技巧**

```bash
# 按内容分割日志文件（每遇到ERROR就分割）
csplit server.log '/ERROR/' '{*}' --prefix=error_part_
# 按日期分割日志
awk '/2023-01-01/{n++}{print > "log_part_" n ".txt"}' access.log
```

### 5.2 文件比较三兄弟


**🔸 diff - 详细差异分析**
```
diff像文档的修改跟踪功能，详细显示两个文件的差异
适合：代码比较、配置文件变更检查
```

```bash
# 基本比较
diff file1.txt file2.txt

# 并排显示差异
diff -y file1.txt file2.txt

# 忽略大小写差异
diff -i file1.txt file2.txt

# 生成补丁文件
diff -u old.txt new.txt > changes.patch
```

**🔸 cmp - 二进制文件比较**
```
cmp专门比较二进制文件，精确到字节级别
适合：验证文件完整性、检查文件是否相同
```

```bash
# 检查两个文件是否完全相同
cmp file1.bin file2.bin
# 如果相同：无输出
# 如果不同：显示第一个不同的位置

# 显示所有不同的字节位置
cmp -l file1.txt file2.txt
```

**🔸 comm - 共同行比较**
```
comm比较两个已排序文件的共同行和独有行
输出三列：只在file1中、只在file2中、两个文件都有
```

```bash
# 准备已排序的文件
sort list1.txt > sorted1.txt
sort list2.txt > sorted2.txt

# 三列输出比较
comm sorted1.txt sorted2.txt
# 第1列：只在file1中
# 第2列：只在file2中  
# 第3列：两文件共有

# 只显示共同行
comm -12 sorted1.txt sorted2.txt

# 只显示file1独有的行
comm -23 sorted1.txt sorted2.txt
```

---

## 6. 🌐 编码转换与格式处理


### 6.1 iconv - 字符编码转换专家


**🔸 字符编码的重要性**
```
常见编码问题：
- 中文显示乱码：GBK vs UTF-8
- 文件在不同系统间传输出现乱码
- 网页显示异常：编码声明与实际不符

iconv就是解决这些编码问题的瑞士军刀
```

**🔧 编码转换实操**

```bash
# 查看系统支持的编码
iconv -l | head -20

# 基本转换格式
iconv -f 源编码 -t 目标编码 输入文件 > 输出文件

# GBK转UTF-8（处理中文乱码）
iconv -f GBK -t UTF-8 chinese_gbk.txt > chinese_utf8.txt

# UTF-8转GBK
iconv -f UTF-8 -t GBK chinese_utf8.txt > chinese_gbk.txt

# 批量转换多个文件
for file in *.txt; do
    iconv -f GBK -t UTF-8 "$file" > "${file%.txt}_utf8.txt"
done
```

**⚠️ 编码检测与处理**

```bash
# 检测文件编码（需要安装chardet）
file -i filename.txt
# filename.txt: text/plain; charset=utf-8

# 处理编码错误（忽略无法转换的字符）
iconv -f GBK -t UTF-8//IGNORE input.txt > output.txt

# 替换无法转换的字符
iconv -f GBK -t UTF-8//TRANSLIT input.txt > output.txt
```

**🎯 实际应用场景**

```bash
# 网站迁移：批量转换网页编码
find /var/www -name "*.html" | while read file; do
    iconv -f GBK -t UTF-8 "$file" > "${file}.new"
    mv "${file}.new" "$file"
done

# 数据库导入前的编码转换
iconv -f UTF-8 -t GBK data.csv > data_gbk.csv
```

---

## 7. 🚀 实际应用场景


### 7.1 日志分析综合案例


**📊 场景：分析Web服务器访问日志**

```bash
# 假设日志格式：IP 时间 方法 URL 状态码 大小
# 192.168.1.1 [01/Jan/2023:12:00:00] GET /index.html 200 1024

# 1. 提取访问最多的IP（前10名）
cat access.log | cut -d' ' -f1 | sort | uniq -c | sort -nr | head -10

# 2. 统计状态码分布
cat access.log | cut -d' ' -f5 | sort | uniq -c | sort -nr

# 3. 找出访问量最大的页面
cat access.log | cut -d' ' -f4 | sort | uniq -c | sort -nr | head -10

# 4. 按小时统计访问量
cat access.log | cut -d'[' -f2 | cut -d':' -f2 | sort | uniq -c

# 5. 找出404错误的页面
grep " 404 " access.log | cut -d' ' -f4 | sort | uniq -c
```

### 7.2 数据清洗实战


**🛠️ 场景：清理CSV数据文件**

```bash
# 原始CSV文件可能存在的问题：
# - 重复行
# - 空行
# - 编码问题
# - 格式不统一

# 数据清洗流水线
clean_csv() {
    local input_file=$1
    local output_file=$2
    
    # 1. 转换编码
    iconv -f GBK -t UTF-8 "$input_file" > temp1.csv
    
    # 2. 删除空行
    grep -v '^$' temp1.csv > temp2.csv
    
    # 3. 统一分隔符（把分号替换为逗号）
    tr ';' ',' < temp2.csv > temp3.csv
    
    # 4. 去重
    sort temp3.csv | uniq > temp4.csv
    
    # 5. 最终输出
    mv temp4.csv "$output_file"
    
    # 清理临时文件
    rm -f temp*.csv
}

# 使用函数
clean_csv dirty_data.csv clean_data.csv
```

### 7.3 系统监控脚本


**📈 场景：系统资源统计报告**

```bash
#!/bin/bash
# 生成系统资源报告

generate_report() {
    local report_file="system_report_$(date +%Y%m%d).txt"
    
    echo "=== 系统资源报告 ===" > "$report_file"
    echo "生成时间: $(date)" >> "$report_file"
    echo "" >> "$report_file"
    
    # CPU使用率前10的进程
    echo "CPU使用率TOP 10:" >> "$report_file"
    ps aux --sort=-pcpu | head -11 | tail -10 | \
    cut -c1-11,65- >> "$report_file"
    echo "" >> "$report_file"
    
    # 内存使用率前10的进程  
    echo "内存使用率TOP 10:" >> "$report_file"
    ps aux --sort=-pmem | head -11 | tail -10 | \
    cut -c1-11,65- >> "$report_file"
    echo "" >> "$report_file"
    
    # 磁盘使用情况
    echo "磁盘使用情况:" >> "$report_file"
    df -h | grep -v tmpfs | grep -v udev >> "$report_file"
    
    echo "报告已生成: $report_file"
}

generate_report
```

---

## 8. 📋 核心要点总结


### 8.1 必须掌握的核心概念


```
🔸 sort：排序的万能工具，记住-n(数字)、-r(反向)、-k(列)、-t(分隔符)
🔸 uniq：去重专家，必须先排序，-c统计、-d找重复、-u找唯一
🔸 cut：精确提取工具，-d指定分隔符、-f选择字段、-c选择字符
🔸 tr：字符级处理，替换、删除、压缩，一步到位
🔸 join：智能文件连接，像数据库连接，必须预先排序
🔸 paste：简单文件合并，按行对应拼接
```

### 8.2 关键理解要点


**🔹 命令组合的威力**
```
单个命令功能有限，组合使用威力无穷：
sort + uniq = 排序去重
cut + sort + uniq -c = 统计分析
tr + cut = 数据清洗
grep + cut + sort = 日志分析

掌握管道思维：一个命令的输出是下个命令的输入
```

**🔹 数据处理的一般流程**
```
1. 数据清洗：tr处理格式，删除无关字符
2. 数据提取：cut选择需要的列或字符
3. 数据排序：sort按需要的规则排序  
4. 数据统计：uniq进行计数和去重
5. 数据输出：重定向到文件或传递给下个命令
```

**🔹 文件比较选择策略**
```
diff：查看详细差异，适合文本文件
cmp：检查文件是否相同，适合二进制文件
comm：比较已排序文件的共同部分，适合列表比较
```

### 8.3 实际应用指导


**📊 文本处理最佳实践**

```bash
# 模板：日志分析通用流程
analyze_log() {
    local log_file=$1
    local field=$2  # 要分析的字段位置
    
    echo "分析文件: $log_file"
    echo "统计字段: $field"
    echo "==================="
    
    # 提取 -> 排序 -> 统计 -> 排序(按频率) -> 显示前10
    cut -d' ' -f"$field" "$log_file" | \
    sort | \
    uniq -c | \
    sort -nr | \
    head -10
}

# 使用示例
analyze_log access.log 1  # 分析IP访问频率
analyze_log access.log 4  # 分析访问页面频率
```

**🛠️ 数据处理工具箱**

```bash
# 快速查看数据结构
preview_data() {
    local file=$1
    echo "文件: $file"
    echo "行数: $(wc -l < "$file")"
    echo "列数: $(head -1 "$file" | tr -cd ',' | wc -c)"
    echo "前5行:"
    head -5 "$file"
    echo "后5行:"  
    tail -5 "$file"
}

# 快速数据统计
quick_stats() {
    local file=$1
    local col=$2
    echo "第${col}列统计信息:"
    cut -d, -f"$col" "$file" | sort -n | uniq -c | sort -nr
}
```

### 8.4 记忆口诀


**🎯 核心命令记忆法**

```
sort排序很重要，-n数字-r倒，-k指定列，-t分隔号
uniq去重要排序，-c计数-d重复，-u唯一很清楚
cut切割定位准，-d分隔-f字段，-c字符位置论
tr转换字符串，删除替换压缩匀，大小写转换很迅速
join连接像数据库，两文件要排序，共同字段来做主
paste简单做合并，按行拼接不挑剔
```

**💡 处理流程记忆法**

```
文本处理三步走：
第一步：清理（tr删除转换）
第二步：提取（cut选择字段）  
第三步：统计（sort+uniq分析）

数据分析五步法：
清洗->提取->排序->统计->输出
```

**核心记忆**：
- 文本处理是Shell的强项，命令组合威力无穷
- 数据必须先清洗再分析，格式统一是前提
- 排序是很多操作的前提，特别是uniq和join
- 管道思维很重要：前一个命令的输出是后一个的输入