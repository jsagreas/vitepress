---
title: 3、日志查询-语法
---
## 📚 目录

1. [LogQL查询语法概述](#1-LogQL查询语法概述)
2. [日志流选择器详解](#2-日志流选择器详解)
3. [关键字搜索技巧](#3-关键字搜索技巧)
4. [正则表达式过滤](#4-正则表达式过滤)
5. [字段提取与解析](#5-字段提取与解析)
6. [聚合统计操作](#6-聚合统计操作)
7. [实际应用场景](#7-实际应用场景)
8. [核心要点总结](#8-核心要点总结)

---

## 1. 🔍 LogQL查询语法概述


### 1.1 什么是LogQL


**💡 简单理解**：LogQL就是Grafana用来查询日志的专用语言，就像SQL查询数据库一样，LogQL用来查询日志数据。

```
通俗比喻：
如果把日志想象成一本厚厚的书
那么LogQL就是帮你快速找到特定内容的"目录索引"

传统找日志：一页页翻找 ❌
LogQL查日志：直接定位 ✅
```

**🎯 核心作用**：
- **精确定位**：从海量日志中快速找到想要的信息
- **灵活筛选**：支持多种条件组合查询
- **统计分析**：对日志进行计数、聚合等操作
- **实时监控**：配合Grafana实现日志的实时展示

### 1.2 LogQL的基本构成


**📋 查询结构**：
```
日志流选择器 + 过滤器 + 操作符
     ↓           ↓        ↓
  选择日志源   筛选内容   统计操作
```

**🔸 基础语法框架**：
```logql
{标签="值"} |= "关键字" | 统计函数
    ↓           ↓           ↓
  选择器     过滤器      操作符
```

### 1.3 日志查询的工作流程


```
步骤流程：
用户输入 → LogQL解析 → 匹配日志流 → 应用过滤 → 返回结果

详细过程：
①选择日志源：通过标签选择器定位具体的日志流
②过滤内容：使用关键字或正则表达式筛选
③处理数据：提取字段、统计计算
④展示结果：在Grafana面板中可视化展示
```

---

## 2. 📊 日志流选择器详解


### 2.1 什么是日志流选择器


**💭 通俗解释**：日志流选择器就像是给日志贴的"标签纸"，通过这些标签来快速找到你想要的那堆日志。

```
实际场景类比：
图书馆找书 → 先看分类标签（文学、科技、历史）
日志查询 → 先看日志标签（应用名、环境、主机）

标签示例：
app="nginx"          ← 应用程序标签
env="production"     ← 环境标签  
host="server-01"     ← 主机标签
level="error"        ← 日志级别标签
```

### 2.2 基础选择器语法


**🔸 单标签选择**：
```logql
{app="nginx"}                # 选择nginx应用的所有日志
{env="production"}           # 选择生产环境的日志
{level="error"}              # 选择错误级别的日志
```

**🔸 多标签组合**：
```logql
{app="nginx", env="production"}              # 同时满足两个条件
{app="nginx", env="production", level="error"} # 三个条件都要满足
```

### 2.3 标签匹配操作符


| 操作符 | **含义** | **示例** | **说明** |
|--------|---------|----------|----------|
| `=` | **完全匹配** | `{app="nginx"}` | `标签值必须完全等于nginx` |
| `!=` | **不等于** | `{app!="test"}` | `排除app为test的日志` |
| `=~` | **正则匹配** | `{app=~"nginx.*"}` | `app值以nginx开头的都匹配` |
| `!~` | **正则不匹配** | `{app!~"test.*"}` | `排除app值以test开头的` |

**💡 实际应用示例**：
```logql
# 查看所有web应用的错误日志
{app=~"web.*", level="error"}

# 排除测试环境的所有日志
{env!="test"}

# 查看特定服务器组的日志
{host=~"prod-server-[0-9]+"}
```

### 2.4 选择器的性能优化


**⚡ 性能建议**：
- **精确优于模糊**：优先使用`=`而不是`=~`
- **标签组合**：多个精确标签比单个模糊标签效率高
- **常用标签在前**：将选择性高的标签放在前面

```logql
✅ 推荐写法：
{app="nginx", env="production", level="error"}

❌ 避免写法：
{app=~".*", env=~".*"} |= "error"
```

---

## 3. 🔎 关键字搜索技巧


### 3.1 基础关键字搜索


**💡 简单理解**：关键字搜索就像在文档里用Ctrl+F查找特定词汇，找到包含这个词的所有行。

**🔸 包含关键字**：
```logql
{app="nginx"} |= "error"        # 包含"error"的日志行
{app="nginx"} |= "404"          # 包含"404"的日志行
{app="nginx"} |= "timeout"      # 包含"timeout"的日志行
```

**🔸 不包含关键字**：
```logql
{app="nginx"} != "debug"        # 不包含"debug"的日志行
{app="nginx"} != "info"         # 排除信息级别日志
```

### 3.2 多关键字组合搜索


**📋 组合搜索类型**：

```
搜索组合示例：
用户访问错误 → "error" + "user" + "login"
系统性能问题 → "slow" + "timeout" + "performance"
安全相关 → "auth" + "fail" + "security"
```

**🔸 多条件AND组合**：
```logql
{app="nginx"} |= "error" |= "user"          # 同时包含error和user
{app="nginx"} |= "404" |= "login"           # 同时包含404和login
{app="nginx"} |= "timeout" |= "database"    # 同时包含timeout和database
```

**🔸 多条件OR组合**：
```logql
{app="nginx"} |~ "error|fail|timeout"       # 包含任意一个关键字
{app="nginx"} |~ "404|500|502"              # 包含任意一个状态码
```

### 3.3 大小写敏感处理


**🔸 大小写敏感**（默认）：
```logql
{app="nginx"} |= "Error"        # 只匹配"Error"，不匹配"error"
{app="nginx"} |= "ERROR"        # 只匹配"ERROR"
```

**🔸 大小写不敏感**：
```logql
{app="nginx"} |~ "(?i)error"    # 匹配error、Error、ERROR等
{app="nginx"} |~ "(?i)timeout"  # 忽略大小写匹配timeout
```

### 3.4 关键字搜索最佳实践


**🎯 实用技巧**：

```
业务场景搜索策略：

错误排查：
{app="web"} |= "error" |= "500"

用户行为分析：
{app="web"} |= "user" |= "login"

性能监控：
{app="api"} |~ "slow|timeout|latency"

安全审计：
{app="auth"} |~ "fail|denied|unauthorized"
```

---

## 4. 🎯 正则表达式过滤


### 4.1 正则表达式基础


**💭 通俗理解**：正则表达式就像一个"智能搜索模式"，不只是找固定的词，还能找符合某种规律的内容。

```
生活中的类比：
找电话号码 → 不用一个个写具体号码，用模式"3位-4位-4位"
找邮箱地址 → 用模式"字母@字母.com"
找IP地址 → 用模式"数字.数字.数字.数字"
```

**🔸 正则表达式操作符**：
- `|~` ：正则匹配（包含）
- `!~` ：正则不匹配（排除）

### 4.2 常用正则模式


**📋 实用正则表达式**：

| **模式** | **含义** | **示例** |
|----------|----------|----------|
| `\d+` | **数字** | `匹配123、456等` |
| `[0-9]+` | **数字（同上）** | `匹配任意数字` |
| `\w+` | **字母数字下划线** | `匹配user_123` |
| `.*` | **任意字符** | `匹配任何内容` |
| `^` | **行开始** | `^ERROR匹配行首的ERROR` |
| `$` | **行结束** | `OK$匹配行尾的OK` |

**🔸 IP地址匹配**：
```logql
{app="nginx"} |~ "\d+\.\d+\.\d+\.\d+"      # 匹配IP地址格式
{app="nginx"} |~ "192\.168\.\d+\.\d+"       # 匹配192.168网段
```

**🔸 HTTP状态码匹配**：
```logql
{app="nginx"} |~ "status:[45]\d\d"          # 匹配4xx和5xx状态码
{app="nginx"} |~ "\"[45]\d\d\""             # 匹配带引号的错误状态码
```

### 4.3 业务场景正则应用


**🎯 实际业务案例**：

**①用户ID提取**：
```logql
{app="user-service"} |~ "user_id:\d+"       # 匹配用户ID
{app="user-service"} |~ "uid=[a-zA-Z0-9]+"  # 匹配用户标识
```

**②时间戳匹配**：
```logql
{app="api"} |~ "\d{4}-\d{2}-\d{2}"          # 匹配日期格式2024-01-15
{app="api"} |~ "\d{2}:\d{2}:\d{2}"          # 匹配时间格式14:30:25
```

**③邮箱和手机号**：
```logql
{app="auth"} |~ "\w+@\w+\.\w+"              # 简单邮箱格式
{app="auth"} |~ "1[3-9]\d{9}"               # 中国手机号格式
```

### 4.4 正则表达式性能优化


**⚡ 性能提示**：
- **尽量精确**：避免使用`.*`这样的贪婪匹配
- **固定前缀**：先用关键字过滤，再用正则
- **简单优先**：能用关键字搜索就不用正则

```logql
✅ 高效写法：
{app="nginx"} |= "error" |~ "code:[45]\d\d"

❌ 低效写法：
{app="nginx"} |~ ".*error.*code:[45]\d\d.*"
```

---

## 5. ⚙️ 字段提取与解析


### 5.1 什么是字段提取


**💡 通俗解释**：字段提取就像把一句话拆分成几个部分，然后给每个部分起个名字，方便后续使用。

```
原始日志示例：
"2024-01-15 14:30:25 ERROR user_service login failed for user_id:12345"

字段提取后：
时间: "2024-01-15 14:30:25"
级别: "ERROR"  
服务: "user_service"
操作: "login failed"
用户ID: "12345"
```

### 5.2 JSON日志解析


**🔸 JSON格式日志**：
```json
{"timestamp":"2024-01-15T14:30:25Z","level":"ERROR","service":"user","message":"login failed","user_id":12345}
```

**🔸 JSON字段提取**：
```logql
{app="user-service"} | json                          # 自动解析所有JSON字段
{app="user-service"} | json level, user_id          # 只解析指定字段
{app="user-service"} | json | level="ERROR"         # 解析后按level过滤
```

### 5.3 结构化日志解析


**🔸 logfmt格式解析**：
```
# 原始日志
level=error ts=2024-01-15T14:30:25Z caller=main.go:25 msg="database connection failed"

# LogQL解析
{app="api"} | logfmt                                 # 解析logfmt格式
{app="api"} | logfmt | level="error"                # 解析后筛选错误
```

**🔸 自定义格式解析**：
```logql
# 使用regexp提取字段
{app="nginx"} | regexp "(?P<ip>\d+\.\d+\.\d+\.\d+).*\"(?P<method>\w+)\s+(?P<path>\S+)"

# 提取后的字段可以直接使用
{app="nginx"} | regexp "..." | method="GET"
{app="nginx"} | regexp "..." | ip="192.168.1.100"
```

### 5.4 字段类型转换


**🔸 字符串转数字**：
```logql
{app="api"} | json | response_time_ms > 1000         # 数值比较
{app="api"} | json | status_code >= 400             # 状态码数值比较
```

**🔸 时间字段处理**：
```logql
{app="api"} | json timestamp | timestamp > "2024-01-15T14:00:00Z"
```

---

## 6. 📈 聚合统计操作


### 6.1 基础计数统计


**💡 简单理解**：聚合统计就是对日志进行"数数"和"分组统计"，帮你了解日志的整体情况。

**🔸 基础计数**：
```logql
# 统计错误日志总数
count_over_time({app="nginx"} |= "error" [5m])

# 统计特定状态码数量  
count_over_time({app="nginx"} |~ "\"[45]\d\d\"" [1h])

# 统计用户登录次数
count_over_time({app="auth"} |= "login" [1d])
```

### 6.2 速率计算


**🔸 错误率统计**：
```logql
# 每秒错误数
rate({app="nginx"} |= "error" [5m])

# 每分钟请求数
rate({app="nginx"} [1m]) * 60

# HTTP错误率（4xx、5xx）
rate({app="nginx"} |~ "\"[45]\d\d\"" [5m])
```

### 6.3 分组聚合


**🔸 按字段分组统计**：
```logql
# 按HTTP状态码分组统计
sum by (status_code) (count_over_time({app="nginx"} | json [5m]))

# 按用户ID分组统计访问次数
sum by (user_id) (count_over_time({app="api"} | json [1h]))

# 按主机分组统计错误数
sum by (host) (count_over_time({app="api"} |= "error" [5m]))
```

### 6.4 聚合函数详解


**📊 常用聚合函数**：

| **函数** | **作用** | **示例** |
|----------|----------|----------|
| `sum()` | **求和** | `sum(count_over_time(...))` |
| `avg()` | **平均值** | `avg(response_time)` |
| `max()` | **最大值** | `max(response_time)` |
| `min()` | **最小值** | `min(response_time)` |
| `topk()` | **取前K个** | `topk(5, sum by(path)(...))` |

**🎯 实际应用示例**：
```logql
# Top 5 访问最多的API接口
topk(5, sum by (path) (count_over_time({app="api"} | json [1h])))

# 平均响应时间
avg(avg_over_time({app="api"} | json | unwrap response_time [5m]))

# 错误率超过阈值的服务
sum by (service) (rate({level="error"} [5m])) > 0.01
```

---

## 7. 🎯 实际应用场景


### 7.1 故障排查场景


**🚨 系统故障快速定位**：

```logql
# 场景1：查找最近5分钟的所有错误
{app="web-server"} |= "error" | line_format "{{.timestamp}} {{.level}} {{.message}}"

# 场景2：特定用户的错误日志
{app="user-service"} | json | user_id="12345" | level="ERROR"

# 场景3：数据库连接错误
{app=~".*-service"} |~ "database.*connect.*fail"
```

### 7.2 性能监控场景


**⚡ 性能瓶颈分析**：

```logql
# 慢查询检测（响应时间>1秒）
{app="api"} | json | response_time > 1000

# API接口性能排行
topk(10, avg by (endpoint) (avg_over_time({app="api"} | json | unwrap response_time [5m])))

# 高并发期间的错误率
rate({app="web"} |~ "\"[45]\d\d\"" [1m]) / rate({app="web"} [1m])
```

### 7.3 业务监控场景


**📊 业务指标统计**：

```logql
# 用户登录成功率
(count_over_time({app="auth"} |= "login success" [1h])) / 
(count_over_time({app="auth"} |= "login" [1h]))

# 订单处理情况
sum by (status) (count_over_time({app="order"} | json [1h]))

# 支付成功率
count_over_time({app="payment"} |= "payment success" [1h]) /
count_over_time({app="payment"} |= "payment" [1h])
```

### 7.4 安全监控场景


**🔒 安全事件检测**：

```logql
# 暴力破解检测
count by (ip) (count_over_time({app="auth"} |= "login failed" [5m])) > 10

# 异常访问模式
{app="nginx"} |~ "\"(POST|PUT|DELETE).*admin.*\""

# 可疑IP地址
sum by (remote_ip) (count_over_time({app="nginx"} |~ "\"[45]\d\d\"" [1h])) > 100
```

---

## 8. 📋 核心要点总结


### 8.1 必须掌握的核心概念


```
🔸 LogQL本质：专门用于查询日志的语言，类似SQL查数据库
🔸 查询结构：选择器 + 过滤器 + 操作符的三层结构
🔸 选择器：通过标签快速定位目标日志流
🔸 过滤器：使用关键字和正则表达式筛选内容
🔸 字段提取：将非结构化日志转为结构化数据
🔸 聚合统计：对日志进行计数、求和、分组等操作
```

### 8.2 关键理解要点


**🔹 LogQL的查询思路**：
```
第一步：选择日志源（哪个应用、哪个环境）
第二步：过滤内容（包含什么关键字）
第三步：提取字段（需要哪些具体信息）
第四步：统计分析（计数、求和、分组）
```

**🔹 性能优化原则**：
```
精确胜过模糊：能用等号就不用正则
标签优于内容：先用标签选择，再用内容过滤  
简单优于复杂：复杂查询拆分成多个简单查询
```

**🔹 实际应用策略**：
```
故障排查：关注错误关键字和异常模式
性能监控：重点关注响应时间和请求量
业务分析：统计成功率和转化率
安全审计：监控失败登录和异常访问
```

### 8.3 学习进阶路径


**📈 学习建议**：
- **基础阶段**：熟练掌握选择器和关键字搜索
- **进阶阶段**：学会正则表达式和字段提取
- **高级阶段**：掌握复杂聚合和业务场景应用
- **实战阶段**：结合Grafana面板创建监控大屏

### 8.4 常见问题与解决


**🔧 常见查询问题**：
```
查询慢：检查标签选择器是否够精确
结果少：检查关键字是否存在大小写问题
解析错误：确认日志格式和解析器匹配
统计不准：检查时间范围和聚合函数使用
```

**核心记忆要点**：
- LogQL = 选择器 + 过滤器 + 操作符
- 标签选择要精确，关键字搜索要准确
- 正则表达式很强大，但要注意性能
- 字段提取是进阶功能，聚合统计是实用技能
- 实际应用中要结合业务场景灵活运用