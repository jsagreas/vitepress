---
title: 3、高级部署策略
---
## 📚 目录

1. [高级部署策略概述](#1-高级部署策略概述)
2. [蓝绿部署实现](#2-蓝绿部署实现)
3. [滚动更新策略](#3-滚动更新策略)
4. [金丝雀部署配置](#4-金丝雀部署配置)
5. [A/B测试部署](#5-AB测试部署)
6. [零停机部署](#6-零停机部署)
7. [自动回滚机制](#7-自动回滚机制)
8. [核心要点总结](#8-核心要点总结)

---

## 1. 🎯 高级部署策略概述


### 1.1 什么是高级部署策略


**简单理解**：就像更换商店橱窗的不同方法，有的一次性全换（传统部署），有的一半一半换（蓝绿部署），有的一点点换（滚动更新），有的先换一个角落让客户试试（金丝雀部署）。

**核心目标**：
- **降低风险** - 减少部署失败对用户的影响
- **保证可用性** - 用户感觉不到服务中断
- **快速恢复** - 出问题能立即回到之前的版本
- **渐进验证** - 先小范围测试，再全量发布

### 1.2 部署策略对比


```
传统部署：旧系统关闭 → 新系统启动 → 用户等待
                ❌ 有停机时间    ❌ 风险大

蓝绿部署：保持旧系统 → 启动新系统 → 流量切换
                ✅ 零停机      ❌ 资源消耗大

滚动更新：逐步替换服务器 → 一台台升级
                ✅ 资源节省    ⚠️ 升级时间长

金丝雀部署：小部分用户 → 验证稳定 → 全量发布
                ✅ 风险最小    ⚠️ 配置复杂
```

### 1.3 选择合适的部署策略


| 场景 | **推荐策略** | **原因** |
|------|-------------|----------|
| 🏪 **电商网站** | `蓝绿部署` | `不能停机，交易量大` |
| 📱 **移动应用API** | `滚动更新` | `用户分布广，逐步验证` |
| 🎮 **游戏服务** | `金丝雀部署` | `用户体验敏感，需要测试` |
| 💼 **企业内部系统** | `蓝绿部署` | `稳定性优先，资源充足` |
| 🔧 **开发测试环境** | `直接部署` | `快速迭代，影响范围小` |

---

## 2. 🔄 蓝绿部署实现


### 2.1 蓝绿部署原理


**形象理解**：就像有两个完全一样的舞台，一个舞台正在表演（绿色环境），另一个舞台准备新节目（蓝色环境）。新节目准备好后，观众瞬间从绿色舞台切换到蓝色舞台，完全无感知。

```
部署前状态：
用户流量 → 负载均衡器 → 绿色环境（V1.0）
                      蓝色环境（空闲）

部署新版本：
用户流量 → 负载均衡器 → 绿色环境（V1.0）
                      蓝色环境（V2.0 部署中...）

切换完成：
用户流量 → 负载均衡器 → 蓝色环境（V2.0）
                      绿色环境（V1.0 待清理）
```

### 2.2 GitLab CI/CD 蓝绿部署配置


**核心配置思路**：
1. **并行环境** - 维护两个相同的生产环境
2. **流量切换** - 通过负载均衡器控制流量方向
3. **环境标记** - 用变量标记当前活跃环境
4. **快速回滚** - 一键切换回上一个环境

```yaml
# .gitlab-ci.yml 蓝绿部署示例
stages:
  - build
  - deploy-blue
  - test-blue
  - switch-traffic
  - cleanup

variables:
  DOCKER_IMAGE: "$CI_REGISTRY_IMAGE:$CI_COMMIT_SHA"
  
build:
  stage: build
  script:
    - docker build -t $DOCKER_IMAGE .
    - docker push $DOCKER_IMAGE

# 部署到蓝色环境
deploy-to-blue:
  stage: deploy-blue
  script:
    - echo "部署到蓝色环境..."
    - kubectl set image deployment/app-blue app=$DOCKER_IMAGE
    - kubectl rollout status deployment/app-blue
  environment:
    name: production-blue
    url: https://blue.myapp.com

# 测试蓝色环境
test-blue-environment:
  stage: test-blue
  script:
    - echo "测试蓝色环境健康状况..."
    - curl -f https://blue.myapp.com/health
    - pytest tests/integration/
  dependencies:
    - deploy-to-blue

# 切换流量到蓝色环境
switch-to-blue:
  stage: switch-traffic
  script:
    - echo "切换流量到蓝色环境..."
    - kubectl patch service app-service -p '{"spec":{"selector":{"version":"blue"}}}'
    - echo "✅ 流量切换完成"
  when: manual  # 手动确认切换
  environment:
    name: production
    url: https://myapp.com

# 清理绿色环境
cleanup-green:
  stage: cleanup
  script:
    - echo "清理旧的绿色环境..."
    - kubectl scale deployment app-green --replicas=0
  when: manual
```

### 2.3 蓝绿部署的关键配置


**环境变量管理**：
```yaml
# 环境变量定义
variables:
  ACTIVE_COLOR: "green"  # 当前活跃环境
  INACTIVE_COLOR: "blue"  # 待部署环境
  
# 动态切换逻辑
before_script:
  - |
    if [ "$ACTIVE_COLOR" = "green" ]; then
      DEPLOY_TARGET="blue"
      ACTIVE_TARGET="green"
    else
      DEPLOY_TARGET="green"
      ACTIVE_TARGET="blue"
    fi
```

**健康检查配置**：
```yaml
health-check:
  script:
    - |
      echo "等待服务启动..."
      for i in {1..30}; do
        if curl -f https://$DEPLOY_TARGET.myapp.com/health; then
          echo "✅ 健康检查通过"
          break
        fi
        echo "⏳ 等待中... ($i/30)"
        sleep 10
      done
```

### 2.4 蓝绿部署优势与注意事项


**✅ 主要优势**：
- **零停机时间** - 用户完全无感知切换
- **快速回滚** - 秒级切换回上一版本
- **完整测试** - 新版本可以充分测试
- **风险隔离** - 新旧版本完全隔离

**⚠️ 注意事项**：
- **资源成本** - 需要双倍的服务器资源
- **数据同步** - 数据库变更需要兼容新旧版本
- **状态管理** - 用户会话可能中断
- **配置复杂** - 需要精心设计负载均衡器

---

## 3. 🔄 滚动更新策略


### 3.1 滚动更新原理


**形象理解**：就像更换公交车队，不是一次性把所有旧车都换掉，而是先换一辆，确认没问题后再换下一辆，逐步完成整个车队的更新。乘客（用户）在这个过程中依然能正常坐车。

```
滚动更新过程：
初始状态：    [V1] [V1] [V1] [V1]    ← 4个V1版本实例
第1步：       [V2] [V1] [V1] [V1]    ← 更新1个实例
第2步：       [V2] [V2] [V1] [V1]    ← 更新2个实例
第3步：       [V2] [V2] [V2] [V1]    ← 更新3个实例
最终状态：    [V2] [V2] [V2] [V2]    ← 全部更新完成
```

### 3.2 GitLab CI/CD 滚动更新配置


**核心思路**：
1. **分批更新** - 一次只更新部分实例
2. **健康检查** - 确保新实例正常后继续
3. **自动控制** - Kubernetes自动管理更新过程
4. **失败停止** - 有问题立即停止更新

```yaml
# 滚动更新部署配置
rolling-deploy:
  stage: deploy
  script:
    - echo "开始滚动更新..."
    - |
      # 配置滚动更新策略
      kubectl patch deployment myapp -p '{
        "spec": {
          "strategy": {
            "type": "RollingUpdate",
            "rollingUpdate": {
              "maxUnavailable": 1,
              "maxSurge": 1
            }
          }
        }
      }'
    - kubectl set image deployment/myapp app=$DOCKER_IMAGE
    - kubectl rollout status deployment/myapp --timeout=600s
  environment:
    name: production
    url: https://myapp.com

# 监控更新进度
monitor-rollout:
  stage: deploy
  script:
    - |
      echo "监控滚动更新进度..."
      while true; do
        STATUS=$(kubectl rollout status deployment/myapp --timeout=30s)
        if [[ $STATUS == *"successfully rolled out"* ]]; then
          echo "✅ 滚动更新成功完成"
          break
        elif [[ $STATUS == *"failed"* ]]; then
          echo "❌ 滚动更新失败"
          kubectl rollout undo deployment/myapp
          exit 1
        fi
        echo "⏳ 更新进行中..."
        sleep 10
      done
  dependencies:
    - rolling-deploy
```

### 3.3 滚动更新参数详解


**关键参数说明**：

**maxUnavailable（最大不可用实例数）**：
```yaml
# 示例：总共4个实例，maxUnavailable: 1
# 意思是：更新过程中最多只能有1个实例不可用
# 保证至少3个实例在运行，服务不中断

maxUnavailable: 1        # 数值：具体实例数
maxUnavailable: "25%"    # 百分比：总实例的25%
```

**maxSurge（最大额外实例数）**：
```yaml
# 示例：总共4个实例，maxSurge: 1
# 意思是：更新过程中最多可以多创建1个实例
# 总实例数可以临时达到5个

maxSurge: 1              # 数值：具体实例数
maxSurge: "25%"          # 百分比：总实例的25%
```

**实际更新策略示例**：
```yaml
# 保守策略（稳定优先）
rollingUpdate:
  maxUnavailable: 0      # 不允许实例不可用
  maxSurge: 1            # 一次只多创建1个实例

# 快速策略（速度优先）
rollingUpdate:
  maxUnavailable: "50%"  # 允许一半实例不可用
  maxSurge: "50%"        # 可以多创建一半实例

# 平衡策略（推荐）
rollingUpdate:
  maxUnavailable: "25%"  # 允许25%实例不可用
  maxSurge: "25%"        # 可以多创建25%实例
```

### 3.4 滚动更新的健康检查


```yaml
# Kubernetes部署配置
apiVersion: apps/v1
kind: Deployment
metadata:
  name: myapp
spec:
  replicas: 4
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
      maxSurge: 1
  template:
    spec:
      containers:
      - name: app
        image: myapp:latest
        # 健康检查配置
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
```

---

## 4. 🎯 金丝雀部署配置


### 4.1 金丝雀部署原理


**名字来源**：以前矿工下井前会带一只金丝雀，因为金丝雀对有毒气体很敏感，如果金丝雀有异常，矿工就知道环境有危险。

**部署理解**：新版本就像金丝雀，先让一小部分用户（比如5%）使用新版本，如果没问题，再逐步扩大到更多用户，最终全量发布。

```
金丝雀部署流程：
阶段1：95%用户 → 旧版本     5%用户 → 新版本（金丝雀）
阶段2：80%用户 → 旧版本    20%用户 → 新版本
阶段3：50%用户 → 旧版本    50%用户 → 新版本
阶段4：0%用户  → 旧版本   100%用户 → 新版本（完成）
```

### 4.2 GitLab CI/CD 金丝雀部署配置


**基本金丝雀部署**：
```yaml
# 金丝雀部署到5%流量
deploy-canary:
  stage: deploy
  script:
    - echo "部署金丝雀版本（5%流量）..."
    - kubectl apply -f k8s/canary-deployment.yaml
    - |
      # 配置流量分配：95% stable, 5% canary
      kubectl patch virtualservice myapp-vs -p '{
        "spec": {
          "http": [{
            "match": [{"headers": {"canary": {"exact": "true"}}}],
            "route": [{"destination": {"host": "myapp-canary"}}]
          }, {
            "route": [
              {"destination": {"host": "myapp-stable"}, "weight": 95},
              {"destination": {"host": "myapp-canary"}, "weight": 5}
            ]
          }]
        }
      }'
  environment:
    name: production-canary
    url: https://myapp.com

# 监控金丝雀指标
monitor-canary:
  stage: test
  script:
    - echo "监控金丝雀版本指标..."
    - |
      # 检查错误率
      ERROR_RATE=$(curl -s "http://prometheus:9090/api/v1/query?query=rate(http_requests_total{version=\"canary\",status=~\"5..\"}[5m])" | jq '.data.result[0].value[1]')
      if (( $(echo "$ERROR_RATE > 0.01" | bc -l) )); then
        echo "❌ 金丝雀版本错误率过高: $ERROR_RATE"
        exit 1
      fi
      echo "✅ 金丝雀版本表现良好"
  dependencies:
    - deploy-canary
```

### 4.3 自动化金丝雀发布


**渐进式流量切换**：
```yaml
# 阶段1：5%流量
canary-5-percent:
  stage: canary-5
  script:
    - echo "金丝雀部署：5%流量"
    - kubectl apply -f k8s/canary-5percent.yaml
    - sleep 300  # 等待5分钟观察
  environment:
    name: canary-5

# 阶段2：20%流量
canary-20-percent:
  stage: canary-20
  script:
    - echo "扩大金丝雀：20%流量"
    - kubectl apply -f k8s/canary-20percent.yaml
    - sleep 300  # 等待5分钟观察
  dependencies:
    - canary-5-percent
  environment:
    name: canary-20

# 阶段3：50%流量
canary-50-percent:
  stage: canary-50
  script:
    - echo "扩大金丝雀：50%流量"
    - kubectl apply -f k8s/canary-50percent.yaml
    - sleep 300
  dependencies:
    - canary-20-percent
  when: manual  # 手动确认

# 阶段4：全量发布
promote-to-stable:
  stage: promote
  script:
    - echo "提升为稳定版本：100%流量"
    - kubectl apply -f k8s/stable-deployment.yaml
    - kubectl delete -f k8s/canary-deployment.yaml
  dependencies:
    - canary-50-percent
  when: manual
```

### 4.4 金丝雀部署的监控指标


**关键监控指标**：
```yaml
# 监控脚本示例
canary-health-check:
  script:
    - |
      echo "检查金丝雀版本健康指标..."
      
      # 1. 错误率检查
      ERROR_RATE=$(prometheus_query "rate(http_requests_total{version=\"canary\",status=~\"5..\"}[5m])")
      echo "错误率: $ERROR_RATE"
      
      # 2. 响应时间检查
      RESPONSE_TIME=$(prometheus_query "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket{version=\"canary\"}[5m]))")
      echo "95%响应时间: $RESPONSE_TIME"
      
      # 3. 内存使用率检查
      MEMORY_USAGE=$(prometheus_query "container_memory_usage_bytes{pod=~\".*canary.*\"}")
      echo "内存使用: $MEMORY_USAGE"
      
      # 判断是否继续
      if [[ $(echo "$ERROR_RATE > 0.01" | bc -l) -eq 1 ]]; then
        echo "❌ 金丝雀版本错误率过高，停止发布"
        exit 1
      fi
      
      echo "✅ 金丝雀版本指标正常"
```

---

## 5. 🧪 A/B测试部署


### 5.1 A/B测试部署原理


**简单理解**：就像店家想测试两种不同的橱窗装饰效果，把顾客分成两组，A组看到装饰方案A，B组看到装饰方案B，然后比较哪种装饰带来更多销售。

**技术实现**：同时运行两个版本的应用，根据用户特征（如地理位置、用户ID等）将用户分配到不同版本，收集数据比较效果。

```
A/B测试流程：
用户访问 → 分流规则判断 → A版本（50%用户）
                       → B版本（50%用户）
         ↓
    收集用户行为数据 → 分析对比 → 选择更好的版本
```

### 5.2 GitLab CI/CD A/B测试配置


**基于Header的A/B分流**：
```yaml
# 部署A/B测试版本
deploy-ab-test:
  stage: deploy
  script:
    - echo "部署A/B测试版本..."
    - kubectl apply -f k8s/version-a.yaml
    - kubectl apply -f k8s/version-b.yaml
    - |
      # 配置A/B测试路由规则
      kubectl apply -f - <<EOF
      apiVersion: networking.istio.io/v1alpha3
      kind: VirtualService
      metadata:
        name: myapp-ab-test
      spec:
        hosts:
        - myapp.com
        http:
        - match:
          - headers:
              ab-test-group:
                exact: "A"
          route:
          - destination:
              host: myapp-version-a
        - match:
          - headers:
              ab-test-group:
                exact: "B"
          route:
          - destination:
              host: myapp-version-b
        - route:  # 默认50/50分流
          - destination:
              host: myapp-version-a
            weight: 50
          - destination:
              host: myapp-version-b
            weight: 50
      EOF
  environment:
    name: ab-test

# 用户分组逻辑配置
configure-user-assignment:
  stage: configure
  script:
    - |
      echo "配置用户分组逻辑..."
      # 基于用户ID的哈希分组
      kubectl create configmap ab-test-config --from-literal=assignment-rule="
      function assignGroup(userId) {
        const hash = simpleHash(userId);
        return hash % 2 === 0 ? 'A' : 'B';
      }"
```

### 5.3 智能A/B测试分流


**基于用户特征的分流**：
```yaml
# 高级A/B测试配置
smart-ab-deployment:
  script:
    - |
      # 应用智能分流规则
      kubectl apply -f - <<EOF
      apiVersion: networking.istio.io/v1alpha3
      kind: VirtualService
      metadata:
        name: smart-ab-test
      spec:
        hosts:
        - myapp.com
        http:
        # VIP用户使用稳定版本A
        - match:
          - headers:
              user-tier:
                exact: "vip"
          route:
          - destination:
              host: myapp-version-a
        # 新用户使用新版本B进行测试
        - match:
          - headers:
              user-type:
                exact: "new"
          route:
          - destination:
              host: myapp-version-b
        # 其他用户随机分配
        - route:
          - destination:
              host: myapp-version-a
            weight: 60
          - destination:
              host: myapp-version-b
            weight: 40
      EOF
```

### 5.4 A/B测试数据收集


**数据收集配置**：
```yaml
# A/B测试数据收集
collect-ab-metrics:
  stage: analyze
  script:
    - |
      echo "收集A/B测试数据..."
      
      # 版本A的指标
      CONVERSION_RATE_A=$(prometheus_query "
        sum(rate(conversion_events_total{version=\"A\"}[1h])) / 
        sum(rate(page_views_total{version=\"A\"}[1h]))
      ")
      
      # 版本B的指标
      CONVERSION_RATE_B=$(prometheus_query "
        sum(rate(conversion_events_total{version=\"B\"}[1h])) / 
        sum(rate(page_views_total{version=\"B\"}[1h]))
      ")
      
      echo "版本A转化率: $CONVERSION_RATE_A"
      echo "版本B转化率: $CONVERSION_RATE_B"
      
      # 生成报告
      cat > ab_test_report.json <<EOF
      {
        "test_duration": "24h",
        "version_a": {
          "conversion_rate": $CONVERSION_RATE_A,
          "error_rate": $(prometheus_query "rate(errors_total{version=\"A\"}[1h])"),
          "avg_response_time": $(prometheus_query "avg(response_time{version=\"A\"})")
        },
        "version_b": {
          "conversion_rate": $CONVERSION_RATE_B,
          "error_rate": $(prometheus_query "rate(errors_total{version=\"B\"}[1h])"),
          "avg_response_time": $(prometheus_query "avg(response_time{version=\"B\"})")
        }
      }
      EOF
  artifacts:
    reports:
      junit: ab_test_report.json
```

---

## 6. ⚡ 零停机部署


### 6.1 零停机部署的核心要求


**用户体验目标**：用户在整个部署过程中完全感觉不到服务中断，就像网站从来没有停过一样。

**技术要求**：
- **服务连续性** - 始终有可用的服务实例
- **数据一致性** - 数据库变更不影响正在处理的请求
- **会话保持** - 用户登录状态等不丢失
- **优雅关闭** - 正在处理的请求完成后再关闭旧实例

### 6.2 零停机部署的关键技术


**1. 健康检查配置**：
```yaml
# 应用健康检查
health-check-config:
  script:
    - |
      # 配置细致的健康检查
      kubectl apply -f - <<EOF
      apiVersion: v1
      kind: Service
      metadata:
        name: myapp-service
      spec:
        ports:
        - port: 80
          targetPort: 8080
        selector:
          app: myapp
      ---
      apiVersion: apps/v1
      kind: Deployment
      metadata:
        name: myapp
      spec:
        replicas: 3
        template:
          spec:
            containers:
            - name: app
              image: myapp:latest
              ports:
              - containerPort: 8080
              # 存活探针：应用是否正常运行
              livenessProbe:
                httpGet:
                  path: /health
                  port: 8080
                initialDelaySeconds: 60    # 应用启动需要时间
                periodSeconds: 10          # 每10秒检查一次
                timeoutSeconds: 5          # 5秒无响应算失败
                failureThreshold: 3        # 连续3次失败才重启
              # 就绪探针：应用是否准备好接收流量
              readinessProbe:
                httpGet:
                  path: /ready
                  port: 8080
                initialDelaySeconds: 10    # 10秒后开始检查
                periodSeconds: 5           # 每5秒检查一次
                timeoutSeconds: 3          # 3秒无响应算未就绪
                failureThreshold: 2        # 连续2次失败算未就绪
              # 优雅关闭配置
              lifecycle:
                preStop:
                  exec:
                    command: ["/bin/sh", "-c", "sleep 15"]  # 给负载均衡器时间移除实例
      EOF
```

**2. 数据库兼容性策略**：
```yaml
# 数据库迁移策略
database-migration:
  stage: pre-deploy
  script:
    - |
      echo "执行向前兼容的数据库迁移..."
      
      # 第一阶段：添加新字段（可选）
      psql -d $DATABASE_URL -c "
        ALTER TABLE users 
        ADD COLUMN new_field VARCHAR(255) DEFAULT 'default_value';
      "
      
      # 第二阶段：创建新索引（在线创建）
      psql -d $DATABASE_URL -c "
        CREATE INDEX CONCURRENTLY idx_users_new_field 
        ON users(new_field);
      "
      
      echo "✅ 数据库迁移完成，新旧版本都兼容"

# 部署后清理（可选）
post-deploy-cleanup:
  stage: post-deploy
  script:
    - |
      echo "清理不再需要的旧字段..."
      # 这一步通常在下一个版本中执行
      # psql -d $DATABASE_URL -c "ALTER TABLE users DROP COLUMN old_field;"
  when: manual  # 手动执行
```

### 6.3 会话保持策略


**Sticky Session配置**：
```yaml
# 会话亲和性配置
session-affinity:
  script:
    - |
      # 配置基于Cookie的会话保持
      kubectl apply -f - <<EOF
      apiVersion: networking.istio.io/v1alpha3
      kind: DestinationRule
      metadata:
        name: myapp-session-affinity
      spec:
        host: myapp-service
        trafficPolicy:
          consistentHash:
            httpCookieName: "JSESSIONID"
            ttl: 3600s  # Cookie有效期1小时
      EOF
      
      echo "✅ 配置会话保持，确保用户请求路由到同一实例"
```

---

## 7. 🔄 自动回滚机制


### 7.1 自动回滚的触发条件


**什么时候需要自动回滚**：
- **健康检查失败** - 新版本启动后一直无法通过健康检查
- **错误率激增** - 新版本的错误率超过设定阈值
- **响应时间过长** - 新版本响应时间明显变慢
- **业务指标下降** - 转化率、点击率等关键指标下降

### 7.2 GitLab CI/CD 自动回滚配置


**基于健康检查的自动回滚**：
```yaml
# 部署并监控
deploy-with-auto-rollback:
  stage: deploy
  script:
    - echo "部署新版本..."
    - kubectl set image deployment/myapp app=$DOCKER_IMAGE
    - |
      # 等待部署完成
      if ! kubectl rollout status deployment/myapp --timeout=300s; then
        echo "❌ 部署超时，执行自动回滚"
        kubectl rollout undo deployment/myapp
        exit 1
      fi
      
      echo "✅ 部署成功，开始健康监控..."
      
      # 监控5分钟，如果失败则回滚
      for i in {1..30}; do
        sleep 10
        
        # 检查Pod状态
        FAILED_PODS=$(kubectl get pods -l app=myapp --field-selector=status.phase!=Running | wc -l)
        if [ $FAILED_PODS -gt 1 ]; then
          echo "❌ 发现失败的Pod，执行自动回滚"
          kubectl rollout undo deployment/myapp
          exit 1
        fi
        
        # 检查健康端点
        if ! curl -f http://myapp.com/health; then
          echo "❌ 健康检查失败，执行自动回滚"
          kubectl rollout undo deployment/myapp
          exit 1
        fi
        
        echo "⏳ 健康监控中... ($i/30)"
      done
      
      echo "✅ 部署成功且稳定运行"
  environment:
    name: production
    url: https://myapp.com
```

**基于业务指标的智能回滚**：
```yaml
# 智能监控和回滚
intelligent-rollback:
  stage: monitor
  script:
    - |
      echo "开始智能监控..."
      
      # 获取基线指标（部署前30分钟的平均值）
      BASELINE_ERROR_RATE=$(prometheus_query "avg_over_time(rate(http_requests_total{status=~\"5..\"}[30m])[30m:])")
      BASELINE_RESPONSE_TIME=$(prometheus_query "avg_over_time(histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))[30m:])")
      
      echo "基线错误率: $BASELINE_ERROR_RATE"
      echo "基线响应时间: $BASELINE_RESPONSE_TIME"
      
      # 监控新版本指标
      for i in {1..60}; do  # 监控10分钟
        sleep 10
        
        # 当前指标
        CURRENT_ERROR_RATE=$(prometheus_query "rate(http_requests_total{status=~\"5..\"}[5m])")
        CURRENT_RESPONSE_TIME=$(prometheus_query "histogram_quantile(0.95, rate(http_request_duration_seconds_bucket[5m]))")
        
        echo "当前错误率: $CURRENT_ERROR_RATE"
        echo "当前响应时间: $CURRENT_RESPONSE_TIME"
        
        # 判断是否需要回滚
        if (( $(echo "$CURRENT_ERROR_RATE > $BASELINE_ERROR_RATE * 2" | bc -l) )); then
          echo "❌ 错误率比基线高2倍，触发自动回滚"
          kubectl rollout undo deployment/myapp
          exit 1
        fi
        
        if (( $(echo "$CURRENT_RESPONSE_TIME > $BASELINE_RESPONSE_TIME * 1.5" | bc -l) )); then
          echo "❌ 响应时间比基线慢50%，触发自动回滚"
          kubectl rollout undo deployment/myapp
          exit 1
        fi
        
        echo "⏳ 指标正常，继续监控... ($i/60)"
      done
      
      echo "✅ 新版本指标稳定，部署成功"
  dependencies:
    - deploy-with-auto-rollback
```

### 7.3 回滚策略配置


**多层次回滚策略**：
```yaml
# 回滚策略定义
rollback-strategies:
  # 快速回滚：立即回到上一版本
  quick-rollback:
    script:
      - echo "执行快速回滚..."
      - kubectl rollout undo deployment/myapp
      - kubectl rollout status deployment/myapp --timeout=300s
    when: manual
  
  # 指定版本回滚
  rollback-to-version:
    script:
      - echo "回滚到指定版本..."
      - kubectl rollout undo deployment/myapp --to-revision=$ROLLBACK_REVISION
      - kubectl rollout status deployment/myapp --timeout=300s
    when: manual
    variables:
      ROLLBACK_REVISION: "1"  # 可以通过变量指定版本
  
  # 安全回滚：回滚前先验证
  safe-rollback:
    script:
      - |
        echo "执行安全回滚..."
        
        # 获取当前版本信息
        CURRENT_REVISION=$(kubectl rollout history deployment/myapp | tail -1 | awk '{print $1}')
        PREVIOUS_REVISION=$((CURRENT_REVISION - 1))
        
        echo "当前版本: $CURRENT_REVISION"
        echo "回滚目标: $PREVIOUS_REVISION"
        
        # 确认回滚
        read -p "确认回滚到版本 $PREVIOUS_REVISION? (y/N): " confirm
        if [[ $confirm != "y" ]]; then
          echo "回滚已取消"
          exit 0
        fi
        
        # 执行回滚
        kubectl rollout undo deployment/myapp
        kubectl rollout status deployment/myapp --timeout=300s
        
        echo "✅ 回滚完成"
    when: manual
```

### 7.4 回滚后的验证


```yaml
# 回滚后验证
post-rollback-verification:
  stage: verify
  script:
    - |
      echo "验证回滚结果..."
      
      # 等待服务稳定
      sleep 30
      
      # 验证服务可用性
      for i in {1..10}; do
        if curl -f http://myapp.com/health; then
          echo "✅ 健康检查通过"
          break
        fi
        echo "⏳ 等待服务恢复... ($i/10)"
        sleep 10
      done
      
      # 验证关键功能
      if curl -f http://myapp.com/api/test; then
        echo "✅ API功能正常"
      else
        echo "❌ API功能异常，需要人工介入"
        exit 1
      fi
      
      # 发送通知
      curl -X POST $SLACK_WEBHOOK -d '{
        "text": "🔄 自动回滚完成，服务已恢复正常",
        "username": "GitLab CI/CD"
      }'
      
      echo "✅ 回滚验证完成"
  dependencies:
    - quick-rollback
```

---

## 8. 📋 核心要点总结


### 8.1 部署策略选择指南


| 场景需求 | **推荐策略** | **核心优势** | **注意事项** |
|---------|-------------|-------------|-------------|
| **零停机要求** | `蓝绿部署` | `瞬间切换，完全无感` | `资源成本双倍` |
| **资源限制** | `滚动更新` | `节省资源，渐进替换` | `更新时间较长` |
| **风险敏感** | `金丝雀部署` | `小范围验证，风险可控` | `配置相对复杂` |
| **功能测试** | `A/B测试` | `数据驱动决策` | `需要监控分析` |
| **快速迭代** | `直接部署` | `简单快速` | `适合开发环境` |

### 8.2 必须掌握的核心概念


**🔸 部署策略本质**：
```
传统部署 = 一次性全换（简单但有风险）
蓝绿部署 = 准备新环境再切换（安全但耗资源）
滚动更新 = 逐步替换（平衡安全和资源）
金丝雀部署 = 小范围试验（最安全）
A/B测试 = 对比验证（数据驱动）
```

**🔸 关键技术要点**：
- **健康检查**：确保新版本正常运行
- **流量控制**：精确控制用户访问分配
- **监控指标**：实时监控部署效果
- **自动回滚**：问题发生时快速恢复
- **数据兼容**：确保新旧版本数据一致

### 8.3 实践建议


**🎯 新手起步建议**：
1. **从简单开始** - 先掌握滚动更新，再学习复杂策略
2. **重视监控** - 没有监控的部署是盲目的
3. **多练习回滚** - 确保出问题时能快速恢复
4. **渐进改进** - 根据业务需求逐步引入高级策略

**⚠️ 避免常见误区**：
- **不要为了技术而技术** - 选择适合业务需求的策略
- **不要忽视数据兼容性** - 数据库变更要考虑向前兼容
- **不要缺少监控** - 部署不等于上线，要持续监控
- **不要忘记文档** - 记录部署策略和应急预案

**核心记忆口诀**：
```
部署策略要选对，业务需求是第一
蓝绿切换零停机，金丝雀测试风险低
滚动更新省资源，A/B测试看数据
监控回滚不能少，稳定服务最重要
```