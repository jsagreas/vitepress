---
title: 2、分片策略与路由机制
---
## 📚 目录

1. [分片基础概念](#1-分片基础概念)
2. [主分片与副本分片](#2-主分片与副本分片)
3. [分片路由机制](#3-分片路由机制)
4. [分片分配与均衡](#4-分片分配与均衡)
5. [热温冷架构](#5-热温冷架构)
6. [分片性能优化](#6-分片性能优化)
7. [故障处理与恢复](#7-故障处理与恢复)
8. [核心要点总结](#8-核心要点总结)

---

## 1. 🧩 分片基础概念


### 1.1 什么是分片


> 💡 **生活类比**  
> 把分片想象成图书馆：一个大图书馆的书太多了，我们把书分门别类放到不同的书架上。每个书架就是一个分片，这样找书更快，管理也更方便。

**🔸 分片的本质含义**

```
分片(Shard) = 数据的水平切分
目的：将大索引拆分成小块，分散存储和处理

现实场景：
假设你有1000万条用户数据
├── 分片0：存储用户ID 0-249万
├── 分片1：存储用户ID 250-499万  
├── 分片2：存储用户ID 500-749万
└── 分片3：存储用户ID 750-1000万
```

**🎯 分片的核心价值**

| 价值维度 | **说明** | **实际好处** |
|---------|----------|-------------|
| 📊 **水平扩展** | `突破单机存储限制` | `单台服务器2TB → 集群100TB` |
| ⚡ **并行处理** | `多个分片同时工作` | `搜索速度提升3-5倍` |
| 🔄 **负载分散** | `请求分散到不同节点` | `避免单点性能瓶颈` |
| 🛡️ **容错能力** | `分片故障不影响整体` | `单个节点宕机服务继续` |

### 1.2 分片的工作原理


**📋 分片工作流程图**

```
写入数据流程：
客户端 ──[写入文档]──> 协调节点
   |                      |
   |                      ├─ 计算路由(hash)
   |                      ├─ 定位目标分片
   |                      └─ 转发到主分片
   |
   └─ 主分片 ──同步──> 副本分片1
       |              └─> 副本分片2
       |
       └─ 返回写入成功

查询数据流程：
客户端 ──[搜索请求]──> 协调节点
   |                      |
   |                      ├─ 广播到所有分片
   |                      ├─ 收集各分片结果
   |                      ├─ 合并排序结果
   |                      └─ 返回最终结果
```

**🔍 深入理解：分片就是独立的Lucene索引**

```
每个分片实际上是：
┌─────────────────┐
│   分片(Shard)   │
│  ┌───────────┐  │
│  │ Lucene索引 │  │ ← 完整的搜索引擎
│  │  ·段文件   │  │
│  │  ·索引文件 │  │
│  │  ·存储文件 │  │
│  └───────────┘  │
└─────────────────┘

多个分片组成完整的Elasticsearch索引
```

---

## 2. 🎭 主分片与副本分片


### 2.1 主分片设计策略


> 🧠 **记忆口诀**  
> 主分片一旦定，终身不能变；副本随时调，弹性很方便

**🔸 主分片数量设计原则**

```
🎯 经验法则：
• 分片大小：20GB - 40GB 为最佳
• 节点配比：分片数 ≤ 节点数 × 20
• 性能考虑：过多分片影响查询性能
• 扩展预留：预估3-5年的数据增长

实际计算示例：
预估数据量：500GB
理想分片大小：30GB
分片数量：500 ÷ 30 ≈ 17个
建议设置：18个主分片（预留空间）
```

**💾 不同业务场景的分片策略**

| 业务类型 | **数据特点** | **推荐分片数** | **理由说明** |
|---------|-------------|--------------|-------------|
| 📰 **日志分析** | `写多读少，按时间分割` | `5-10个` | `利用时间索引轮转` |
| 🛒 **电商搜索** | `读多写少，数据相对稳定` | `3-6个` | `保证查询性能` |
| 📊 **实时监控** | `高频写入，短期存储` | `1-3个` | `减少协调开销` |
| 📚 **知识库** | `数据量大，更新较少` | `10-20个` | `充分利用并行能力` |

### 2.2 副本分片配置


**🔄 副本分片的作用机制**

```
副本分片 = 主分片的完整备份

工作分工：
主分片职责：
├── 处理所有写入操作
├── 协调副本同步
└── 参与读取操作

副本分片职责：
├── 接收主分片同步
├── 分担读取压力  
└── 主分片故障时接管
```

**⚖️ 副本数量权衡**

```
📈 副本数量影响分析：

副本数 = 0：
✅ 优点：写入性能最高，存储成本最低
❌ 缺点：无容错能力，节点故障丢数据

副本数 = 1：
✅ 优点：平衡性能和安全，推荐配置
⚖️ 缺点：适度的存储成本增加

副本数 ≥ 2：
✅ 优点：极高可用性，适合金融等行业
❌ 缺点：写入性能下降，存储成本倍增
```

### 2.3 分片大小控制


**📏 分片大小的性能影响**

> 🔍 **技术类比**  
> 分片就像餐厅的服务员：太少了忙不过来，太多了互相碰撞影响效率

```
🔄 分片大小与性能关系：

分片过小（< 5GB）：
├── 查询需要跨越更多分片
├── 协调开销增加
├── 内存碎片化严重
└── 🐌 整体性能下降

分片过大（> 50GB）：
├── 单分片处理时间长
├── 故障恢复缓慢
├── 内存占用过高
└── ⚡ 响应时间变慢

最佳分片大小（20-40GB）：
├── 平衡查询和存储效率
├── 合理的内存使用
├── 快速故障恢复
└── 🎯 性能最优化
```

**📊 分片大小监控与调整**

```bash
# 查看索引分片大小分布
GET /_cat/shards/my_index?v&h=index,shard,prirep,store&s=store

# 示例输出解读
index    shard prirep store
my_index 0     p      25.2gb  ← 主分片，大小合适
my_index 0     r      25.2gb  ← 副本分片，大小一致
my_index 1     p      31.8gb  ← 稍大但可接受
my_index 2     p      45.1gb  ← 过大，需要关注
```

---

## 3. 🧭 分片路由机制


### 3.1 默认路由算法


**🔢 路由算法核心原理**

> 💡 **生活类比**  
> 路由算法就像邮局分拣：根据邮编(文档ID)计算应该投递到哪个邮递员(分片)那里

```
🎯 默认路由公式：
shard_id = hash(document_id) % primary_shards_count

具体过程：
1. 获取文档ID：user_12345
2. 计算哈希值：hash("user_12345") = 987654321
3. 取模运算：987654321 % 5 = 1
4. 路由结果：文档存储在分片1

这样保证：
✅ 相同ID总是路由到同一分片
✅ 文档分布相对均匀
✅ 查询时能快速定位分片
```

**🔍 路由过程可视化**

```
文档路由示例（5个主分片）：

文档A (ID: user_001) ──hash──> 12345 % 5 = 0 ──> 分片0
文档B (ID: user_002) ──hash──> 23456 % 5 = 1 ──> 分片1  
文档C (ID: user_003) ──hash──> 34567 % 5 = 2 ──> 分片2
文档D (ID: user_004) ──hash──> 45678 % 5 = 3 ──> 分片3
文档E (ID: user_005) ──hash──> 56789 % 5 = 4 ──> 分片4

分布结果：
分片0: [user_001]
分片1: [user_002] 
分片2: [user_003]
分片3: [user_004]
分片4: [user_005]
```

### 3.2 自定义路由策略


**🎯 为什么需要自定义路由**

```
默认路由的局限性：
├── 相关文档可能分散在不同分片
├── 查询需要跨多个分片
├── 网络开销和协调成本增加
└── 影响查询性能

自定义路由的优势：
├── 相关数据聚集在同一分片
├── 查询只需访问特定分片
├── 减少网络传输
└── 显著提升查询性能
```

**🔧 自定义路由实现**

```json
# 按用户ID路由，相同用户的数据存储在同一分片
PUT my_index/_doc/1?routing=user_123
{
  "user_id": "user_123",
  "action": "login",
  "timestamp": "2024-01-01T10:00:00"
}

PUT my_index/_doc/2?routing=user_123  
{
  "user_id": "user_123",
  "action": "purchase",
  "timestamp": "2024-01-01T11:00:00"
}

# 查询时指定相同路由值，只查询特定分片
GET my_index/_search?routing=user_123
{
  "query": {
    "term": { "user_id": "user_123" }
  }
}
```

**📊 路由策略对比**

| 路由方式 | **数据分布** | **查询性能** | **适用场景** |
|---------|-------------|-------------|-------------|
| 🎲 **默认路由** | `均匀分布` | `需跨分片查询` | `通用场景，数据关联性低` |
| 👤 **用户路由** | `按用户聚集` | `单分片查询` | `用户行为分析，个人数据` |
| 📅 **时间路由** | `按时间聚集` | `时间范围查询快` | `日志分析，时序数据` |
| 🏢 **租户路由** | `按租户隔离` | `租户内查询快` | `SaaS多租户应用` |

---

## 4. ⚖️ 分片分配与均衡


### 4.1 分片分配策略


**🎯 Elasticsearch的智能分配**

> 🏗️ **工程类比**  
> 分片分配就像工地调度：合理安排工人(分片)到不同工地(节点)，确保工作量均衡，工期最短

```
🤖 分配策略核心原则：

1. 均衡性原则：
   ├── 每个节点分片数量尽量相等
   ├── 避免数据倾斜和热点
   └── 充分利用集群资源

2. 容错性原则：  
   ├── 主副分片不在同一节点
   ├── 跨机架/可用区分布
   └── 故障域隔离

3. 性能原则：
   ├── 考虑节点硬件差异
   ├── 网络拓扑优化
   └── 磁盘空间利用率
```

**🔧 分配参数配置**

```json
# 集群级别分配设置
PUT /_cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.node_concurrent_recoveries": 2,
    "cluster.routing.allocation.cluster_concurrent_rebalance": 2,
    "cluster.routing.allocation.disk.watermark.low": "85%",
    "cluster.routing.allocation.disk.watermark.high": "90%"
  }
}

# 索引级别分配控制
PUT my_index/_settings
{
  "index.routing.allocation.include._name": "node-1,node-2",
  "index.routing.allocation.exclude._name": "node-3"
}
```

### 4.2 分片均衡机制


**🔄 自动均衡工作原理**

```
🎯 均衡触发条件：
├── 新节点加入集群
├── 节点离开集群  
├── 分片分布不均
└── 磁盘使用率差异大

均衡过程示例：
初始状态（不均衡）：
节点A: [分片0, 分片1, 分片2] ← 负载重
节点B: [分片3]               ← 负载轻
节点C: [分片4]               ← 负载轻

均衡后状态：
节点A: [分片0, 分片1]        ← 均衡
节点B: [分片2, 分片3]        ← 均衡  
节点C: [分片4]               ← 均衡
```

**⚙️ 均衡策略配置**

```json
# 配置均衡策略
PUT /_cluster/settings
{
  "persistent": {
    "cluster.routing.rebalance.enable": "all",
    "cluster.routing.allocation.allow_rebalance": "indices_all_active",
    "cluster.routing.allocation.balance.shard": 0.45,
    "cluster.routing.allocation.balance.index": 0.55
  }
}
```

### 4.3 分片迁移过程


**🚚 分片迁移详细流程**

```
分片迁移步骤：

1. 准备阶段
   ├── 选择源节点和目标节点
   ├── 检查目标节点资源
   └── 创建迁移计划

2. 数据传输阶段  
   ├── 在目标节点创建新分片
   ├── 复制数据文件（段文件）
   ├── 同步增量变更
   └── 验证数据完整性

3. 切换阶段
   ├── 停止向旧分片写入
   ├── 更新路由表
   ├── 删除旧分片
   └── 完成迁移

迁移示例流程图：
节点A           节点B
[分片1] ────复制数据───> [分片1']
   │                      │
   │                      ├─ 同步增量
   │                      ├─ 验证完整性
   │                      └─ 激活分片
   │
   └── 删除旧分片
```

---

## 5. 🌡️ 热温冷架构


### 5.1 热温冷架构概念


> 🏠 **生活类比**  
> 就像家里的储物：常用的放客厅(热)，偶尔用的放储藏室(温)，几乎不用的放地下室(冷)

**🔥 数据生命周期管理**

```
🌡️ 数据温度分层：

热数据 (Hot Tier)：
├── 时间范围：最近7-30天
├── 访问频率：高频读写
├── 硬件要求：SSD + 高CPU + 大内存
├── 应用场景：实时搜索、仪表板
└── 成本特点：高性能高成本

温数据 (Warm Tier)：
├── 时间范围：1-12个月
├── 访问频率：偶尔查询
├── 硬件要求：SATA + 中等配置
├── 应用场景：历史分析、合规查询
└── 成本特点：性能适中成本较低

冷数据 (Cold Tier)：
├── 时间范围：1年以上
├── 访问频率：极少访问
├── 硬件要求：大容量存储
├── 应用场景：长期归档、审计
└── 成本特点：低性能低成本
```

### 5.2 热温冷架构实施


**🏗️ 节点角色配置**

```json
# 热节点配置
# elasticsearch.yml
node.roles: ["data_hot"]
node.attr.data: hot

# 温节点配置  
node.roles: ["data_warm"]
node.attr.data: warm

# 冷节点配置
node.roles: ["data_cold"] 
node.attr.data: cold
```

**🔄 Index Lifecycle Management (ILM) 策略**

```json
# 创建ILM策略
PUT _ilm/policy/logs_policy
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_size": "50GB",
            "max_age": "7d"
          }
        }
      },
      "warm": {
        "min_age": "7d",
        "actions": {
          "allocate": {
            "require": { "data": "warm" }
          },
          "shrink": {
            "number_of_shards": 1
          }
        }
      },
      "cold": {
        "min_age": "90d", 
        "actions": {
          "allocate": {
            "require": { "data": "cold" }
          }
        }
      },
      "delete": {
        "min_age": "365d"
      }
    }
  }
}
```

**📊 架构收益分析**

| 架构方式 | **存储成本** | **查询性能** | **管理复杂度** |
|---------|-------------|-------------|--------------|
| 🔥 **单热层** | `高成本` | `高性能` | `简单管理` |
| 🌡️ **热温冷分层** | `成本优化60%` | `分层性能` | `中等复杂度` |
| ❄️ **完全冷存储** | `最低成本` | `低性能` | `简单但慢` |

---

## 6. ⚡ 分片性能优化


### 6.1 分片容量规划


**📏 容量规划计算方法**

```
🎯 分片容量规划公式：

理想分片数 = 总数据量 ÷ 目标分片大小
考虑因素修正：
├── 数据增长率：×(1 + 年增长率)^预估年数
├── 副本因子：×(1 + 副本数)  
├── 索引开销：×1.2 (倒排索引等开销)
└── 安全边际：×1.3 (预留缓冲)

实际计算示例：
当前数据：500GB
年增长率：50%
预估时间：3年
副本数：1
计算过程：
500GB × (1+0.5)³ × 2 × 1.2 × 1.3 ≈ 3,515GB
目标分片大小：30GB
建议分片数：3,515 ÷ 30 ≈ 117个
```

### 6.2 分片性能监控


**📊 关键性能指标**

```bash
# 分片级别性能监控
GET /_cat/shards?v&h=index,shard,prirep,node,store,docs.count&s=store:desc

# 索引级别统计信息
GET /my_index/_stats/store,docs,indexing,search

# 节点级别分片分布
GET /_cat/allocation?v&h=node,shards,disk.used_percent
```

**🎯 性能优化检查清单**

```
📋 分片性能检查：

✅ 分片大小检查：
├── 单分片 < 50GB
├── 文档数 < 2亿条  
├── 分片数 ≤ 节点数 × 20
└── 内存使用率 < 85%

✅ 分布均衡检查：
├── 各节点分片数相近
├── 磁盘使用率均衡
├── CPU使用率均衡  
└── 无明显热点节点

✅ 查询性能检查：
├── 平均查询时间 < 100ms
├── 95%查询时间 < 500ms
├── 跨分片查询比例 < 30%
└── 缓存命中率 > 90%
```

### 6.3 分片优化技巧


**🔧 实用优化策略**

```json
# 1. 合理设置分片数
PUT my_index
{
  "settings": {
    "number_of_shards": 5,
    "number_of_replicas": 1,
    "refresh_interval": "30s"
  }
}

# 2. 使用自定义路由
PUT my_index/_doc/1?routing=user_123
{
  "user_id": "user_123",
  "content": "用户数据"
}

# 3. 批量操作优化
POST /_bulk
{"index": {"_index": "my_index", "_id": "1", "routing": "user_123"}}
{"user_id": "user_123", "data": "批量数据1"}
{"index": {"_index": "my_index", "_id": "2", "routing": "user_123"}}  
{"user_id": "user_123", "data": "批量数据2"}
```

---

## 7. 🛠️ 故障处理与恢复


### 7.1 分片故障类型


**⚠️ 常见分片故障场景**

```
🔴 分片故障分类：

1. 未分配分片 (Unassigned)：
   ├── 原因：节点下线、磁盘满、内存不足
   ├── 影响：数据不可用或只读
   └── 紧急程度：高

2. 初始化分片 (Initializing)：
   ├── 原因：分片正在恢复或迁移
   ├── 影响：暂时性能下降
   └── 紧急程度：中

3. 重定位分片 (Relocating)：
   ├── 原因：集群再平衡过程
   ├── 影响：轻微性能影响
   └── 紧急程度：低
```

### 7.2 分片恢复流程


**🔄 自动恢复机制**

```
分片恢复优先级：

1. 主分片恢复：
   节点重启 ──> 检查本地分片 ──> 恢复索引元数据
       │             │              │
       │             └── 重放事务日志 ──┘
       │                      │
       └── 标记分片为活跃 ─────┘

2. 副本分片恢复：
   主分片就绪 ──> 选择恢复源 ──> 复制段文件
       │             │              │
       │             └── 同步增量数据 ──┘
       │                      │
       └── 验证数据完整性 ─────┘

恢复时间估算：
本地恢复：几秒到几分钟（依赖磁盘速度）
网络恢复：几分钟到几小时（依赖数据量和网络）
```

### 7.3 故障排查与处理


**🔍 故障诊断步骤**

```bash
# 1. 检查集群健康状态
GET /_cluster/health?level=shards

# 2. 查看未分配分片详情
GET /_cluster/allocation/explain
{
  "index": "my_index",
  "shard": 0,
  "primary": true
}

# 3. 强制分配分片（谨慎使用）
POST /_cluster/reroute
{
  "commands": [
    {
      "allocate_empty_primary": {
        "index": "my_index",
        "shard": 0,
        "node": "node-1",
        "accept_data_loss": true
      }
    }
  ]
}
```

**🚨 应急处理指南**

| 故障类型 | **处理优先级** | **处理方法** | **注意事项** |
|---------|--------------|-------------|-------------|
| 🔴 **主分片丢失** | `🚨 紧急` | `提升副本为主分片` | `可能有数据丢失` |
| 🟡 **副本分片丢失** | `⚠️ 重要` | `重新创建副本` | `影响可用性` |
| 🟢 **分片重定位** | `💡 关注` | `等待自动完成` | `监控进度即可` |

---

## 8. 📋 核心要点总结


### 8.1 必须掌握的核心概念


```
🔸 分片本质：数据的水平切分，实现分布式存储和处理
🔸 主副分片：主分片负责写入，副本分片负责容错和读取分流
🔸 路由机制：通过哈希算法将文档分配到具体分片
🔸 分配策略：集群自动管理分片分布，确保均衡和容错
🔸 热温冷架构：根据数据访问频率分层存储，优化成本
🔸 性能优化：合理规划分片大小和数量，避免过度分片
```

### 8.2 实践经验总结


**🎯 分片设计黄金法则**

```
📏 分片大小黄金法则：
• 单分片最佳大小：20-40GB
• 文档数量上限：每分片2000万条
• 分片数量控制：集群分片总数 < 节点数 × 20

🎯 路由策略选择：
• 默认路由：适用于大多数通用场景
• 自定义路由：用于有明确数据关联的场景
• 时间路由：适用于日志和时序数据

⚖️ 性能与成本平衡：
• 热数据用SSD：保证查询性能
• 温数据用SATA：平衡性能和成本
• 冷数据用大容量：最低成本存储
```

### 8.3 避免常见误区


**❌ 分片设计误区**

```
误区1：分片数越多越好
✅ 正确：适量分片，避免协调开销

误区2：所有数据都用热存储
✅ 正确：根据访问模式分层存储

误区3：忽略副本分片的作用
✅ 正确：合理配置副本，平衡性能和可用性

误区4：分片大小不控制
✅ 正确：定期监控分片大小，及时调整策略
```

### 8.4 运维最佳实践


**🔧 日常运维建议**

```
📊 监控指标：
• 分片健康状态：每5分钟检查
• 分片大小分布：每天统计
• 节点负载均衡：每小时监控
• 查询性能指标：实时监控

🛠️ 维护操作：
• 定期清理过期索引：释放存储空间
• 优化分片分配：避免热点节点
• 调整ILM策略：适应业务变化
• 备份重要数据：确保数据安全
```

**🧠 核心记忆口诀**：
- 分片设计要合理，大小控制二三十
- 主副分片配合好，读写分离性能高  
- 路由策略选对路，查询性能不用愁
- 热温冷层分得清，成本性能两兼顾