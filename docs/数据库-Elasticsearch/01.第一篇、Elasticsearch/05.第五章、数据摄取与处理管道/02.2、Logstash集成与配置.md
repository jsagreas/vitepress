---
title: 2、Logstash集成与配置
---
## 📚 目录

1. [Logstash概述与架构](#1-Logstash概述与架构)
2. [Input插件详解](#2-Input插件详解)
3. [Filter插件核心功能](#3-Filter插件核心功能)
4. [Output插件配置](#4-Output插件配置)
5. [Pipeline管道机制](#5-Pipeline管道机制)
6. [数据处理实战案例](#6-数据处理实战案例)
7. [性能优化与监控](#7-性能优化与监控)
8. [核心要点总结](#8-核心要点总结)

---

## 1. 🔄 Logstash概述与架构


### 1.1 什么是Logstash


**🔸 简单理解**
```
Logstash就像一个强大的数据搬运工：
- 从各种地方收集数据（日志文件、数据库、消息队列等）
- 对数据进行清洗和加工（格式转换、字段提取、数据过滤）
- 把处理好的数据送到目标地（Elasticsearch、文件、数据库等）
```

> 💡 **生活类比**：Logstash就像工厂的流水线，原材料从一端进入，经过各种加工处理，最终变成成品从另一端输出。

**🔸 核心价值**
- **数据收集**：支持200+种数据源
- **数据转换**：强大的数据处理能力
- **数据分发**：可以同时输出到多个目标

### 1.2 Logstash架构原理


**🏗️ 三层架构图示**
```
┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│   Input     │───▶│   Filter    │───▶│   Output    │
│  数据输入    │    │  数据处理    │    │  数据输出    │
└─────────────┘    └─────────────┘    └─────────────┘
       ▲                  ▲                  ▲
       │                  │                  │
┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│日志文件      │    │格式解析      │    │Elasticsearch│
│数据库       │    │字段提取      │    │文件存储      │
│消息队列      │    │数据过滤      │    │其他系统      │
└─────────────┘    └─────────────┘    └─────────────┘
```

**🔸 工作流程详解**
```
数据流向过程：
1. Input阶段：接收原始数据
2. Filter阶段：解析和转换数据  
3. Output阶段：发送处理后的数据

每个阶段都可以配置多个插件，形成强大的数据处理管道
```

### 1.3 Logstash与ELK的关系


**📊 ELK架构位置**
```
日志源 → Logstash → Elasticsearch → Kibana
  ▲        ▲           ▲            ▲
数据产生   数据处理    数据存储     数据展示

Logstash在ELK中的作用：
- 数据收集器：从各种源头收集数据
- 数据处理器：清洗和格式化数据
- 数据路由器：将数据发送到正确的目标
```

---

## 2. 📥 Input插件详解


### 2.1 Input插件基本概念


**🔸 什么是Input插件**
```
Input插件负责从数据源读取数据，相当于Logstash的"嘴巴"
- 决定从哪里获取数据
- 控制数据读取的方式和频率
- 支持多种数据源同时读取
```

### 2.2 File插件 - 读取日志文件


**📁 File插件基础配置**
```ruby
input {
  file {
    path => "/var/log/app/*.log"      # 日志文件路径
    start_position => "beginning"     # 从文件开头读取
    sincedb_path => "/dev/null"      # 不记录读取位置
    codec => "plain"                 # 纯文本格式
  }
}
```

**🔸 关键参数说明**
- **`path`**: 指定要读取的文件路径，支持通配符
- **`start_position`**: 设置读取起始位置
  - `beginning`: 从文件开头读取（适合测试）
  - `end`: 从文件末尾读取（适合生产）
- **`sincedb_path`**: 记录文件读取进度的位置

**📝 实际应用示例**
```ruby
input {
  file {
    path => [
      "/var/log/nginx/access.log",
      "/var/log/nginx/error.log"
    ]
    type => "nginx"                   # 给数据打标签
    start_position => "end"           # 只读取新内容
  }
}
```

### 2.3 Beats插件 - 接收Filebeat数据


**🎵 Beats插件配置**
```ruby
input {
  beats {
    port => 5044                      # 监听端口
    type => "beats"                   # 数据类型标识
  }
}
```

> 💡 **使用场景**：当你使用Filebeat收集日志时，Logstash通过Beats插件接收这些数据进行进一步处理。

### 2.4 Syslog插件 - 接收系统日志


**📡 Syslog插件配置**
```ruby
input {
  syslog {
    port => 514                       # 标准syslog端口
    type => "syslog"
  }
}
```

### 2.5 多Input配置示例


**🔄 同时处理多种数据源**
```ruby
input {
  # 读取应用日志文件
  file {
    path => "/app/logs/*.log"
    type => "application"
  }
  
  # 接收Filebeat数据
  beats {
    port => 5044
    type => "filebeat"
  }
  
  # 接收系统日志
  syslog {
    port => 514
    type => "system"
  }
}
```

---

## 3. 🔧 Filter插件核心功能


### 3.1 Filter插件作用机制


**🔸 什么是Filter插件**
```
Filter插件是Logstash的"大脑"，负责：
- 解析数据格式（JSON、XML、日志格式等）
- 提取有用字段
- 转换数据类型
- 过滤无用数据
- 添加新字段
```

### 3.2 Grok插件 - 强大的模式匹配


**🎯 Grok基本概念**
```
Grok用正则表达式从非结构化文本中提取结构化数据
相当于把一行日志文本变成一个个有意义的字段
```

**📝 Grok基础语法**
```ruby
%{PATTERN_NAME:field_name}

常用内置模式：
%{WORD:username}        # 匹配单词
%{NUMBER:count}         # 匹配数字  
%{IP:client_ip}         # 匹配IP地址
%{TIMESTAMP_ISO8601:timestamp}  # 匹配时间戳
```

**🌟 实际应用示例**
```ruby
filter {
  grok {
    match => { 
      "message" => "%{IP:client_ip} - - \[%{HTTPDATE:timestamp}\] \"%{WORD:method} %{URIPATH:request} HTTP/%{NUMBER:http_version}\" %{NUMBER:status_code} %{NUMBER:bytes}"
    }
  }
}
```

**📊 处理前后对比**
```
处理前的原始日志：
192.168.1.100 - - [25/Dec/2023:10:00:01 +0000] "GET /api/users HTTP/1.1" 200 1234

处理后的结构化数据：
{
  "client_ip": "192.168.1.100",
  "timestamp": "25/Dec/2023:10:00:01 +0000",
  "method": "GET",
  "request": "/api/users",
  "http_version": "1.1",
  "status_code": "200",
  "bytes": "1234"
}
```

### 3.3 Mutate插件 - 字段操作工具


**🛠️ Mutate基本功能**
```
Mutate插件用于修改字段，包括：
- 重命名字段
- 删除字段
- 转换数据类型
- 添加、更新字段
- 字符串操作
```

**📝 常用操作示例**
```ruby
filter {
  mutate {
    # 重命名字段
    rename => { "old_field" => "new_field" }
    
    # 删除不需要的字段
    remove_field => [ "message", "@version" ]
    
    # 转换数据类型
    convert => { 
      "status_code" => "integer"
      "bytes" => "integer"
    }
    
    # 添加新字段
    add_field => { "processed_by" => "logstash" }
    
    # 字符串操作
    uppercase => [ "method" ]
    strip => [ "username" ]
  }
}
```

### 3.4 Date插件 - 时间字段处理


**⏰ Date插件的重要性**
```
Date插件用于解析时间字段，这很重要因为：
- Elasticsearch需要正确的时间戳进行时间序列分析
- 错误的时间格式会导致索引问题
- 正确的时间字段是日志分析的基础
```

**📅 Date插件配置**
```ruby
filter {
  date {
    match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
    target => "@timestamp"
  }
}
```

**🔸 参数说明**
- **`match`**: 指定源字段和时间格式
- **`target`**: 指定解析后存储的字段名

### 3.5 条件判断 - 智能数据处理


**🤔 什么是条件判断**
```
条件判断让Logstash能够根据数据内容采取不同的处理策略
就像编程中的if-else语句
```

**📝 条件判断语法**
```ruby
filter {
  if [type] == "nginx" {
    grok {
      match => { "message" => "%{NGINX_ACCESS}" }
    }
  } else if [type] == "application" {
    json {
      source => "message"
    }
  }
  
  # 根据状态码添加标签
  if [status_code] >= 400 {
    mutate {
      add_tag => [ "error" ]
    }
  }
}
```

### 3.6 综合Filter配置示例


**🎯 完整的Nginx日志处理流程**
```ruby
filter {
  # 只处理nginx类型的数据
  if [type] == "nginx" {
    
    # 解析nginx访问日志格式
    grok {
      match => { 
        "message" => "%{IPORHOST:client_ip} - %{DATA:username} \[%{HTTPDATE:timestamp}\] \"%{WORD:method} %{DATA:request} HTTP/%{NUMBER:http_version}\" %{NUMBER:status_code} %{NUMBER:body_bytes_sent} \"%{DATA:http_referer}\" \"%{DATA:http_user_agent}\""
      }
    }
    
    # 解析时间字段
    date {
      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
    }
    
    # 转换数据类型
    mutate {
      convert => { 
        "status_code" => "integer"
        "body_bytes_sent" => "integer"
      }
      remove_field => [ "message" ]
    }
    
    # 根据状态码添加标签
    if [status_code] >= 400 {
      mutate {
        add_tag => [ "error" ]
      }
    }
  }
}
```

---

## 4. 📤 Output插件配置


### 4.1 Output插件基本概念


**🔸 什么是Output插件**
```
Output插件是Logstash的"手"，负责把处理好的数据发送到目标系统
- 可以同时发送到多个目标
- 支持各种存储和分析系统
- 提供数据传输的可靠性保证
```

### 4.2 Elasticsearch插件 - 核心输出目标


**🔍 Elasticsearch插件基础配置**
```ruby
output {
  elasticsearch {
    hosts => ["localhost:9200"]      # ES集群地址
    index => "logstash-%{+YYYY.MM.dd}"  # 索引名称模式
    document_type => "_doc"          # 文档类型
  }
}
```

**🎯 高级配置选项**
```ruby
output {
  elasticsearch {
    hosts => ["es1:9200", "es2:9200", "es3:9200"]
    index => "%{type}-%{+YYYY.MM.dd}"     # 根据类型创建不同索引
    template_name => "logstash"           # 模板名称
    template_pattern => "logstash-*"      # 模板匹配模式
    manage_template => true               # 自动管理模板
  }
}
```

**📊 索引命名策略**
```
按日期分割索引的好处：
- nginx-2023.12.25  ← 便于按日期查询和管理
- app-2023.12.25    ← 便于设置不同的保留策略
- error-2023.12.25  ← 便于单独分析错误日志

这样可以：
✅ 提高查询性能
✅ 便于数据管理
✅ 支持滚动删除旧数据
```

### 4.3 File插件 - 文件输出


**📁 File插件配置**
```ruby
output {
  file {
    path => "/var/log/logstash/output-%{+YYYY-MM-dd}.log"
    codec => "json_lines"            # JSON格式输出
  }
}
```

> 💡 **使用场景**：用于数据备份、调试输出或者发送到不支持的第三方系统。

### 4.4 条件输出 - 智能路由


**🔀 根据条件选择输出目标**
```ruby
output {
  # 错误日志发送到专门的索引
  if "error" in [tags] {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "error-logs-%{+YYYY.MM.dd}"
    }
  }
  
  # 正常日志发送到常规索引
  if [type] == "nginx" {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "nginx-logs-%{+YYYY.MM.dd}"
    }
  }
  
  # 同时输出到文件用于备份
  file {
    path => "/backup/logs/%{type}-%{+YYYY-MM-dd}.log"
  }
}
```

---

## 5. 🚀 Pipeline管道机制


### 5.1 什么是Pipeline


**🔸 Pipeline基本概念**
```
Pipeline是Logstash的核心概念，表示一个完整的数据处理流程：

数据源 → Input → Filter → Output → 目标系统

一个Logstash实例可以运行多个Pipeline，每个Pipeline处理不同类型的数据
```

### 5.2 单Pipeline配置


**📝 基础Pipeline配置文件**
```ruby
# /etc/logstash/conf.d/nginx.conf
input {
  file {
    path => "/var/log/nginx/access.log"
    type => "nginx-access"
    start_position => "end"
  }
}

filter {
  if [type] == "nginx-access" {
    grok {
      match => { "message" => "%{NGINX_ACCESS}" }
    }
    date {
      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
    }
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "nginx-%{+YYYY.MM.dd}"
  }
}
```

### 5.3 多Pipeline配置


**🔄 pipelines.yml配置文件**
```yaml
# /etc/logstash/pipelines.yml
- pipeline.id: nginx-pipeline
  path.config: "/etc/logstash/conf.d/nginx.conf"
  pipeline.workers: 2
  
- pipeline.id: app-pipeline  
  path.config: "/etc/logstash/conf.d/app.conf"
  pipeline.workers: 4
  
- pipeline.id: error-pipeline
  path.config: "/etc/logstash/conf.d/error.conf"
  pipeline.workers: 1
```

**🔸 多Pipeline的优势**
```
资源隔离：
- 不同Pipeline使用独立的线程池
- 一个Pipeline出现问题不影响其他Pipeline
- 可以为不同Pipeline分配不同的资源

配置管理：
- 每种数据类型有独立的配置文件
- 便于维护和修改
- 支持热重载特定Pipeline
```

### 5.4 Pipeline性能调优


**⚡ 关键性能参数**
```yaml
# logstash.yml
pipeline.workers: 4              # 处理线程数
pipeline.batch.size: 125         # 批处理大小
pipeline.batch.delay: 50         # 批处理延迟（毫秒）
queue.type: persisted           # 使用持久化队列
queue.max_bytes: 1gb            # 队列最大大小
```

**📊 参数调优指南**
```
pipeline.workers（工作线程数）：
- 设置为CPU核心数的1-2倍
- 对于CPU密集型处理增加workers
- 对于IO密集型处理适度增加

pipeline.batch.size（批处理大小）：
- 较大值提高吞吐量但增加延迟
- 较小值降低延迟但影响吞吐量
- 典型值：125-1000

pipeline.batch.delay（批处理延迟）：
- 控制等待批次填满的时间
- 低延迟要求设置较小值
- 高吞吐要求设置较大值
```

---

## 6. 🛠️ 数据处理实战案例


### 6.1 案例1：Web服务器日志分析


**📋 业务场景**
```
需求：分析Nginx访问日志，监控网站性能和用户行为
数据源：/var/log/nginx/access.log
目标：创建可视化仪表板，监控PV、UV、响应时间等指标
```

**🔧 完整配置方案**
```ruby
input {
  file {
    path => "/var/log/nginx/access.log"
    type => "nginx"
    start_position => "end"
    sincedb_path => "/var/lib/logstash/sincedb_nginx"
  }
}

filter {
  if [type] == "nginx" {
    # 解析nginx日志格式
    grok {
      match => { 
        "message" => "%{IPORHOST:client_ip} - %{DATA:username} \[%{HTTPDATE:timestamp}\] \"%{WORD:method} %{DATA:request} HTTP/%{NUMBER:http_version}\" %{NUMBER:response_code} %{NUMBER:bytes} \"%{DATA:referrer}\" \"%{DATA:user_agent}\" %{NUMBER:request_time}"
      }
    }
    
    # 解析时间
    date {
      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
    }
    
    # 数据类型转换
    mutate {
      convert => {
        "response_code" => "integer"
        "bytes" => "integer"
        "request_time" => "float"
      }
    }
    
    # 提取URL路径和参数
    if [request] {
      grok {
        match => { "request" => "(?<url_path>[^?]*)" }
      }
    }
    
    # 根据状态码分类
    if [response_code] >= 400 {
      mutate { add_tag => ["error"] }
    } else if [response_code] >= 300 {
      mutate { add_tag => ["redirect"] }
    } else {
      mutate { add_tag => ["success"] }
    }
    
    # 根据响应时间分类
    if [request_time] > 2.0 {
      mutate { add_tag => ["slow"] }
    }
    
    # 解析用户代理信息
    useragent {
      source => "user_agent"
      target => "ua"
    }
    
    # 清理不需要的字段
    mutate {
      remove_field => ["message", "host", "@version"]
    }
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "nginx-access-%{+YYYY.MM.dd}"
  }
}
```

### 6.2 案例2：应用程序错误日志监控


**📋 业务场景**
```
需求：监控Java应用程序错误日志，及时发现和报警系统异常
数据源：应用程序JSON格式日志
目标：提取错误信息，分类处理，设置告警规则
```

**🔧 配置方案**
```ruby
input {
  file {
    path => "/app/logs/application.log"
    type => "application"
    codec => "json"
  }
}

filter {
  if [type] == "application" {
    # 解析日期
    date {
      match => [ "timestamp", "yyyy-MM-dd HH:mm:ss.SSS" ]
    }
    
    # 根据日志级别处理
    if [level] == "ERROR" {
      mutate {
        add_tag => ["error", "alert"]
      }
      
      # 提取异常信息
      if [exception] {
        grok {
          match => { 
            "exception" => "(?<exception_class>[a-zA-Z.]+Exception): %{GREEDYDATA:exception_message}"
          }
        }
      }
    } else if [level] == "WARN" {
      mutate {
        add_tag => ["warning"]
      }
    }
    
    # 添加环境标识
    mutate {
      add_field => { "environment" => "production" }
    }
  }
}

output {
  # 错误日志单独存储
  if "error" in [tags] {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "app-errors-%{+YYYY.MM.dd}"
    }
  }
  
  # 所有日志存储
  elasticsearch {
    hosts => ["localhost:9200"] 
    index => "app-logs-%{+YYYY.MM.dd}"
  }
}
```

### 6.3 案例3：多源数据整合


**📋 业务场景**
```
需求：整合来自不同系统的日志数据，进行关联分析
数据源：Web服务器、应用服务器、数据库日志
目标：统一格式，便于跨系统分析
```

**🔧 统一配置方案**
```ruby
input {
  # Web服务器日志
  file {
    path => "/var/log/nginx/*.log"
    type => "web"
  }
  
  # 应用服务器日志
  file {
    path => "/app/logs/*.log"
    type => "app"
    codec => "json"
  }
  
  # 数据库日志
  file {
    path => "/var/log/mysql/*.log"
    type => "database"
  }
}

filter {
  # 添加统一的元数据
  mutate {
    add_field => {
      "environment" => "production"
      "datacenter" => "dc1"
    }
  }
  
  # 根据类型处理
  if [type] == "web" {
    grok {
      match => { "message" => "%{NGINX_ACCESS}" }
    }
  } else if [type] == "app" {
    # JSON格式已自动解析
    date {
      match => [ "timestamp", "yyyy-MM-dd HH:mm:ss.SSS" ]
    }
  } else if [type] == "database" {
    grok {
      match => { "message" => "%{MYSQL_ERROR}" }
    }
  }
  
  # 统一时间字段格式
  if ![timestamp] {
    mutate {
      add_field => { "timestamp" => "%{@timestamp}" }
    }
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "%{type}-logs-%{+YYYY.MM.dd}"
  }
}
```

---

## 7. 📊 性能优化与监控


### 7.1 Logstash性能监控


**📈 关键监控指标**
```
吞吐量指标：
- events.in：输入事件数/秒
- events.filtered：过滤事件数/秒  
- events.out：输出事件数/秒

延迟指标：
- pipeline.events.duration_in_millis：处理延迟
- pipeline.queue.push_duration_in_millis：队列推送延迟

资源使用：
- jvm.memory.heap_used_percent：JVM堆内存使用率
- process.cpu.percent：CPU使用率
```

**🔍 监控API使用**
```bash
# 查看节点信息
curl -X GET "localhost:9600/_node/stats"

# 查看管道状态
curl -X GET "localhost:9600/_node/stats/pipelines"

# 查看JVM信息
curl -X GET "localhost:9600/_node/stats/jvm"
```

### 7.2 性能调优策略


**⚡ JVM优化配置**
```bash
# /etc/logstash/jvm.options
-Xms2g          # 初始堆大小
-Xmx2g          # 最大堆大小  
-XX:+UseG1GC    # 使用G1垃圾收集器
```

**🔧 Pipeline优化配置**
```yaml
# logstash.yml
pipeline.workers: 8                    # 根据CPU核心数调整
pipeline.batch.size: 1000             # 增大批处理大小
pipeline.batch.delay: 10              # 减少批处理延迟
queue.type: persisted                 # 使用持久化队列
queue.max_bytes: 2gb                  # 增大队列大小
queue.checkpoint.writes: 1024         # 检查点写入频率
```

### 7.3 背压处理机制


**🔸 什么是背压**
```
背压是指当下游系统（如Elasticsearch）处理速度跟不上上游数据产生速度时，
系统自动调节处理速度的机制，防止内存溢出。
```

**🛡️ 背压处理策略**
```yaml
# 队列配置
queue.type: persisted                 # 持久化队列防止数据丢失
queue.max_bytes: 2gb                  # 限制队列大小
queue.checkpoint.writes: 1024         # 定期写入检查点

# 输出配置  
output {
  elasticsearch {
    hosts => ["localhost:9200"]
    # 批量发送配置
    template_name => "logstash"
    template_overwrite => true
    # 重试配置
    retry_on_conflict => 3
  }
}
```

### 7.4 常见性能问题解决


**🐛 问题1：处理延迟过高**
```
症状：数据处理延迟超过几分钟
原因：
- Filter处理逻辑过于复杂
- Grok正则表达式效率低
- 输出目标响应慢

解决方案：
✅ 优化Grok模式，避免过度复杂的正则
✅ 增加Pipeline workers数量
✅ 使用条件判断减少不必要的处理
✅ 检查输出目标性能
```

**🐛 问题2：内存使用过高**
```
症状：JVM堆内存持续增长
原因：
- 批处理大小设置过大
- 队列积压严重
- 存在内存泄漏

解决方案：
✅ 减小pipeline.batch.size
✅ 增加JVM堆内存
✅ 监控队列大小和积压情况
✅ 检查Filter插件是否存在内存泄漏
```

**🐛 问题3：数据丢失**
```
症状：输出的数据量少于输入
原因：
- Grok解析失败导致数据被丢弃
- 输出目标连接失败
- 队列配置不当

解决方案：
✅ 使用持久化队列
✅ 添加解析失败的错误处理
✅ 配置输出重试机制
✅ 监控处理成功率
```

---

## 8. 📋 核心要点总结


### 8.1 必须掌握的核心概念


```
🔸 Logstash架构：Input→Filter→Output三层处理模式
🔸 插件体系：200+插件覆盖各种数据源和目标系统
🔸 Pipeline机制：独立的数据处理流水线，支持多管道并行
🔸 Grok解析：强大的正则表达式数据提取工具
🔸 条件判断：基于数据内容的智能处理路由
🔸 性能调优：通过参数配置优化处理性能
```

### 8.2 关键技能要点


**🔹 配置文件编写能力**
```
基础技能：
- 熟练编写Input、Filter、Output配置
- 掌握常用插件的参数配置
- 理解配置文件的语法规则

进阶技能：
- 设计复杂的数据处理逻辑
- 优化Grok模式提高解析效率
- 配置多Pipeline处理不同数据流
```

**🔹 数据处理思维**
```
分析数据特征：
- 识别数据格式（结构化/非结构化）
- 确定关键字段和信息
- 设计合适的解析策略

设计处理流程：
- 选择合适的Input插件
- 设计Filter处理逻辑
- 配置Output路由策略
```

### 8.3 最佳实践建议


**📝 配置文件管理**
```
组织结构：
✅ 按数据类型分别创建配置文件
✅ 使用有意义的文件名和注释
✅ 版本控制管理配置变更

测试验证：
✅ 使用小数据集测试配置正确性
✅ 验证数据解析结果
✅ 检查输出格式和内容
```

**⚡ 性能优化原则**
```
资源分配：
✅ 根据数据量合理设置Pipeline workers
✅ 为不同重要性的数据分配不同资源
✅ 监控系统资源使用情况

处理效率：
✅ 优化Grok正则表达式性能
✅ 使用条件判断避免不必要处理
✅ 合理设置批处理参数
```

**🔍 监控告警体系**
```
关键监控指标：
📊 数据处理吞吐量和延迟
📊 错误率和解析成功率
📊 系统资源使用情况
📊 队列积压和背压状态

告警策略：
🚨 处理延迟超过阈值
🚨 错误率超过正常范围
🚨 内存使用率过高
🚨 队列积压严重
```

### 8.4 学习路径建议


**📚 入门阶段（1-2周）**
```
🎯 学习目标：
- 理解Logstash基本概念和架构
- 掌握基础的Input、Filter、Output配置
- 能够处理简单的日志文件

🛠️ 实践项目：
- 配置读取本地日志文件
- 使用Grok解析简单的日志格式
- 将数据发送到Elasticsearch
```

**📈 进阶阶段（2-4周）**
```  
🎯 学习目标：
- 掌握复杂的数据处理逻辑
- 理解性能优化和监控
- 能够设计生产级别的配置

🛠️ 实践项目：
- 处理多种格式的日志数据
- 配置多Pipeline处理不同数据流
- 实施性能监控和告警
```

**🚀 高级阶段（持续学习）**
```
🎯 学习目标：
- 深入理解Logstash内部机制
- 具备故障排查和性能调优能力
- 能够指导团队使用和优化

🛠️ 实践方向：
- 大规模集群部署和管理
- 自定义插件开发
- 与其他ELK组件深度整合
```

**🧠 核心记忆口诀**
```
Logstash数据搬运工，三层架构要记牢
Input收集Filter处理，Output发送到目标
Grok解析最强大，正则提取有妙招
Pipeline管道并行跑，性能监控不能少
配置测试要仔细，生产运行才可靠
```