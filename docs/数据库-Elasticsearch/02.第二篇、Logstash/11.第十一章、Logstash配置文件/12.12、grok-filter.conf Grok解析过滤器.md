---
title: 12、grok-filter.conf Grok解析过滤器
---
## 📚 目录

1. [Grok过滤器基础概念](#1-grok过滤器基础概念)
2. [Apache访问日志解析实战](#2-apache访问日志解析实战)
3. [Nginx日志解析配置](#3-nginx日志解析配置)
4. [自定义Grok模式详解](#4-自定义grok模式详解)
5. [命名捕获组高级用法](#5-命名捕获组高级用法)
6. [性能优化与故障处理](#6-性能优化与故障处理)
7. [实用配置样例集合](#7-实用配置样例集合)
8. [核心要点总结](#8-核心要点总结)

---

## 1. 🔍 Grok过滤器基础概念


### 1.1 什么是Grok过滤器


**简单理解**：Grok就像一个"文本翻译器"，它能把混乱的日志文本变成结构化的数据。

```
原始日志（人类难读）：
127.0.0.1 - - [25/Dec/2023:10:00:00 +0000] "GET /index.html HTTP/1.1" 200 2326

经过Grok处理（结构化）：
{
  "clientip": "127.0.0.1",
  "timestamp": "25/Dec/2023:10:00:00 +0000",
  "method": "GET",
  "request": "/index.html",
  "httpversion": "1.1",
  "response": "200",
  "bytes": "2326"
}
```

### 1.2 Grok的工作原理


**核心机制**：Grok使用**正则表达式**的预定义模式来匹配和提取文本中的字段。

```
工作流程图示：
                    
原始日志文本  →  Grok模式匹配  →  提取字段  →  结构化输出
    ↓              ↓              ↓           ↓
"192.168.1.1"  %{IP:clientip}    clientip    {"clientip":"192.168.1.1"}
```

**预定义模式示例**：
- `%{IP}` - 匹配IP地址
- `%{WORD}` - 匹配单词  
- `%{NUMBER}` - 匹配数字
- `%{TIMESTAMP_ISO8601}` - 匹配ISO时间格式

### 1.3 基础配置文件结构


```ruby
# 基础Grok过滤器配置
filter {
  grok {
    # 匹配模式 - 告诉Grok如何解析日志
    match => { "message" => "%{COMBINEDAPACHELOG}" }
    
    # 失败标签 - 解析失败时添加标签
    tag_on_failure => ["_grokparsefailure"]
    
    # 字段覆盖 - 是否覆盖已存在的字段
    overwrite => ["host", "message"]
  }
}
```

**配置说明**：
- `match` - 核心配置，定义匹配规则
- `tag_on_failure` - 解析失败时的标记
- `overwrite` - 控制字段覆盖行为

---

## 2. 🌐 Apache访问日志解析实战


### 2.1 Apache日志格式分析


**标准Apache访问日志格式**：
```
LogFormat "%h %l %u %t \"%r\" %>s %O \"%{Referer}i\" \"%{User-Agent}i\"" combined
```

**实际日志示例**：
```
192.168.1.100 - - [25/Dec/2023:10:15:30 +0000] "GET /products/laptop HTTP/1.1" 200 3456 "https://www.google.com" "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
```

**字段含义解析**：
| 位置 | 字段名 | 含义 | 示例值 |
|------|--------|------|--------|
| 1 | **客户端IP** | 访问者的IP地址 | `192.168.1.100` |
| 2 | **远程登录名** | RFC 1413身份（通常为-） | `-` |
| 3 | **认证用户** | HTTP认证用户名 | `-` |
| 4 | **时间戳** | 请求时间 | `[25/Dec/2023:10:15:30 +0000]` |
| 5 | **请求行** | HTTP方法+URL+版本 | `"GET /products/laptop HTTP/1.1"` |
| 6 | **响应码** | HTTP状态码 | `200` |
| 7 | **响应大小** | 返回字节数 | `3456` |
| 8 | **引用页** | 来源页面 | `"https://www.google.com"` |
| 9 | **用户代理** | 浏览器信息 | `"Mozilla/5.0..."` |

### 2.2 Apache日志解析配置


**完整配置文件** `apache-access-grok.conf`：

```ruby
input {
  file {
    path => "/var/log/apache2/access.log"
    start_position => "beginning"
    type => "apache-access"
  }
}

filter {
  # 只处理Apache访问日志
  if [type] == "apache-access" {
    
    grok {
      # 使用内置的Apache日志模式
      match => { 
        "message" => "%{COMBINEDAPACHELOG}" 
      }
      
      # 解析失败时的处理
      tag_on_failure => ["apache_grok_failure"]
      
      # 覆盖原始message字段
      overwrite => ["message"]
    }
    
    # 转换响应码为数字类型
    mutate {
      convert => { 
        "response" => "integer"
        "bytes" => "integer"
      }
    }
    
    # 解析时间戳
    date {
      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
      target => "@timestamp"
    }
    
    # 添加有用的标签
    if [response] >= 400 {
      mutate {
        add_tag => ["error"]
      }
    }
    
    if [response] >= 500 {
      mutate {
        add_tag => ["server_error"]
      }
    }
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "apache-logs-%{+YYYY.MM.dd}"
  }
  
  # 调试输出（可选）
  stdout {
    codec => rubydebug
  }
}
```

### 2.3 自定义Apache日志模式


**场景**：如果你的Apache日志格式与标准不同，需要自定义模式。

```ruby
# 自定义日志格式示例
# LogFormat "%h %t %m %U %s %b %D" custom

filter {
  grok {
    # 自定义匹配模式
    match => { 
      "message" => "%{IP:clientip} \[%{HTTPDATE:timestamp}\] %{WORD:method} %{URIPATH:request} %{INT:response} %{INT:bytes} %{INT:duration}"
    }
    
    tag_on_failure => ["custom_apache_grok_failure"]
  }
  
  # 数据类型转换
  mutate {
    convert => {
      "response" => "integer"
      "bytes" => "integer"
      "duration" => "integer"
    }
  }
}
```

---

## 3. 🔧 Nginx日志解析配置


### 3.1 Nginx日志格式理解


**标准Nginx访问日志格式**：
```nginx
log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                '$status $body_bytes_sent "$http_referer" '
                '"$http_user_agent" "$http_x_forwarded_for"';
```

**实际日志示例**：
```
203.0.113.195 - - [17/May/2023:08:05:32 +0000] "GET /favicon.ico HTTP/1.1" 200 946 "https://example.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_6) AppleWebKit/537.36" "-"
```

### 3.2 Nginx日志解析配置


**基础Nginx解析配置**：

```ruby
input {
  beats {
    port => 5044
    type => "nginx-access"
  }
}

filter {
  if [type] == "nginx-access" {
    
    grok {
      # Nginx访问日志模式
      match => { 
        "message" => "%{IPORHOST:remote_addr} - %{DATA:remote_user} \[%{HTTPDATE:time_local}\] \"%{WORD:method} %{DATA:request} HTTP/%{NUMBER:http_version}\" %{INT:status} %{INT:body_bytes_sent} \"%{DATA:http_referer}\" \"%{DATA:http_user_agent}\" \"%{DATA:http_x_forwarded_for}\""
      }
      
      tag_on_failure => ["nginx_grok_failure"]
    }
    
    # 数据类型转换
    mutate {
      convert => {
        "status" => "integer"
        "body_bytes_sent" => "integer"
        "http_version" => "float"
      }
    }
    
    # 时间戳处理
    date {
      match => [ "time_local", "dd/MMM/yyyy:HH:mm:ss Z" ]
      target => "@timestamp"
    }
    
    # 添加业务标签
    if [status] >= 400 {
      mutate { add_tag => ["http_error"] }
    }
    
    if [status] == 404 {
      mutate { add_tag => ["not_found"] }
    }
    
    # 处理X-Forwarded-For（真实客户端IP）
    if [http_x_forwarded_for] != "-" {
      mutate {
        add_field => { "real_client_ip" => "%{http_x_forwarded_for}" }
      }
    } else {
      mutate {
        add_field => { "real_client_ip" => "%{remote_addr}" }
      }
    }
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "nginx-logs-%{+YYYY.MM.dd}"
  }
}
```

### 3.3 Nginx错误日志解析


**Nginx错误日志格式不同**，需要单独处理：

```ruby
filter {
  if [type] == "nginx-error" {
    
    grok {
      # Nginx错误日志模式
      match => {
        "message" => "%{DATA:timestamp} \[%{DATA:log_level}\] %{INT:pid}#%{INT:tid}: (\*%{INT:connection_id} )?%{GREEDYDATA:error_message}"
      }
      
      tag_on_failure => ["nginx_error_grok_failure"]
    }
    
    # 时间戳转换
    date {
      match => [ "timestamp", "yyyy/MM/dd HH:mm:ss" ]
      target => "@timestamp"
    }
    
    # 错误级别分类
    if [log_level] == "emerg" or [log_level] == "alert" or [log_level] == "crit" {
      mutate { add_tag => ["critical_error"] }
    }
    
    if [log_level] == "error" {
      mutate { add_tag => ["error"] }
    }
    
    if [log_level] == "warn" {
      mutate { add_tag => ["warning"] }
    }
  }
}
```

---

## 4. 🛠️ 自定义Grok模式详解


### 4.1 创建自定义模式文件


**为什么需要自定义模式**：
- 应用程序有特殊的日志格式
- 内置模式不够精确
- 需要提取特定的业务字段

**创建模式文件**：

```bash
# 创建自定义模式目录
mkdir -p /etc/logstash/patterns

# 创建模式文件
vim /etc/logstash/patterns/myapp_patterns
```

**自定义模式文件内容**：

```
# /etc/logstash/patterns/myapp_patterns

# 应用程序特有的模式
MYAPP_TIMESTAMP %{YEAR}-%{MONTHNUM}-%{MONTHDAY} %{TIME}
MYAPP_LOGLEVEL (DEBUG|INFO|WARN|ERROR|FATAL)
MYAPP_COMPONENT \[%{DATA:component}\]
MYAPP_USERID User:%{INT:user_id}
MYAPP_SESSIONID Session:%{DATA:session_id}

# 业务相关模式
ORDER_ID Order#%{INT:order_id}
PAYMENT_STATUS Payment:(SUCCESS|FAILED|PENDING)
EMAIL_ADDR %{EMAILADDRESS:email}

# 复合模式
MYAPP_LOGLINE %{MYAPP_TIMESTAMP:timestamp} %{MYAPP_LOGLEVEL:level} %{MYAPP_COMPONENT} %{GREEDYDATA:message}
```

### 4.2 使用自定义模式


**配置文件中引用自定义模式**：

```ruby
filter {
  grok {
    # 指定自定义模式目录
    patterns_dir => ["/etc/logstash/patterns"]
    
    # 使用自定义模式
    match => {
      "message" => "%{MYAPP_LOGLINE}"
    }
    
    tag_on_failure => ["custom_grok_failure"]
  }
  
  # 进一步解析业务字段
  if "_grokparsefailure" not in [tags] {
    grok {
      patterns_dir => ["/etc/logstash/patterns"]
      match => {
        "message" => "%{ORDER_ID}.*%{PAYMENT_STATUS:payment_result}"
      }
      tag_on_failure => ["business_grok_failure"]
    }
  }
}
```

### 4.3 实际应用示例


**假设应用日志格式**：
```
2023-12-25 10:30:45 INFO [OrderService] User:12345 Session:abc123xyz Order#67890 Payment:SUCCESS Processing completed
```

**对应的Grok配置**：

```ruby
filter {
  grok {
    patterns_dir => ["/etc/logstash/patterns"]
    
    match => {
      "message" => "%{MYAPP_TIMESTAMP:log_time} %{MYAPP_LOGLEVEL:log_level} %{MYAPP_COMPONENT} %{MYAPP_USERID} %{MYAPP_SESSIONID} %{ORDER_ID} %{PAYMENT_STATUS:payment_status} %{GREEDYDATA:details}"
    }
  }
  
  # 时间戳转换
  date {
    match => [ "log_time", "yyyy-MM-dd HH:mm:ss" ]
    target => "@timestamp"
  }
  
  # 业务逻辑标签
  if [payment_status] == "SUCCESS" {
    mutate { add_tag => ["payment_success"] }
  } else {
    mutate { add_tag => ["payment_issue"] }
  }
}
```

---

## 5. 🎯 命名捕获组高级用法


### 5.1 命名捕获组基础


**什么是命名捕获组**：给匹配到的内容起一个有意义的名字，方便后续使用。

**基本语法**：
```
%{PATTERN:field_name}
```

**示例对比**：
```ruby
# 不使用命名捕获组（不推荐）
match => { "message" => "%{IP} %{WORD} %{INT}" }
# 结果：字段名不明确

# 使用命名捕获组（推荐）
match => { "message" => "%{IP:client_ip} %{WORD:method} %{INT:status_code}" }
# 结果：字段名清晰明确
```

### 5.2 嵌套和条件捕获


**多级命名捕获**：

```ruby
filter {
  grok {
    match => {
      "message" => "%{IP:network.client_ip} - %{DATA:auth.username} \[%{HTTPDATE:request.timestamp}\] \"%{WORD:request.method} %{URIPATH:request.path}(?:%{URIPARAM:request.params})? HTTP/%{NUMBER:request.version}\" %{INT:response.status} %{INT:response.bytes}"
    }
  }
}
```

**条件捕获组**：

```ruby
filter {
  grok {
    # 可选字段的处理
    match => {
      "message" => "%{IP:clientip}(?: %{WORD:ident})?(?: %{WORD:auth})? \[%{HTTPDATE:timestamp}\]"
    }
  }
  
  # 根据是否存在字段进行后续处理
  if [ident] {
    mutate {
      add_tag => ["has_ident"]
    }
  }
  
  if [auth] and [auth] != "-" {
    mutate {
      add_tag => ["authenticated_user"]
    }
  }
}
```

### 5.3 字段重命名和转换


**智能字段处理**：

```ruby
filter {
  grok {
    match => {
      "message" => "%{COMBINEDAPACHELOG}"
    }
  }
  
  # 重命名字段（更符合业务含义）
  mutate {
    rename => {
      "clientip" => "client_ip_address"
      "verb" => "http_method"
      "request" => "request_path"
      "bytes" => "response_size_bytes"
    }
  }
  
  # 数据类型转换
  mutate {
    convert => {
      "response" => "integer"
      "response_size_bytes" => "integer"
    }
  }
  
  # 添加计算字段
  if [response_size_bytes] > 1048576 {  # 1MB
    mutate {
      add_field => { "large_response" => "true" }
      add_tag => ["large_file_transfer"]
    }
  }
}
```

---

## 6. ⚡ 性能优化与故障处理


### 6.1 性能优化配置


**超时设置**：

```ruby
filter {
  grok {
    match => { "message" => "%{COMBINEDAPACHELOG}" }
    
    # 超时设置（毫秒）
    timeout_millis => 30000
    
    # 模式匹配超时后的处理
    tag_on_timeout => ["grok_timeout"]
  }
}
```

**模式目录优化**：

```ruby
filter {
  grok {
    # 缓存模式以提高性能
    patterns_dir => ["/etc/logstash/patterns"]
    
    # 启用模式缓存
    patterns_files_glob => "*.patterns"
    
    match => { "message" => "%{CUSTOM_PATTERN}" }
  }
}
```

### 6.2 故障处理策略


**分层故障处理**：

```ruby
filter {
  # 第一层：尝试最常见的模式
  grok {
    match => { "message" => "%{COMBINEDAPACHELOG}" }
    tag_on_failure => ["try_common_pattern"]
  }
  
  # 第二层：如果第一层失败，尝试简化模式
  if "try_common_pattern" in [tags] {
    grok {
      match => { 
        "message" => "%{IP:clientip} %{GREEDYDATA:raw_request}"
      }
      tag_on_failure => ["try_basic_pattern"]
      remove_tag => ["try_common_pattern"]
    }
  }
  
  # 第三层：最后的兜底策略
  if "try_basic_pattern" in [tags] {
    mutate {
      add_field => { "parse_status" => "failed" }
      add_field => { "original_message" => "%{message}" }
      remove_tag => ["try_basic_pattern"]
      add_tag => ["grok_complete_failure"]
    }
  }
}
```

### 6.3 调试和监控


**调试配置**：

```ruby
filter {
  grok {
    match => { "message" => "%{COMBINEDAPACHELOG}" }
    
    # 保留原始消息用于调试
    keep_empty_captures => true
    
    # 详细的失败标签
    tag_on_failure => ["_grokparsefailure_%{[@metadata][input]}"]
  }
  
  # 添加调试信息
  if "_grokparsefailure" in [tags] {
    mutate {
      add_field => { 
        "debug_message" => "Grok parsing failed for: %{message}"
        "debug_timestamp" => "%{@timestamp}"
      }
    }
  }
}
```

**监控输出**：

```ruby
output {
  # 正常数据输出到Elasticsearch
  if "_grokparsefailure" not in [tags] {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "parsed-logs-%{+YYYY.MM.dd}"
    }
  }
  
  # 解析失败的数据单独存储
  if "_grokparsefailure" in [tags] {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "failed-logs-%{+YYYY.MM.dd}"
    }
    
    # 同时输出到文件便于调试
    file {
      path => "/var/log/logstash/grok-failures.log"
      codec => json_lines
    }
  }
}
```

---

## 7. 📦 实用配置样例集合


### 7.1 Java应用日志解析


**Java堆栈异常日志处理**：

```ruby
filter {
  # 多行日志合并（Java异常堆栈）
  multiline {
    pattern => "^\d{4}-\d{2}-\d{2}"
    negate => true
    what => "previous"
  }
  
  grok {
    match => {
      "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} \[%{DATA:thread}\] %{DATA:logger} - %{GREEDYDATA:log_message}"
    }
    tag_on_failure => ["java_grok_failure"]
  }
  
  # 异常检测
  if [log_message] =~ /Exception|Error|Throwable/ {
    mutate {
      add_tag => ["java_exception"]
    }
  }
}
```

### 7.2 数据库日志解析


**MySQL慢查询日志**：

```ruby
filter {
  if [type] == "mysql-slow" {
    grok {
      match => {
        "message" => "# Time: %{TIMESTAMP_ISO8601:query_time}\n# User@Host: %{DATA:user}\[%{DATA:db}\] @ %{DATA:host} \[%{IP:client_ip}\]\n# Query_time: %{NUMBER:query_duration:float} Lock_time: %{NUMBER:lock_time:float} Rows_sent: %{INT:rows_sent} Rows_examined: %{INT:rows_examined}\n%{GREEDYDATA:sql_query}"
      }
    }
    
    # 慢查询分类
    if [query_duration] > 10 {
      mutate { add_tag => ["very_slow_query"] }
    } else if [query_duration] > 5 {
      mutate { add_tag => ["slow_query"] }
    }
  }
}
```

### 7.3 系统日志解析


**系统Syslog格式**：

```ruby
filter {
  if [type] == "syslog" {
    grok {
      match => {
        "message" => "%{SYSLOGTIMESTAMP:timestamp} %{IPORHOST:server} %{PROG:program}(?:\[%{POSINT:pid}\])?: %{GREEDYDATA:log_message}"
      }
    }
    
    # 优先级解析
    if [program] == "kernel" {
      mutate { add_tag => ["kernel_log"] }
    }
    
    if [log_message] =~ /error|Error|ERROR/ {
      mutate { add_tag => ["system_error"] }
    }
  }
}
```

---

## 8. 📋 核心要点总结


### 8.1 必须掌握的核心概念


```
🔸 Grok本质：正则表达式的预定义模式库，用于结构化文本数据
🔸 工作原理：pattern matching → field extraction → structured output
🔸 配置要素：match规则、失败处理、字段转换、性能优化
🔸 应用场景：Web服务器日志、应用程序日志、系统日志解析
🔸 最佳实践：分层解析、故障兜底、性能监控、调试友好
```

### 8.2 关键配置参数速查


| 参数 | 作用 | 使用场景 | 示例值 |
|------|------|----------|--------|
| **match** | 定义解析模式 | 核心配置 | `{ "message" => "%{COMBINEDAPACHELOG}" }` |
| **tag_on_failure** | 解析失败标签 | 故障处理 | `["_grokparsefailure"]` |
| **overwrite** | 字段覆盖控制 | 字段管理 | `["host", "message"]` |
| **patterns_dir** | 自定义模式目录 | 扩展功能 | `["/etc/logstash/patterns"]` |
| **timeout_millis** | 解析超时设置 | 性能优化 | `30000` |
| **keep_empty_captures** | 保留空捕获 | 调试模式 | `true` |

### 8.3 常见问题解决指南


**🔹 解析失败排查步骤**：
```
1. 检查日志格式是否与模式匹配
2. 验证自定义模式文件语法
3. 启用调试输出查看原始消息
4. 使用Grok调试工具测试模式
5. 实施分层解析策略
```

**🔹 性能优化要点**：
```
1. 合理设置超时时间
2. 使用特定模式而非通用模式
3. 避免过度复杂的正则表达式
4. 缓存自定义模式文件
5. 监控解析成功率和耗时
```

**🔹 字段管理策略**：
```
1. 使用有意义的字段命名
2. 及时进行数据类型转换
3. 清理不需要的临时字段
4. 添加业务逻辑标签
5. 保留原始数据用于排错
```

### 8.4 实际应用建议


**适用场景判断**：
- ✅ **结构化日志解析**：Web服务器、应用程序日志
- ✅ **字段提取需求**：需要将文本转为结构化数据
- ✅ **批量处理**：大量相似格式的日志文件
- ✅ **实时分析**：配合Elasticsearch进行实时搜索分析

**配置最佳实践**：
- 🎯 **从简单开始**：先用内置模式，再考虑自定义
- 🎯 **分层处理**：主要模式 + 备用模式 + 兜底策略
- 🎯 **性能监控**：关注解析成功率和处理延迟
- 🎯 **文档维护**：记录自定义模式的用途和样例

**核心记忆**：
- Grok是文本结构化的瑞士军刀
- 模式匹配决定解析效果
- 故障处理保证系统稳定性
- 性能优化提升处理效率