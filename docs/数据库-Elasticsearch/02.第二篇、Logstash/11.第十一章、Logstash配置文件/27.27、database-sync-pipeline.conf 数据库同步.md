---
title: 27ã€database-sync-pipeline.conf æ•°æ®åº“åŒæ­¥
---
## ğŸ“š ç›®å½•

1. [æ•°æ®åº“åŒæ­¥åŸºç¡€æ¦‚å¿µ](#1-æ•°æ®åº“åŒæ­¥åŸºç¡€æ¦‚å¿µ)
2. [å¢é‡æ•°æ®åŒæ­¥å®ç°](#2-å¢é‡æ•°æ®åŒæ­¥å®ç°)
3. [ä¸»é”®å†²çªå¤„ç†ç­–ç•¥](#3-ä¸»é”®å†²çªå¤„ç†ç­–ç•¥)
4. [æ•°æ®å˜æ›´æ£€æµ‹æœºåˆ¶](#4-æ•°æ®å˜æ›´æ£€æµ‹æœºåˆ¶)
5. [å¤§è¡¨åˆ†é¡µå¤„ç†æŠ€æœ¯](#5-å¤§è¡¨åˆ†é¡µå¤„ç†æŠ€æœ¯)
6. [å…³è”è¡¨æ•°æ®åŒæ­¥](#6-å…³è”è¡¨æ•°æ®åŒæ­¥)
7. [æ•°æ®ä¸€è‡´æ€§ä¿è¯](#7-æ•°æ®ä¸€è‡´æ€§ä¿è¯)
8. [åŒæ­¥çŠ¶æ€ç›‘æ§](#8-åŒæ­¥çŠ¶æ€ç›‘æ§)
9. [é”™è¯¯æ¢å¤æœºåˆ¶](#9-é”™è¯¯æ¢å¤æœºåˆ¶)
10. [æ ¸å¿ƒè¦ç‚¹æ€»ç»“](#10-æ ¸å¿ƒè¦ç‚¹æ€»ç»“)

---

## 1. ğŸ—„ï¸ æ•°æ®åº“åŒæ­¥åŸºç¡€æ¦‚å¿µ


### 1.1 ä»€ä¹ˆæ˜¯æ•°æ®åº“åŒæ­¥


**ğŸ”¸ å®šä¹‰**ï¼šæ•°æ®åº“åŒæ­¥å°±æ˜¯è®©ä¸¤ä¸ªæˆ–å¤šä¸ªæ•°æ®åº“çš„æ•°æ®ä¿æŒä¸€è‡´ï¼Œå°±åƒä¸¤é¢é•œå­äº’ç›¸ç…§æ˜ ä¸€æ ·ã€‚

```
ç®€å•ç†è§£ï¼š
æºæ•°æ®åº“ â†’ [Logstashå¤„ç†] â†’ ç›®æ ‡æ•°æ®åº“
   MySQL                      Elasticsearch

ç”¨é€”ï¼š
âœ… æ•°æ®å¤‡ä»½ï¼šæŠŠé‡è¦æ•°æ®å¤åˆ¶ä¸€ä»½
âœ… æ•°æ®åˆ†æï¼šæŠŠä¸šåŠ¡æ•°æ®å¯¼å…¥åˆ†æç³»ç»Ÿ  
âœ… æœç´¢ä¼˜åŒ–ï¼šæŠŠå…³ç³»æ•°æ®å¯¼å…¥æœç´¢å¼•æ“
âœ… è¯»å†™åˆ†ç¦»ï¼šå‡è½»ä¸»æ•°æ®åº“å‹åŠ›
```

### 1.2 åŒæ­¥æ–¹å¼å¯¹æ¯”


| åŒæ­¥ç±»å‹ | **å·¥ä½œåŸç†** | **é€‚ç”¨åœºæ™¯** | **ä¼˜ç¼ºç‚¹** |
|---------|-------------|-------------|-----------|
| ğŸ”„ **å…¨é‡åŒæ­¥** | `æ¯æ¬¡åŒæ­¥æ‰€æœ‰æ•°æ®` | `åˆå§‹åŒ–ã€å°æ•°æ®é‡` | `ç®€å•ä½†è€—æ—¶ï¼Œé€‚åˆå¯åŠ¨æ—¶` |
| âš¡ **å¢é‡åŒæ­¥** | `åªåŒæ­¥å˜åŒ–çš„æ•°æ®` | `æ—¥å¸¸è¿è¡Œã€å¤§æ•°æ®é‡` | `é«˜æ•ˆä½†å¤æ‚ï¼Œé€‚åˆæŒç»­è¿è¡Œ` |
| ğŸ• **å®æ—¶åŒæ­¥** | `æ•°æ®å˜åŒ–ç«‹å³åŒæ­¥` | `å¼ºä¸€è‡´æ€§è¦æ±‚` | `åŠæ—¶ä½†èµ„æºæ¶ˆè€—å¤§` |

### 1.3 æ•°æ®åº“åŒæ­¥æ¶æ„å›¾


```
ä¸šåŠ¡åº”ç”¨å†™å…¥               æ•°æ®åŒæ­¥æµç¨‹                ç›®æ ‡ç³»ç»ŸæŸ¥è¯¢
     â†“                        â†“                         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MySQL     â”‚ â”€â”€â”€â”€â†’   â”‚  Logstash   â”‚ â”€â”€â”€â”€â†’   â”‚Elasticsearchâ”‚
â”‚(æºæ•°æ®åº“)    â”‚         â”‚(æ•°æ®å¤„ç†å™¨)  â”‚         â”‚(ç›®æ ‡ç³»ç»Ÿ)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â†‘                        â†‘                         â†‘
   CRUDæ“ä½œ              æ•°æ®è½¬æ¢å¤„ç†              æœç´¢åˆ†ææŸ¥è¯¢
```

---

## 2. âš¡ å¢é‡æ•°æ®åŒæ­¥å®ç°


### 2.1 å¢é‡åŒæ­¥åŸç†


**ğŸ”¸ æ ¸å¿ƒæ€æƒ³**ï¼šåªå¤„ç†è‡ªä¸Šæ¬¡åŒæ­¥åå‘ç”Ÿå˜åŒ–çš„æ•°æ®ï¼Œé¿å…é‡å¤å¤„ç†ã€‚

```
å¢é‡åŒæ­¥ç­–ç•¥ï¼š

æ—¶é—´æˆ³ç­–ç•¥ï¼š
- è®°å½•ä¸Šæ¬¡åŒæ­¥æ—¶é—´ï¼š2024-09-21 10:00:00
- æŸ¥è¯¢æ¡ä»¶ï¼šWHERE updated_at > '2024-09-21 10:00:00'
- ä¼˜ç‚¹ï¼šç®€å•æ˜“å®ç°
- ç¼ºç‚¹ï¼šä¾èµ–æ—¶é—´å­—æ®µ

ç‰ˆæœ¬å·ç­–ç•¥ï¼š
- è®°å½•ä¸Šæ¬¡åŒæ­¥ç‰ˆæœ¬ï¼šversion = 12345
- æŸ¥è¯¢æ¡ä»¶ï¼šWHERE version > 12345  
- ä¼˜ç‚¹ï¼šç²¾ç¡®æ§åˆ¶
- ç¼ºç‚¹ï¼šéœ€è¦ç»´æŠ¤ç‰ˆæœ¬å­—æ®µ

è‡ªå¢IDç­–ç•¥ï¼š
- è®°å½•ä¸Šæ¬¡åŒæ­¥IDï¼šlast_id = 98765
- æŸ¥è¯¢æ¡ä»¶ï¼šWHERE id > 98765
- ä¼˜ç‚¹ï¼šæ€§èƒ½å¥½ï¼Œå¯é 
- ç¼ºç‚¹ï¼šåªèƒ½å¤„ç†æ–°å¢æ•°æ®
```

### 2.2 åŸºç¡€å¢é‡åŒæ­¥é…ç½®


```ruby
# åŸºç¡€å¢é‡åŒæ­¥é…ç½®æ–‡ä»¶
input {
  jdbc {
    # æ•°æ®åº“è¿æ¥ä¿¡æ¯
    jdbc_driver_library => "/opt/logstash/drivers/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/business_db"
    jdbc_user => "logstash_user"
    jdbc_password => "secure_password"
    
    # å¢é‡åŒæ­¥æŸ¥è¯¢è¯­å¥
    statement => "
      SELECT id, name, email, phone, status, created_at, updated_at 
      FROM users 
      WHERE updated_at > :sql_last_value 
      ORDER BY updated_at ASC
    "
    
    # ğŸ”¸ å…³é”®é…ç½®ï¼šè·Ÿè¸ªå­—æ®µå’Œæ–‡ä»¶
    tracking_column => "updated_at"          # ç”¨äºè·Ÿè¸ªçš„å­—æ®µ
    tracking_column_type => "timestamp"      # å­—æ®µç±»å‹
    last_run_metadata_path => "/data/logstash/.logstash_jdbc_last_run"  # è®°å½•æ–‡ä»¶
    
    # åŒæ­¥é¢‘ç‡è®¾ç½®
    schedule => "*/5 * * * *"               # æ¯5åˆ†é’Ÿæ‰§è¡Œä¸€æ¬¡
    use_column_value => true                # ä½¿ç”¨åˆ—å€¼ä½œä¸ºè·Ÿè¸ªç‚¹
  }
}

filter {
  # æ·»åŠ åŒæ­¥æ—¶é—´æˆ³
  ruby {
    code => "event.set('sync_timestamp', Time.now.utc.iso8601)"
  }
  
  # æ•°æ®æ¸…æ´—å’Œæ ¼å¼åŒ–
  mutate {
    convert => { "id" => "integer" }
    convert => { "status" => "integer" }
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "user_sync_%{+YYYY.MM.dd}"
    document_id => "%{id}"                  # ä½¿ç”¨æ•°æ®åº“IDä½œä¸ºæ–‡æ¡£ID
  }
  
  # è°ƒè¯•è¾“å‡ºï¼ˆç”Ÿäº§ç¯å¢ƒå¯å…³é—­ï¼‰
  stdout { 
    codec => rubydebug {
      metadata => false
    }
  }
}
```

### 2.3 é«˜çº§å¢é‡åŒæ­¥é…ç½®


```ruby
# æ”¯æŒåˆ é™¤æ£€æµ‹çš„å¢é‡åŒæ­¥
input {
  jdbc {
    jdbc_driver_library => "/opt/logstash/drivers/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/business_db"
    jdbc_user => "logstash_user"
    jdbc_password => "secure_password"
    
    # ğŸ”¸ å¤åˆæŸ¥è¯¢ï¼šå¤„ç†æ–°å¢å’Œä¿®æ”¹
    statement => "
      SELECT 
        id, name, email, phone, status, 
        created_at, updated_at, deleted_at,
        CASE 
          WHEN deleted_at IS NOT NULL THEN 'delete'
          WHEN created_at > :sql_last_value THEN 'insert'
          ELSE 'update'
        END as sync_action
      FROM users 
      WHERE 
        updated_at > :sql_last_value 
        OR deleted_at > :sql_last_value
      ORDER BY updated_at ASC
    "
    
    tracking_column => "updated_at"
    tracking_column_type => "timestamp"
    last_run_metadata_path => "/data/logstash/.user_sync_last_run"
    schedule => "*/2 * * * *"
    use_column_value => true
  }
}

filter {
  # ğŸ”¸ æ ¹æ®æ“ä½œç±»å‹å¤„ç†æ•°æ®
  if [sync_action] == "delete" {
    # æ ‡è®°ä¸ºåˆ é™¤æ“ä½œ
    mutate {
      add_field => { "[@metadata][action]" => "delete" }
    }
  } else {
    # æ­£å¸¸çš„æ’å…¥æˆ–æ›´æ–°æ“ä½œ
    mutate {
      add_field => { "[@metadata][action]" => "index" }
    }
  }
  
  # æ•°æ®æ ¼å¼åŒ–
  date {
    match => [ "created_at", "yyyy-MM-dd HH:mm:ss" ]
    target => "created_time"
  }
  
  date {
    match => [ "updated_at", "yyyy-MM-dd HH:mm:ss" ]
    target => "updated_time"
  }
}

output {
  # ğŸ”¸ æ ¹æ®æ“ä½œç±»å‹é€‰æ‹©è¾“å‡ºåŠ¨ä½œ
  if [@metadata][action] == "delete" {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "user_sync_%{+YYYY.MM.dd}"
      document_id => "%{id}"
      action => "delete"                   # æ‰§è¡Œåˆ é™¤æ“ä½œ
    }
  } else {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "user_sync_%{+YYYY.MM.dd}"
      document_id => "%{id}"
      action => "index"                    # æ‰§è¡Œç´¢å¼•æ“ä½œ
    }
  }
}
```

---

## 3. ğŸ”§ ä¸»é”®å†²çªå¤„ç†ç­–ç•¥


### 3.1 ä¸»é”®å†²çªåœºæ™¯åˆ†æ


**ğŸ”¸ å¸¸è§å†²çªæƒ…å†µ**ï¼š

```
åœºæ™¯åˆ†æï¼š

1ï¸âƒ£ æ•°æ®é‡å¤å¯¼å…¥ï¼š
   - åŒä¸€æ¡è®°å½•è¢«å¤šæ¬¡åŒæ­¥
   - åŸå› ï¼šåŒæ­¥ä»»åŠ¡é‡å¯ã€ç½‘ç»œé‡è¯•ç­‰

2ï¸âƒ£ å¤šæºæ•°æ®å†²çªï¼š
   - ä¸åŒæ•°æ®æºæœ‰ç›¸åŒIDçš„è®°å½•
   - åŸå› ï¼šå¤šä¸ªç³»ç»Ÿä½¿ç”¨ç›¸åŒçš„IDè§„åˆ™

3ï¸âƒ£ æ—¶åºé—®é¢˜ï¼š
   - è¾ƒæ–°çš„æ•°æ®å…ˆåˆ°è¾¾ï¼Œæ—§æ•°æ®ååˆ°è¾¾
   - åŸå› ï¼šç½‘ç»œå»¶è¿Ÿã€å¤„ç†é¡ºåºç­‰
```

### 3.2 å†²çªå¤„ç†ç­–ç•¥é…ç½®


```ruby
# ç­–ç•¥ä¸€ï¼šè¦†ç›–æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "products"
    document_id => "%{id}"
    # é»˜è®¤è¡Œä¸ºï¼šç›´æ¥è¦†ç›–å·²å­˜åœ¨çš„æ–‡æ¡£
  }
}

# ç­–ç•¥äºŒï¼šä»…åˆ›å»ºæ¨¡å¼
output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "products"
    document_id => "%{id}"
    action => "create"                     # ä»…åˆ›å»ºï¼Œå­˜åœ¨åˆ™å¤±è´¥
    # é€‚ç”¨åœºæ™¯ï¼šç¡®ä¿æ•°æ®å”¯ä¸€æ€§
  }
}

# ç­–ç•¥ä¸‰ï¼šæ¡ä»¶æ›´æ–°æ¨¡å¼
filter {
  # ğŸ”¸ æ·»åŠ ç‰ˆæœ¬æ§åˆ¶å­—æ®µ
  ruby {
    code => "event.set('doc_version', event.get('updated_at').to_i)"
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "products"
    document_id => "%{id}"
    
    # ğŸ”¸ ä½¿ç”¨è„šæœ¬è¿›è¡Œæ¡ä»¶æ›´æ–°
    script => "
      if (ctx._source.doc_version == null || 
          params.doc_version > ctx._source.doc_version) {
        ctx._source.putAll(params);
      } else {
        ctx.op = 'noop';  // è·³è¿‡æ›´æ–°
      }
    "
    script_lang => "painless"
  }
}
```

### 3.3 é«˜çº§å†²çªå¤„ç†


```ruby
# å¤åˆä¸»é”®å¤„ç†æ–¹æ¡ˆ
filter {
  # ğŸ”¸ ç”Ÿæˆå¤åˆä¸»é”®
  mutate {
    add_field => { 
      "composite_id" => "%{tenant_id}_%{user_id}_%{product_id}" 
    }
  }
  
  # æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
  if ![tenant_id] or ![user_id] or ![product_id] {
    mutate {
      add_tag => ["invalid_composite_key"]
    }
  }
}

output {
  # æ­£å¸¸æ•°æ®å¤„ç†
  if "invalid_composite_key" not in [tags] {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "orders"
      document_id => "%{composite_id}"
      
      # ğŸ”¸ ä½¿ç”¨è„šæœ¬å¤„ç†ä¸šåŠ¡é€»è¾‘å†²çª
      script => "
        // æ£€æŸ¥è®¢å•çŠ¶æ€ï¼Œåªå…è®¸çŠ¶æ€å‡çº§
        if (ctx._source.status == null) {
          ctx._source.putAll(params);
        } else {
          int oldStatus = ctx._source.status;
          int newStatus = params.status;
          if (newStatus >= oldStatus) {
            ctx._source.putAll(params);
          } else {
            ctx.op = 'noop';
          }
        }
      "
    }
  } else {
    # æ— æ•ˆæ•°æ®è¾“å‡ºåˆ°é”™è¯¯æ—¥å¿—
    file {
      path => "/var/log/logstash/invalid_composite_keys.log"
      codec => json_lines
    }
  }
}
```

---

## 4. ğŸ” æ•°æ®å˜æ›´æ£€æµ‹æœºåˆ¶


### 4.1 å˜æ›´æ£€æµ‹ç­–ç•¥


**ğŸ”¸ æ ¸å¿ƒæ–¹æ³•**ï¼š

```
æ£€æµ‹æ–¹æ³•å¯¹æ¯”ï¼š

ğŸ“… æ—¶é—´æˆ³æ£€æµ‹ï¼š
- åŸç†ï¼šæ¯”è¾ƒ updated_at å­—æ®µ
- ä¼˜ç‚¹ï¼šç®€å•ã€ç›´è§‚ã€æ€§èƒ½å¥½
- ç¼ºç‚¹ï¼šä¾èµ–åº”ç”¨å±‚ç»´æŠ¤æ—¶é—´æˆ³

ğŸ”¢ ç‰ˆæœ¬å·æ£€æµ‹ï¼š
- åŸç†ï¼šæ¯”è¾ƒ version å­—æ®µ
- ä¼˜ç‚¹ï¼šç²¾ç¡®ã€å¯æ§
- ç¼ºç‚¹ï¼šéœ€è¦é¢å¤–å­˜å‚¨ç©ºé—´

ğŸ” æ ¡éªŒå’Œæ£€æµ‹ï¼š
- åŸç†ï¼šè®¡ç®—æ•°æ®å†…å®¹å“ˆå¸Œå€¼
- ä¼˜ç‚¹ï¼šæ£€æµ‹ä»»æ„å­—æ®µå˜æ›´
- ç¼ºç‚¹ï¼šè®¡ç®—å¼€é”€å¤§

ğŸ“Š æ—¥å¿—è§£ææ£€æµ‹ï¼š
- åŸç†ï¼šè§£ææ•°æ®åº“å˜æ›´æ—¥å¿—
- ä¼˜ç‚¹ï¼šå®æ—¶ã€å®Œæ•´
- ç¼ºç‚¹ï¼šå¤æ‚ã€ä¾èµ–æ•°æ®åº“ç‰¹æ€§
```

### 4.2 æ—¶é—´æˆ³å˜æ›´æ£€æµ‹


```ruby
# åŸºäºæ—¶é—´æˆ³çš„å˜æ›´æ£€æµ‹
input {
  jdbc {
    jdbc_driver_library => "/opt/logstash/drivers/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/business_db"
    jdbc_user => "sync_user"
    jdbc_password => "sync_pass"
    
    # ğŸ”¸ æ£€æµ‹æœ€è¿‘å˜æ›´çš„æ•°æ®
    statement => "
      SELECT 
        id, product_name, category_id, price, stock,
        created_at, updated_at,
        -- è®¡ç®—å˜æ›´ç±»å‹
        CASE 
          WHEN created_at = updated_at THEN 'new'
          ELSE 'modified'
        END as change_type
      FROM products 
      WHERE updated_at > :sql_last_value
      ORDER BY updated_at ASC
    "
    
    tracking_column => "updated_at"
    tracking_column_type => "timestamp"
    last_run_metadata_path => "/data/logstash/.product_changes_last_run"
    schedule => "*/1 * * * *"              # æ¯åˆ†é’Ÿæ£€æµ‹ä¸€æ¬¡
    use_column_value => true
  }
}

filter {
  # ğŸ”¸ å˜æ›´æ—¶é—´åˆ†æ
  ruby {
    code => "
      created = LogStash::Timestamp.new(event.get('created_at'))
      updated = LogStash::Timestamp.new(event.get('updated_at'))
      
      # è®¡ç®—æ•°æ®å¹´é¾„ï¼ˆå°æ—¶ï¼‰
      data_age_hours = (Time.now - created.time) / 3600
      event.set('data_age_hours', data_age_hours.round(2))
      
      # è®¡ç®—æœ€åä¿®æ”¹é—´éš”ï¼ˆåˆ†é’Ÿï¼‰
      if created.time != updated.time
        last_change_minutes = (Time.now - updated.time) / 60
        event.set('last_change_minutes', last_change_minutes.round(2))
      end
    "
  }
  
  # ğŸ”¸ æ ¹æ®å˜æ›´é¢‘ç‡åˆ†ç±»
  if [last_change_minutes] and [last_change_minutes] < 60 {
    mutate { add_tag => ["hot_data"] }      # çƒ­æ•°æ®ï¼š1å°æ—¶å†…æœ‰å˜æ›´
  } else if [data_age_hours] and [data_age_hours] < 24 {
    mutate { add_tag => ["recent_data"] }   # æ–°æ•°æ®ï¼š24å°æ—¶å†…åˆ›å»º
  } else {
    mutate { add_tag => ["cold_data"] }     # å†·æ•°æ®ï¼šå…¶ä»–
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    # ğŸ”¸ æ ¹æ®æ•°æ®æ¸©åº¦ä½¿ç”¨ä¸åŒç´¢å¼•
    index => "products_%{[tags][0]}_%{+YYYY.MM.dd}"
    document_id => "%{id}"
  }
}
```

### 4.3 æ ¡éªŒå’Œå˜æ›´æ£€æµ‹


```ruby
# åŸºäºå†…å®¹æ ¡éªŒå’Œçš„å˜æ›´æ£€æµ‹
input {
  jdbc {
    jdbc_driver_library => "/opt/logstash/drivers/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/business_db"
    jdbc_user => "sync_user"
    jdbc_password => "sync_pass"
    
    # ğŸ”¸ æŸ¥è¯¢æ•°æ®å¹¶è®¡ç®—æ ¡éªŒå’Œ
    statement => "
      SELECT 
        id, name, email, phone, address, status,
        updated_at,
        -- MySQLè®¡ç®—å†…å®¹æ ¡éªŒå’Œ
        MD5(CONCAT_WS('|', name, email, phone, address, status)) as content_hash
      FROM customers 
      WHERE updated_at > :sql_last_value
      ORDER BY updated_at ASC
    "
    
    tracking_column => "updated_at"
    tracking_column_type => "timestamp"
    last_run_metadata_path => "/data/logstash/.customer_hash_last_run"
    schedule => "*/3 * * * *"
    use_column_value => true
  }
}

filter {
  # ğŸ”¸ æ£€æŸ¥å†…å®¹æ˜¯å¦çœŸæ­£å‘ç”Ÿå˜åŒ–
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "customers"
    query => "id:%{id}"
    fields => { "content_hash" => "stored_hash" }
    add_tag => ["hash_checked"]
  }
  
  # æ¯”è¾ƒæ ¡éªŒå’Œ
  if [stored_hash] and [stored_hash] == [content_hash] {
    # å†…å®¹æœªå˜åŒ–ï¼Œè·³è¿‡åŒæ­¥
    mutate {
      add_tag => ["no_content_change"]
    }
  } else {
    # å†…å®¹æœ‰å˜åŒ–ï¼Œéœ€è¦åŒæ­¥
    mutate {
      add_tag => ["content_changed"]
    }
  }
}

output {
  # åªåŒæ­¥çœŸæ­£æœ‰å†…å®¹å˜åŒ–çš„æ•°æ®
  if "content_changed" in [tags] {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "customers"
      document_id => "%{id}"
    }
  }
  
  # è®°å½•è·³è¿‡çš„åŒæ­¥
  if "no_content_change" in [tags] {
    file {
      path => "/var/log/logstash/skipped_sync.log"
      codec => json_lines
    }
  }
}
```

---

## 5. ğŸ“„ å¤§è¡¨åˆ†é¡µå¤„ç†æŠ€æœ¯


### 5.1 åˆ†é¡µå¤„ç†åŸç†


**ğŸ”¸ ä¸ºä»€ä¹ˆéœ€è¦åˆ†é¡µ**ï¼š

```
å¤§è¡¨é—®é¢˜ï¼š
ğŸ”¸ å†…å­˜æº¢å‡ºï¼šä¸€æ¬¡æŸ¥è¯¢ç™¾ä¸‡æ¡è®°å½•ä¼šè€—å°½å†…å­˜
ğŸ”¸ è¶…æ—¶é—®é¢˜ï¼šå¤§æŸ¥è¯¢å¯èƒ½è¶…è¿‡æ•°æ®åº“è¿æ¥è¶…æ—¶æ—¶é—´
ğŸ”¸ é”å®šé—®é¢˜ï¼šé•¿æ—¶é—´æŸ¥è¯¢å¯èƒ½é”å®šè¡¨ï¼Œå½±å“ä¸šåŠ¡
ğŸ”¸ ç½‘ç»œä¼ è¾“ï¼šå¤§é‡æ•°æ®ä¼ è¾“å¯èƒ½å¯¼è‡´ç½‘ç»œæ‹¥å¡

åˆ†é¡µè§£å†³æ–¹æ¡ˆï¼š
âœ… åˆ†æ‰¹å¤„ç†ï¼šæ¯æ¬¡åªå¤„ç†ä¸€å°æ‰¹æ•°æ®
âœ… å†…å­˜å¯æ§ï¼šå†…å­˜ä½¿ç”¨é‡å›ºå®š
âœ… ä¸­æ–­æ¢å¤ï¼šå¯ä»¥ä»ä¸­æ–­ç‚¹ç»§ç»­å¤„ç†
âœ… å¹¶å‘å‹å¥½ï¼šå‡å°‘å¯¹ä¸šåŠ¡ç³»ç»Ÿçš„å½±å“
```

### 5.2 åŸºç¡€åˆ†é¡µé…ç½®


```ruby
# åŸºç¡€åˆ†é¡µæŸ¥è¯¢é…ç½®
input {
  jdbc {
    jdbc_driver_library => "/opt/logstash/drivers/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/large_db"
    jdbc_user => "readonly_user"
    jdbc_password => "readonly_pass"
    
    # ğŸ”¸ åˆ†é¡µæŸ¥è¯¢è¯­å¥
    statement => "
      SELECT 
        id, order_number, customer_id, total_amount, 
        order_status, created_at, updated_at
      FROM orders 
      WHERE id > :sql_last_value 
      ORDER BY id ASC 
      LIMIT 1000                           -- æ¯é¡µ1000æ¡è®°å½•
    "
    
    # ğŸ”¸ åˆ†é¡µé…ç½®
    tracking_column => "id"                # ä½¿ç”¨IDä½œä¸ºåˆ†é¡µæ ‡è®°
    tracking_column_type => "numeric"      # æ•°å€¼ç±»å‹
    last_run_metadata_path => "/data/logstash/.orders_paging_last_run"
    
    # ğŸ”¸ æ€§èƒ½ä¼˜åŒ–é…ç½®
    jdbc_fetch_size => 1000               # JDBCè·å–å¤§å°
    jdbc_page_size => 1000                # é¡µé¢å¤§å°
    
    schedule => "0 2 * * *"               # æ¯å¤©å‡Œæ™¨2ç‚¹æ‰§è¡Œ
    use_column_value => true
  }
}

filter {
  # ğŸ”¸ æ·»åŠ åˆ†é¡µå¤„ç†æ ‡è®°
  ruby {
    code => "
      event.set('page_batch_id', (event.get('id').to_i / 1000).to_i + 1)
      event.set('processing_timestamp', Time.now.utc.iso8601)
    "
  }
  
  # æ•°æ®æ¸…æ´—
  mutate {
    convert => { "total_amount" => "float" }
    convert => { "order_status" => "integer" }
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "orders_batch_%{page_batch_id}"
    document_id => "%{id}"
  }
  
  # ğŸ”¸ åˆ†é¡µè¿›åº¦ç›‘æ§
  file {
    path => "/var/log/logstash/orders_paging_progress.log"
    codec => line {
      format => "[%{+ISO8601}] Processed batch %{page_batch_id}, last ID: %{id}"
    }
  }
}
```

### 5.3 é«˜çº§åˆ†é¡µå¤„ç†


```ruby
# æ™ºèƒ½åˆ†é¡µå¤„ç†é…ç½®
input {
  jdbc {
    jdbc_driver_library => "/opt/logstash/drivers/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/warehouse_db?useCursorFetch=true"
    jdbc_user => "etl_user"
    jdbc_password => "etl_pass"
    
    # ğŸ”¸ è‡ªé€‚åº”åˆ†é¡µæŸ¥è¯¢
    statement => "
      SELECT 
        id, product_id, warehouse_id, quantity, 
        unit_price, batch_number, expiry_date,
        created_at, updated_at,
        -- è®¡ç®—è®°å½•å¯†åº¦ï¼Œç”¨äºè°ƒæ•´åˆ†é¡µå¤§å°
        COUNT(*) OVER(PARTITION BY DATE(created_at)) as daily_count
      FROM inventory_transactions 
      WHERE id > :sql_last_value 
      ORDER BY id ASC 
      LIMIT :page_size                     -- åŠ¨æ€é¡µé¢å¤§å°
    "
    
    # ğŸ”¸ åŠ¨æ€å‚æ•°é…ç½®
    parameters => {
      "page_size" => 2000                  # é»˜è®¤é¡µé¢å¤§å°
    }
    
    tracking_column => "id"
    tracking_column_type => "numeric"
    last_run_metadata_path => "/data/logstash/.inventory_smart_paging_last_run"
    
    # ğŸ”¸ è¿æ¥æ± ä¼˜åŒ–
    jdbc_connection_timeout => 300         # è¿æ¥è¶…æ—¶5åˆ†é’Ÿ
    jdbc_validation_timeout => 60          # éªŒè¯è¶…æ—¶1åˆ†é’Ÿ
    jdbc_pool_timeout => 60                # è¿æ¥æ± è¶…æ—¶1åˆ†é’Ÿ
    
    schedule => "0 */4 * * *"              # æ¯4å°æ—¶æ‰§è¡Œä¸€æ¬¡
    use_column_value => true
  }
}

filter {
  # ğŸ”¸ æ™ºèƒ½åˆ†é¡µå¤§å°è°ƒæ•´
  ruby {
    code => "
      daily_count = event.get('daily_count').to_i
      current_page_size = 2000
      
      # æ ¹æ®æ•°æ®å¯†åº¦è°ƒæ•´é¡µé¢å¤§å°
      if daily_count > 100000
        # é«˜å¯†åº¦æ•°æ®ï¼Œä½¿ç”¨å°é¡µé¢
        recommended_page_size = 500
      elsif daily_count > 10000
        # ä¸­ç­‰å¯†åº¦æ•°æ®ï¼Œä½¿ç”¨ä¸­ç­‰é¡µé¢
        recommended_page_size = 1000
      else
        # ä½å¯†åº¦æ•°æ®ï¼Œä½¿ç”¨å¤§é¡µé¢
        recommended_page_size = 5000
      end
      
      event.set('recommended_page_size', recommended_page_size)
      event.set('data_density', daily_count)
    "
  }
  
  # ğŸ”¸ å¤„ç†è¿‡æœŸæ•°æ®æ ‡è®°
  if [expiry_date] {
    ruby {
      code => "
        expiry = Date.parse(event.get('expiry_date').to_s)
        today = Date.today
        
        if expiry < today
          event.set('is_expired', true)
          event.set('days_expired', (today - expiry).to_i)
        else
          event.set('is_expired', false)
          event.set('days_to_expiry', (expiry - today).to_i)
        end
      "
    }
  }
  
  # æ•°æ®è½¬æ¢
  mutate {
    convert => { "quantity" => "integer" }
    convert => { "unit_price" => "float" }
  }
}

output {
  # ğŸ”¸ æ ¹æ®æ•°æ®ç‰¹æ€§ä½¿ç”¨ä¸åŒç´¢å¼•ç­–ç•¥
  if [is_expired] == true {
    # è¿‡æœŸæ•°æ®å­˜å‚¨åˆ°ä¸“é—¨çš„ç´¢å¼•
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "inventory_expired_%{+YYYY.MM}"
      document_id => "%{id}"
    }
  } else {
    # æ­£å¸¸æ•°æ®æŒ‰æ—¶é—´åˆ†ç´¢å¼•
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "inventory_active_%{+YYYY.MM.dd}"
      document_id => "%{id}"
    }
  }
  
  # ğŸ”¸ åˆ†é¡µæ€§èƒ½ç›‘æ§
  if [recommended_page_size] != 2000 {
    file {
      path => "/var/log/logstash/page_size_recommendations.log"
      codec => json_lines
    }
  }
}
```

---

## 6. ğŸ”— å…³è”è¡¨æ•°æ®åŒæ­¥


### 6.1 å…³è”åŒæ­¥ç­–ç•¥


**ğŸ”¸ å¸¸è§å…³è”æ¨¡å¼**ï¼š

```
å…³è”ç±»å‹åˆ†æï¼š

ğŸ“‹ ä¸€å¯¹ä¸€å…³è”ï¼š
- ç”¨æˆ· â†” ç”¨æˆ·è¯¦æƒ…
- è®¢å• â†” è®¢å•è¯¦æƒ…
- ç­–ç•¥ï¼šJOINæŸ¥è¯¢æˆ–åˆ†æ­¥éª¤è·å–

ğŸ“‘ ä¸€å¯¹å¤šå…³è”ï¼š
- ç”¨æˆ· â†’ å¤šä¸ªè®¢å•
- è®¢å• â†’ å¤šä¸ªè®¢å•é¡¹
- ç­–ç•¥ï¼šåµŒå¥—æŸ¥è¯¢æˆ–åˆ†åˆ«åŒæ­¥

ğŸ”— å¤šå¯¹å¤šå…³è”ï¼š
- ç”¨æˆ· â†” è§’è‰²
- å•†å“ â†” æ ‡ç­¾
- ç­–ç•¥ï¼šä¸­é—´è¡¨å¤„ç†

ğŸŒ² å±‚çº§å…³è”ï¼š
- éƒ¨é—¨æ ‘ç»“æ„
- å•†å“åˆ†ç±»æ ‘
- ç­–ç•¥ï¼šé€’å½’æŸ¥è¯¢æˆ–è·¯å¾„ç¼–ç 
```

### 6.2 ä¸€å¯¹å¤šå…³è”åŒæ­¥


```ruby
# è®¢å•åŠè®¢å•é¡¹å…³è”åŒæ­¥
input {
  jdbc {
    jdbc_driver_library => "/opt/logstash/drivers/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/ecommerce_db"
    jdbc_user => "sync_user"
    jdbc_password => "sync_pass"
    
    # ğŸ”¸ å…³è”æŸ¥è¯¢è·å–å®Œæ•´è®¢å•ä¿¡æ¯
    statement => "
      SELECT 
        -- è®¢å•ä¸»è¡¨ä¿¡æ¯
        o.id as order_id,
        o.order_number,
        o.customer_id,
        o.total_amount,
        o.order_status,
        o.created_at as order_created_at,
        o.updated_at as order_updated_at,
        
        -- å®¢æˆ·ä¿¡æ¯
        c.name as customer_name,
        c.email as customer_email,
        c.phone as customer_phone,
        
        -- è®¢å•é¡¹ä¿¡æ¯ï¼ˆJSONèšåˆï¼‰
        COALESCE(
          JSON_ARRAYAGG(
            JSON_OBJECT(
              'item_id', oi.id,
              'product_id', oi.product_id,
              'product_name', p.name,
              'quantity', oi.quantity,
              'unit_price', oi.unit_price,
              'subtotal', oi.quantity * oi.unit_price
            )
          ), 
          JSON_ARRAY()
        ) as order_items
        
      FROM orders o
      LEFT JOIN customers c ON o.customer_id = c.id
      LEFT JOIN order_items oi ON o.id = oi.order_id
      LEFT JOIN products p ON oi.product_id = p.id
      WHERE o.updated_at > :sql_last_value
      GROUP BY o.id, c.id
      ORDER BY o.updated_at ASC
    "
    
    tracking_column => "order_updated_at"
    tracking_column_type => "timestamp"
    last_run_metadata_path => "/data/logstash/.order_relations_last_run"
    schedule => "*/5 * * * *"
    use_column_value => true
  }
}

filter {
  # ğŸ”¸ è§£æJSONæ ¼å¼çš„è®¢å•é¡¹
  json {
    source => "order_items"
    target => "items"
  }
  
  # ğŸ”¸ è®¡ç®—è®¢å•ç»Ÿè®¡ä¿¡æ¯
  ruby {
    code => "
      items = event.get('items') || []
      
      # ç»Ÿè®¡è®¢å•é¡¹æ•°é‡å’Œé‡‘é¢
      item_count = items.length
      calculated_total = 0
      
      items.each do |item|
        calculated_total += item['subtotal'].to_f if item['subtotal']
      end
      
      event.set('item_count', item_count)
      event.set('calculated_total', calculated_total)
      
      # æ£€æŸ¥é‡‘é¢ä¸€è‡´æ€§
      original_total = event.get('total_amount').to_f
      if (original_total - calculated_total).abs > 0.01
        event.set('amount_mismatch', true)
        event.set('amount_difference', original_total - calculated_total)
      end
    "
  }
  
  # ğŸ”¸ è®¢å•åˆ†ç±»æ ‡è®°
  if [item_count] == 0 {
    mutate { add_tag => ["empty_order"] }
  } else if [item_count] == 1 {
    mutate { add_tag => ["single_item_order"] }
  } else if [item_count] > 10 {
    mutate { add_tag => ["bulk_order"] }
  } else {
    mutate { add_tag => ["normal_order"] }
  }
  
  # é‡‘é¢å¼‚å¸¸æ ‡è®°
  if [amount_mismatch] {
    mutate { add_tag => ["amount_error"] }
  }
}

output {
  # æ­£å¸¸è®¢å•åŒæ­¥
  if "amount_error" not in [tags] and "empty_order" not in [tags] {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "orders_with_items_%{+YYYY.MM.dd}"
      document_id => "%{order_id}"
    }
  }
  
  # å¼‚å¸¸è®¢å•å•ç‹¬å¤„ç†
  if "amount_error" in [tags] or "empty_order" in [tags] {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "orders_exceptions"
      document_id => "%{order_id}"
    }
    
    # åŒæ—¶è®°å½•åˆ°é”™è¯¯æ—¥å¿—
    file {
      path => "/var/log/logstash/order_exceptions.log"
      codec => json_lines
    }
  }
}
```

### 6.3 å¤šå¯¹å¤šå…³è”åŒæ­¥


```ruby
# ç”¨æˆ·è§’è‰²æƒé™å…³è”åŒæ­¥
input {
  jdbc {
    jdbc_driver_library => "/opt/logstash/drivers/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/auth_db"
    jdbc_user => "auth_sync"
    jdbc_password => "auth_pass"
    
    # ğŸ”¸ å¤æ‚å¤šå¯¹å¤šå…³è”æŸ¥è¯¢
    statement => "
      SELECT 
        -- ç”¨æˆ·åŸºæœ¬ä¿¡æ¯
        u.id as user_id,
        u.username,
        u.email,
        u.status as user_status,
        u.created_at,
        u.updated_at,
        
        -- è§’è‰²ä¿¡æ¯ï¼ˆJSONèšåˆï¼‰
        COALESCE(
          JSON_ARRAYAGG(
            DISTINCT JSON_OBJECT(
              'role_id', r.id,
              'role_name', r.name,
              'role_code', r.code,
              'role_level', r.level
            )
          ),
          JSON_ARRAY()
        ) as user_roles,
        
        -- æƒé™ä¿¡æ¯ï¼ˆJSONèšåˆï¼‰
        COALESCE(
          JSON_ARRAYAGG(
            DISTINCT JSON_OBJECT(
              'permission_id', p.id,
              'permission_name', p.name,
              'permission_code', p.code,
              'resource_type', p.resource_type,
              'action_type', p.action_type
            )
          ),
          JSON_ARRAY()
        ) as user_permissions
        
      FROM users u
      LEFT JOIN user_roles ur ON u.id = ur.user_id
      LEFT JOIN roles r ON ur.role_id = r.id
      LEFT JOIN role_permissions rp ON r.id = rp.role_id
      LEFT JOIN permissions p ON rp.permission_id = p.id
      WHERE u.updated_at > :sql_last_value
      GROUP BY u.id
      ORDER BY u.updated_at ASC
    "
    
    tracking_column => "updated_at"
    tracking_column_type => "timestamp"
    last_run_metadata_path => "/data/logstash/.user_auth_relations_last_run"
    schedule => "*/10 * * * *"
    use_column_value => true
  }
}

filter {
  # ğŸ”¸ è§£æå…³è”æ•°æ®
  json {
    source => "user_roles"
    target => "roles"
  }
  
  json {
    source => "user_permissions"
    target => "permissions"
  }
  
  # ğŸ”¸ åˆ†æç”¨æˆ·æƒé™çº§åˆ«
  ruby {
    code => "
      roles = event.get('roles') || []
      permissions = event.get('permissions') || []
      
      # è®¡ç®—è§’è‰²ç»Ÿè®¡
      role_count = roles.length
      max_role_level = 0
      role_codes = []
      
      roles.each do |role|
        level = role['role_level'].to_i
        max_role_level = level if level > max_role_level
        role_codes << role['role_code']
      end
      
      # è®¡ç®—æƒé™ç»Ÿè®¡
      permission_count = permissions.length
      resource_types = []
      action_types = []
      
      permissions.each do |perm|
        resource_types << perm['resource_type']
        action_types << perm['action_type']
      end
      
      # è®¾ç½®ç»Ÿè®¡å­—æ®µ
      event.set('role_count', role_count)
      event.set('permission_count', permission_count)
      event.set('max_role_level', max_role_level)
      event.set('role_codes', role_codes.uniq)
      event.set('resource_types', resource_types.uniq)
      event.set('action_types', action_types.uniq)
      
      # ç”¨æˆ·åˆ†ç±»
      if max_role_level >= 90
        event.set('user_category', 'admin')
      elsif max_role_level >= 50
        event.set('user_category', 'manager')
      elsif permission_count > 0
        event.set('user_category', 'user')
      else
        event.set('user_category', 'guest')
      end
    "
  }
  
  # ğŸ”¸ ç‰¹æ®Šæƒé™æ ‡è®°
  if "admin" in [role_codes] {
    mutate { add_tag => ["super_admin"] }
  }
  
  if "system" in [resource_types] {
    mutate { add_tag => ["system_access"] }
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    # ğŸ”¸ æŒ‰ç”¨æˆ·ç±»åˆ«ä½¿ç”¨ä¸åŒç´¢å¼•
    index => "users_%{user_category}_%{+YYYY.MM.dd}"
    document_id => "%{user_id}"
  }
  
  # ğŸ”¸ é«˜æƒé™ç”¨æˆ·é¢å¤–å®¡è®¡
  if "super_admin" in [tags] or "system_access" in [tags] {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "user_audit_high_privilege"
      document_id => "%{user_id}_%{+yyyyMMddHHmmss}"
    }
  }
}
```

---

## 7. ğŸ”’ æ•°æ®ä¸€è‡´æ€§ä¿è¯


### 7.1 ä¸€è‡´æ€§çº§åˆ«å®šä¹‰


**ğŸ”¸ ä¸€è‡´æ€§ç±»å‹**ï¼š

```
ä¸€è‡´æ€§çº§åˆ«è¯´æ˜ï¼š

ğŸ”„ æœ€ç»ˆä¸€è‡´æ€§ï¼š
- å«ä¹‰ï¼šæ•°æ®æœ€ç»ˆä¼šè¾¾åˆ°ä¸€è‡´çŠ¶æ€
- ç‰¹ç‚¹ï¼šå…è®¸ä¸´æ—¶ä¸ä¸€è‡´
- é€‚ç”¨ï¼šå¤§å¤šæ•°ä¸šåŠ¡åœºæ™¯
- ç¤ºä¾‹ï¼šç”¨æˆ·ä¿¡æ¯åŒæ­¥ã€å•†å“ä¿¡æ¯

âš¡ å¼ºä¸€è‡´æ€§ï¼š
- å«ä¹‰ï¼šä»»ä½•æ—¶åˆ»æ•°æ®éƒ½ä¿æŒä¸€è‡´
- ç‰¹ç‚¹ï¼šåŒæ­¥æ“ä½œï¼Œæ€§èƒ½å½±å“å¤§
- é€‚ç”¨ï¼šå…³é”®ä¸šåŠ¡æ•°æ®
- ç¤ºä¾‹ï¼šè®¢å•çŠ¶æ€ã€åº“å­˜æ•°é‡

ğŸ• ä¼šè¯ä¸€è‡´æ€§ï¼š
- å«ä¹‰ï¼šåŒä¸€ä¼šè¯å†…ä¿è¯ä¸€è‡´æ€§
- ç‰¹ç‚¹ï¼šç”¨æˆ·æ„ŸçŸ¥ä¸€è‡´
- é€‚ç”¨ï¼šç”¨æˆ·ç›¸å…³æ“ä½œ
- ç¤ºä¾‹ï¼šè´­ç‰©è½¦ã€ä¸ªäººè®¾ç½®

ğŸ“Š å•è°ƒä¸€è‡´æ€§ï¼š
- å«ä¹‰ï¼šæ•°æ®å˜æ›´æŒ‰æ—¶é—´é¡ºåºç”Ÿæ•ˆ
- ç‰¹ç‚¹ï¼šä¸ä¼šå‡ºç°å›é€€ç°è±¡
- é€‚ç”¨ï¼šæ—¶åºæ•æ„Ÿæ•°æ®
- ç¤ºä¾‹ï¼šæ—¥å¿—æ•°æ®ã€ç›‘æ§æ•°æ®
```

### 7.2 äº‹åŠ¡ä¸€è‡´æ€§é…ç½®


```ruby
# äº‹åŠ¡çº§åˆ«çš„ä¸€è‡´æ€§ä¿è¯
input {
  jdbc {
    jdbc_driver_library => "/opt/logstash/drivers/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/financial_db?useSSL=true&serverTimezone=UTC"
    jdbc_user => "financial_sync"
    jdbc_password => "secure_financial_pass"
    
    # ğŸ”¸ ä½¿ç”¨äº‹åŠ¡ä¿è¯æ•°æ®ä¸€è‡´æ€§
    statement => "
      SELECT 
        t.id as transaction_id,
        t.account_id,
        t.transaction_type,
        t.amount,
        t.balance_before,
        t.balance_after,
        t.transaction_time,
        t.reference_number,
        
        -- è´¦æˆ·ä¿¡æ¯
        a.account_number,
        a.account_type,
        a.current_balance,
        
        -- éªŒè¯æ•°æ®ä¸€è‡´æ€§
        CASE 
          WHEN a.current_balance = t.balance_after THEN 'consistent'
          ELSE 'inconsistent'
        END as consistency_status,
        
        ABS(a.current_balance - t.balance_after) as balance_difference
        
      FROM financial_transactions t
      INNER JOIN accounts a ON t.account_id = a.id
      WHERE t.transaction_time > :sql_last_value
        AND t.status = 'completed'
      ORDER BY t.transaction_time ASC
    "
    
    tracking_column => "transaction_time"
    tracking_column_type => "timestamp"
    last_run_metadata_path => "/data/logstash/.financial_consistency_last_run"
    
    # ğŸ”¸ äº‹åŠ¡éš”ç¦»çº§åˆ«é…ç½®
    jdbc_default_timezone => "UTC"
    schedule => "*/30 * * * * *"             # æ¯30ç§’åŒæ­¥ä¸€æ¬¡
    use_column_value => true
  }
}

filter {
  # ğŸ”¸ ä¸€è‡´æ€§æ£€æŸ¥
  if [consistency_status] == "inconsistent" {
    mutate {
      add_tag => ["data_inconsistency"]
    }
    
    # ğŸ”¸ è®¡ç®—ä¸ä¸€è‡´ä¸¥é‡ç¨‹åº¦
    ruby {
      code => "
        difference = event.get('balance_difference').to_f
        amount = event.get('amount').to_f
        
        if difference > amount * 0.01  # å·®å¼‚è¶…è¿‡äº¤æ˜“é‡‘é¢çš„1%
          event.set('inconsistency_level', 'critical')
        elsif difference > 1.0         # å·®å¼‚è¶…è¿‡1å…ƒ
          event.set('inconsistency_level', 'warning')
        else
          event.set('inconsistency_level', 'minor')
        end
      "
    }
  }
  
  # ğŸ”¸ æ•°æ®å®Œæ•´æ€§éªŒè¯
  ruby {
    code => "
      balance_before = event.get('balance_before').to_f
      balance_after = event.get('balance_after').to_f
      amount = event.get('amount').to_f
      transaction_type = event.get('transaction_type')
      
      # éªŒè¯ä½™é¢è®¡ç®—é€»è¾‘
      expected_balance = balance_before
      
      case transaction_type
      when 'deposit', 'credit'
        expected_balance += amount
      when 'withdraw', 'debit'
        expected_balance -= amount
      end
      
      if (expected_balance - balance_after).abs < 0.01
        event.set('balance_calculation_valid', true)
      else
        event.set('balance_calculation_valid', false)
        event.set('expected_balance', expected_balance)
        event.add_tag(['calculation_error'])
      end
    "
  }
  
  # ğŸ”¸ æ—¶é—´æˆ³éªŒè¯
  ruby {
    code => "
      transaction_time = event.get('transaction_time')
      current_time = Time.now
      
      # æ£€æŸ¥æœªæ¥æ—¶é—´æˆ³
      if Time.parse(transaction_time.to_s) > current_time
        event.add_tag(['future_timestamp'])
      end
      
      # æ£€æŸ¥è¿‡è€çš„æ•°æ®ï¼ˆè¶…è¿‡24å°æ—¶ï¼‰
      if current_time - Time.parse(transaction_time.to_s) > 86400
        event.add_tag(['old_transaction'])
      end
    "
  }
}

output {
  # ğŸ”¸ æ­£å¸¸ä¸€è‡´çš„æ•°æ®
  if "data_inconsistency" not in [tags] and "calculation_error" not in [tags] {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "financial_transactions_%{+YYYY.MM.dd}"
      document_id => "%{transaction_id}"
      
      # è®¾ç½®å†™å…¥ä¸€è‡´æ€§çº§åˆ«
      doc_as_upsert => true
    }
  }
  
  # ğŸ”¸ æ•°æ®ä¸ä¸€è‡´çš„è®°å½•
  if "data_inconsistency" in [tags] {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "financial_inconsistencies"
      document_id => "%{transaction_id}"
    }
    
    # ğŸ”¸ criticalçº§åˆ«çš„ä¸ä¸€è‡´éœ€è¦ç«‹å³å‘Šè­¦
    if [inconsistency_level] == "critical" {
      http {
        url => "http://alert-system.internal/api/alerts"
        http_method => "post"
        headers => {
          "Content-Type" => "application/json"
        }
        mapping => {
          "alert_type" => "financial_inconsistency"
          "severity" => "critical"
          "transaction_id" => "%{transaction_id}"
          "account_id" => "%{account_id}"
          "balance_difference" => "%{balance_difference}"
          "timestamp" => "%{@timestamp}"
        }
      }
    }
  }
  
  # ğŸ”¸ è®¡ç®—é”™è¯¯çš„è®°å½•
  if "calculation_error" in [tags] {
    file {
      path => "/var/log/logstash/financial_calculation_errors.log"
      codec => json_lines
    }
  }
}
```

### 7.3 è¡¥å¿æœºåˆ¶é…ç½®


```ruby
# æ•°æ®ä¸€è‡´æ€§è¡¥å¿æœºåˆ¶
input {
  # ğŸ”¸ å®šæœŸä¸€è‡´æ€§æ£€æŸ¥ä»»åŠ¡
  jdbc {
    jdbc_driver_library => "/opt/logstash/drivers/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/business_db"
    jdbc_user => "consistency_checker"
    jdbc_password => "checker_pass"
    
    # ğŸ”¸ æŸ¥æ‰¾å¯èƒ½çš„ä¸ä¸€è‡´æ•°æ®
    statement => "
      SELECT 
        'source' as data_source,
        id, name, email, status, updated_at,
        MD5(CONCAT(name, email, status)) as source_hash
      FROM users 
      WHERE updated_at > DATE_SUB(NOW(), INTERVAL 1 HOUR)
      
      UNION ALL
      
      SELECT 
        'target' as data_source,
        id, name, email, status, updated_at,
        MD5(CONCAT(name, email, status)) as target_hash
      FROM users_replica
      WHERE updated_at > DATE_SUB(NOW(), INTERVAL 1 HOUR)
    "
    
    schedule => "0 */1 * * *"              # æ¯å°æ—¶æ‰§è¡Œä¸€æ¬¡
    last_run_metadata_path => "/data/logstash/.consistency_check_last_run"
  }
}

filter {
  # ğŸ”¸ æ•°æ®ä¸€è‡´æ€§æ¯”è¾ƒ
  aggregate {
    task_id => "%{id}"
    code => "
      # æ”¶é›†åŒä¸€IDçš„æºæ•°æ®å’Œç›®æ ‡æ•°æ®
      if event.get('data_source') == 'source'
        map['source_hash'] = event.get('source_hash')
        map['source_data'] = event.to_hash
      else
        map['target_hash'] = event.get('target_hash')
        map['target_data'] = event.to_hash
      end
      
      # å½“æ”¶é›†åˆ°ä¸¤è¾¹æ•°æ®æ—¶è¿›è¡Œæ¯”è¾ƒ
      if map['source_hash'] && map['target_hash']
        if map['source_hash'] != map['target_hash']
          # å‘ç°ä¸ä¸€è‡´ï¼Œåˆ›å»ºä¿®å¤äº‹ä»¶
          new_event = LogStash::Event.new
          new_event.set('inconsistency_detected', true)
          new_event.set('user_id', event.get('id'))
          new_event.set('source_hash', map['source_hash'])
          new_event.set('target_hash', map['target_hash'])
          new_event.set('source_data', map['source_data'])
          new_event.set('target_data', map['target_data'])
          new_event.set('detection_time', Time.now.utc.iso8601)
          
          yield new_event
        end
        
        # æ¸…ç†å·²å¤„ç†çš„æ•°æ®
        map.clear
      end
      
      event.cancel()  # å–æ¶ˆåŸå§‹äº‹ä»¶
    "
    push_previous_map_as_event => false
    timeout => 300
  }
  
  # ğŸ”¸ ç”Ÿæˆä¿®å¤ç­–ç•¥
  if [inconsistency_detected] {
    ruby {
      code => "
        source_data = event.get('source_data')
        target_data = event.get('target_data')
        
        # æ¯”è¾ƒæ›´æ–°æ—¶é—´ï¼Œé€‰æ‹©è¾ƒæ–°çš„æ•°æ®ä½œä¸ºæ­£ç¡®æ•°æ®
        source_time = Time.parse(source_data['updated_at'].to_s)
        target_time = Time.parse(target_data['updated_at'].to_s)
        
        if source_time > target_time
          event.set('repair_action', 'update_target')
          event.set('correct_data', source_data)
        else
          event.set('repair_action', 'update_source')
          event.set('correct_data', target_data)
        end
        
        event.set('time_difference_seconds', (source_time - target_time).abs)
      "
    }
  }
}

output {
  # ğŸ”¸ è®°å½•å‘ç°çš„ä¸ä¸€è‡´é—®é¢˜
  if [inconsistency_detected] {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "data_consistency_issues"
      document_id => "%{user_id}_%{+yyyyMMddHHmmss}"
    }
    
    # ğŸ”¸ è‡ªåŠ¨ä¿®å¤æœºåˆ¶ï¼ˆè°¨æ…ä½¿ç”¨ï¼‰
    if [time_difference_seconds] and [time_difference_seconds] < 3600 {  # 1å°æ—¶å†…çš„å·®å¼‚æ‰è‡ªåŠ¨ä¿®å¤
      if [repair_action] == "update_target" {
        elasticsearch {
          hosts => ["localhost:9200"]
          index => "users_replica"
          document_id => "%{user_id}"
          action => "update"
          doc => "%{correct_data}"
        }
      }
      
      # è®°å½•è‡ªåŠ¨ä¿®å¤æ“ä½œ
      file {
        path => "/var/log/logstash/auto_repair_actions.log"
        codec => json_lines
      }
    }
  }
}
```

---

## 8. ğŸ“Š åŒæ­¥çŠ¶æ€ç›‘æ§


### 8.1 ç›‘æ§æŒ‡æ ‡ä½“ç³»


**ğŸ”¸ å…³é”®ç›‘æ§æŒ‡æ ‡**ï¼š

```
ç›‘æ§ç»´åº¦åˆ†æï¼š

ğŸ“ˆ æ€§èƒ½æŒ‡æ ‡ï¼š
- åŒæ­¥é€Ÿåº¦ï¼šè®°å½•/ç§’
- å»¶è¿Ÿæ—¶é—´ï¼šæ•°æ®æ–°é²œåº¦
- ååé‡ï¼šå¤„ç†é‡ç»Ÿè®¡
- èµ„æºä½¿ç”¨ï¼šCPUã€å†…å­˜ã€ç½‘ç»œ

ğŸ“Š è´¨é‡æŒ‡æ ‡ï¼š
- æˆåŠŸç‡ï¼šåŒæ­¥æˆåŠŸæ¯”ä¾‹
- é”™è¯¯ç‡ï¼šå„ç±»é”™è¯¯ç»Ÿè®¡
- æ•°æ®å®Œæ•´æ€§ï¼šä¸¢å¤±/é‡å¤æ£€æµ‹
- ä¸€è‡´æ€§åå·®ï¼šæºç›®æ ‡æ•°æ®å¯¹æ¯”

âš ï¸ å¼‚å¸¸æŒ‡æ ‡ï¼š
- åŒæ­¥ä¸­æ–­ï¼šå¼‚å¸¸åœæ­¢æ¬¡æ•°
- è¶…æ—¶äº‹ä»¶ï¼šé•¿æ—¶é—´æ— å“åº”
- æ•°æ®å¼‚å¸¸ï¼šæ ¼å¼/ç±»å‹é”™è¯¯
- è¿æ¥é—®é¢˜ï¼šç½‘ç»œ/æ•°æ®åº“æ•…éšœ

ğŸ• ä¸šåŠ¡æŒ‡æ ‡ï¼š
- æ•°æ®æ–°é²œåº¦ï¼šæœ€åæ›´æ–°æ—¶é—´
- è¦†ç›–èŒƒå›´ï¼šåŒæ­¥æ•°æ®èŒƒå›´
- å½±å“è¯„ä¼°ï¼šä¸šåŠ¡å½±å“ç¨‹åº¦
```

### 8.2 å®æ—¶ç›‘æ§é…ç½®


```ruby
# åŒæ­¥çŠ¶æ€å®æ—¶ç›‘æ§
input {
  jdbc {
    jdbc_driver_library => "/opt/logstash/drivers/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/monitoring_db"
    jdbc_user => "monitor_user"
    jdbc_password => "monitor_pass"
    
    statement => "
      SELECT 
        id, table_name, sync_type, status,
        records_processed, records_failed,
        start_time, end_time, duration_seconds,
        last_record_timestamp, next_schedule_time,
        error_message, performance_metrics,
        created_at, updated_at
      FROM sync_monitoring_log 
      WHERE updated_at > :sql_last_value
      ORDER BY updated_at ASC
    "
    
    tracking_column => "updated_at"
    tracking_column_type => "timestamp"
    last_run_metadata_path => "/data/logstash/.sync_monitoring_last_run"
    schedule => "*/1 * * * *"               # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
    use_column_value => true
    
    # æ·»åŠ åŒæ­¥ç›‘æ§æ ‡è¯†
    add_field => { "[@metadata][pipeline_type]" => "sync_monitoring" }
  }
}

filter {
  # ğŸ”¸ è®¡ç®—ç›‘æ§æŒ‡æ ‡
  ruby {
    code => "
      # è®¡ç®—åŒæ­¥æ€§èƒ½æŒ‡æ ‡
      duration = event.get('duration_seconds').to_f
      processed = event.get('records_processed').to_i
      failed = event.get('records_failed').to_i
      
      if duration > 0
        # è®¡ç®—å¤„ç†é€Ÿåº¦ï¼ˆè®°å½•/ç§’ï¼‰
        processing_rate = processed / duration
        event.set('processing_rate_per_second', processing_rate.round(2))
        
        # è®¡ç®—æˆåŠŸç‡
        total_records = processed + failed
        if total_records > 0
          success_rate = (processed.to_f / total_records * 100).round(2)
          event.set('success_rate_percentage', success_rate)
        end
      end
      
      # è®¡ç®—æ•°æ®æ–°é²œåº¦ï¼ˆåˆ†é’Ÿï¼‰
      last_record_time = event.get('last_record_timestamp')
      if last_record_time
        freshness_minutes = (Time.now - Time.parse(last_record_time.to_s)) / 60
        event.set('data_freshness_minutes', freshness_minutes.round(2))
      end
      
      # åŒæ­¥çŠ¶æ€åˆ†ç±»
      status = event.get('status')
      case status
      when 'completed'
        if failed == 0
          event.set('sync_health', 'healthy')
        else
          event.set('sync_health', 'warning')
        end
      when 'failed', 'error'
        event.set('sync_health', 'critical')
      when 'running'
        event.set('sync_health', 'active')
      else
        event.set('sync_health', 'unknown')
      end
    "
  }
  
  # ğŸ”¸ æ€§èƒ½é˜ˆå€¼å‘Šè­¦åˆ¤æ–­
  if [processing_rate_per_second] and [processing_rate_per_second] < 10 {
    mutate { add_tag => ["slow_performance"] }
  }
  
  if [success_rate_percentage] and [success_rate_percentage] < 95 {
    mutate { add_tag => ["low_success_rate"] }
  }
  
  if [data_freshness_minutes] and [data_freshness_minutes] > 60 {
    mutate { add_tag => ["stale_data"] }
  }
  
  # ğŸ”¸ é”™è¯¯æ¨¡å¼åˆ†æ
  if [error_message] {
    # åˆ†ç±»å¸¸è§é”™è¯¯
    if [error_message] =~ /connection/i {
      mutate { add_tag => ["connection_error"] }
    } else if [error_message] =~ /timeout/i {
      mutate { add_tag => ["timeout_error"] }
    } else if [error_message] =~ /sql/i {
      mutate { add_tag => ["sql_error"] }
    } else {
      mutate { add_tag => ["other_error"] }
    }
  }
}

output {
  # ğŸ”¸ ç›‘æ§æ•°æ®å­˜å‚¨
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "sync_monitoring_%{+YYYY.MM.dd}"
    document_id => "%{id}"
  }
  
  # ğŸ”¸ æ€§èƒ½å‘Šè­¦
  if "slow_performance" in [tags] or "low_success_rate" in [tags] {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "sync_performance_alerts"
      document_id => "%{table_name}_%{+yyyyMMddHHmm}"
    }
    
    # HTTPå‘Šè­¦é€šçŸ¥
    http {
      url => "http://alert-manager.internal/api/alerts"
      http_method => "post"
      headers => { "Content-Type" => "application/json" }
      mapping => {
        "alert_type" => "sync_performance"
        "table_name" => "%{table_name}"
        "processing_rate" => "%{processing_rate_per_second}"
        "success_rate" => "%{success_rate_percentage}"
        "severity" => "warning"
        "timestamp" => "%{@timestamp}"
      }
    }
  }
  
  # ğŸ”¸ æ•°æ®æ–°é²œåº¦å‘Šè­¦
  if "stale_data" in [tags] {
    file {
      path => "/var/log/logstash/stale_data_alerts.log"
      codec => line {
        format => "[%{+ISO8601}] STALE DATA: %{table_name} - last update %{data_freshness_minutes} minutes ago"
      }
    }
  }
}
```

### 8.3 ç›‘æ§ä»ªè¡¨æ¿æ•°æ®


```ruby
# ç”Ÿæˆç›‘æ§ä»ªè¡¨æ¿æ•°æ®
input {
  # ğŸ”¸ å®šæœŸèšåˆç»Ÿè®¡ä»»åŠ¡
  http_poller {
    urls => {
      sync_stats => {
        method => "get"
        url => "http://localhost:9200/sync_monitoring_*/_search"
        headers => { "Content-Type" => "application/json" }
        body => '{
          "size": 0,
          "aggs": {
            "by_table": {
              "terms": { "field": "table_name.keyword", "size": 100 },
              "aggs": {
                "avg_processing_rate": { "avg": { "field": "processing_rate_per_second" } },
                "avg_success_rate": { "avg": { "field": "success_rate_percentage" } },
                "total_processed": { "sum": { "field": "records_processed" } },
                "total_failed": { "sum": { "field": "records_failed" } },
                "last_sync": { "max": { "field": "end_time" } }
              }
            },
            "overall_stats": {
              "global": {},
              "aggs": {
                "total_tables": { "cardinality": { "field": "table_name.keyword" } },
                "healthy_syncs": {
                  "filter": { "term": { "sync_health.keyword": "healthy" } },
                  "aggs": { "count": { "value_count": { "field": "id" } } }
                },
                "failed_syncs": {
                  "filter": { "term": { "sync_health.keyword": "critical" } },
                  "aggs": { "count": { "value_count": { "field": "id" } } }
                }
              }
            }
          }
        }'
      }
    }
    request_timeout => 60
    schedule => { "every" => "5m" }
    codec => "json"
    add_field => { "[@metadata][data_type]" => "dashboard_stats" }
  }
}

filter {
  # ğŸ”¸ è§£æèšåˆç»Ÿè®¡ç»“æœ
  if [@metadata][data_type] == "dashboard_stats" {
    ruby {
      code => "
        # è§£æèšåˆç»“æœ
        aggs = event.get('[aggregations]')
        
        if aggs
          # æ•´ä½“ç»Ÿè®¡
          overall = aggs['overall_stats']
          if overall
            event.set('total_tables', overall['total_tables']['value'])
            event.set('healthy_syncs', overall['healthy_syncs']['count']['value'])
            event.set('failed_syncs', overall['failed_syncs']['count']['value'])
          end
          
          # å„è¡¨ç»Ÿè®¡
          by_table = aggs['by_table']['buckets']
          table_stats = []
          
          by_table.each do |bucket|
            table_stat = {
              'table_name' => bucket['key'],
              'sync_count' => bucket['doc_count'],
              'avg_processing_rate' => bucket['avg_processing_rate']['value'],
              'avg_success_rate' => bucket['avg_success_rate']['value'],
              'total_processed' => bucket['total_processed']['value'],
              'total_failed' => bucket['total_failed']['value'],
              'last_sync_time' => bucket['last_sync']['value_as_string']
            }
            table_stats << table_stat
          end
          
          event.set('table_statistics', table_stats)
          
          # è®¡ç®—å¥åº·åº¦æŒ‡æ ‡
          total_syncs = event.get('healthy_syncs').to_i + event.get('failed_syncs').to_i
          if total_syncs > 0
            health_percentage = (event.get('healthy_syncs').to_f / total_syncs * 100).round(2)
            event.set('overall_health_percentage', health_percentage)
          end
          
          # è®¾ç½®ä»ªè¡¨æ¿æ—¶é—´æˆ³
          event.set('dashboard_generated_at', Time.now.utc.iso8601)
        end
      "
    }
  }
}

output {
  # ğŸ”¸ ä»ªè¡¨æ¿æ•°æ®å­˜å‚¨
  if [@metadata][data_type] == "dashboard_stats" {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "sync_dashboard"
      document_id => "current_stats"
      action => "index"
    }
    
    # åŒæ—¶ä¿å­˜å†å²å¿«ç…§
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "sync_dashboard_history_%{+YYYY.MM.dd}"
      document_id => "%{+yyyyMMddHHmmss}"
    }
  }
}
```

---

## 9. ğŸ”„ é”™è¯¯æ¢å¤æœºåˆ¶


### 9.1 é”™è¯¯åˆ†ç±»ä¸å¤„ç†ç­–ç•¥


**ğŸ”¸ é”™è¯¯ç±»å‹åˆ†æ**ï¼š

```
é”™è¯¯åˆ†ç±»ä½“ç³»ï¼š

ğŸ”Œ è¿æ¥ç±»é”™è¯¯ï¼š
- æ•°æ®åº“è¿æ¥è¶…æ—¶
- ç½‘ç»œä¸­æ–­
- è®¤è¯å¤±è´¥
- å¤„ç†ï¼šé‡è¯•æœºåˆ¶ + è¿æ¥æ± ç®¡ç†

â° è¶…æ—¶ç±»é”™è¯¯ï¼š
- æŸ¥è¯¢æ‰§è¡Œè¶…æ—¶
- å“åº”ç­‰å¾…è¶…æ—¶
- å¤„ç†ï¼šè°ƒæ•´è¶…æ—¶å‚æ•° + åˆ†æ‰¹å¤„ç†

ğŸ“Š æ•°æ®ç±»é”™è¯¯ï¼š
- æ•°æ®æ ¼å¼é”™è¯¯
- çº¦æŸå†²çª
- ç¼–ç é—®é¢˜
- å¤„ç†ï¼šæ•°æ®æ¸…æ´— + å¼‚å¸¸æ•°æ®éš”ç¦»

âš™ï¸ ç³»ç»Ÿç±»é”™è¯¯ï¼š
- å†…å­˜ä¸è¶³
- ç£ç›˜ç©ºé—´æ»¡
- æƒé™é—®é¢˜
- å¤„ç†ï¼šèµ„æºç›‘æ§ + é™çº§ç­–ç•¥
```

### 9.2 è‡ªåŠ¨é‡è¯•æœºåˆ¶


```ruby
# æ™ºèƒ½é‡è¯•æœºåˆ¶é…ç½®
input {
  jdbc {
    jdbc_driver_library => "/opt/logstash/drivers/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/business_db?autoReconnect=true&failOverReadOnly=false&maxReconnects=3"
    jdbc_user => "resilient_user"
    jdbc_password => "resilient_pass"
    
    statement => "
      SELECT id, name, status, data_content, updated_at
      FROM business_data 
      WHERE updated_at > :sql_last_value
      ORDER BY updated_at ASC
    "
    
    tracking_column => "updated_at"
    tracking_column_type => "timestamp"
    last_run_metadata_path => "/data/logstash/.resilient_sync_last_run"
    
    # ğŸ”¸ è¿æ¥é‡è¯•é…ç½®
    jdbc_connection_timeout => 30
    jdbc_validation_timeout => 10
    jdbc_pool_timeout => 30
    
    # ğŸ”¸ é”™è¯¯æ¢å¤è®¾ç½®
    schedule => "*/2 * * * *"
    use_column_value => true
  }
}

filter {
  # ğŸ”¸ æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
  if ![id] or ![name] {
    mutate {
      add_tag => ["incomplete_data"]
      add_field => { "error_type" => "missing_required_fields" }
    }
  }
  
  # ğŸ”¸ æ•°æ®æ ¼å¼éªŒè¯
  if [data_content] {
    ruby {
      code => "
        begin
          # å°è¯•è§£æJSONæ ¼å¼æ•°æ®
          JSON.parse(event.get('data_content'))
          event.set('data_format_valid', true)
        rescue JSON::ParserError => e
          event.set('data_format_valid', false)
          event.set('json_parse_error', e.message)
          event.add_tag(['json_format_error'])
        end
      "
    }
  }
  
  # ğŸ”¸ æ·»åŠ é‡è¯•è®¡æ•°å™¨
  ruby {
    code => "
      # ä»æ–‡ä»¶è¯»å–é‡è¯•æ¬¡æ•°
      retry_file = '/tmp/logstash_retry_count_' + event.get('id').to_s
      
      if File.exist?(retry_file)
        retry_count = File.read(retry_file).to_i
        event.set('retry_count', retry_count)
      else
        event.set('retry_count', 0)
      end
    "
  }
}

output {
  # ğŸ”¸ æ­£å¸¸æ•°æ®å¤„ç†
  if "incomplete_data" not in [tags] and "json_format_error" not in [tags] {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "business_data_%{+YYYY.MM.dd}"
      document_id => "%{id}"
      
      # ğŸ”¸ å¤±è´¥é‡è¯•é…ç½®
      retry_on_conflict => 3
      retry_on_failure => 5
    }
    
    # ğŸ”¸ æˆåŠŸåæ¸…ç†é‡è¯•è®¡æ•°
    ruby {
      code => "
        retry_file = '/tmp/logstash_retry_count_' + event.get('id').to_s
        File.delete(retry_file) if File.exist?(retry_file)
      "
    }
  }
  
  # ğŸ”¸ é”™è¯¯æ•°æ®é‡è¯•æœºåˆ¶
  if ("incomplete_data" in [tags] or "json_format_error" in [tags]) and [retry_count] < 3 {
    # å¢åŠ é‡è¯•è®¡æ•°
    ruby {
      code => "
        retry_count = event.get('retry_count').to_i + 1
        retry_file = '/tmp/logstash_retry_count_' + event.get('id').to_s
        File.write(retry_file, retry_count.to_s)
        event.set('retry_count', retry_count)
      "
    }
    
    # å»¶è¿Ÿé‡è¯•é˜Ÿåˆ—
    redis {
      host => "localhost"
      port => 6379
      data_type => "list"
      key => "logstash_retry_queue"
      codec => json
    }
    
    # è®°å½•é‡è¯•æ—¥å¿—
    file {
      path => "/var/log/logstash/retry_attempts.log"
      codec => line {
        format => "[%{+ISO8601}] Retry attempt %{retry_count} for record %{id}: %{error_type}"
      }
    }
  }
  
  # ğŸ”¸ è¶…è¿‡é‡è¯•æ¬¡æ•°çš„æ•°æ®
  if [retry_count] >= 3 {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "failed_data_archive"
      document_id => "%{id}"
    }
    
    # æ¸…ç†é‡è¯•æ–‡ä»¶
    ruby {
      code => "
        retry_file = '/tmp/logstash_retry_count_' + event.get('id').to_s
        File.delete(retry_file) if File.exist?(retry_file)
      "
    }
    
    # å‘é€å¤±è´¥å‘Šè­¦
    http {
      url => "http://alert-system.internal/api/data-failures"
      http_method => "post"
      headers => { "Content-Type" => "application/json" }
      mapping => {
        "failed_record_id" => "%{id}"
        "error_type" => "%{error_type}"
        "retry_count" => "%{retry_count}"
        "timestamp" => "%{@timestamp}"
      }
    }
  }
}
```

### 9.3 æ–­ç‚¹ç»­ä¼ æœºåˆ¶


```ruby
# æ–­ç‚¹ç»­ä¼ æ¢å¤é…ç½®
input {
  jdbc {
    jdbc_driver_library => "/opt/logstash/drivers/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/large_dataset_db"
    jdbc_user => "recovery_user"
    jdbc_password => "recovery_pass"
    
    # ğŸ”¸ æ”¯æŒæ–­ç‚¹ç»­ä¼ çš„æŸ¥è¯¢
    statement => "
      SELECT 
        id, batch_number, data_payload, checksum,
        processing_status, created_at, updated_at
      FROM large_data_batches 
      WHERE 
        id > :sql_last_value
        AND (processing_status IS NULL OR processing_status = 'pending')
      ORDER BY id ASC 
      LIMIT 500
    "
    
    tracking_column => "id"
    tracking_column_type => "numeric"
    last_run_metadata_path => "/data/logstash/.recovery_sync_last_run"
    
    # ğŸ”¸ æ¢å¤æ£€æŸ¥é…ç½®
    schedule => "*/3 * * * *"
    use_column_value => true
  }
}

filter {
  # ğŸ”¸ æ£€æŸ¥å¤„ç†çŠ¶æ€
  ruby {
    code => "
      # è¯»å–ä¸Šæ¬¡ä¸­æ–­ä¿¡æ¯
      interrupt_file = '/data/logstash/sync_interrupt_info.json'
      
      if File.exist?(interrupt_file)
        begin
          interrupt_info = JSON.parse(File.read(interrupt_file))
          event.set('last_interrupt_time', interrupt_info['interrupt_time'])
          event.set('last_processed_id', interrupt_info['last_processed_id'])
          event.set('recovery_mode', true)
        rescue
          event.set('recovery_mode', false)
        end
      else
        event.set('recovery_mode', false)
      end
    "
  }
  
  # ğŸ”¸ æ•°æ®å®Œæ•´æ€§æ ¡éªŒ
  if [checksum] {
    ruby {
      code => "
        # è®¡ç®—æ•°æ®æ ¡éªŒå’Œ
        data_payload = event.get('data_payload')
        if data_payload
          calculated_checksum = Digest::MD5.hexdigest(data_payload)
          stored_checksum = event.get('checksum')
          
          if calculated_checksum == stored_checksum
            event.set('checksum_valid', true)
          else
            event.set('checksum_valid', false)
            event.add_tag(['checksum_mismatch'])
          end
        end
      "
    }
  }
  
  # ğŸ”¸ æ›´æ–°å¤„ç†çŠ¶æ€
  mutate {
    add_field => { 
      "processing_started_at" => "%{@timestamp}"
      "processing_status" => "in_progress"
    }
  }
}

output {
  # ğŸ”¸ æ­£å¸¸æ•°æ®å¤„ç†
  if "checksum_mismatch" not in [tags] {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "large_dataset_%{+YYYY.MM.dd}"
      document_id => "%{id}"
    }
    
    # ğŸ”¸ æ›´æ–°æºæ•°æ®åº“å¤„ç†çŠ¶æ€
    jdbc {
      driver_class => "com.mysql.cj.jdbc.Driver"
      connection_string => "jdbc:mysql://localhost:3306/large_dataset_db"
      username => "recovery_user"
      password => "recovery_pass"
      statement => [
        "UPDATE large_data_batches SET processing_status = 'completed', processed_at = NOW() WHERE id = ?",
        "%{id}"
      ]
    }
    
    # ğŸ”¸ æ›´æ–°æ–­ç‚¹ç»­ä¼ ä¿¡æ¯
    ruby {
      code => "
        interrupt_info = {
          'last_processed_id' => event.get('id'),
          'last_update_time' => Time.now.utc.iso8601,
          'total_processed' => (event.get('total_processed') || 0) + 1
        }
        
        File.write('/data/logstash/sync_interrupt_info.json', interrupt_info.to_json)
      "
    }
  }
  
  # ğŸ”¸ æ ¡éªŒå¤±è´¥æ•°æ®å¤„ç†
  if "checksum_mismatch" in [tags] {
    # æ ‡è®°ä¸ºå¤±è´¥
    jdbc {
      driver_class => "com.mysql.cj.jdbc.Driver"
      connection_string => "jdbc:mysql://localhost:3306/large_dataset_db"
      username => "recovery_user"
      password => "recovery_pass"
      statement => [
        "UPDATE large_data_batches SET processing_status = 'checksum_failed', failed_at = NOW() WHERE id = ?",
        "%{id}"
      ]
    }
    
    # è®°å½•åˆ°é”™è¯¯æ—¥å¿—
    file {
      path => "/var/log/logstash/checksum_failures.log"
      codec => json_lines
    }
  }
}
```

### 9.4 å¼‚å¸¸å¤„ç†ä¸å‘Šè­¦


```ruby
# å…¨é¢å¼‚å¸¸å¤„ç†é…ç½®
input {
  jdbc {
    jdbc_driver_library => "/opt/logstash/drivers/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    jdbc_connection_string => "jdbc:mysql://localhost:3306/critical_db"
    jdbc_user => "critical_sync"
    jdbc_password => "critical_pass"
    
    statement => "
      SELECT 
        id, transaction_id, amount, account_id, 
        transaction_type, status, created_at, updated_at
      FROM critical_transactions 
      WHERE updated_at > :sql_last_value
      ORDER BY updated_at ASC
    "
    
    tracking_column => "updated_at"
    tracking_column_type => "timestamp"
    last_run_metadata_path => "/data/logstash/.critical_sync_last_run"
    schedule => "*/1 * * * *"
    use_column_value => true
  }
}

filter {
  # ğŸ”¸ å¼‚å¸¸æ£€æµ‹
  ruby {
    code => "
      begin
        # ä¸šåŠ¡è§„åˆ™éªŒè¯
        amount = event.get('amount').to_f
        transaction_type = event.get('transaction_type')
        
        # æ£€æŸ¥é‡‘é¢åˆç†æ€§
        if amount <= 0
          event.add_tag(['invalid_amount'])
          event.set('validation_error', 'Amount must be positive')
        end
        
        # æ£€æŸ¥é‡‘é¢ä¸Šé™ï¼ˆå‡è®¾å•ç¬”ä¸èƒ½è¶…è¿‡100ä¸‡ï¼‰
        if amount > 1000000
          event.add_tag(['excessive_amount'])
          event.set('validation_error', 'Amount exceeds limit')
        end
        
        # æ£€æŸ¥äº¤æ˜“ç±»å‹
        valid_types = ['deposit', 'withdraw', 'transfer', 'payment']
        unless valid_types.include?(transaction_type)
          event.add_tag(['invalid_transaction_type'])
          event.set('validation_error', 'Invalid transaction type')
        end
        
      rescue => e
        event.add_tag(['processing_exception'])
        event.set('exception_message', e.message)
        event.set('exception_class', e.class.name)
      end
    "
  }
  
  # ğŸ”¸ é”™è¯¯çº§åˆ«åˆ†ç±»
  if "invalid_amount" in [tags] or "invalid_transaction_type" in [tags] {
    mutate { 
      add_field => { "error_severity" => "medium" }
      add_field => { "error_category" => "data_validation" }
    }
  }
  
  if "excessive_amount" in [tags] {
    mutate { 
      add_field => { "error_severity" => "high" }
      add_field => { "error_category" => "business_rule" }
    }
  }
  
  if "processing_exception" in [tags] {
    mutate { 
      add_field => { "error_severity" => "critical" }
      add_field => { "error_category" => "system_error" }
    }
  }
}

output {
  # ğŸ”¸ æ­£å¸¸æ•°æ®å¤„ç†
  if ![error_severity] {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "critical_transactions_%{+YYYY.MM.dd}"
      document_id => "%{transaction_id}"
    }
  }
  
  # ğŸ”¸ å¼‚å¸¸æ•°æ®å¤„ç†
  if [error_severity] {
    # æ‰€æœ‰å¼‚å¸¸éƒ½è®°å½•åˆ°å¼‚å¸¸ç´¢å¼•
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "transaction_exceptions"
      document_id => "%{transaction_id}_%{+yyyyMMddHHmmss}"
    }
    
    # ğŸ”¸ é«˜ä¸¥é‡æ€§é”™è¯¯ç«‹å³å‘Šè­¦
    if [error_severity] == "critical" or [error_severity] == "high" {
      # å‘é€ç´§æ€¥å‘Šè­¦
      http {
        url => "http://alert-system.internal/api/urgent-alerts"
        http_method => "post"
        headers => { "Content-Type" => "application/json" }
        mapping => {
          "alert_type" => "critical_transaction_error"
          "transaction_id" => "%{transaction_id}"
          "account_id" => "%{account_id}"
          "amount" => "%{amount}"
          "error_category" => "%{error_category}"
          "error_message" => "%{validation_error}"
          "severity" => "%{error_severity}"
          "timestamp" => "%{@timestamp}"
        }
      }
      
      # å‘é€çŸ­ä¿¡å‘Šè­¦ï¼ˆæ¨¡æ‹Ÿï¼‰
      file {
        path => "/var/log/logstash/sms_alerts.log"
        codec => line {
          format => "SMS Alert: Critical transaction error - ID: %{transaction_id}, Amount: %{amount}, Error: %{validation_error}"
        }
      }
    }
    
    # ğŸ”¸ ä¸­ç­‰ä¸¥é‡æ€§é”™è¯¯é‚®ä»¶å‘Šè­¦
    if [error_severity] == "medium" {
      file {
        path => "/var/log/logstash/email_alerts.log"
        codec => line {
          format => "Email Alert: Transaction validation error - ID: %{transaction_id}, Error: %{validation_error}"
        }
      }
    }
  }
  
  # ğŸ”¸ ç»Ÿè®¡å¼‚å¸¸æƒ…å†µ
  ruby {
    code => "
      # æ›´æ–°å¼‚å¸¸ç»Ÿè®¡æ–‡ä»¶
      stats_file = '/data/logstash/error_statistics.json'
      
      begin
        if File.exist?(stats_file)
          stats = JSON.parse(File.read(stats_file))
        else
          stats = {
            'total_errors' => 0,
            'by_severity' => {},
            'by_category' => {},
            'last_update' => Time.now.utc.iso8601
          }
        end
        
        if event.get('error_severity')
          stats['total_errors'] += 1
          
          severity = event.get('error_severity')
          stats['by_severity'][severity] = (stats['by_severity'][severity] || 0) + 1
          
          category = event.get('error_category')
          stats['by_category'][category] = (stats['by_category'][category] || 0) + 1
          
          stats['last_update'] = Time.now.utc.iso8601
          
          File.write(stats_file, stats.to_json)
        end
      rescue => e
        # å¿½ç•¥ç»Ÿè®¡æ–‡ä»¶é”™è¯¯ï¼Œé¿å…å½±å“ä¸»æµç¨‹
      end
    "
  }
}
```

---

## 10. ğŸ“‹ æ ¸å¿ƒè¦ç‚¹æ€»ç»“


### 10.1 å¿…é¡»æŒæ¡çš„æ ¸å¿ƒæ¦‚å¿µ


```markdown
ğŸ”¸ **æ•°æ®åº“åŒæ­¥æœ¬è´¨**ï¼šè®©ä¸åŒæ•°æ®åº“çš„æ•°æ®ä¿æŒä¸€è‡´ï¼Œç±»ä¼¼é•œåƒå¤åˆ¶
ğŸ”¸ **å¢é‡åŒæ­¥ç­–ç•¥**ï¼šåªå¤„ç†å˜åŒ–æ•°æ®ï¼Œé€šè¿‡æ—¶é—´æˆ³ã€ç‰ˆæœ¬å·æˆ–IDè¿½è¸ª
ğŸ”¸ **ä¸»é”®å†²çªå¤„ç†**ï¼šè¦†ç›–ã€ä»…åˆ›å»ºã€æ¡ä»¶æ›´æ–°ä¸‰ç§ç­–ç•¥
ğŸ”¸ **å˜æ›´æ£€æµ‹æœºåˆ¶**ï¼šæ—¶é—´æˆ³ã€æ ¡éªŒå’Œã€ç‰ˆæœ¬å·ã€æ—¥å¿—è§£æå››ç§æ–¹æ³•
ğŸ”¸ **åˆ†é¡µå¤„ç†æŠ€æœ¯**ï¼šé˜²æ­¢å¤§è¡¨æŸ¥è¯¢å†…å­˜æº¢å‡ºï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ 
ğŸ”¸ **å…³è”åŒæ­¥æ–¹å¼**ï¼šä¸€å¯¹ä¸€JOINã€ä¸€å¯¹å¤šèšåˆã€å¤šå¯¹å¤šåˆ†æ­¥å¤„ç†
ğŸ”¸ **ä¸€è‡´æ€§ä¿è¯çº§åˆ«**ï¼šæœ€ç»ˆä¸€è‡´æ€§ã€å¼ºä¸€è‡´æ€§ã€ä¼šè¯ä¸€è‡´æ€§
ğŸ”¸ **ç›‘æ§æŒ‡æ ‡ä½“ç³»**ï¼šæ€§èƒ½ã€è´¨é‡ã€å¼‚å¸¸ã€ä¸šåŠ¡å››ä¸ªç»´åº¦
ğŸ”¸ **é”™è¯¯æ¢å¤æœºåˆ¶**ï¼šé‡è¯•ã€æ–­ç‚¹ç»­ä¼ ã€é™çº§ã€å‘Šè­¦å››å±‚é˜²æŠ¤
```

### 10.2 å…³é”®æŠ€æœ¯ç†è§£è¦ç‚¹


**ğŸ”¹ å¢é‡åŒæ­¥çš„æ ¸å¿ƒæ€æƒ³**
```markdown
ä¸ºä»€ä¹ˆéœ€è¦å¢é‡åŒæ­¥ï¼š
âœ… æ•°æ®é‡å¤§ï¼šå…¨é‡åŒæ­¥å¤ªæ…¢ï¼Œå½±å“æ€§èƒ½
âœ… å®æ—¶æ€§è¦æ±‚ï¼šéœ€è¦å¿«é€Ÿåæ˜ æ•°æ®å˜åŒ–
âœ… èµ„æºèŠ‚çº¦ï¼šå‡å°‘ç½‘ç»œä¼ è¾“å’Œå­˜å‚¨å¼€é”€

å®ç°å…³é”®ï¼š
- å¯é çš„å˜æ›´æ ‡è¯†ï¼šupdated_atå­—æ®µæˆ–ç‰ˆæœ¬å·
- å‡†ç¡®çš„æ–­ç‚¹è®°å½•ï¼š.logstash_jdbc_last_runæ–‡ä»¶
- å¤„ç†æ—¶åºé—®é¢˜ï¼šORDER BYç¡®ä¿æŒ‰æ—¶é—´é¡ºåºå¤„ç†
```

**ğŸ”¹ ä¸»é”®å†²çªçš„ä¸šåŠ¡å«ä¹‰**
```markdown
å†²çªåœºæ™¯ç†è§£ï¼š
- æ•°æ®é‡å¤å¯¼å…¥ï¼šç½‘ç»œé‡è¯•å¯¼è‡´çš„é‡å¤å¤„ç†
- å¤šæºæ•°æ®å†²çªï¼šä¸åŒç³»ç»Ÿä½¿ç”¨ç›¸åŒIDè§„åˆ™
- æ—¶åºä¹±åºï¼šç½‘ç»œå»¶è¿Ÿå¯¼è‡´æ–°æ—§æ•°æ®é¡ºåºé¢ å€’

å¤„ç†åŸåˆ™ï¼š
- ä¸šåŠ¡ä¼˜å…ˆï¼šæ ¹æ®ä¸šåŠ¡è§„åˆ™å†³å®šä¿ç•™å“ªä¸ªç‰ˆæœ¬
- æ—¶é—´æˆ³ä¼˜å…ˆï¼šé€šå¸¸ä¿ç•™æœ€æ–°æ—¶é—´çš„æ•°æ®
- ç‰ˆæœ¬æ§åˆ¶ï¼šä½¿ç”¨ç‰ˆæœ¬å·ç¡®ä¿æ›´æ–°çš„æ­£ç¡®æ€§
```

**ğŸ”¹ å¤§è¡¨åˆ†é¡µçš„å®ç”¨ä»·å€¼**
```markdown
å†…å­˜ç®¡ç†ï¼š
- æ§åˆ¶å•æ¬¡æŸ¥è¯¢çš„æ•°æ®é‡
- é¿å…JVMå†…å­˜æº¢å‡º
- ä¿æŒç¨³å®šçš„å†…å­˜ä½¿ç”¨

ä¸šåŠ¡è¿ç»­æ€§ï¼š
- å‡å°‘å¯¹æºæ•°æ®åº“çš„é”å®šæ—¶é—´
- æ”¯æŒä¸­æ–­åç»­ä¼ 
- ä¸å½±å“æ­£å¸¸ä¸šåŠ¡æ“ä½œ

æ€§èƒ½ä¼˜åŒ–ï¼š
- åˆç†çš„åˆ†é¡µå¤§å°ï¼ˆé€šå¸¸1000-5000æ¡ï¼‰
- åŸºäºç´¢å¼•å­—æ®µåˆ†é¡µï¼ˆå¦‚è‡ªå¢IDï¼‰
- é¿å…ä½¿ç”¨OFFSETï¼ˆæ€§èƒ½å·®ï¼‰
```

### 10.3 å®é™…åº”ç”¨åœºæ™¯æŒ‡å¯¼


**ğŸ“Š ä¸šåŠ¡æ•°æ®åŒæ­¥**
```markdown
ç”µå•†ç³»ç»ŸåŒæ­¥ï¼š
- å•†å“ä¿¡æ¯ï¼šæ¯5åˆ†é’Ÿå¢é‡åŒæ­¥
- è®¢å•æ•°æ®ï¼šæ¯1åˆ†é’Ÿå¢é‡åŒæ­¥
- ç”¨æˆ·ä¿¡æ¯ï¼šæ¯10åˆ†é’Ÿå¢é‡åŒæ­¥
- åº“å­˜æ•°æ®ï¼šæ¯30ç§’å¢é‡åŒæ­¥ï¼ˆé«˜é¢‘ï¼‰

é…ç½®è¦ç‚¹ï¼š
- ä½¿ç”¨å¤åˆç´¢å¼•åŠ é€ŸæŸ¥è¯¢
- è®¾ç½®åˆç†çš„åŒæ­¥é¢‘ç‡
- ç›‘æ§æ•°æ®æ–°é²œåº¦
- å»ºç«‹å¼‚å¸¸å‘Šè­¦æœºåˆ¶
```

**ğŸ”„ æ•°æ®ä»“åº“ETL**
```markdown
æ•°æ®ä»“åº“åœºæ™¯ï¼š
- ç»´åº¦è¡¨ï¼šæ¯æ—¥å…¨é‡åŒæ­¥
- äº‹å®è¡¨ï¼šå®æ—¶å¢é‡åŒæ­¥
- å†å²æ•°æ®ï¼šåˆ†æ‰¹å½’æ¡£åŒæ­¥

ä¼˜åŒ–ç­–ç•¥ï¼š
- å¤œé—´æ‰§è¡Œå¤§æ‰¹é‡åŒæ­¥
- ä½¿ç”¨åˆ†åŒºè¡¨æé«˜æ€§èƒ½
- å®ç°æ•°æ®è¡€ç¼˜è¿½è¸ª
- ä¿è¯æ•°æ®è´¨é‡æ£€æŸ¥
```

**ğŸ” æœç´¢å¼•æ“åŒæ­¥**
```markdown
æœç´¢ç³»ç»Ÿåœºæ™¯ï¼š
- å•†å“æœç´¢ï¼šå®æ—¶åŒæ­¥å•†å“å˜æ›´
- å†…å®¹æœç´¢ï¼šå®šæ—¶åŒæ­¥æ–‡ç« å†…å®¹
- ç”¨æˆ·æœç´¢ï¼šæŒ‰éœ€åŒæ­¥ç”¨æˆ·ä¿¡æ¯

æŠ€æœ¯è¦ç‚¹ï¼š
- ä½¿ç”¨document_idé¿å…é‡å¤
- å®ç°æ™ºèƒ½è·¯ç”±åˆ°ä¸åŒç´¢å¼•
- å»ºç«‹æœç´¢ç›¸å…³æ€§ä¼˜åŒ–
- ç›‘æ§æœç´¢æœåŠ¡å¯ç”¨æ€§
```

### 10.4 é…ç½®æœ€ä½³å®è·µ


**âš™ï¸ æ€§èƒ½ä¼˜åŒ–é…ç½®**
```ruby
# é«˜æ€§èƒ½åŒæ­¥é…ç½®æ¨¡æ¿
input {
  jdbc {
    # è¿æ¥æ± ä¼˜åŒ–
    jdbc_connection_timeout => 300
    jdbc_validation_timeout => 60
    jdbc_fetch_size => 1000
    
    # æŸ¥è¯¢ä¼˜åŒ–
    statement => "
      SELECT * FROM table_name 
      WHERE updated_at > :sql_last_value 
      AND updated_at <= NOW() - INTERVAL 30 SECOND  -- é¿å…æ­£åœ¨å†™å…¥çš„æ•°æ®
      ORDER BY updated_at, id ASC 
      LIMIT 2000
    "
    
    # æ—¶é—´é…ç½®
    schedule => "*/2 * * * *"
    use_column_value => true
  }
}
```

**ğŸ”§ é”™è¯¯å¤„ç†é…ç½®**
```ruby
# å®Œå–„çš„é”™è¯¯å¤„ç†æ¨¡æ¿
output {
  elasticsearch {
    hosts => ["localhost:9200"]
    retry_on_conflict => 3
    retry_on_failure => 5
    
    # å¤±è´¥å¤„ç†
    action => "index"
    manage_template => false
  }
  
  # é”™è¯¯æ•°æ®ä¸“é—¨å¤„ç†
  if [_grokparsefailure] or [_jsonparsefailure] {
    file {
      path => "/var/log/logstash/parse_failures_%{+YYYY.MM.dd}.log"
    }
  }
}
```

### 10.5 ç›‘æ§è¿ç»´è¦ç‚¹


**ğŸ“ˆ å…³é”®ç›‘æ§æŒ‡æ ‡**
```markdown
æ€§èƒ½æŒ‡æ ‡ï¼š
- åŒæ­¥é€Ÿåº¦ï¼šrecords/second
- æ•°æ®å»¶è¿Ÿï¼šminutes behind master
- æˆåŠŸç‡ï¼šsuccess percentage
- èµ„æºä½¿ç”¨ï¼šCPUã€å†…å­˜ã€ç½‘ç»œ

è´¨é‡æŒ‡æ ‡ï¼š
- æ•°æ®å®Œæ•´æ€§ï¼šrecord count consistency
- æ•°æ®å‡†ç¡®æ€§ï¼šchecksum validation
- é”™è¯¯ç‡ï¼šerror percentage by type
- æ¢å¤æ—¶é—´ï¼šrecovery time objective

è¿ç»´æŒ‡æ ‡ï¼š
- æœåŠ¡å¯ç”¨æ€§ï¼šuptime percentage
- å‘Šè­¦å“åº”ï¼šalert response time
- æ•…éšœæ¢å¤ï¼šmean time to recovery
```

**ğŸš¨ å‘Šè­¦ç­–ç•¥é…ç½®**
```markdown
ç´§æ€¥å‘Šè­¦ï¼ˆç«‹å³é€šçŸ¥ï¼‰ï¼š
- åŒæ­¥ä¸­æ–­è¶…è¿‡5åˆ†é’Ÿ
- æ•°æ®ä¸ä¸€è‡´è¶…è¿‡1000æ¡
- é”™è¯¯ç‡è¶…è¿‡10%
- ç³»ç»Ÿèµ„æºä½¿ç”¨è¶…è¿‡90%

è­¦å‘Šå‘Šè­¦ï¼ˆ1å°æ—¶å†…å¤„ç†ï¼‰ï¼š
- åŒæ­¥å»¶è¿Ÿè¶…è¿‡30åˆ†é’Ÿ
- é”™è¯¯ç‡è¶…è¿‡5%
- æ€§èƒ½ä¸‹é™è¶…è¿‡50%

ä¿¡æ¯å‘Šè­¦ï¼ˆæ—¥å¸¸å·¡æ£€ï¼‰ï¼š
- åŒæ­¥ä»»åŠ¡å®Œæˆé€šçŸ¥
- æ¯æ—¥æ•°æ®ç»Ÿè®¡æŠ¥å‘Š
- ç³»ç»Ÿå¥åº·çŠ¶æ€æŠ¥å‘Š
```

**æ ¸å¿ƒè®°å¿†å£è¯€**ï¼š
- å¢é‡åŒæ­¥çœèµ„æºï¼Œæ—¶é—´æˆ³ç‰ˆæœ¬è¦é€‰å‡†
- ä¸»é”®å†²çªæœ‰ç­–ç•¥ï¼Œä¸šåŠ¡è§„åˆ™æœ€é‡è¦  
- å¤§è¡¨åˆ†é¡µé˜²æº¢å‡ºï¼Œæ–­ç‚¹ç»­ä¼ ä¿å®Œæ•´
- å…³è”æ•°æ®å·§å¤„ç†ï¼Œä¸€å¯¹å¤šè¦èšåˆå¥½
- ä¸€è‡´æ€§ä¿è¯æœ‰å±‚æ¬¡ï¼Œç›‘æ§å‘Šè­¦ä¸å¯å°‘
- é”™è¯¯æ¢å¤è¦å…¨é¢ï¼Œé‡è¯•é™çº§åŠ å‘Šè­¦