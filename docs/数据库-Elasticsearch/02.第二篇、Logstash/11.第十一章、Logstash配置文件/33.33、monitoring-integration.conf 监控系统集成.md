---
title: 33、monitoring-integration.conf 监控系统集成
---
## 📚 目录

1. [监控系统集成概述](#1-监控系统集成概述)
2. [Prometheus指标输出配置](#2-Prometheus指标输出配置)
3. [Grafana数据源集成](#3-Grafana数据源集成)
4. [Zabbix监控集成](#4-Zabbix监控集成)
5. [Nagios告警转发](#5-Nagios告警转发)
6. [自定义指标与健康检查](#6-自定义指标与健康检查)
7. [性能基线与SLA监控](#7-性能基线与SLA监控)
8. [核心要点总结](#8-核心要点总结)

---

## 1. 🔍 监控系统集成概述


### 1.1 什么是监控系统集成


**简单理解**：就像给你的房子安装各种传感器和报警器一样，监控系统集成就是给Logstash配置各种"探头"，让它能把运行状态报告给不同的监控平台。

```
监控集成的作用：
┌─────────────┐    指标数据    ┌─────────────┐
│   Logstash  │ ────────────> │  监控平台   │
│   运行状态  │               │ (显示图表)  │
└─────────────┘               └─────────────┘
                                     │
                              ┌─────────────┐
                              │   告警通知  │
                              │ (出问题提醒) │
                              └─────────────┘
```

**为什么需要监控集成**：
- 🔸 **及时发现问题**：Logstash出故障能第一时间知道
- 🔸 **性能优化依据**：通过数据分析找出性能瓶颈
- 🔸 **运维自动化**：减少人工巡检，提高效率
- 🔸 **历史趋势分析**：了解系统长期运行规律

### 1.2 常见监控平台特点


| 监控平台 | **特点** | **适用场景** | **优势** |
|---------|---------|-------------|---------|
| **Prometheus** | `指标收集存储` | `云原生环境` | `强大的查询语言` |
| **Grafana** | `数据可视化` | `图表展示` | `美观的仪表板` |
| **Zabbix** | `企业级监控` | `传统IT环境` | `成熟稳定` |
| **Nagios** | `告警通知` | `关键服务监控` | `告警机制完善` |

---

## 2. 📊 Prometheus指标输出配置


### 2.1 Prometheus基础概念


**什么是Prometheus**：想象它是一个专门收集各种仪表数据的"数据收集员"，定期去各个系统那里"抄表"，然后把数据整理存储起来。

**核心概念解释**：
- 🔸 **Metrics（指标）**：就像汽车仪表盘上的各种数值（速度、油量等）
- 🔸 **Labels（标签）**：给指标加上"标签"，比如区分不同的车辆
- 🔸 **Scrape（采集）**：定期去"抄表"的动作

### 2.2 基础Prometheus输出配置


```ruby
# 33-prometheus-basic.conf - 基础Prometheus集成
input {
  beats {
    port => 5044
  }
}

filter {
  # 添加监控标签
  mutate {
    add_field => {
      "logstash_pipeline" => "main"
      "environment" => "production"
    }
  }
}

output {
  # 主要数据输出
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "logs-%{+YYYY.MM.dd}"
  }
  
  # Prometheus指标输出
  http {
    url => "http://prometheus-gateway:9091/metrics/job/logstash/instance/node1"
    http_method => "post"
    format => "json"
    mapping => {
      "pipeline" => "%{logstash_pipeline}"
      "env" => "%{environment}"
      "processed_events" => "%{[@metadata][beat]}"
      "timestamp" => "%{@timestamp}"
    }
  }
}
```

**配置说明**：
- 🔹 **mapping部分**：定义要发送给Prometheus的指标数据
- 🔹 **url部分**：指定Prometheus网关地址
- 🔹 **job和instance**：给监控数据打上"身份标签"

### 2.3 高级Prometheus集成


```ruby
# 33-prometheus-advanced.conf - 高级指标收集
input {
  beats {
    port => 5044
    add_field => { "input_type" => "beats" }
  }
  
  http {
    port => 8080
    add_field => { "input_type" => "http" }
  }
}

filter {
  # 统计处理量
  ruby {
    code => "
      event.set('processing_time', Time.now.to_f - event.get('@timestamp').to_f)
    "
  }
  
  # 错误统计
  if [level] == "ERROR" {
    mutate {
      add_field => { "error_count" => 1 }
    }
  }
}

output {
  # 数据输出
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "logs-%{+YYYY.MM.dd}"
  }
  
  # 详细Prometheus指标
  http {
    url => "http://prometheus-gateway:9091/metrics/job/logstash"
    http_method => "post"
    format => "json"
    mapping => {
      # 基础指标
      "logstash_events_processed_total" => "1"
      "logstash_processing_time_seconds" => "%{processing_time}"
      "logstash_input_type" => "%{input_type}"
      
      # 错误指标
      "logstash_errors_total" => "%{error_count}"
      
      # 标签信息
      "pipeline" => "%{[@metadata][pipeline]}"
      "host" => "%{host[name]}"
    }
  }
}
```

---

## 3. 📈 Grafana数据源集成


### 3.1 Grafana数据源配置


**什么是Grafana**：把它想象成一个"数据大屏幕"，可以把各种监控数据用漂亮的图表显示出来，就像股票大盘的显示屏一样。

### 3.2 Elasticsearch数据源配置


```ruby
# 33-grafana-elasticsearch.conf - 为Grafana准备数据
input {
  beats {
    port => 5044
  }
}

filter {
  # 为Grafana图表准备时间字段
  date {
    match => [ "@timestamp", "ISO8601" ]
    target => "grafana_timestamp"
  }
  
  # 准备图表所需的聚合字段
  mutate {
    add_field => {
      "metric_timestamp" => "%{+YYYY-MM-dd'T'HH:mm:ss.SSS'Z'}"
      "service_name" => "%{[fields][service]}"
      "log_level" => "%{level}"
    }
  }
  
  # 计算响应时间（如果有的话）
  if [response_time] {
    mutate {
      convert => { "response_time" => "float" }
    }
  }
}

output {
  # 输出到专门的Grafana索引
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "grafana-metrics-%{+YYYY.MM.dd}"
    document_type => "_doc"
    template_name => "grafana-metrics"
    template => {
      "index_patterns" => ["grafana-metrics-*"]
      "settings" => {
        "number_of_shards" => 1
        "number_of_replicas" => 0
      }
      "mappings" => {
        "properties" => {
          "@timestamp" => { "type" => "date" }
          "service_name" => { "type" => "keyword" }
          "log_level" => { "type" => "keyword" }
          "response_time" => { "type" => "float" }
          "message" => { "type" => "text" }
        }
      }
    }
  }
}
```

### 3.3 InfluxDB时序数据库集成


```ruby
# 33-grafana-influxdb.conf - 时序数据库输出
input {
  beats {
    port => 5044
  }
}

filter {
  # 提取关键指标
  grok {
    match => {
      "message" => "%{TIMESTAMP_ISO8601:log_time} \[%{WORD:log_level}\] %{GREEDYDATA:log_message}"
    }
  }
  
  # 准备InfluxDB格式的数据
  mutate {
    add_field => {
      "measurement" => "application_logs"
      "tag_service" => "%{[fields][service]}"
      "tag_environment" => "%{[fields][env]}"
    }
  }
}

output {
  # 输出到InfluxDB
  influxdb {
    host => "localhost"
    port => 8086
    database => "logstash_metrics"
    username => "logstash"
    password => "password123"
    
    # 指标定义
    measurement => "%{measurement}"
    tags => {
      "service" => "%{tag_service}"
      "environment" => "%{tag_environment}"
      "level" => "%{log_level}"
    }
    fields => {
      "message_count" => 1
      "processing_time" => "%{[@metadata][processing_time]}"
    }
  }
}
```

---

## 4. 🔔 Zabbix监控集成


### 4.1 Zabbix集成原理


**Zabbix是什么**：想象它是一个"全能管家"，不仅能监控各种设备，还能在出问题时主动通知你，就像智能家居系统一样。

### 4.2 Zabbix Sender集成


```ruby
# 33-zabbix-integration.conf - Zabbix监控集成
input {
  beats {
    port => 5044
  }
}

filter {
  # 统计错误和警告
  if [level] in ["ERROR", "WARN"] {
    mutate {
      add_field => { "alert_level" => "%{level}" }
    }
  }
  
  # 计算处理延迟
  ruby {
    code => "
      now = Time.now.to_f
      log_time = event.get('@timestamp').to_f
      event.set('processing_delay', now - log_time)
    "
  }
}

output {
  # 主要数据输出
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "logs-%{+YYYY.MM.dd}"
  }
  
  # 发送关键指标到Zabbix
  if [alert_level] {
    exec {
      command => "zabbix_sender -z zabbix-server -s logstash-node1 -k 'logstash.alert[%{alert_level}]' -o '1'"
    }
  }
  
  # 发送性能指标到Zabbix
  exec {
    command => "zabbix_sender -z zabbix-server -s logstash-node1 -k 'logstash.processing_delay' -o '%{processing_delay}'"
  }
}
```

### 4.3 Zabbix HTTP监控


```ruby
# 33-zabbix-http.conf - 通过HTTP发送监控数据
input {
  beats {
    port => 5044
  }
}

filter {
  # 聚合统计信息
  aggregate {
    task_id => "%{[@metadata][beat]}-%{[agent][hostname]}"
    code => "
      map['total_events'] ||= 0
      map['error_events'] ||= 0
      map['total_events'] += 1
      
      if event.get('level') == 'ERROR'
        map['error_events'] += 1
      end
      
      event.set('total_count', map['total_events'])
      event.set('error_count', map['error_events'])
    "
    push_map_as_event_on_timeout => true
    timeout => 60
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "logs-%{+YYYY.MM.dd}"
  }
  
  # 发送聚合数据到Zabbix
  if [@metadata][aggregated] {
    http {
      url => "http://zabbix-server:10051/zabbix"
      http_method => "post"
      format => "json"
      mapping => {
        "request" => "sender data"
        "data" => [
          {
            "host" => "%{[agent][hostname]}"
            "key" => "logstash.events.total"
            "value" => "%{total_count}"
          },
          {
            "host" => "%{[agent][hostname]}"
            "key" => "logstash.events.errors"
            "value" => "%{error_count}"
          }
        ]
      }
    }
  }
}
```

---

## 5. 🚨 Nagios告警转发


### 5.1 Nagios告警原理


**Nagios的作用**：把它想象成一个"专业的安保系统"，专门负责在出现严重问题时发出警报，通过各种方式（邮件、短信等）通知管理员。

### 5.2 关键事件告警配置


```ruby
# 33-nagios-alerts.conf - Nagios告警转发
input {
  beats {
    port => 5044
  }
}

filter {
  # 识别关键错误
  if [level] == "ERROR" or [level] == "FATAL" {
    mutate {
      add_field => { 
        "nagios_alert" => "true"
        "nagios_service" => "%{[fields][service]}"
        "nagios_host" => "%{[agent][hostname]}"
      }
    }
  }
  
  # 检查特定错误模式
  if [message] =~ /OutOfMemoryError|Connection refused|Timeout/ {
    mutate {
      add_field => { 
        "nagios_critical" => "true"
        "alert_type" => "CRITICAL"
      }
    }
  } else if [level] == "ERROR" {
    mutate {
      add_field => { "alert_type" => "WARNING" }
    }
  }
}

output {
  # 正常日志输出
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "logs-%{+YYYY.MM.dd}"
  }
  
  # 发送告警到Nagios
  if [nagios_alert] == "true" {
    # 通过NSCA发送被动检查结果
    exec {
      command => "echo '%{nagios_host}\t%{nagios_service}\t%{alert_type}\t%{message}' | /usr/local/nagios/bin/send_nsca -H nagios-server -c /usr/local/nagios/etc/send_nsca.cfg"
    }
  }
}
```

### 5.3 服务状态监控


```ruby
# 33-nagios-service-status.conf - 服务状态监控
input {
  beats {
    port => 5044
  }
}

filter {
  # 服务启动/停止检测
  if [message] =~ /Starting|Started/ {
    mutate {
      add_field => { 
        "service_status" => "OK"
        "nagios_message" => "Service %{[fields][service]} is running normally"
      }
    }
  } else if [message] =~ /Stopping|Stopped|Shutdown/ {
    mutate {
      add_field => { 
        "service_status" => "CRITICAL"
        "nagios_message" => "Service %{[fields][service]} has stopped"
      }
    }
  }
  
  # 性能指标检查
  if [response_time] {
    if [response_time] > 5000 {
      mutate {
        add_field => { 
          "service_status" => "WARNING"
          "nagios_message" => "Response time is high: %{response_time}ms"
        }
      }
    }
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "logs-%{+YYYY.MM.dd}"
  }
  
  # 发送服务状态到Nagios
  if [service_status] {
    http {
      url => "http://nagios-server:8080/nagios/cgi-bin/cmd.cgi"
      http_method => "post"
      form => {
        "cmd_typ" => "30"  # PROCESS_SERVICE_CHECK_RESULT
        "cmd_mod" => "2"
        "host" => "%{[agent][hostname]}"
        "service" => "%{[fields][service]}"
        "plugin_state" => "%{service_status}"
        "plugin_output" => "%{nagios_message}"
      }
    }
  }
}
```

---

## 6. 📏 自定义指标与健康检查


### 6.1 自定义指标定义


**什么是自定义指标**：就像在汽车上安装额外的仪表一样，除了标准的速度、油量表，你还可以加装发动机温度、胎压等专门的监控表。

### 6.2 业务指标收集


```ruby
# 33-custom-metrics.conf - 自定义业务指标
input {
  beats {
    port => 5044
  }
}

filter {
  # 用户行为指标
  if [fields][log_type] == "user_action" {
    grok {
      match => {
        "message" => "User %{WORD:user_id} performed %{WORD:action} on %{WORD:resource}"
      }
    }
    
    mutate {
      add_field => { 
        "metric_type" => "user_behavior"
        "metric_name" => "user_action_count"
        "metric_value" => "1"
      }
    }
  }
  
  # 业务交易指标
  if [fields][log_type] == "transaction" {
    grok {
      match => {
        "message" => "Transaction %{WORD:transaction_id} amount %{NUMBER:amount:float} status %{WORD:status}"
      }
    }
    
    mutate {
      add_field => { 
        "metric_type" => "business"
        "metric_name" => "transaction_amount"
        "metric_value" => "%{amount}"
      }
    }
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "logs-%{+YYYY.MM.dd}"
  }
  
  # 输出自定义指标
  if [metric_type] {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "metrics-%{+YYYY.MM.dd}"
      document_type => "_doc"
      document_id => "%{metric_type}-%{metric_name}-%{+YYYY.MM.dd.HH.mm}"
    }
  }
}
```

### 6.3 健康检查配置


```ruby
# 33-health-check.conf - 系统健康检查
input {
  beats {
    port => 5044
  }
  
  # 定期健康检查输入
  exec {
    command => "curl -s http://localhost:9600/_node/stats/pipeline"
    interval => 30
    codec => "json"
    add_field => { "check_type" => "pipeline_health" }
  }
}

filter {
  # 处理健康检查数据
  if [check_type] == "pipeline_health" {
    # 检查管道状态
    if [pipeline][main][events][in] {
      ruby {
        code => "
          events_in = event.get('[pipeline][main][events][in]')
          events_out = event.get('[pipeline][main][events][out]')
          
          if events_in > 0
            throughput = (events_out.to_f / events_in.to_f) * 100
            event.set('pipeline_throughput', throughput)
            
            if throughput < 95
              event.set('health_status', 'WARNING')
              event.set('health_message', 'Pipeline throughput is low')
            else
              event.set('health_status', 'OK')
              event.set('health_message', 'Pipeline is healthy')
            end
          end
        "
      }
    }
  }
}

output {
  # 正常日志
  if [check_type] != "pipeline_health" {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "logs-%{+YYYY.MM.dd}"
    }
  }
  
  # 健康检查结果
  if [check_type] == "pipeline_health" {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "health-check-%{+YYYY.MM.dd}"
    }
    
    # 如果有问题，发送告警
    if [health_status] != "OK" {
      http {
        url => "http://alert-manager:9093/api/v1/alerts"
        http_method => "post"
        format => "json"
        mapping => {
          "alerts" => [
            {
              "labels" => {
                "alertname" => "LogstashHealthCheck"
                "service" => "logstash"
                "severity" => "%{health_status}"
              }
              "annotations" => {
                "summary" => "%{health_message}"
                "description" => "Logstash pipeline health check failed"
              }
            }
          ]
        }
      }
    }
  }
}
```

---

## 7. 📊 性能基线与SLA监控


### 7.1 性能基线概念


**什么是性能基线**：就像给汽车做"体检"时建立的"健康档案"一样，记录系统正常运行时的各项指标，作为判断是否异常的标准。

**SLA监控**：Service Level Agreement，服务水平协议监控，就是监控系统是否达到了承诺的服务标准。

### 7.2 性能基线收集


```ruby
# 33-performance-baseline.conf - 性能基线数据收集
input {
  beats {
    port => 5044
  }
  
  # 定期收集系统指标
  exec {
    command => "cat /proc/meminfo | grep MemAvailable"
    interval => 60
    add_field => { "metric_source" => "system" }
  }
}

filter {
  # 处理系统指标
  if [metric_source] == "system" {
    grok {
      match => {
        "message" => "MemAvailable:\s+%{NUMBER:memory_available:int} kB"
      }
    }
    
    mutate {
      add_field => { 
        "baseline_metric" => "memory_available"
        "baseline_value" => "%{memory_available}"
        "baseline_timestamp" => "%{@timestamp}"
      }
    }
  }
  
  # 计算处理性能指标
  ruby {
    code => "
      # 计算每分钟处理事件数
      event.set('events_per_minute', 1)
      
      # 记录处理时间
      start_time = event.get('[@metadata][received_at]') || Time.now
      process_time = Time.now - start_time.to_f
      event.set('processing_time_ms', process_time * 1000)
    "
  }
}

output {
  # 正常日志输出
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "logs-%{+YYYY.MM.dd}"
  }
  
  # 基线数据存储
  if [baseline_metric] {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "performance-baseline-%{+YYYY.MM}"
      document_type => "_doc"
    }
  }
}
```

### 7.3 SLA监控配置


```ruby
# 33-sla-monitoring.conf - SLA服务水平监控
input {
  beats {
    port => 5044
  }
}

filter {
  # 可用性监控（99.9%正常运行时间）
  if [level] in ["ERROR", "FATAL"] {
    mutate {
      add_field => { 
        "sla_violation" => "availability"
        "sla_impact" => "high"
      }
    }
  }
  
  # 响应时间监控（<2秒响应时间）
  if [response_time] {
    if [response_time] > 2000 {
      mutate {
        add_field => { 
          "sla_violation" => "response_time"
          "sla_impact" => "medium"
          "sla_threshold" => "2000ms"
          "sla_actual" => "%{response_time}ms"
        }
      }
    }
  }
  
  # 吞吐量监控（>1000 TPS）
  aggregate {
    task_id => "throughput_check"
    code => "
      map['event_count'] ||= 0
      map['start_time'] ||= Time.now
      map['event_count'] += 1
      
      # 每分钟检查一次
      if Time.now - map['start_time'] >= 60
        tps = map['event_count'] / 60.0
        event.set('current_tps', tps)
        
        if tps < 1000
          event.set('sla_violation', 'throughput')
          event.set('sla_impact', 'high')
          event.set('sla_threshold', '1000 TPS')
          event.set('sla_actual', tps)
        end
        
        # 重置计数器
        map['event_count'] = 0
        map['start_time'] = Time.now
      end
    "
    push_map_as_event_on_timeout => true
    timeout => 60
  }
}

output {
  # 正常日志输出
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "logs-%{+YYYY.MM.dd}"
  }
  
  # SLA违规记录
  if [sla_violation] {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "sla-violations-%{+YYYY.MM.dd}"
      document_type => "_doc"
    }
    
    # 发送SLA告警
    http {
      url => "http://sla-monitor:8080/violations"
      http_method => "post"
      format => "json"
      mapping => {
        "violation_type" => "%{sla_violation}"
        "impact_level" => "%{sla_impact}"
        "threshold" => "%{sla_threshold}"
        "actual_value" => "%{sla_actual}"
        "timestamp" => "%{@timestamp}"
        "service" => "%{[fields][service]}"
      }
    }
  }
}
```

---

## 8. 📋 核心要点总结


### 8.1 监控集成要点


**🔸 选择合适的监控平台**
```
根据环境选择：
• 云原生环境 → Prometheus + Grafana
• 传统企业环境 → Zabbix + Nagios
• 混合环境 → 多平台组合使用
```

**🔸 关键指标设计**
- **系统指标**：CPU、内存、磁盘、网络
- **业务指标**：事务量、用户数、错误率
- **性能指标**：响应时间、吞吐量、延迟

### 8.2 实施建议


**监控配置原则**：
1. **从简单开始**：先配置基础监控，再逐步完善
2. **避免过度监控**：只监控真正重要的指标
3. **设置合理阈值**：避免误报和漏报
4. **定期审查优化**：根据实际情况调整配置

**告警策略设计**：
```
告警级别设计：
┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│   INFO      │    │  WARNING    │    │  CRITICAL   │
│   信息提醒   │    │   需要关注   │    │   紧急处理   │
└─────────────┘    └─────────────┘    └─────────────┘
       │                   │                   │
   日志记录          邮件通知            短信+电话
```

### 8.3 最佳实践


**🔹 数据存储策略**
- **热数据**：最近7天，高分辨率存储
- **温数据**：最近30天，中等分辨率
- **冷数据**：超过30天，低分辨率或归档

**🔹 性能优化建议**
- 使用批量发送减少网络开销
- 合理设置采集间隔，避免过于频繁
- 定期清理历史数据，控制存储成本

**核心记忆要点**：
- 监控集成是运维自动化的基础
- 选择合适的监控工具组合很重要
- 关键是要监控对业务有意义的指标
- 告警要及时、准确，避免"狼来了"效应