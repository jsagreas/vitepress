---
title: 3、数据库与队列输入
---
## 📚 目录

1. [JDBC数据库输入详解](#1-jdbc数据库输入详解)
2. [增量同步策略实战](#2-增量同步策略实战)
3. [Kafka消息队列输入](#3-kafka消息队列输入)
4. [Redis队列输入配置](#4-redis队列输入配置)
5. [调度与轮询机制](#5-调度与轮询机制)
6. [连接池管理优化](#6-连接池管理优化)
7. [核心要点总结](#7-核心要点总结)

---

## 1. 📊 JDBC数据库输入详解


### 1.1 什么是JDBC输入插件


**🔸 简单理解**
JDBC插件就像是Logstash的"数据库读取器"，它能够连接各种数据库（MySQL、PostgreSQL、Oracle等），把数据库里的数据"搬运"到Elasticsearch中。

```
简单比喻：
数据库 = 仓库里的货物
JDBC插件 = 搬运工
Logstash = 运输车
Elasticsearch = 目标仓库

JDBC插件负责从数据库仓库里取出数据，交给Logstash运输车运送
```

### 1.2 基础配置入门


**🔧 最简单的MySQL连接示例**
```ruby
input {
  jdbc {
    # 数据库连接信息（就像门牌号）
    jdbc_connection_string => "jdbc:mysql://localhost:3306/mydb"
    jdbc_user => "root"
    jdbc_password => "123456"
    
    # 数据库驱动（告诉Logstash用什么方式连接）
    jdbc_driver_library => "/path/to/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    
    # 要执行的SQL查询
    statement => "SELECT * FROM users WHERE created_at > :sql_last_value"
    
    # 多久查询一次（5分钟）
    schedule => "*/5 * * * *"
  }
}
```

> 💡 **新手提示**：`:sql_last_value` 是Logstash的"记忆功能"，它会记住上次查询到哪里了，避免重复读取相同数据。

### 1.3 配置参数详解


**📋 核心参数说明**

| 参数名称 | **作用说明** | **举例** |
|---------|-------------|---------|
| `jdbc_connection_string` | `数据库地址，像门牌号一样` | `jdbc:mysql://192.168.1.100:3306/shop` |
| `jdbc_user` | `数据库用户名` | `logstash_user` |
| `jdbc_password` | `数据库密码` | `mypassword123` |
| `statement` | `要执行的SQL语句` | `SELECT * FROM orders WHERE updated_at > ?` |
| `schedule` | `多久执行一次查询` | `*/10 * * * *`（每10分钟） |
| `use_column_value` | `是否使用字段值做增量` | `true`（推荐开启） |
| `tracking_column` | `用来追踪的字段名` | `updated_at` 或 `id` |

### 1.4 实际应用场景


**🎯 典型使用场景**

```
电商订单同步：
┌─────────────┐    JDBC     ┌─────────────┐    输出    ┌─────────────┐
│   MySQL     │ ---------> │  Logstash   │ -------> │Elasticsearch│
│  订单表     │  读取新订单  │   处理数据   │  存储索引  │   订单索引   │
└─────────────┘            └─────────────┘          └─────────────┘

用户行为分析：
数据库用户表 → Logstash → ES用户画像索引

日志审计：
操作记录表 → Logstash → ES审计日志索引
```

---

## 2. 🔄 增量同步策略实战


### 2.1 什么是增量同步


**🔸 通俗解释**
增量同步就像"只搬运新货物"的策略。想象你是仓库管理员，每天只需要把新到的货物搬到另一个仓库，而不是每次都把所有货物重新搬一遍。

```
全量同步 vs 增量同步：

全量同步（不推荐）：
每次都查询：SELECT * FROM users
问题：数据重复，性能差

增量同步（推荐）：
第一次：SELECT * FROM users WHERE id > 0
第二次：SELECT * FROM users WHERE id > 1000
第三次：SELECT * FROM users WHERE id > 2000
优势：只处理新数据，高效节能
```

### 2.2 基于ID的增量同步


**⚡ 最常用的增量方式**
```ruby
input {
  jdbc {
    jdbc_connection_string => "jdbc:mysql://localhost:3306/ecommerce"
    jdbc_user => "logstash"
    jdbc_password => "password"
    jdbc_driver_library => "/opt/logstash/mysql-connector.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    
    # 基于ID增量查询
    statement => "SELECT * FROM products WHERE id > :sql_last_value ORDER BY id"
    
    # 启用增量追踪
    use_column_value => true
    tracking_column => "id"
    tracking_column_type => "numeric"
    
    # 记录状态的文件位置
    last_run_metadata_path => "/opt/logstash/metadata/.logstash_jdbc_last_run"
    
    schedule => "*/2 * * * *"
  }
}
```

> 📖 **概念解释**：`tracking_column` 就是Logstash的"书签"，它记录读到了哪一行，下次从这里继续读。

### 2.3 基于时间的增量同步


**📅 适合有更新时间字段的表**
```ruby
input {
  jdbc {
    jdbc_connection_string => "jdbc:mysql://localhost:3306/blog"
    jdbc_user => "reader"
    jdbc_password => "secret"
    jdbc_driver_library => "/opt/logstash/mysql-connector.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    
    # 基于时间增量查询
    statement => "SELECT * FROM articles WHERE updated_at > :sql_last_value ORDER BY updated_at"
    
    use_column_value => true
    tracking_column => "updated_at"
    tracking_column_type => "timestamp"
    
    # 设置时区
    jdbc_default_timezone => "Asia/Shanghai"
    
    schedule => "0 */1 * * *"  # 每小时执行一次
  }
}
```

### 2.4 增量同步最佳实践


**💎 实用技巧和注意事项**

```ruby
# 高级增量配置示例
input {
  jdbc {
    jdbc_connection_string => "jdbc:mysql://localhost:3306/crm"
    jdbc_user => "logstash_reader"
    jdbc_password => "secure_pass"
    jdbc_driver_library => "/opt/logstash/drivers/mysql-connector.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    
    # 复杂查询：处理软删除的数据
    statement => "
      SELECT id, name, email, status, created_at, updated_at 
      FROM customers 
      WHERE updated_at > :sql_last_value 
      AND status != 'deleted'
      ORDER BY updated_at
    "
    
    use_column_value => true
    tracking_column => "updated_at"
    tracking_column_type => "timestamp"
    
    # 设置查询超时（30秒）
    jdbc_fetch_size => 1000
    jdbc_page_size => 1000
    
    # 清理连接
    clean_run => false
    
    schedule => "*/5 * * * *"
  }
}
```

> ⚠️ **注意事项**：
> - 追踪字段必须是**递增的**（ID自增或时间戳）
> - 不要删除 `last_run_metadata_path` 文件，否则会重新全量同步
> - 建议在追踪字段上建立数据库索引

---

## 3. 🚀 Kafka消息队列输入


### 3.1 Kafka输入插件简介


**🔸 什么是Kafka输入**
Kafka就像一个"超级快递中转站"，应用程序把消息投递到Kafka，Logstash从Kafka取出消息进行处理。这种方式特别适合处理大量实时数据。

```
消息流转示意图：
应用A ──┐
应用B ──┼──> Kafka消息队列 ──> Logstash ──> Elasticsearch
应用C ──┘     (中转缓冲)       (数据处理)    (最终存储)

优势：
✅ 解耦：应用和Logstash不直接连接
✅ 缓冲：Kafka能暂存大量消息
✅ 可靠：消息不会丢失
✅ 扩展：可以多个Logstash实例消费
```

### 3.2 基础Kafka输入配置


**🔧 简单的Kafka消费者配置**
```ruby
input {
  kafka {
    # Kafka集群地址（可以多个）
    bootstrap_servers => "localhost:9092"
    
    # 要消费的主题名称
    topics => ["user-events", "order-logs"]
    
    # 消费者组ID（同组的消费者会分摊消息）
    group_id => "logstash-group-1"
    
    # 消息格式（通常是JSON）
    codec => "json"
    
    # 从哪里开始消费（earliest=从头开始，latest=最新消息）
    auto_offset_reset => "earliest"
  }
}
```

### 3.3 Kafka高级配置


**⚡ 生产环境配置示例**
```ruby
input {
  kafka {
    bootstrap_servers => ["kafka1:9092", "kafka2:9092", "kafka3:9092"]
    topics => ["application-logs"]
    group_id => "logstash-logs-processor"
    
    # 消费者配置
    consumer_threads => 4        # 并发消费线程数
    fetch_min_bytes => 1024     # 最小拉取字节数
    fetch_max_wait_ms => 1000   # 最大等待时间
    
    # 安全配置
    security_protocol => "SASL_SSL"
    sasl_mechanism => "PLAIN"
    sasl_jaas_config => "org.apache.kafka.common.security.plain.PlainLoginModule required username='user' password='pass';"
    
    # 解码配置
    codec => "json"
    
    # 偏移量管理
    enable_auto_commit => true
    auto_commit_interval_ms => 5000
  }
}
```

### 3.4 Kafka使用场景


**🎯 实际应用案例**

```
微服务日志收集：
┌──────────────┐    ┌──────────────┐    ┌──────────────┐
│  用户服务     │───▶│   Kafka      │───▶│  Logstash    │
│  订单服务     │───▶│   Topic:     │───▶│   日志处理    │
│  支付服务     │───▶│  app-logs    │───▶│              │
└──────────────┘    └──────────────┘    └──────────────┘

实时数据分析：
业务系统 → Kafka → Logstash → Elasticsearch → Kibana仪表板

事件驱动架构：
用户行为 → Kafka → Logstash → 多个目标存储系统
```

> 💡 **使用建议**：Kafka适合高吞吐量的实时数据处理，如果你的数据量很大且需要实时性，Kafka是最佳选择。

---

## 4. 🔑 Redis队列输入配置


### 4.1 Redis输入插件概述


**🔸 Redis作为消息队列**
Redis就像一个"高速收发室"，应用程序把消息放到Redis的列表里，Logstash从列表中取出消息处理。相比Kafka，Redis更简单，适合中小规模的数据处理。

```
Redis队列工作模式：
应用程序 ──LPUSH──> Redis List ──RPOP──> Logstash ──> 目标系统
         (放入消息)             (取出消息)

常用场景：
- 日志收集
- 任务队列
- 简单消息传递
```

### 4.2 基础Redis输入配置


**🔧 简单的Redis列表消费**
```ruby
input {
  redis {
    # Redis服务器地址
    host => "localhost"
    port => 6379
    
    # 认证密码（如果设置了）
    password => "myredispassword"
    
    # 数据类型：list（列表）
    data_type => "list"
    
    # 队列名称
    key => "logstash-queue"
    
    # 解码方式
    codec => "json"
    
    # 阻塞超时时间（秒）
    timeout => 5
  }
}
```

### 4.3 Redis发布订阅模式


**📡 Pub/Sub模式配置**
```ruby
input {
  redis {
    host => "redis.example.com"
    port => 6379
    password => "secretpass"
    
    # 使用发布订阅模式
    data_type => "channel"
    
    # 订阅的频道名称
    key => "logs-channel"
    
    codec => "json"
  }
}
```

### 4.4 Redis模式对比


**📊 不同Redis数据类型的使用场景**

| 模式类型 | **工作方式** | **适用场景** | **优缺点** |
|---------|-------------|-------------|-----------|
| **List队列** | `LPUSH/RPOP操作` | `简单任务队列` | `✅简单可靠 ❌单消费者` |
| **Pub/Sub** | `发布订阅消息` | `实时通知广播` | `✅多订阅者 ❌消息可能丢失` |
| **Stream** | `流式消息队列` | `高级消息处理` | `✅功能强大 ❌配置复杂` |

### 4.5 Redis连接优化


**⚡ 生产环境配置优化**
```ruby
input {
  redis {
    host => "redis-cluster.internal"
    port => 6379
    password => "${REDIS_PASSWORD}"  # 使用环境变量
    
    data_type => "list"
    key => "app-logs-queue"
    
    # 连接池设置
    db => 0
    reconnect_interval => 1
    
    # 批量处理优化
    batch_count => 100    # 一次处理100条消息
    
    # 线程设置
    threads => 2
    
    codec => "json"
  }
}
```

---

## 5. ⏰ 调度与轮询机制


### 5.1 调度机制详解


**🔸 什么是调度**
调度就是"定时执行任务"的机制，就像闹钟一样，告诉Logstash什么时候去数据库查询数据。Logstash使用类似Linux的cron格式来设置调度时间。

```
Cron格式说明：
* * * * *
│ │ │ │ │
│ │ │ │ └─ 星期几 (0-7, 0和7都表示周日)
│ │ │ └─── 月份 (1-12)
│ │ └───── 日期 (1-31)
│ └─────── 小时 (0-23)
└───────── 分钟 (0-59)
```

### 5.2 常用调度表达式


**📅 实用的调度配置示例**

```ruby
# 每5分钟执行一次
schedule => "*/5 * * * *"

# 每小时的第10分钟执行
schedule => "10 * * * *"

# 每天凌晨2点执行
schedule => "0 2 * * *"

# 每周一早上8点执行
schedule => "0 8 * * 1"

# 工作日每小时执行
schedule => "0 * * * 1-5"
```

**🎯 业务场景对应的调度策略**
```
实时数据（用户行为）：
schedule => "*/1 * * * *"  # 每分钟

一般业务数据（订单、商品）：
schedule => "*/5 * * * *"  # 每5分钟

历史数据分析：
schedule => "0 2 * * *"    # 每天凌晨2点

报表数据：
schedule => "0 1 * * 0"    # 每周日凌晨1点
```

### 5.3 轮询策略优化


**⚡ 避免数据库压力的技巧**

```ruby
input {
  jdbc {
    jdbc_connection_string => "jdbc:mysql://db.example.com:3306/production"
    jdbc_user => "readonly_user"
    jdbc_password => "safe_password"
    jdbc_driver_library => "/opt/logstash/mysql-connector.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    
    # 优化查询：只查询必要字段
    statement => "
      SELECT id, title, price, status, updated_at 
      FROM products 
      WHERE updated_at > :sql_last_value 
      AND status IN ('active', 'pending')
      ORDER BY updated_at 
      LIMIT 1000
    "
    
    # 合理的调度间隔
    schedule => "*/10 * * * *"  # 10分钟一次，避免频繁查询
    
    # 分页查询，避免一次加载过多数据
    jdbc_page_size => 1000
    jdbc_fetch_size => 1000
    
    use_column_value => true
    tracking_column => "updated_at"
    tracking_column_type => "timestamp"
  }
}
```

> ⚠️ **注意事项**：
> - 调度频率要考虑数据库性能
> - 避免在业务高峰期执行大量查询
> - 使用只读账户，设置合适的权限

---

## 6. 🏊 连接池管理优化


### 6.1 什么是连接池


**🔸 连接池概念**
连接池就像"出租车调度站"，预先准备好一些数据库连接，需要时直接使用，用完放回池子里。这样避免了频繁建立和关闭连接的开销。

```
没有连接池的问题：
每次查询 → 建立连接 → 执行SQL → 关闭连接
问题：频繁连接开销大，性能差

使用连接池的优势：
连接池 ← 复用连接 ← Logstash查询
   ↓
预先建立的连接们（空闲等待被使用）
优势：连接复用，性能好，资源控制
```

### 6.2 JDBC连接池配置


**🔧 连接池参数优化**
```ruby
input {
  jdbc {
    jdbc_connection_string => "jdbc:mysql://localhost:3306/mydb?useSSL=false&serverTimezone=UTC"
    jdbc_user => "logstash_user"
    jdbc_password => "password123"
    jdbc_driver_library => "/opt/logstash/mysql-connector.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    
    # 连接池配置
    connection_retry_attempts => 3          # 连接重试次数
    connection_retry_attempts_wait_time => 5 # 重试间隔（秒）
    
    # 查询超时设置
    jdbc_fetch_size => 1000                 # 每次获取记录数
    jdbc_page_size => 1000                  # 分页大小
    
    # SQL查询
    statement => "SELECT * FROM orders WHERE updated_at > :sql_last_value ORDER BY updated_at"
    
    # 调度配置
    schedule => "*/5 * * * *"
    
    use_column_value => true
    tracking_column => "updated_at"
    tracking_column_type => "timestamp"
  }
}
```

### 6.3 连接字符串优化


**⚡ MySQL连接参数调优**
```ruby
# 优化的MySQL连接字符串
jdbc_connection_string => "
  jdbc:mysql://db-server:3306/production?
  useSSL=true&
  serverTimezone=Asia/Shanghai&
  useUnicode=true&
  characterEncoding=utf8mb4&
  autoReconnect=true&
  failOverReadOnly=false&
  maxReconnects=3&
  initialTimeout=2&
  connectTimeout=10000&
  socketTimeout=30000&
  useCompression=true
"
```

**📋 关键参数说明**

| 参数名称 | **作用** | **推荐值** |
|---------|---------|-----------|
| `connectTimeout` | `连接超时时间` | `10000`（10秒） |
| `socketTimeout` | `读取超时时间` | `30000`（30秒） |
| `autoReconnect` | `自动重连` | `true` |
| `useCompression` | `数据压缩` | `true`（大数据量时） |
| `useUnicode` | `Unicode支持` | `true` |

### 6.4 多数据源连接管理


**🔗 多个数据库输入配置**
```ruby
# 主业务数据库
input {
  jdbc {
    jdbc_connection_string => "jdbc:mysql://main-db:3306/business"
    jdbc_user => "business_reader"
    jdbc_password => "pass1"
    jdbc_driver_library => "/opt/logstash/mysql-connector.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    
    statement => "SELECT * FROM orders WHERE updated_at > :sql_last_value"
    schedule => "*/2 * * * *"
    
    use_column_value => true
    tracking_column => "updated_at"
    tracking_column_type => "timestamp"
    last_run_metadata_path => "/data/logstash/.orders_last_run"
    
    # 添加标签区分数据源
    add_field => { "data_source" => "main_business" }
  }
}

# 日志数据库
input {
  jdbc {
    jdbc_connection_string => "jdbc:postgresql://log-db:5432/logs"
    jdbc_user => "log_reader"
    jdbc_password => "pass2"
    jdbc_driver_library => "/opt/logstash/postgresql.jar"
    jdbc_driver_class => "org.postgresql.Driver"
    
    statement => "SELECT * FROM application_logs WHERE created_at > :sql_last_value"
    schedule => "*/1 * * * *"  # 日志数据更频繁
    
    use_column_value => true
    tracking_column => "created_at"
    tracking_column_type => "timestamp"
    last_run_metadata_path => "/data/logstash/.logs_last_run"
    
    add_field => { "data_source" => "application_logs" }
  }
}
```

---

## 7. 📋 核心要点总结


### 7.1 必须掌握的基本概念


```
🔸 JDBC输入：从数据库读取数据的主要方式，需要配置连接信息和SQL查询
🔸 增量同步：只处理新增或更新的数据，避免重复处理，提高效率
🔸 Kafka输入：处理高吞吐量实时消息的最佳选择，适合微服务架构
🔸 Redis输入：轻量级消息队列，配置简单，适合中小规模应用
🔸 调度机制：控制数据读取频率，平衡实时性和系统负载
🔸 连接池：优化数据库连接性能，避免频繁建立连接的开销
```

### 7.2 关键配置要点


**🔹 JDBC配置核心**
```
必备配置：
- 连接字符串、用户名密码（门票）
- 驱动包路径和类名（工具）
- SQL查询语句（任务清单）
- 调度时间（执行计划）

增量配置：
- use_column_value = true（启用增量）
- tracking_column（追踪字段）
- last_run_metadata_path（状态保存位置）
```

**🔹 队列输入选择**
```
选择Kafka的情况：
✅ 数据量大（每秒千条以上）
✅ 需要高可靠性
✅ 多个消费者处理
✅ 微服务架构

选择Redis的情况：
✅ 配置简单快速
✅ 数据量中等
✅ 已有Redis基础设施
✅ 实时性要求高
```

### 7.3 性能优化策略


**⚡ 数据库输入优化**
```
查询优化：
- 在tracking_column上建索引
- 使用LIMIT限制单次查询量
- 避免SELECT *，只查询需要的字段
- 合理设置jdbc_page_size

调度优化：
- 根据数据更新频率设置schedule
- 避免业务高峰期执行重查询
- 使用只读账户，降低风险

连接优化：
- 设置合适的超时时间
- 启用连接重试机制
- 使用连接池复用连接
```

### 7.4 实际应用建议


**🎯 根据业务场景选择方案**

```
电商订单同步：
方案：JDBC + 增量同步
频率：每5分钟
追踪：updated_at字段

实时用户行为：
方案：Kafka输入
特点：高吞吐量，实时处理

系统监控日志：
方案：Redis List
特点：简单可靠，快速配置

数据仓库ETL：
方案：JDBC + 定时调度
频率：每天凌晨执行
```

**核心记忆要点**：
- **JDBC输入**：数据库数据的搬运工，配置连接信息和SQL查询
- **增量同步**：只处理新数据，节省资源提高效率
- **Kafka/Redis**：处理实时消息流，各有优势场景
- **调度轮询**：控制执行频率，平衡实时性和性能
- **连接管理**：优化数据库连接，提升整体性能

> 💡 **学习建议**：先从简单的JDBC输入开始实践，掌握基本配置后再尝试增量同步，最后根据实际需求选择合适的队列输入方案。