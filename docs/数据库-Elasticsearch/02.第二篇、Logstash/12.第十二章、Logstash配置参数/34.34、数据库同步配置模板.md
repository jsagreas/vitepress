---
title: 34、数据库同步配置模板
---
## 📚 目录

1. [数据库同步基础概念](#1-数据库同步基础概念)
2. [MySQL数据同步配置](#2-MySQL数据同步配置)
3. [PostgreSQL数据采集](#3-PostgreSQL数据采集)
4. [增量数据同步策略](#4-增量数据同步策略)
5. [大表分页查询技巧](#5-大表分页查询技巧)
6. [数据变更检测机制](#6-数据变更检测机制)
7. [错误重试机制](#7-错误重试机制)
8. [核心要点总结](#8-核心要点总结)

---

## 1. 📊 数据库同步基础概念


### 1.1 什么是数据库同步


**简单理解**：数据库同步就像是把一个图书馆里的书籍信息复制到另一个图书馆的过程

```
原理示意：
数据库A (源头)    →    Logstash (搬运工)    →    目标系统 (目的地)
   |                        |                      |
 用户数据              读取、处理、转换           Elasticsearch
 订单数据                   |                    或其他系统
 商品数据              按照规则同步
```

**核心作用**：
- **数据备份**：防止重要数据丢失
- **系统解耦**：让不同系统独立工作
- **数据分析**：把业务数据导入分析系统
- **搜索优化**：将关系型数据导入搜索引擎

### 1.2 Logstash在数据库同步中的角色


**生活类比**：Logstash就像是一个**智能快递员**
- 定时去数据库"取件"（读取数据）
- 按照地址标签"分拣"（数据处理）
- 准确送达目的地（写入目标系统）

**技术特点**：
```
优势：
✅ 配置简单，不需要写复杂代码
✅ 支持多种数据库类型
✅ 内置数据转换功能
✅ 支持增量同步，效率高
✅ 有重试机制，保证可靠性

适用场景：
🎯 定时数据同步
🎯 数据迁移项目
🎯 实时数据分析
🎯 数据备份归档
```

---

## 2. 🐬 MySQL数据同步配置


### 2.1 基础连接配置


**配置说明**：就像给Logstash一把钥匙，让它能进入MySQL数据库

```ruby
# mysql-sync.conf
input {
  jdbc {
    # 数据库连接信息（相当于门牌号）
    jdbc_connection_string => "jdbc:mysql://localhost:3306/mydb?useSSL=false"
    jdbc_user => "logstash_user"           # 用户名
    jdbc_password => "your_password"        # 密码
    jdbc_driver_library => "/path/to/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.jdbc.Driver"
    
    # 查询语句（告诉Logstash要拿什么数据）
    statement => "SELECT * FROM users WHERE updated_at > :sql_last_value"
    
    # 同步频率（多久检查一次新数据）
    schedule => "*/5 * * * *"              # 每5分钟执行一次
    
    # 增量同步字段（用来判断哪些是新数据）
    use_column_value => true
    tracking_column => "updated_at"         # 跟踪字段
    tracking_column_type => "timestamp"     # 字段类型
    
    # 记录上次同步位置的文件
    last_run_metadata_path => "/opt/logstash/metadata/.logstash_jdbc_last_run"
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "users"                        # 目标索引名
    document_id => "%{id}"                  # 使用数据库ID作为文档ID
  }
}
```

### 2.2 完整用户表同步示例


**业务场景**：把MySQL用户表数据同步到Elasticsearch，用于用户搜索功能

```ruby
input {
  jdbc {
    # === 连接配置 ===
    jdbc_connection_string => "jdbc:mysql://localhost:3306/ecommerce"
    jdbc_user => "sync_user"
    jdbc_password => "sync123456"
    jdbc_driver_library => "/usr/share/logstash/lib/mysql-connector-java.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    
    # === 查询配置 ===
    statement => "
      SELECT 
        id, username, email, phone, status, 
        created_at, updated_at,
        CONCAT(first_name, ' ', last_name) as full_name
      FROM users 
      WHERE updated_at > :sql_last_value 
      ORDER BY updated_at ASC
    "
    
    # === 调度配置 ===
    schedule => "*/2 * * * *"                # 每2分钟同步一次
    use_column_value => true
    tracking_column => "updated_at"
    tracking_column_type => "timestamp"
    last_run_metadata_path => "/var/lib/logstash/users_last_run"
    
    # === 性能优化 ===
    jdbc_fetch_size => 1000                 # 每次获取1000条记录
    jdbc_page_size => 1000
  }
}

filter {
  # 数据清洗和转换
  mutate {
    # 移除不需要的字段
    remove_field => ["@version", "@timestamp"]
    
    # 添加同步时间标记
    add_field => { "sync_time" => "%{+yyyy-MM-dd HH:mm:ss}" }
  }
  
  # 处理用户状态
  if [status] == "1" {
    mutate { add_field => { "status_text" => "active" } }
  } else {
    mutate { add_field => { "status_text" => "inactive" } }
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "users-%{+yyyy.MM}"             # 按月分索引
    document_id => "%{id}"
    
    # 更新策略：如果文档存在则更新
    action => "index"
  }
  
  # 同时输出到控制台（调试用）
  stdout { 
    codec => rubydebug { metadata => false } 
  }
}
```

### 2.3 多表关联同步


**复杂场景**：同步用户信息时，同时包含用户的订单统计

```ruby
input {
  jdbc {
    jdbc_connection_string => "jdbc:mysql://localhost:3306/shop"
    jdbc_user => "logstash"
    jdbc_password => "password"
    jdbc_driver_library => "/opt/logstash/lib/mysql-connector.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    
    # 复杂查询：关联多个表
    statement => "
      SELECT 
        u.id, u.username, u.email, u.updated_at,
        COUNT(o.id) as order_count,
        COALESCE(SUM(o.total_amount), 0) as total_spent,
        MAX(o.created_at) as last_order_date
      FROM users u
      LEFT JOIN orders o ON u.id = o.user_id
      WHERE u.updated_at > :sql_last_value
      GROUP BY u.id, u.username, u.email, u.updated_at
      ORDER BY u.updated_at ASC
    "
    
    schedule => "*/10 * * * *"               # 每10分钟同步
    use_column_value => true
    tracking_column => "updated_at"
    tracking_column_type => "timestamp"
    last_run_metadata_path => "/var/lib/logstash/user_orders_sync"
  }
}

filter {
  # 计算用户等级
  if [total_spent] {
    if [total_spent] > 10000 {
      mutate { add_field => { "user_level" => "VIP" } }
    } else if [total_spent] > 1000 {
      mutate { add_field => { "user_level" => "Gold" } }
    } else {
      mutate { add_field => { "user_level" => "Silver" } }
    }
  }
}
```

---

## 3. 🐘 PostgreSQL数据采集


### 3.1 PostgreSQL基础配置


**与MySQL的区别**：PostgreSQL的配置稍有不同，但原理相同

```ruby
input {
  jdbc {
    # PostgreSQL连接配置
    jdbc_connection_string => "jdbc:postgresql://localhost:5432/mydb"
    jdbc_user => "postgres"
    jdbc_password => "password"
    jdbc_driver_library => "/opt/logstash/lib/postgresql-jdbc.jar"
    jdbc_driver_class => "org.postgresql.Driver"
    
    # PostgreSQL特有的查询语法
    statement => "
      SELECT * FROM products 
      WHERE updated_at > :sql_last_value
      ORDER BY updated_at ASC
    "
    
    schedule => "*/5 * * * *"
    use_column_value => true
    tracking_column => "updated_at"
    tracking_column_type => "timestamp"
    last_run_metadata_path => "/var/lib/logstash/pg_products_sync"
  }
}
```

### 3.2 处理PostgreSQL特殊数据类型


**场景说明**：PostgreSQL支持很多特殊数据类型，需要特别处理

```ruby
input {
  jdbc {
    jdbc_connection_string => "jdbc:postgresql://localhost:5432/analytics"
    jdbc_user => "analytics_user"
    jdbc_password => "analytics_pass"
    jdbc_driver_library => "/opt/logstash/lib/postgresql.jar"
    jdbc_driver_class => "org.postgresql.Driver"
    
    statement => "
      SELECT 
        id, 
        title,
        tags,                           -- PostgreSQL数组类型
        metadata,                       -- JSONB类型
        coordinates,                    -- Point类型
        price_range,                    -- NumRange类型
        created_at::text as created_time  -- 时间转换为文本
      FROM products 
      WHERE updated_at > :sql_last_value
    "
    
    schedule => "*/3 * * * *"
    use_column_value => true
    tracking_column => "updated_at"
    tracking_column_type => "timestamp"
  }
}

filter {
  # 处理PostgreSQL数组
  if [tags] {
    # 将PostgreSQL数组格式转换为JSON数组
    mutate {
      gsub => [
        "tags", "^\{", "[",
        "tags", "\}$", "]",
        "tags", '\"', '"'
      ]
    }
    
    json {
      source => "tags"
      target => "tag_list"
    }
  }
  
  # 处理JSONB数据
  if [metadata] {
    json {
      source => "metadata"
      target => "product_meta"
    }
  }
}
```

---

## 4. ⚡ 增量数据同步策略


### 4.1 基于时间戳的增量同步


**核心原理**：就像看报纸一样，每次只看比上次看的时间更新的内容

```
增量同步流程：
第一次运行：
├── 读取所有数据 (WHERE updated_at > '1970-01-01')
├── 记录最后一条数据的时间: 2024-01-15 10:30:00
└── 保存到元数据文件

第二次运行：
├── 读取元数据文件，获取上次时间: 2024-01-15 10:30:00
├── 查询新数据 (WHERE updated_at > '2024-01-15 10:30:00')
├── 更新最后时间: 2024-01-15 11:00:00
└── 保存新的元数据
```

### 4.2 配置时间戳增量同步


```ruby
input {
  jdbc {
    jdbc_connection_string => "jdbc:mysql://localhost:3306/orders"
    jdbc_user => "sync_user"
    jdbc_password => "password"
    jdbc_driver_library => "/opt/logstash/lib/mysql-connector.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    
    # 关键配置：增量查询
    statement => "
      SELECT 
        order_id, customer_id, total_amount, status, updated_at
      FROM orders 
      WHERE updated_at > :sql_last_value  -- 这是关键！
      ORDER BY updated_at ASC              -- 必须排序
    "
    
    # 增量同步必须配置
    use_column_value => true               # 启用列值跟踪
    tracking_column => "updated_at"        # 跟踪哪个字段
    tracking_column_type => "timestamp"    # 字段类型
    
    # 元数据存储位置
    last_run_metadata_path => "/var/lib/logstash/orders_last_run"
    
    # 同步频率
    schedule => "*/1 * * * *"              # 每分钟检查一次
  }
}
```

### 4.3 基于ID的增量同步


**适用场景**：当表没有更新时间字段时，可以使用自增ID

```ruby
input {
  jdbc {
    jdbc_connection_string => "jdbc:mysql://localhost:3306/logs"
    jdbc_user => "logstash"
    jdbc_password => "password"
    jdbc_driver_library => "/opt/logstash/lib/mysql-connector.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    
    statement => "
      SELECT log_id, user_id, action, details, created_at
      FROM user_logs 
      WHERE log_id > :sql_last_value
      ORDER BY log_id ASC
    "
    
    # 基于ID的增量配置
    use_column_value => true
    tracking_column => "log_id"           # 跟踪ID字段
    tracking_column_type => "numeric"     # 数字类型
    
    last_run_metadata_path => "/var/lib/logstash/logs_by_id"
    schedule => "*/30 * * * * *"          # 每30秒检查
  }
}
```

### 4.4 混合增量策略


**复杂场景**：结合时间戳和状态字段的增量同步

```ruby
input {
  jdbc {
    statement => "
      SELECT * FROM transactions 
      WHERE (updated_at > :sql_last_value) 
         OR (status = 'pending' AND created_at > DATE_SUB(NOW(), INTERVAL 1 HOUR))
      ORDER BY updated_at ASC
    "
    
    # 多重增量条件说明：
    # 1. updated_at > :sql_last_value  -- 常规增量
    # 2. status = 'pending'            -- 处理中的交易要持续监控
    # 3. created_at > 1小时前           -- 避免处理太老的pending记录
    
    use_column_value => true
    tracking_column => "updated_at"
    tracking_column_type => "timestamp"
    schedule => "*/2 * * * *"
  }
}
```

---

## 5. 📄 大表分页查询技巧


### 5.1 为什么需要分页


**问题说明**：就像搬家一样，一次搬太多东西会累坏，分批搬更安全

```
大表问题：
❌ 一次查询1000万条记录
   ├── 内存占用过大 (可能几GB)
   ├── 查询超时
   ├── 数据库压力过大
   └── 网络传输缓慢

✅ 分页查询 (每次1000条)
   ├── 内存占用可控 (几MB)
   ├── 查询速度快
   ├── 对数据库友好
   └── 支持断点续传
```

### 5.2 基础分页配置


```ruby
input {
  jdbc {
    jdbc_connection_string => "jdbc:mysql://localhost:3306/bigdata"
    jdbc_user => "readonly_user"
    jdbc_password => "password"
    jdbc_driver_library => "/opt/logstash/lib/mysql-connector.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    
    statement => "
      SELECT user_id, action_type, timestamp, details
      FROM user_actions 
      WHERE timestamp > :sql_last_value
      ORDER BY timestamp ASC
    "
    
    # === 分页关键配置 ===
    jdbc_paging_enabled => true            # 启用分页
    jdbc_page_size => 5000                 # 每页5000条记录
    jdbc_fetch_size => 5000                # 每次从数据库获取5000条
    
    # === 性能优化 ===
    jdbc_pool_timeout => 300               # 连接池超时时间(秒)
    jdbc_validation_timeout => 3600        # 连接验证超时
    
    use_column_value => true
    tracking_column => "timestamp"
    tracking_column_type => "timestamp"
    schedule => "0 */6 * * *"              # 每6小时运行一次
  }
}
```

### 5.3 大表优化实践


**场景：同步千万级订单表**

```ruby
input {
  jdbc {
    # 连接配置优化
    jdbc_connection_string => "jdbc:mysql://localhost:3306/orders?useSSL=false&useServerPrepStmts=true"
    jdbc_user => "batch_user"
    jdbc_password => "batch_password"
    jdbc_driver_library => "/opt/logstash/lib/mysql-connector.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    
    # 优化的查询语句
    statement => "
      SELECT /*+ USE_INDEX(orders, idx_updated_at) */
        order_id, customer_id, order_status, total_amount, 
        created_at, updated_at
      FROM orders 
      WHERE updated_at > :sql_last_value
        AND updated_at < DATE_ADD(:sql_last_value, INTERVAL 1 DAY)  -- 限制时间范围
      ORDER BY updated_at ASC, order_id ASC                        -- 双字段排序确保稳定
    "
    
    # 分页和性能配置
    jdbc_paging_enabled => true
    jdbc_page_size => 2000                 # 适中的页面大小
    jdbc_fetch_size => 2000
    
    # 连接池配置
    connection_retry_attempts => 3         # 重试3次
    connection_retry_attempts_wait_time => 5  # 重试间隔5秒
    
    use_column_value => true
    tracking_column => "updated_at"
    tracking_column_type => "timestamp"
    
    # 降低同步频率，减少数据库压力
    schedule => "0 */3 * * *"              # 每3小时同步一次
  }
}

filter {
  # 分批处理，避免ES写入压力过大
  if [order_status] == "completed" {
    mutate { 
      add_tag => ["completed_order"] 
      add_field => { "process_priority" => "low" }
    }
  } else {
    mutate { 
      add_tag => ["active_order"] 
      add_field => { "process_priority" => "high" }
    }
  }
}

output {
  # 分开写入不同索引，分散压力
  if "completed_order" in [tags] {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "orders-completed-%{+yyyy.MM}"
      document_id => "%{order_id}"
    }
  } else {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "orders-active-%{+yyyy.MM.dd}"
      document_id => "%{order_id}"
    }
  }
}
```

---

## 6. 🔍 数据变更检测机制


### 6.1 触发器方式检测变更


**原理说明**：在数据库中设置"监听器"，当数据变化时自动记录

```sql
-- 在MySQL中创建变更日志表
CREATE TABLE data_change_log (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    table_name VARCHAR(100) NOT NULL,
    record_id BIGINT NOT NULL,
    operation_type ENUM('INSERT', 'UPDATE', 'DELETE') NOT NULL,
    changed_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    old_values JSON,
    new_values JSON,
    INDEX idx_changed_at (changed_at),
    INDEX idx_table_record (table_name, record_id)
);

-- 为用户表创建更新触发器
DELIMITER $$
CREATE TRIGGER users_update_trigger
    AFTER UPDATE ON users
    FOR EACH ROW
BEGIN
    INSERT INTO data_change_log (
        table_name, record_id, operation_type, old_values, new_values
    ) VALUES (
        'users', 
        NEW.id, 
        'UPDATE',
        JSON_OBJECT('name', OLD.name, 'email', OLD.email, 'status', OLD.status),
        JSON_OBJECT('name', NEW.name, 'email', NEW.email, 'status', NEW.status)
    );
END$$
DELIMITER ;
```

**Logstash配置监听变更日志**：

```ruby
input {
  jdbc {
    jdbc_connection_string => "jdbc:mysql://localhost:3306/mydb"
    jdbc_user => "logstash"
    jdbc_password => "password"
    jdbc_driver_library => "/opt/logstash/lib/mysql-connector.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    
    # 查询变更日志
    statement => "
      SELECT 
        id, table_name, record_id, operation_type,
        old_values, new_values, changed_at
      FROM data_change_log 
      WHERE id > :sql_last_value
      ORDER BY id ASC
    "
    
    use_column_value => true
    tracking_column => "id"
    tracking_column_type => "numeric"
    schedule => "*/10 * * * * *"          # 每10秒检查一次
  }
}

filter {
  # 解析JSON格式的变更数据
  if [old_values] {
    json {
      source => "old_values"
      target => "before"
    }
  }
  
  if [new_values] {
    json {
      source => "new_values"
      target => "after"
    }
  }
  
  # 根据操作类型处理
  if [operation_type] == "UPDATE" {
    # 找出具体变更的字段
    ruby {
      code => "
        before = event.get('before') || {}
        after = event.get('after') || {}
        changed_fields = []
        
        after.each do |key, value|
          if before[key] != value
            changed_fields << key
          end
        end
        
        event.set('changed_fields', changed_fields)
      "
    }
  }
}
```

### 6.2 版本号方式检测变更


**适用场景**：表中有版本号字段，每次更新版本号+1

```ruby
input {
  jdbc {
    statement => "
      SELECT 
        id, username, email, status, version, updated_at
      FROM users 
      WHERE version > :sql_last_value
      ORDER BY version ASC
    "
    
    use_column_value => true
    tracking_column => "version"          # 跟踪版本号
    tracking_column_type => "numeric"
    schedule => "*/1 * * * *"
  }
}

filter {
  # 计算版本差异
  if [version] and [version] > 1 {
    mutate {
      add_field => { "is_updated" => "true" }
      add_field => { "change_type" => "modification" }
    }
  } else {
    mutate {
      add_field => { "is_updated" => "false" }
      add_field => { "change_type" => "creation" }
    }
  }
}
```

### 6.3 哈希值检测变更


**高级技巧**：计算记录的哈希值，哈希值变化说明数据变更

```ruby
input {
  jdbc {
    statement => "
      SELECT 
        id, name, email, phone, address, updated_at,
        MD5(CONCAT(name, email, phone, IFNULL(address, ''))) as data_hash
      FROM customers 
      WHERE updated_at > :sql_last_value
      ORDER BY updated_at ASC
    "
    
    use_column_value => true
    tracking_column => "updated_at"
    tracking_column_type => "timestamp"
    schedule => "*/5 * * * *"
  }
}

filter {
  # 与之前的哈希值对比（需要先查询ES中的旧哈希值）
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "customers"
    query => "id:%{id}"
    fields => { "data_hash" => "old_hash" }
  }
  
  # 对比哈希值
  if [data_hash] != [old_hash] {
    mutate {
      add_field => { "has_changes" => "true" }
      add_tag => ["data_changed"]
    }
  } else {
    mutate {
      add_field => { "has_changes" => "false" }
      add_tag => ["no_change"]
    }
  }
}

output {
  # 只同步有变化的数据
  if "data_changed" in [tags] {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "customers"
      document_id => "%{id}"
    }
  }
}
```

---

## 7. 🔄 错误重试机制


### 7.1 基础重试配置


**重要性**：网络不稳定、数据库连接中断等问题很常见，重试机制让同步更可靠

```ruby
input {
  jdbc {
    jdbc_connection_string => "jdbc:mysql://localhost:3306/shop"
    jdbc_user => "sync_user"
    jdbc_password => "password"
    jdbc_driver_library => "/opt/logstash/lib/mysql-connector.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    
    statement => "SELECT * FROM products WHERE updated_at > :sql_last_value"
    
    # === 重试配置 ===
    connection_retry_attempts => 5         # 连接失败时重试5次
    connection_retry_attempts_wait_time => 10  # 每次重试间隔10秒
    
    # === 超时配置 ===
    jdbc_pool_timeout => 300               # 连接池超时5分钟
    jdbc_validation_timeout => 30          # 连接验证超时30秒
    
    use_column_value => true
    tracking_column => "updated_at"
    tracking_column_type => "timestamp"
    schedule => "*/5 * * * *"
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "products"
    document_id => "%{id}"
    
    # ES写入重试配置
    retry_on_conflict => 3                 # 文档冲突时重试3次
    retry_max_interval => 5                # 最大重试间隔5秒
  }
}
```

### 7.2 智能重试策略


**指数退避重试**：失败次数越多，等待时间越长

```ruby
input {
  jdbc {
    jdbc_connection_string => "jdbc:mysql://db-server:3306/ecommerce"
    jdbc_user => "readonly"
    jdbc_password => "readonly123"
    jdbc_driver_library => "/opt/logstash/lib/mysql-connector.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    
    statement => "
      SELECT order_id, customer_id, total, status, created_at
      FROM orders 
      WHERE created_at > :sql_last_value
      ORDER BY created_at ASC
    "
    
    # 渐进式重试配置
    connection_retry_attempts => 10        # 最多重试10次
    connection_retry_attempts_wait_time => 5   # 基础等待时间5秒
    
    # 实际等待时间会是: 5秒, 10秒, 20秒, 40秒...
    
    # 健康检查
    jdbc_validate_connection => true       # 启用连接验证
    jdbc_validation_timeout => 60         # 验证超时1分钟
    
    use_column_value => true
    tracking_column => "created_at"
    tracking_column_type => "timestamp"
    schedule => "*/3 * * * *"
  }
}
```

### 7.3 错误处理和日志记录


**完整的错误处理方案**：

```ruby
input {
  jdbc {
    # 数据库配置
    jdbc_connection_string => "jdbc:mysql://primary-db:3306/analytics"
    jdbc_user => "analytics_reader"
    jdbc_password => "${DB_PASSWORD}"      # 使用环境变量
    jdbc_driver_library => "/opt/logstash/lib/mysql-connector.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    
    statement => "
      SELECT 
        event_id, user_id, event_type, event_data, 
        created_at, updated_at
      FROM user_events 
      WHERE updated_at > :sql_last_value
      ORDER BY updated_at ASC
    "
    
    # 全面的重试和超时配置
    connection_retry_attempts => 8
    connection_retry_attempts_wait_time => 10
    jdbc_pool_timeout => 600              # 10分钟超时
    jdbc_validation_timeout => 120        # 2分钟验证超时
    
    # 分页防止超时
    jdbc_paging_enabled => true
    jdbc_page_size => 1000
    
    use_column_value => true
    tracking_column => "updated_at"
    tracking_column_type => "timestamp"
    schedule => "*/2 * * * *"
  }
}

filter {
  # 添加错误处理标记
  mutate {
    add_field => { 
      "sync_attempt" => "1" 
      "sync_timestamp" => "%{+yyyy-MM-dd HH:mm:ss}"
    }
  }
  
  # 数据验证
  if ![event_id] or ![user_id] {
    mutate {
      add_tag => ["invalid_data"]
      add_field => { "error_reason" => "missing_required_fields" }
    }
  }
}

output {
  # 正常数据写入ES
  if "invalid_data" not in [tags] {
    elasticsearch {
      hosts => ["es-cluster:9200"]
      index => "user-events-%{+yyyy.MM}"
      document_id => "%{event_id}"
      
      # ES重试配置
      retry_on_conflict => 5
      retry_max_interval => 10
      
      # 批量写入优化
      flush_size => 500
      idle_flush_time => 10
    }
  }
  
  # 错误数据写入错误队列
  if "invalid_data" in [tags] {
    file {
      path => "/var/log/logstash/sync_errors_%{+yyyy-MM-dd}.log"
      codec => json_lines
    }
  }
  
  # 监控输出（可选）
  if [sync_attempt] == "1" {
    http {
      url => "http://monitoring-service/logstash/sync-status"
      http_method => "post"
      content_type => "application/json"
      mapping => {
        "pipeline" => "user-events-sync"
        "timestamp" => "%{sync_timestamp}"
        "record_count" => "1"
        "status" => "success"
      }
    }
  }
}
```

### 7.4 故障转移配置


**多数据库源故障转移**：主库挂了自动切换到备库

```ruby
input {
  # 主数据库配置
  jdbc {
    id => "primary_db"
    jdbc_connection_string => "jdbc:mysql://primary-db:3306/production"
    jdbc_user => "logstash_primary"
    jdbc_password => "primary_password"
    jdbc_driver_library => "/opt/logstash/lib/mysql-connector.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    
    statement => "SELECT * FROM orders WHERE updated_at > :sql_last_value"
    
    connection_retry_attempts => 3        # 主库只重试3次
    connection_retry_attempts_wait_time => 5
    
    use_column_value => true
    tracking_column => "updated_at" 
    tracking_column_type => "timestamp"
    schedule => "*/2 * * * *"
    
    # 主库标记
    add_field => { "data_source" => "primary" }
  }
  
  # 备份数据库配置（延迟5分钟启动）
  jdbc {
    id => "backup_db"
    jdbc_connection_string => "jdbc:mysql://backup-db:3306/production"
    jdbc_user => "logstash_backup"
    jdbc_password => "backup_password"
    jdbc_driver_library => "/opt/logstash/lib/mysql-connector.jar"
    jdbc_driver_class => "com.mysql.cj.jdbc.Driver"
    
    # 同样的查询
    statement => "SELECT * FROM orders WHERE updated_at > :sql_last_value"
    
    connection_retry_attempts => 8        # 备库多重试几次
    connection_retry_attempts_wait_time => 10
    
    use_column_value => true
    tracking_column => "updated_at"
    tracking_column_type => "timestamp"
    schedule => "5-59/2 * * * *"          # 从第5分钟开始，避免与主库冲突
    
    # 备库标记
    add_field => { "data_source" => "backup" }
  }
}

filter {
  # 去重逻辑（避免主备库数据重复）
  fingerprint {
    source => ["order_id", "updated_at"]
    target => "[@metadata][fingerprint]"
    method => "SHA1"
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "orders"
    document_id => "%{[@metadata][fingerprint]}"  # 使用指纹作为文档ID避免重复
  }
}
```

---

## 8. 📋 核心要点总结


### 8.1 必须掌握的核心概念


```
🔸 数据库同步：让不同系统间的数据保持一致的过程
🔸 增量同步：只同步变化的数据，提高效率
🔸 JDBC插件：Logstash连接关系型数据库的桥梁
🔸 元数据文件：记录上次同步位置，实现断点续传
🔸 分页查询：分批处理大量数据，避免内存溢出
🔸 重试机制：提高同步可靠性，应对网络异常
```

### 8.2 关键配置参数记忆


**🔹 连接配置要点**
```
必填参数：
• jdbc_connection_string  - 数据库连接地址
• jdbc_user/jdbc_password - 认证信息  
• jdbc_driver_library     - 驱动包路径
• jdbc_driver_class       - 驱动类名
• statement               - SQL查询语句
```

**🔹 增量同步配置**
```
核心配置：
• use_column_value => true           - 启用列值跟踪
• tracking_column => "updated_at"    - 跟踪字段名
• tracking_column_type => "timestamp" - 字段类型
• last_run_metadata_path => "路径"    - 元数据存储位置
```

**🔹 性能优化配置**
```
重要参数：
• jdbc_paging_enabled => true  - 启用分页
• jdbc_page_size => 1000      - 每页记录数
• jdbc_fetch_size => 1000     - 每次获取数量
• schedule => "*/5 * * * *"   - 同步频率
```

### 8.3 最佳实践建议


**📊 配置建议表格**

| 数据量级 | 页面大小 | 同步频率 | 重试次数 | 适用场景 |
|---------|---------|---------|---------|---------|
| **小表** (<1万) | 500-1000 | 每分钟 | 3次 | 用户表、配置表 |
| **中表** (1万-100万) | 1000-5000 | 每5分钟 | 5次 | 订单表、商品表 |
| **大表** (100万-1000万) | 2000-5000 | 每小时 | 8次 | 日志表、交易表 |
| **超大表** (>1000万) | 5000-10000 | 每6小时 | 10次 | 历史数据表 |

**🔧 故障排查清单**
```
常见问题检查：
□ 数据库连接是否正常？
□ 驱动包是否存在且版本匹配？
□ 用户权限是否足够？
□ SQL语句语法是否正确？
□ 网络是否稳定？
□ 磁盘空间是否充足？
□ 内存是否足够？
□ 目标索引是否存在？
```

**⚡ 性能优化技巧**
```
数据库端优化：
• 为tracking_column建立索引
• 使用只读账户减少锁等待
• 合理设置数据库连接池大小
• 避免在业务高峰期同步

Logstash端优化：
• 调整JVM内存大小
• 使用SSD存储元数据文件
• 合理设置worker数量
• 监控管道性能指标

网络优化：
• 使用内网连接减少延迟
• 启用压缩减少传输量
• 配置合适的超时时间
```

### 8.4 实际应用价值


**🎯 业务场景应用**
- **电商系统**：商品信息同步到搜索引擎
- **CRM系统**：客户数据实时备份和分析
- **日志系统**：业务日志归档和检索
- **数据仓库**：多源数据整合分析

**🔧 运维实践价值**
- **数据备份**：自动化数据备份策略
- **系统解耦**：降低系统间依赖关系
- **性能优化**：减少数据库查询压力
- **监控告警**：及时发现数据同步异常

**💡 学习建议**
```
学习路径：
1. 先掌握基础配置（连接、简单查询）
2. 理解增量同步原理和配置
3. 学习性能优化技巧
4. 掌握错误处理和监控
5. 实践复杂业务场景

重点关注：
• SQL语句的编写和优化
• 增量字段的选择和索引
• 错误日志的分析和处理
• 监控指标的设置和观察
```

**核心记忆**：
- 数据库同步如搬家，分批进行效率高
- 增量同步看时间，记录位置很关键  
- 分页查询控内存，重试机制保可靠
- 性能优化靠索引，监控告警不可少