---
title: 33、日志文件采集配置模板
---
## 📚 目录

1. [日志采集基础概念](#1-日志采集基础概念)
2. [Apache访问日志解析](#2-Apache访问日志解析)
3. [Nginx日志处理配置](#3-Nginx日志处理配置)
4. [应用程序日志采集](#4-应用程序日志采集)
5. [系统日志处理](#5-系统日志处理)
6. [错误日志过滤](#6-错误日志过滤)
7. [多格式日志统一处理](#7-多格式日志统一处理)
8. [核心要点总结](#8-核心要点总结)

---

## 1. 📊 日志采集基础概念


### 1.1 什么是日志采集


**通俗理解**：就像收集不同种类的垃圾一样，我们需要收集服务器上各种各样的日志文件

```
现实生活中：                   日志采集中：
厨余垃圾 → 厨余垃圾桶          Apache日志 → Logstash
可回收垃圾 → 回收垃圾桶        Nginx日志 → Logstash  
有害垃圾 → 有害垃圾桶          应用日志 → Logstash
                              ↓
垃圾处理厂统一处理              Elasticsearch统一存储分析
```

### 1.2 日志采集的核心流程


**🔄 三步走策略**：
```
步骤1：读取日志文件（input输入）
       ↓
步骤2：解析和处理日志（filter过滤）
       ↓ 
步骤3：发送到存储系统（output输出）
```

**💡 为什么需要日志采集**：
- **问题排查**：当网站出错时，通过日志找原因
- **性能监控**：看哪些页面访问最多，响应最慢
- **安全分析**：发现可疑的访问行为
- **业务分析**：了解用户访问习惯和偏好

### 1.3 常见日志类型概览


| 日志类型 | **作用说明** | **典型位置** | **主要内容** |
|---------|------------|-------------|-------------|
| 🌐 **Web服务器日志** | 记录网站访问情况 | `/var/log/apache2/` | 访问IP、请求URL、响应状态 |
| 🛠 **应用程序日志** | 记录程序运行状态 | `/var/log/app/` | 错误信息、业务流程记录 |
| 🖥 **系统日志** | 记录操作系统事件 | `/var/log/syslog` | 系统启动、用户登录等 |
| ⚠️ **错误日志** | 记录各种错误信息 | `/var/log/error.log` | 程序崩溃、配置错误等 |

---

## 2. 🌐 Apache访问日志解析


### 2.1 Apache日志长什么样


**📝 典型的Apache访问日志**：
```
192.168.1.100 - - [25/Dec/2023:10:00:00 +0000] "GET /index.html HTTP/1.1" 200 1234 "http://www.example.com" "Mozilla/5.0"
```

**🔍 这串字符是什么意思**：
- `192.168.1.100` - 访问者的IP地址（谁来访问）
- `[25/Dec/2023:10:00:00 +0000]` - 访问时间（什么时候访问）
- `"GET /index.html HTTP/1.1"` - 请求方法和页面（访问了什么）
- `200` - 响应状态码（访问是否成功，200表示成功）
- `1234` - 响应大小（发送了多少字节的数据）
- `"http://www.example.com"` - 来源页面（从哪里跳转过来）
- `"Mozilla/5.0"` - 浏览器信息（用什么浏览器访问）

### 2.2 Apache日志采集配置


**📋 基础配置模板**：
```ruby
input {
  file {
    path => "/var/log/apache2/access.log"
    start_position => "beginning"
    type => "apache-access"
  }
}

filter {
  if [type] == "apache-access" {
    grok {
      match => { 
        "message" => "%{COMBINEDAPACHELOG}" 
      }
    }
    
    # 将时间字段转换为标准格式
    date {
      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
    }
    
    # 将响应状态码转换为数字
    mutate {
      convert => { "response" => "integer" }
      convert => { "bytes" => "integer" }
    }
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "apache-logs-%{+YYYY.MM.dd}"
  }
}
```

**🎯 配置详解**：

**input部分解释**：
- `path` - 告诉Logstash去哪里找Apache日志文件
- `start_position` - 从文件开头开始读取（beginning）还是从末尾（end）
- `type` - 给这类日志打个标签，方便后面处理

**filter部分解释**：
- `grok` - 这是Logstash的"翻译官"，把一行日志拆分成各个字段
- `%{COMBINEDAPACHELOG}` - 这是预定义的Apache日志格式模板
- `date` - 把日志中的时间字符串转换成Elasticsearch能理解的时间格式
- `mutate` - 数据类型转换，把字符串的状态码转换成数字

### 2.3 常见Apache日志问题处理


**⚠️ 问题1：日志格式不匹配**
```ruby
filter {
  grok {
    match => { "message" => "%{COMBINEDAPACHELOG}" }
    # 如果匹配失败，添加错误标记
    tag_on_failure => ["_grokparsefailure_apache"]
  }
  
  # 处理解析失败的情况
  if "_grokparsefailure_apache" in [tags] {
    mutate {
      add_field => { "parse_error" => "Apache log format mismatch" }
    }
  }
}
```

**⚠️ 问题2：过滤掉无用的请求**
```ruby
filter {
  # 过滤掉静态文件请求（图片、CSS、JS等）
  if [request] =~ /\.(css|js|png|jpg|ico|gif)$/ {
    drop { }
  }
  
  # 过滤掉搜索引擎爬虫
  if [agent] =~ /bot|spider|crawler/i {
    mutate {
      add_tag => ["bot"]
    }
  }
}
```

---

## 3. 🔧 Nginx日志处理配置


### 3.1 Nginx日志特点


**📝 Nginx访问日志示例**：
```
10.0.0.1 - - [21/Sep/2023:12:00:00 +0000] "GET /api/users HTTP/1.1" 200 456 "-" "curl/7.68.0" "0.123"
```

**🆚 Nginx vs Apache日志的区别**：
- **相似点**：都记录IP、时间、请求、状态码等基本信息
- **不同点**：Nginx通常会额外记录响应时间、上游服务器信息等

### 3.2 Nginx日志采集配置


**📋 完整配置示例**：
```ruby
input {
  file {
    path => "/var/log/nginx/access.log"
    start_position => "beginning"
    type => "nginx-access"
  }
}

filter {
  if [type] == "nginx-access" {
    grok {
      match => { 
        "message" => "%{NGINXACCESS}" 
      }
    }
    
    # 处理时间字段
    date {
      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
    }
    
    # 数据类型转换
    mutate {
      convert => { "response" => "integer" }
      convert => { "bytes" => "integer" }
      convert => { "responsetime" => "float" }
    }
    
    # 添加响应时间分类
    if [responsetime] {
      if [responsetime] < 0.1 {
        mutate { add_tag => ["fast_response"] }
      } else if [responsetime] < 1.0 {
        mutate { add_tag => ["normal_response"] }
      } else {
        mutate { add_tag => ["slow_response"] }
      }
    }
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "nginx-logs-%{+YYYY.MM.dd}"
  }
}
```

**🎯 配置重点说明**：

**响应时间分析**：
- `快速响应` - 小于0.1秒，用户感觉很流畅
- `正常响应` - 0.1-1秒之间，可以接受
- `慢响应` - 超过1秒，需要优化

### 3.3 Nginx错误日志处理


**📋 错误日志配置**：
```ruby
input {
  file {
    path => "/var/log/nginx/error.log"
    start_position => "beginning"
    type => "nginx-error"
  }
}

filter {
  if [type] == "nginx-error" {
    grok {
      match => { 
        "message" => "%{NGINXERROR}" 
      }
    }
    
    # 根据错误级别添加标签
    if [severity] == "error" {
      mutate { add_tag => ["critical"] }
    } else if [severity] == "warn" {
      mutate { add_tag => ["warning"] }
    }
  }
}
```

---

## 4. 💻 应用程序日志采集


### 4.1 应用日志的特点


**💡 什么是应用程序日志**：
就是你自己写的程序（比如Java、Python、Node.js程序）产生的日志，记录程序运行过程中的各种信息。

**📝 典型的应用日志格式**：
```
2023-09-21 14:30:15 INFO  [UserService] - User login successful: userId=12345
2023-09-21 14:30:20 ERROR [OrderService] - Failed to process order: orderId=67890, error=Payment timeout
2023-09-21 14:30:25 DEBUG [DatabaseConnection] - Query executed in 145ms
```

### 4.2 Java应用日志采集


**📋 Java应用日志配置**：
```ruby
input {
  file {
    path => "/var/log/myapp/application.log"
    start_position => "beginning"
    type => "java-app"
    # 处理多行日志（比如异常堆栈）
    codec => multiline {
      pattern => "^\d{4}-\d{2}-\d{2}"
      negate => true
      what => "previous"
    }
  }
}

filter {
  if [type] == "java-app" {
    grok {
      match => { 
        "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} \[%{DATA:class}\] - %{GREEDYDATA:log_message}" 
      }
    }
    
    # 时间处理
    date {
      match => [ "timestamp", "yyyy-MM-dd HH:mm:ss" ]
    }
    
    # 根据日志级别分类
    if [level] == "ERROR" {
      mutate { add_tag => ["error", "needs_attention"] }
    } else if [level] == "WARN" {
      mutate { add_tag => ["warning"] }
    } else if [level] == "INFO" {
      mutate { add_tag => ["info"] }
    }
    
    # 提取用户ID和订单ID等业务信息
    if [log_message] =~ /userId=(\d+)/ {
      grok {
        match => { "log_message" => "userId=%{NUMBER:user_id}" }
      }
    }
    
    if [log_message] =~ /orderId=(\d+)/ {
      grok {
        match => { "log_message" => "orderId=%{NUMBER:order_id}" }
      }
    }
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "java-app-logs-%{+YYYY.MM.dd}"
  }
}
```

**🎯 重点说明**：

**multiline处理**：
- Java程序出错时，通常会打印很多行的错误堆栈
- `multiline`插件可以把这些多行内容合并成一条日志记录
- `pattern` - 用正则表达式识别新日志的开始（通常是时间戳）

**业务信息提取**：
- 从日志内容中提取有用的业务信息（如用户ID、订单ID）
- 这样就可以按用户或订单来查询和分析日志

### 4.3 Python应用日志采集


**📋 Python应用日志配置**：
```ruby
input {
  file {
    path => "/var/log/python-app/*.log"
    start_position => "beginning"
    type => "python-app"
  }
}

filter {
  if [type] == "python-app" {
    # Python日志通常格式更灵活
    grok {
      match => { 
        "message" => "%{TIMESTAMP_ISO8601:timestamp} - %{WORD:logger} - %{LOGLEVEL:level} - %{GREEDYDATA:log_message}" 
      }
    }
    
    date {
      match => [ "timestamp", "yyyy-MM-dd HH:mm:ss,SSS" ]
    }
    
    # 检测是否包含异常信息
    if [log_message] =~ /Traceback|Exception|Error:/ {
      mutate { add_tag => ["exception", "critical"] }
    }
  }
}
```

---

## 5. 🖥 系统日志处理


### 5.1 系统日志概述


**💡 什么是系统日志**：
操作系统（Linux、Windows等）自动产生的日志，记录系统级别的事件，比如：
- 用户登录/登出
- 系统启动/关闭
- 硬件故障
- 安全事件

**📍 Linux系统日志位置**：
```
/var/log/syslog          - 系统主日志
/var/log/auth.log        - 认证相关日志
/var/log/kern.log        - 内核日志
/var/log/cron.log        - 定时任务日志
/var/log/mail.log        - 邮件系统日志
```

### 5.2 系统日志采集配置


**📋 syslog采集配置**：
```ruby
input {
  file {
    path => "/var/log/syslog"
    start_position => "beginning"
    type => "syslog"
  }
}

filter {
  if [type] == "syslog" {
    grok {
      match => { 
        "message" => "%{SYSLOGTIMESTAMP:timestamp} %{IPORHOST:host} %{DATA:program}: %{GREEDYDATA:log_message}" 
      }
    }
    
    date {
      match => [ "timestamp", "MMM dd HH:mm:ss", "MMM  d HH:mm:ss" ]
    }
    
    # 根据程序类型分类
    if [program] == "sshd" {
      mutate { add_tag => ["ssh", "security"] }
    } else if [program] == "sudo" {
      mutate { add_tag => ["sudo", "security"] }
    } else if [program] == "cron" {
      mutate { add_tag => ["cron", "scheduled"] }
    }
    
    # 检测安全相关事件
    if [log_message] =~ /Failed password|Invalid user|authentication failure/ {
      mutate { add_tag => ["security_alert", "failed_login"] }
    }
  }
}

output {
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "syslog-%{+YYYY.MM.dd}"
  }
}
```

### 5.3 认证日志专门处理


**📋 auth.log采集配置**：
```ruby
input {
  file {
    path => "/var/log/auth.log"
    start_position => "beginning"
    type => "auth-log"
  }
}

filter {
  if [type] == "auth-log" {
    grok {
      match => { 
        "message" => "%{SYSLOGTIMESTAMP:timestamp} %{IPORHOST:host} %{DATA:program}\[%{POSINT:pid}\]: %{GREEDYDATA:log_message}" 
      }
    }
    
    # 提取登录相关信息
    if [log_message] =~ /Accepted password for/ {
      grok {
        match => { "log_message" => "Accepted password for %{USERNAME:username} from %{IP:source_ip}" }
      }
      mutate { add_tag => ["successful_login"] }
    } else if [log_message] =~ /Failed password for/ {
      grok {
        match => { "log_message" => "Failed password for %{USERNAME:username} from %{IP:source_ip}" }
      }
      mutate { add_tag => ["failed_login", "security_alert"] }
    }
  }
}
```

---

## 6. ⚠️ 错误日志过滤


### 6.1 错误日志的重要性


**💡 为什么错误日志很重要**：
错误日志就像汽车的故障警示灯，能第一时间告诉我们系统哪里出了问题：
- **及时发现问题** - 系统出错时立即知道
- **快速定位原因** - 错误信息帮助找到根本原因
- **预防严重故障** - 小错误不处理可能导致大问题

### 6.2 错误级别分类


**📊 常见错误级别**：
```
FATAL/CRITICAL  - 致命错误，系统崩溃
ERROR          - 严重错误，功能无法正常工作
WARN/WARNING   - 警告，有潜在问题但还能工作
INFO           - 信息，正常运行状态
DEBUG          - 调试信息，开发时使用
```

### 6.3 错误日志过滤配置


**📋 智能错误过滤配置**：
```ruby
input {
  file {
    path => [
      "/var/log/*/error.log",
      "/var/log/application/errors/*.log"
    ]
    start_position => "beginning"
    type => "error-log"
  }
}

filter {
  if [type] == "error-log" {
    # 基础解析
    grok {
      match => { 
        "message" => "%{TIMESTAMP_ISO8601:timestamp} \[%{LOGLEVEL:level}\] %{GREEDYDATA:error_message}" 
      }
    }
    
    date {
      match => [ "timestamp", "yyyy-MM-dd HH:mm:ss" ]
    }
    
    # 错误严重程度分类
    if [level] in ["FATAL", "CRITICAL"] {
      mutate { 
        add_tag => ["critical_error", "immediate_attention"]
        add_field => { "priority" => "P1" }
      }
    } else if [level] == "ERROR" {
      mutate { 
        add_tag => ["error", "needs_attention"]
        add_field => { "priority" => "P2" }
      }
    } else if [level] in ["WARN", "WARNING"] {
      mutate { 
        add_tag => ["warning"]
        add_field => { "priority" => "P3" }
      }
    }
    
    # 过滤掉已知的无害错误
    if [error_message] =~ /Connection reset by peer/ and [level] == "INFO" {
      drop { }
    }
    
    # 检测特定错误模式
    if [error_message] =~ /OutOfMemoryError|MemoryError/ {
      mutate { add_tag => ["memory_issue", "performance"] }
    } else if [error_message] =~ /Connection timeout|ConnectException/ {
      mutate { add_tag => ["network_issue", "connectivity"] }
    } else if [error_message] =~ /SQLException|Database/ {
      mutate { add_tag => ["database_issue", "data"] }
    }
    
    # 提取错误中的关键信息
    if [error_message] =~ /Exception in thread/ {
      grok {
        match => { "error_message" => "Exception in thread \"%{DATA:thread_name}\" %{JAVACLASS:exception_class}" }
      }
    }
  }
}

output {
  # 关键错误发送到告警系统
  if "critical_error" in [tags] {
    email {
      to => "admin@company.com"
      subject => "CRITICAL ERROR: %{error_message}"
      body => "Critical error detected at %{timestamp}"
    }
  }
  
  # 所有错误发送到Elasticsearch
  elasticsearch {
    hosts => ["localhost:9200"]
    index => "error-logs-%{+YYYY.MM.dd}"
  }
}
```

### 6.4 错误聚合和去重


**📋 避免错误日志刷屏**：
```ruby
filter {
  # 为相似错误生成fingerprint
  fingerprint {
    source => ["level", "error_message"]
    target => "[@metadata][fingerprint]"
    method => "MD5"
  }
  
  # 使用throttle插件控制重复错误的频率
  throttle {
    before_count => 3
    after_count => 1
    period => 300
    max_age => 600
    key => "%{[@metadata][fingerprint]}"
    add_tag => ["throttled"]
  }
}
```

**💡 配置说明**：
- `before_count => 3` - 前3次正常处理
- `after_count => 1` - 之后每1次处理一次
- `period => 300` - 每5分钟为一个周期
- 这样可以避免相同错误刷屏，但仍能了解错误趋势

---

## 7. 🔄 多格式日志统一处理


### 7.1 为什么需要统一处理


**💡 现实场景**：
一个典型的Web应用系统可能有：
- **前端服务器** - Nginx日志
- **应用服务器** - Java/Python应用日志  
- **数据库服务器** - MySQL/PostgreSQL日志
- **缓存服务器** - Redis日志
- **消息队列** - RabbitMQ/Kafka日志

**🎯 统一处理的好处**：
- **统一时间格式** - 所有日志使用相同的时间标准
- **统一字段名称** - 便于跨系统查询和分析
- **统一存储位置** - 在一个地方查看所有系统状态
- **关联分析** - 可以追踪一个用户请求在各个系统中的路径

### 7.2 多输入源配置


**📋 统一采集配置框架**：
```ruby
input {
  # Nginx访问日志
  file {
    path => "/var/log/nginx/access.log"
    type => "nginx-access"
    tags => ["web-server", "nginx"]
  }
  
  # Apache访问日志
  file {
    path => "/var/log/apache2/access.log"
    type => "apache-access"
    tags => ["web-server", "apache"]
  }
  
  # Java应用日志
  file {
    path => "/var/log/myapp/application.log"
    type => "java-app"
    tags => ["application", "java"]
    codec => multiline {
      pattern => "^\d{4}-\d{2}-\d{2}"
      negate => true
      what => "previous"
    }
  }
  
  # 系统日志
  file {
    path => "/var/log/syslog"
    type => "syslog"
    tags => ["system", "linux"]
  }
  
  # 数据库日志
  file {
    path => "/var/log/mysql/error.log"
    type => "mysql-error"
    tags => ["database", "mysql"]
  }
}
```

### 7.3 统一字段映射


**📋 字段标准化处理**：
```ruby
filter {
  # 为所有日志添加基础信息
  mutate {
    add_field => { 
      "log_source" => "%{host}"
      "log_type" => "%{type}"
      "processed_at" => "%{@timestamp}"
    }
  }
  
  # 根据不同类型进行相应处理
  if [type] == "nginx-access" {
    grok {
      match => { "message" => "%{NGINXACCESS}" }
    }
    # 统一字段名称
    mutate {
      rename => { "clientip" => "client_ip" }
      rename => { "response" => "status_code" }
      rename => { "bytes" => "response_size" }
    }
  } else if [type] == "apache-access" {
    grok {
      match => { "message" => "%{COMBINEDAPACHELOG}" }
    }
    # 统一字段名称
    mutate {
      rename => { "clientip" => "client_ip" }
      rename => { "response" => "status_code" }
      rename => { "bytes" => "response_size" }
    }
  } else if [type] == "java-app" {
    grok {
      match => { 
        "message" => "%{TIMESTAMP_ISO8601:timestamp} %{LOGLEVEL:level} \[%{DATA:class}\] - %{GREEDYDATA:log_message}" 
      }
    }
    # 统一字段名称
    mutate {
      rename => { "level" => "log_level" }
      rename => { "class" => "source_class" }
    }
  }
  
  # 统一时间处理
  if [timestamp] {
    date {
      match => [ 
        "timestamp", 
        "dd/MMM/yyyy:HH:mm:ss Z",      # Apache/Nginx格式
        "yyyy-MM-dd HH:mm:ss",         # Java应用格式
        "MMM dd HH:mm:ss"              # Syslog格式
      ]
    }
  }
  
  # 添加通用标签
  mutate {
    add_tag => ["processed", "unified"]
  }
}
```

### 7.4 智能路由和存储


**📋 根据日志类型智能存储**：
```ruby
output {
  # Web服务器日志
  if "web-server" in [tags] {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "web-logs-%{+YYYY.MM.dd}"
      template_name => "web-logs"
    }
  }
  
  # 应用程序日志
  if "application" in [tags] {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "app-logs-%{+YYYY.MM.dd}"
      template_name => "app-logs"
    }
  }
  
  # 系统日志
  if "system" in [tags] {
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "system-logs-%{+YYYY.MM.dd}"
      template_name => "system-logs"
    }
  }
  
  # 错误级别的日志额外处理
  if [log_level] in ["ERROR", "FATAL", "CRITICAL"] {
    # 发送到告警系统
    http {
      url => "http://alert-system:8080/alert"
      http_method => "post"
      format => "json"
    }
    
    # 同时存储到专门的错误索引
    elasticsearch {
      hosts => ["localhost:9200"]
      index => "error-logs-%{+YYYY.MM.dd}"
    }
  }
  
  # 调试输出（可选）
  stdout { 
    codec => rubydebug 
  }
}
```

### 7.5 性能优化配置


**📋 大量日志处理优化**：
```ruby
# 在logstash.yml中配置
pipeline.workers: 4                    # 并行处理线程数
pipeline.batch.size: 1000             # 批处理大小
pipeline.batch.delay: 50              # 批处理延迟（毫秒）

# 在配置文件中优化
filter {
  # 使用条件判断减少不必要的处理
  if [type] in ["nginx-access", "apache-access"] {
    # 只对Web日志进行这些处理
    geoip {
      source => "client_ip"
      target => "geoip"
    }
  }
  
  # 跳过某些不重要的日志
  if [message] =~ /health-check|ping|favicon/ {
    drop { }
  }
}
```

---

## 8. 📋 核心要点总结


### 8.1 日志采集配置精要


**🔸 配置三要素**：
```
输入(Input)：告诉Logstash去哪里找日志文件
处理(Filter)：告诉Logstash如何解析和处理日志
输出(Output)：告诉Logstash把处理好的日志发送到哪里
```

**🔸 常用配置参数**：
- `path` - 日志文件路径，支持通配符
- `type` - 日志类型标签，用于后续处理
- `start_position` - 读取起始位置（beginning/end）
- `tags` - 自定义标签，便于分类和过滤

### 8.2 解析模式选择指南


| 日志类型 | **推荐解析方式** | **关键配置** | **注意事项** |
|---------|----------------|-------------|-------------|
| 🌐 **Web服务器** | `%{COMBINEDAPACHELOG}` | grok预定义模式 | 注意时间格式差异 |
| 💻 **应用程序** | 自定义grok模式 | multiline处理 | 处理异常堆栈信息 |
| 🖥 **系统日志** | `%{SYSLOGTIMESTAMP}` | 标准syslog格式 | 关注安全相关事件 |
| ⚠️ **错误日志** | 按级别分类 | 错误聚合去重 | 设置告警阈值 |

### 8.3 性能优化要点


**⚡ 提升处理速度的关键配置**：
```ruby
# 批处理优化
pipeline.batch.size: 1000        # 增大批处理大小
pipeline.workers: 4              # 增加工作线程

# 过滤优化
if [message] =~ /static|assets/ {  # 提前过滤不需要的日志
  drop { }
}

# 字段优化
mutate {
  remove_field => ["host", "path"]   # 删除不需要的字段
}
```

**💾 存储优化策略**：
- **按日期分索引** - `index => "logs-%{+YYYY.MM.dd}"`
- **按类型分索引** - 不同类型日志存储到不同索引
- **设置索引模板** - 预定义字段类型和映射
- **定期清理** - 删除老旧的日志索引

### 8.4 实际部署建议


**🛠 配置文件组织**：
```
/etc/logstash/conf.d/
├── 01-input.conf          # 输入配置
├── 10-filter-web.conf     # Web日志过滤
├── 11-filter-app.conf     # 应用日志过滤
├── 12-filter-system.conf  # 系统日志过滤
└── 99-output.conf         # 输出配置
```

**📊 监控指标**：
- **处理速度** - 每秒处理的日志条数
- **错误率** - grok解析失败的比例
- **延迟** - 从日志产生到索引的时间差
- **资源使用** - CPU和内存使用情况

### 8.5 常见问题解决


**❓ 问题1：日志解析失败**
```ruby
# 解决方案：添加解析失败处理
grok {
  match => { "message" => "%{COMBINEDAPACHELOG}" }
  tag_on_failure => ["_grokparsefailure"]
}

if "_grokparsefailure" in [tags] {
  mutate {
    add_field => { "parse_error" => "true" }
  }
}
```

**❓ 问题2：时间字段处理错误**
```ruby
# 解决方案：提供多种时间格式
date {
  match => [ 
    "timestamp", 
    "dd/MMM/yyyy:HH:mm:ss Z",
    "yyyy-MM-dd HH:mm:ss",
    "ISO8601"
  ]
}
```

**❓ 问题3：日志量太大影响性能**
```ruby
# 解决方案：添加采样和过滤
if [status_code] == 200 and [request] =~ /\.(css|js|png)$/ {
  # 对静态资源请求进行采样，只保留10%
  if [random] > 0.9 {
    drop { }
  }
}
```

**核心记忆口诀**：
- 输入输出定方向，过滤处理在中央
- 格式解析选对路，时间字段要规范  
- 错误分级做告警，性能优化不能忘
- 多种日志统一处，智能路由各归档