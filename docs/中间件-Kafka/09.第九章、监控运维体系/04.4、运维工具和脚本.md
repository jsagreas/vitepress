---
title: 4ã€è¿ç»´å·¥å…·å’Œè„šæœ¬
---
## ğŸ“š ç›®å½•

1. [Kafkaå‘½ä»¤è¡Œå·¥å…·å…¨å®¶æ¡¶](#1-kafkaå‘½ä»¤è¡Œå·¥å…·å…¨å®¶æ¡¶)
2. [AdminClientç®¡ç†API](#2-adminclientç®¡ç†api)
3. [è¿ç»´è„šæœ¬ç¼–å†™å®æˆ˜](#3-è¿ç»´è„šæœ¬ç¼–å†™å®æˆ˜)
4. [è‡ªåŠ¨åŒ–éƒ¨ç½²ä¸é…ç½®ç®¡ç†](#4-è‡ªåŠ¨åŒ–éƒ¨ç½²ä¸é…ç½®ç®¡ç†)
5. [ç›‘æ§å‘Šè­¦è„šæœ¬](#5-ç›‘æ§å‘Šè­¦è„šæœ¬)
6. [å¤‡ä»½æ¢å¤è„šæœ¬](#6-å¤‡ä»½æ¢å¤è„šæœ¬)
7. [æ•…éšœå¤„ç†è„šæœ¬](#7-æ•…éšœå¤„ç†è„šæœ¬)
8. [æ ¸å¿ƒè¦ç‚¹æ€»ç»“](#8-æ ¸å¿ƒè¦ç‚¹æ€»ç»“)

---

## 1. ğŸ› ï¸ Kafkaå‘½ä»¤è¡Œå·¥å…·å…¨å®¶æ¡¶


### 1.1 å·¥å…·æ¦‚è§ˆ


**ä»€ä¹ˆæ˜¯Kafkaå‘½ä»¤è¡Œå·¥å…·ï¼Ÿ**
```
ç®€å•ç†è§£ï¼šå°±åƒæ“ä½œç”µè„‘ç”¨é¼ æ ‡ï¼Œæ“ä½œKafkaç”¨å‘½ä»¤è¡Œå·¥å…·
ä½œç”¨ï¼šä¸ç”¨å†™ä»£ç å°±èƒ½ç®¡ç†Kafkaé›†ç¾¤
ä½ç½®ï¼šå®‰è£…ç›®å½•ä¸‹çš„ bin/ æ–‡ä»¶å¤¹
ç‰¹ç‚¹ï¼šåŠŸèƒ½å…¨é¢ï¼Œæ“ä½œç›´æ¥ï¼Œè¿ç»´å¿…å¤‡
```

**æ ¸å¿ƒå·¥å…·åˆ†ç±»**
```
é›†ç¾¤ç®¡ç†ç±»ï¼š
â”œâ”€â”€ kafka-server-start.sh     â† å¯åŠ¨KafkaæœåŠ¡
â”œâ”€â”€ kafka-server-stop.sh      â† åœæ­¢KafkaæœåŠ¡
â””â”€â”€ kafka-configs.sh          â† é…ç½®ç®¡ç†

Topicç®¡ç†ç±»ï¼š
â”œâ”€â”€ kafka-topics.sh           â† ä¸»é¢˜æ“ä½œ
â”œâ”€â”€ kafka-log-dirs.sh         â† æ—¥å¿—ç›®å½•æŸ¥çœ‹
â””â”€â”€ kafka-dump-log.sh         â† æ—¥å¿—æ–‡ä»¶åˆ†æ

æ•°æ®æ“ä½œç±»ï¼š
â”œâ”€â”€ kafka-console-producer.sh â† å‘½ä»¤è¡Œç”Ÿäº§è€…
â”œâ”€â”€ kafka-console-consumer.sh â† å‘½ä»¤è¡Œæ¶ˆè´¹è€…
â””â”€â”€ kafka-consumer-groups.sh  â† æ¶ˆè´¹ç»„ç®¡ç†

è¿ç»´å·¥å…·ç±»ï¼š
â”œâ”€â”€ kafka-broker-api-versions.sh â† APIç‰ˆæœ¬æŸ¥è¯¢
â”œâ”€â”€ kafka-log-size-checker.sh    â† æ—¥å¿—å¤§å°æ£€æŸ¥
â””â”€â”€ kafka-replica-verification.sh â† å‰¯æœ¬éªŒè¯
```

### 1.2 Topicç®¡ç†å®æˆ˜


**åˆ›å»ºTopicçš„æœ€ä½³å®è·µ**
```bash
# åŸºç¡€åˆ›å»ºï¼ˆç”Ÿäº§ç¯å¢ƒä¸æ¨èï¼‰
kafka-topics.sh --bootstrap-server localhost:9092 \
  --create --topic my-topic

# ç”Ÿäº§ç¯å¢ƒæ ‡å‡†åˆ›å»º
kafka-topics.sh --bootstrap-server localhost:9092 \
  --create \
  --topic user-events \
  --partitions 6 \              # åˆ†åŒºæ•°è¦åˆç†è§„åˆ’
  --replication-factor 3 \      # å‰¯æœ¬æ•°ä¿è¯å¯ç”¨æ€§
  --config retention.ms=604800000 # 7å¤©ä¿ç•™æœŸ
```

**ä¸ºä»€ä¹ˆè¦è®¾ç½®è¿™äº›å‚æ•°ï¼Ÿ**
```
åˆ†åŒºæ•°(partitions)ï¼š
â€¢ ä½œç”¨ï¼šå†³å®šå¹¶å‘å¤„ç†èƒ½åŠ›
â€¢ å»ºè®®ï¼šæ ¹æ®æ¶ˆè´¹è€…æ•°é‡å’Œååé‡éœ€æ±‚è®¾ç½®
â€¢ æ³¨æ„ï¼šåªèƒ½å¢åŠ ä¸èƒ½å‡å°‘

å‰¯æœ¬æ•°(replication-factor)ï¼š
â€¢ ä½œç”¨ï¼šä¿è¯æ•°æ®ä¸ä¸¢å¤±
â€¢ å»ºè®®ï¼šç”Ÿäº§ç¯å¢ƒè‡³å°‘3ä¸ªå‰¯æœ¬
â€¢ å…¬å¼ï¼šå‰¯æœ¬æ•° â‰¤ Brokeræ•°é‡

ä¿ç•™æ—¶é—´(retention.ms)ï¼š
â€¢ ä½œç”¨ï¼šæ§åˆ¶æ•°æ®å­˜å‚¨æ—¶é—´
â€¢ å•ä½ï¼šæ¯«ç§’
â€¢ ç¤ºä¾‹ï¼š604800000ms = 7å¤©
```

**Topicä¿¡æ¯æŸ¥è¯¢æŠ€å·§**
```bash
# æŸ¥çœ‹æ‰€æœ‰Topic
kafka-topics.sh --bootstrap-server localhost:9092 --list

# æŸ¥çœ‹Topicè¯¦ç»†ä¿¡æ¯
kafka-topics.sh --bootstrap-server localhost:9092 \
  --describe --topic user-events

# æŸ¥çœ‹Topicé…ç½®
kafka-configs.sh --bootstrap-server localhost:9092 \
  --entity-type topics --entity-name user-events --describe
```

### 1.3 æ•°æ®æ“ä½œå·¥å…·


**å‘½ä»¤è¡Œç”Ÿäº§è€…ï¼šè°ƒè¯•åˆ©å™¨**
```bash
# åŸºç¡€ä½¿ç”¨
kafka-console-producer.sh --bootstrap-server localhost:9092 \
  --topic test-topic

# å¸¦Keyçš„æ¶ˆæ¯
kafka-console-producer.sh --bootstrap-server localhost:9092 \
  --topic test-topic \
  --property "parse.key=true" \
  --property "key.separator=:"

# è¾“å…¥ç¤ºä¾‹ï¼š
# user1:{"name":"å¼ ä¸‰","age":25}
# user2:{"name":"æå››","age":30}
```

**å‘½ä»¤è¡Œæ¶ˆè´¹è€…ï¼šé—®é¢˜æ’æŸ¥**
```bash
# ä»æœ€æ–°ä½ç½®å¼€å§‹æ¶ˆè´¹
kafka-console-consumer.sh --bootstrap-server localhost:9092 \
  --topic test-topic

# ä»å¤´å¼€å§‹æ¶ˆè´¹æ‰€æœ‰æ•°æ®
kafka-console-consumer.sh --bootstrap-server localhost:9092 \
  --topic test-topic --from-beginning

# æ˜¾ç¤ºKeyå’Œæ—¶é—´æˆ³
kafka-console-consumer.sh --bootstrap-server localhost:9092 \
  --topic test-topic \
  --property print.key=true \
  --property print.timestamp=true
```

**æ¶ˆè´¹ç»„ç®¡ç†ï¼šè¿ç»´å¿…å¤‡**
```bash
# æŸ¥çœ‹æ‰€æœ‰æ¶ˆè´¹ç»„
kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list

# æŸ¥çœ‹æ¶ˆè´¹ç»„è¯¦æƒ…
kafka-consumer-groups.sh --bootstrap-server localhost:9092 \
  --describe --group my-consumer-group

# é‡ç½®æ¶ˆè´¹ä½ç§»ï¼ˆè°¨æ…æ“ä½œï¼ï¼‰
kafka-consumer-groups.sh --bootstrap-server localhost:9092 \
  --group my-consumer-group \
  --reset-offsets --to-earliest \
  --topic test-topic --execute
```

---

## 2. ğŸ”§ AdminClientç®¡ç†API


### 2.1 AdminClientæ˜¯ä»€ä¹ˆï¼Ÿ


**é€šä¿—ç†è§£**
```
ç±»æ¯”ï¼šå‘½ä»¤è¡Œå·¥å…· = æ‰‹å·¥æ“ä½œï¼ŒAdminClient = ç¨‹åºåŒ–æ“ä½œ
ä½œç”¨ï¼šç”¨ä»£ç ç®¡ç†Kafkaé›†ç¾¤ï¼Œå®ç°è‡ªåŠ¨åŒ–è¿ç»´
ä¼˜åŠ¿ï¼šæ›´çµæ´»ã€å¯ç¼–ç¨‹ã€æ˜“é›†æˆåˆ°ç³»ç»Ÿä¸­
é€‚ç”¨ï¼šè¿ç»´å¹³å°ã€ç›‘æ§ç³»ç»Ÿã€è‡ªåŠ¨åŒ–è„šæœ¬
```

### 2.2 Java AdminClientå®æˆ˜


**åŸºç¡€è¿æ¥å’Œé…ç½®**
```java
import org.apache.kafka.clients.admin.*;
import org.apache.kafka.common.config.ConfigResource;

// åˆ›å»ºAdminClient
Properties props = new Properties();
props.put(AdminClientConfig.BOOTSTRAP_SERVERS_CONFIG, "localhost:9092");
props.put(AdminClientConfig.REQUEST_TIMEOUT_MS_CONFIG, "30000");

AdminClient adminClient = AdminClient.create(props);
```

**Topicç®¡ç†æ“ä½œ**
```java
// åˆ›å»ºTopic
public void createTopic(String topicName, int partitions, short replicas) {
    NewTopic newTopic = new NewTopic(topicName, partitions, replicas);
    
    // è®¾ç½®Topicé…ç½®
    Map<String, String> configs = new HashMap<>();
    configs.put("retention.ms", "604800000"); // 7å¤©ä¿ç•™
    configs.put("compression.type", "gzip");   // å¯ç”¨å‹ç¼©
    newTopic.configs(configs);
    
    CreateTopicsResult result = adminClient.createTopics(Arrays.asList(newTopic));
    
    try {
        result.all().get(); // ç­‰å¾…æ“ä½œå®Œæˆ
        System.out.println("Topicåˆ›å»ºæˆåŠŸ: " + topicName);
    } catch (Exception e) {
        System.err.println("Topicåˆ›å»ºå¤±è´¥: " + e.getMessage());
    }
}

// åˆ é™¤Topicï¼ˆè°¨æ…æ“ä½œï¼‰
public void deleteTopic(String topicName) {
    DeleteTopicsResult result = adminClient.deleteTopics(Arrays.asList(topicName));
    try {
        result.all().get();
        System.out.println("Topicåˆ é™¤æˆåŠŸ: " + topicName);
    } catch (Exception e) {
        System.err.println("Topicåˆ é™¤å¤±è´¥: " + e.getMessage());
    }
}
```

**é›†ç¾¤ä¿¡æ¯æŸ¥è¯¢**
```java
// è·å–é›†ç¾¤å…ƒæ•°æ®
public void printClusterInfo() {
    try {
        DescribeClusterResult cluster = adminClient.describeCluster();
        
        // é›†ç¾¤åŸºæœ¬ä¿¡æ¯
        String clusterId = cluster.clusterId().get();
        Node controller = cluster.controller().get();
        Collection<Node> nodes = cluster.nodes().get();
        
        System.out.println("é›†ç¾¤ID: " + clusterId);
        System.out.println("æ§åˆ¶å™¨: " + controller.host() + ":" + controller.port());
        System.out.println("èŠ‚ç‚¹æ•°é‡: " + nodes.size());
        
        // æ‰“å°æ‰€æœ‰èŠ‚ç‚¹ä¿¡æ¯
        for (Node node : nodes) {
            System.out.println("èŠ‚ç‚¹: " + node.id() + " - " + 
                             node.host() + ":" + node.port());
        }
    } catch (Exception e) {
        e.printStackTrace();
    }
}
```

### 2.3 Python AdminClientç¤ºä¾‹


**ç¯å¢ƒå‡†å¤‡**
```bash
# å®‰è£…Python Kafkaåº“
pip install kafka-python
```

**åŸºç¡€æ“ä½œ**
```python
from kafka.admin import KafkaAdminClient, NewTopic, ConfigResource, ConfigResourceType
from kafka.errors import KafkaError

# åˆ›å»ºAdminClient
admin_client = KafkaAdminClient(
    bootstrap_servers='localhost:9092',
    client_id='admin_client'
)

# åˆ›å»ºTopic
def create_topic(topic_name, num_partitions=3, replication_factor=3):
    topic = NewTopic(
        name=topic_name,
        num_partitions=num_partitions,
        replication_factor=replication_factor,
        topic_configs={
            'retention.ms': '604800000',  # 7å¤©ä¿ç•™
            'compression.type': 'gzip'    # å¯ç”¨å‹ç¼©
        }
    )
    
    try:
        result = admin_client.create_topics([topic])
        print(f"Topicåˆ›å»ºæˆåŠŸ: {topic_name}")
    except KafkaError as e:
        print(f"Topicåˆ›å»ºå¤±è´¥: {e}")

# æŸ¥çœ‹Topicåˆ—è¡¨
def list_topics():
    topics = admin_client.list_topics()
    print("Topicåˆ—è¡¨:")
    for topic in topics:
        print(f"  - {topic}")
```

---

## 3. ğŸ“ è¿ç»´è„šæœ¬ç¼–å†™å®æˆ˜


### 3.1 ä¸ºä»€ä¹ˆéœ€è¦è¿ç»´è„šæœ¬ï¼Ÿ


**ç°å®åœºæ™¯**
```
é—®é¢˜ï¼šæ¯å¤©æ‰‹åŠ¨æ£€æŸ¥100+ä¸ªTopicçš„çŠ¶æ€ï¼Ÿç´¯æ­»äººï¼
è§£å†³ï¼šå†™è„šæœ¬è‡ªåŠ¨æ£€æŸ¥ï¼Œå‘ç°é—®é¢˜è‡ªåŠ¨é€šçŸ¥

é—®é¢˜ï¼šåŠå¤œé›†ç¾¤å‡ºæ•…éšœï¼Œè¿ç»´äººå‘˜åœ¨ç¡è§‰ï¼Ÿ
è§£å†³ï¼šæ•…éšœè‡ªæ„ˆè„šæœ¬ï¼Œè‡ªåŠ¨é‡å¯æœåŠ¡

é—®é¢˜ï¼šæ–°äººä¸ç†Ÿæ‚‰å‘½ä»¤ï¼Œæ“ä½œå®¹æ˜“å‡ºé”™ï¼Ÿ
è§£å†³ï¼šæ ‡å‡†åŒ–è„šæœ¬ï¼Œé™ä½æ“ä½œé£é™©
```

### 3.2 Shellè„šæœ¬å®æˆ˜


**é›†ç¾¤å¥åº·æ£€æŸ¥è„šæœ¬**
```bash
#!/bin/bash
# kafka_health_check.sh - Kafkaé›†ç¾¤å¥åº·æ£€æŸ¥

# é…ç½®ä¿¡æ¯
KAFKA_HOME="/opt/kafka"
BOOTSTRAP_SERVERS="localhost:9092,localhost:9093,localhost:9094"
LOG_FILE="/var/log/kafka/health_check.log"

# é¢œè‰²è¾“å‡ºå‡½æ•°
print_status() {
    local status=$1
    local message=$2
    case $status in
        "OK")
            echo -e "\033[32m[OK]\033[0m $message"
            ;;
        "ERROR") 
            echo -e "\033[31m[ERROR]\033[0m $message"
            ;;
        "WARN")
            echo -e "\033[33m[WARN]\033[0m $message"
            ;;
    esac
    echo "$(date): [$status] $message" >> $LOG_FILE
}

# æ£€æŸ¥KafkaæœåŠ¡çŠ¶æ€
check_kafka_service() {
    print_status "INFO" "æ£€æŸ¥KafkaæœåŠ¡çŠ¶æ€..."
    
    # æ£€æŸ¥è¿›ç¨‹æ˜¯å¦è¿è¡Œ
    if pgrep -f "kafka.Kafka" > /dev/null; then
        print_status "OK" "Kafkaè¿›ç¨‹è¿è¡Œæ­£å¸¸"
    else
        print_status "ERROR" "Kafkaè¿›ç¨‹æœªè¿è¡Œ"
        return 1
    fi
    
    # æ£€æŸ¥ç«¯å£æ˜¯å¦ç›‘å¬
    for port in 9092 9093 9094; do
        if netstat -tln | grep ":$port " > /dev/null; then
            print_status "OK" "ç«¯å£ $port ç›‘å¬æ­£å¸¸"
        else
            print_status "ERROR" "ç«¯å£ $port æœªç›‘å¬"
        fi
    done
}

# æ£€æŸ¥TopicçŠ¶æ€
check_topics() {
    print_status "INFO" "æ£€æŸ¥TopicçŠ¶æ€..."
    
    # è·å–Topicåˆ—è¡¨
    topics=$($KAFKA_HOME/bin/kafka-topics.sh --bootstrap-server $BOOTSTRAP_SERVERS --list 2>/dev/null)
    
    if [ $? -eq 0 ]; then
        topic_count=$(echo "$topics" | wc -l)
        print_status "OK" "å‘ç° $topic_count ä¸ªTopic"
        
        # æ£€æŸ¥æ¯ä¸ªTopicçš„è¯¦ç»†ä¿¡æ¯
        echo "$topics" | while read topic; do
            if [ -n "$topic" ]; then
                $KAFKA_HOME/bin/kafka-topics.sh --bootstrap-server $BOOTSTRAP_SERVERS \
                  --describe --topic "$topic" > /dev/null 2>&1
                if [ $? -eq 0 ]; then
                    print_status "OK" "Topic $topic çŠ¶æ€æ­£å¸¸"
                else
                    print_status "ERROR" "Topic $topic çŠ¶æ€å¼‚å¸¸"
                fi
            fi
        done
    else
        print_status "ERROR" "æ— æ³•è·å–Topicåˆ—è¡¨"
    fi
}

# æ£€æŸ¥æ¶ˆè´¹ç»„å»¶è¿Ÿ
check_consumer_lag() {
    print_status "INFO" "æ£€æŸ¥æ¶ˆè´¹ç»„å»¶è¿Ÿ..."
    
    # è·å–æ‰€æœ‰æ¶ˆè´¹ç»„
    groups=$($KAFKA_HOME/bin/kafka-consumer-groups.sh --bootstrap-server $BOOTSTRAP_SERVERS --list 2>/dev/null)
    
    echo "$groups" | while read group; do
        if [ -n "$group" ]; then
            # è·å–æ¶ˆè´¹ç»„è¯¦æƒ…
            lag_info=$($KAFKA_HOME/bin/kafka-consumer-groups.sh --bootstrap-server $BOOTSTRAP_SERVERS \
                      --describe --group "$group" 2>/dev/null | grep -v "Consumer group" | grep -v "TOPIC")
            
            # è§£æå»¶è¿Ÿä¿¡æ¯
            max_lag=$(echo "$lag_info" | awk '{print $5}' | grep -E '^[0-9]+$' | sort -nr | head -1)
            
            if [ -n "$max_lag" ]; then
                if [ "$max_lag" -gt 10000 ]; then
                    print_status "WARN" "æ¶ˆè´¹ç»„ $group æœ€å¤§å»¶è¿Ÿ: $max_lag"
                else
                    print_status "OK" "æ¶ˆè´¹ç»„ $group å»¶è¿Ÿæ­£å¸¸: $max_lag"
                fi
            fi
        fi
    done
}

# ä¸»å‡½æ•°
main() {
    print_status "INFO" "å¼€å§‹Kafkaå¥åº·æ£€æŸ¥..."
    
    check_kafka_service
    check_topics
    check_consumer_lag
    
    print_status "INFO" "å¥åº·æ£€æŸ¥å®Œæˆ"
}

# æ‰§è¡Œä¸»å‡½æ•°
main "$@"
```

**è‡ªåŠ¨é‡å¯è„šæœ¬**
```bash
#!/bin/bash
# kafka_auto_restart.sh - Kafkaè‡ªåŠ¨é‡å¯è„šæœ¬

KAFKA_HOME="/opt/kafka"
CONFIG_FILE="/opt/kafka/config/server.properties"
LOG_FILE="/var/log/kafka/auto_restart.log"
MAX_RESTART_ATTEMPTS=3
RESTART_INTERVAL=60  # é‡å¯é—´éš”60ç§’

# æ—¥å¿—å‡½æ•°
log_message() {
    echo "$(date): $1" | tee -a $LOG_FILE
}

# æ£€æŸ¥Kafkaæ˜¯å¦è¿è¡Œ
is_kafka_running() {
    pgrep -f "kafka.Kafka" > /dev/null
    return $?
}

# åœæ­¢Kafka
stop_kafka() {
    log_message "æ­£åœ¨åœæ­¢Kafka..."
    $KAFKA_HOME/bin/kafka-server-stop.sh
    
    # ç­‰å¾…è¿›ç¨‹å®Œå…¨åœæ­¢
    local count=0
    while is_kafka_running && [ $count -lt 30 ]; do
        sleep 1
        ((count++))
    done
    
    if is_kafka_running; then
        log_message "å¼ºåˆ¶æ€æ­»Kafkaè¿›ç¨‹"
        pkill -f "kafka.Kafka"
        sleep 5
    fi
    
    log_message "Kafkaå·²åœæ­¢"
}

# å¯åŠ¨Kafka
start_kafka() {
    log_message "æ­£åœ¨å¯åŠ¨Kafka..."
    nohup $KAFKA_HOME/bin/kafka-server-start.sh $CONFIG_FILE > /dev/null 2>&1 &
    
    # ç­‰å¾…å¯åŠ¨å®Œæˆ
    local count=0
    while ! is_kafka_running && [ $count -lt 60 ]; do
        sleep 1
        ((count++))
    done
    
    if is_kafka_running; then
        log_message "Kafkaå¯åŠ¨æˆåŠŸ"
        return 0
    else
        log_message "Kafkaå¯åŠ¨å¤±è´¥"
        return 1
    fi
}

# ä¸»é‡å¯é€»è¾‘
main() {
    local attempt=1
    
    while [ $attempt -le $MAX_RESTART_ATTEMPTS ]; do
        log_message "ç¬¬ $attempt æ¬¡é‡å¯å°è¯•..."
        
        # åœæ­¢æœåŠ¡
        stop_kafka
        
        # ç­‰å¾…é—´éš”
        sleep $RESTART_INTERVAL
        
        # å¯åŠ¨æœåŠ¡
        if start_kafka; then
            log_message "é‡å¯æˆåŠŸï¼"
            exit 0
        else
            log_message "ç¬¬ $attempt æ¬¡é‡å¯å¤±è´¥"
            ((attempt++))
        fi
    done
    
    log_message "é‡å¯å¤±è´¥ï¼Œå·²è¾¾åˆ°æœ€å¤§å°è¯•æ¬¡æ•°"
    # å‘é€å‘Šè­¦é€šçŸ¥ï¼ˆå¯é›†æˆé’‰é’‰ã€é‚®ä»¶ç­‰ï¼‰
    exit 1
}

# æ‰§è¡Œé‡å¯
main "$@"
```

### 3.3 Pythonè¿ç»´è„šæœ¬


**æ€§èƒ½ç›‘æ§è„šæœ¬**
```python
#!/usr/bin/env python3
# kafka_monitor.py - Kafkaæ€§èƒ½ç›‘æ§è„šæœ¬

import time
import json
import logging
import smtplib
from datetime import datetime
from kafka import KafkaAdminClient, KafkaConsumer
from kafka.errors import KafkaError
from email.mime.text import MIMEText

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, 
                   format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class KafkaMonitor:
    def __init__(self, bootstrap_servers, alert_email=None):
        self.bootstrap_servers = bootstrap_servers
        self.alert_email = alert_email
        self.admin_client = KafkaAdminClient(
            bootstrap_servers=bootstrap_servers,
            client_id='monitor_client'
        )
    
    def check_cluster_health(self):
        """æ£€æŸ¥é›†ç¾¤å¥åº·çŠ¶æ€"""
        try:
            # è·å–é›†ç¾¤å…ƒæ•°æ®
            cluster_metadata = self.admin_client.describe_cluster()
            nodes = cluster_metadata.nodes
            controller = cluster_metadata.controller
            
            logger.info(f"é›†ç¾¤èŠ‚ç‚¹æ•°: {len(nodes)}")
            logger.info(f"æ§åˆ¶å™¨: {controller.host}:{controller.port}")
            
            return True
        except Exception as e:
            logger.error(f"é›†ç¾¤å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            self.send_alert(f"Kafkaé›†ç¾¤å¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def check_topic_health(self):
        """æ£€æŸ¥Topicå¥åº·çŠ¶æ€"""
        try:
            # è·å–æ‰€æœ‰Topic
            topics = self.admin_client.list_topics()
            logger.info(f"å‘ç° {len(topics)} ä¸ªTopic")
            
            # æ£€æŸ¥Topicè¯¦æƒ…
            for topic in topics:
                topic_metadata = self.admin_client.describe_topics([topic])
                # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤šæ£€æŸ¥é€»è¾‘
                logger.info(f"Topic {topic} çŠ¶æ€æ­£å¸¸")
            
            return True
        except Exception as e:
            logger.error(f"Topicå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            self.send_alert(f"Topicå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def check_consumer_lag(self, max_lag_threshold=10000):
        """æ£€æŸ¥æ¶ˆè´¹å»¶è¿Ÿ"""
        try:
            # è·å–æ‰€æœ‰æ¶ˆè´¹ç»„
            groups = self.admin_client.list_consumer_groups()
            
            for group_info in groups:
                group_id = group_info.group_id
                
                # è·å–æ¶ˆè´¹ç»„è¯¦æƒ…
                group_description = self.admin_client.describe_consumer_groups([group_id])
                
                # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æ£€æŸ¥æ¯ä¸ªåˆ†åŒºçš„å»¶è¿Ÿ
                logger.info(f"æ¶ˆè´¹ç»„ {group_id} æ£€æŸ¥å®Œæˆ")
            
            return True
        except Exception as e:
            logger.error(f"æ¶ˆè´¹å»¶è¿Ÿæ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def send_alert(self, message):
        """å‘é€å‘Šè­¦é‚®ä»¶"""
        if not self.alert_email:
            return
        
        try:
            # è¿™é‡Œç®€åŒ–é‚®ä»¶å‘é€é€»è¾‘
            logger.warning(f"å‘Šè­¦: {message}")
            # å®é™…å®ç°éœ€è¦é…ç½®SMTPæœåŠ¡å™¨
        except Exception as e:
            logger.error(f"å‘é€å‘Šè­¦å¤±è´¥: {e}")
    
    def run_monitor(self, interval=60):
        """è¿è¡Œç›‘æ§å¾ªç¯"""
        logger.info("å¼€å§‹Kafkaç›‘æ§...")
        
        while True:
            try:
                logger.info("æ‰§è¡Œå¥åº·æ£€æŸ¥...")
                
                # æ‰§è¡Œå„é¡¹æ£€æŸ¥
                cluster_ok = self.check_cluster_health()
                topics_ok = self.check_topic_health()
                lag_ok = self.check_consumer_lag()
                
                if cluster_ok and topics_ok and lag_ok:
                    logger.info("æ‰€æœ‰æ£€æŸ¥é€šè¿‡")
                else:
                    logger.warning("å‘ç°å¼‚å¸¸æƒ…å†µ")
                
                # ç­‰å¾…ä¸‹æ¬¡æ£€æŸ¥
                time.sleep(interval)
                
            except KeyboardInterrupt:
                logger.info("ç›‘æ§åœæ­¢")
                break
            except Exception as e:
                logger.error(f"ç›‘æ§å¼‚å¸¸: {e}")
                time.sleep(interval)

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    monitor = KafkaMonitor(
        bootstrap_servers=['localhost:9092'],
        alert_email='admin@company.com'
    )
    monitor.run_monitor(interval=300)  # 5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
```

---

## 4. ğŸš€ è‡ªåŠ¨åŒ–éƒ¨ç½²ä¸é…ç½®ç®¡ç†


### 4.1 ä¸ºä»€ä¹ˆéœ€è¦è‡ªåŠ¨åŒ–ï¼Ÿ


**æ‰‹å·¥éƒ¨ç½²çš„ç—›ç‚¹**
```
äººå·¥é”™è¯¯ï¼šé…ç½®æ–‡ä»¶å†™é”™ä¸€ä¸ªå‚æ•°ï¼Œé›†ç¾¤å¯åŠ¨å¤±è´¥
ç¯å¢ƒå·®å¼‚ï¼šå¼€å‘ç¯å¢ƒOKï¼Œç”Ÿäº§ç¯å¢ƒæœ‰é—®é¢˜
é‡å¤åŠ³åŠ¨ï¼šæ¯æ¬¡éƒ¨ç½²éƒ½è¦æ‰§è¡Œç›¸åŒçš„æ­¥éª¤
éš¾ä»¥å›æ»šï¼šå‡ºé—®é¢˜äº†ä¸çŸ¥é“æ€ä¹ˆå¿«é€Ÿæ¢å¤

è‡ªåŠ¨åŒ–çš„å¥½å¤„ï¼š
âœ“ æ ‡å‡†åŒ–ï¼šç¡®ä¿æ¯æ¬¡éƒ¨ç½²éƒ½ä¸€æ ·
âœ“ å¯é‡å¤ï¼šä»»ä½•äººéƒ½èƒ½æ‰§è¡Œç›¸åŒæ“ä½œ
âœ“ å¯è¿½æº¯ï¼šçŸ¥é“ä»€ä¹ˆæ—¶å€™éƒ¨ç½²äº†ä»€ä¹ˆ
âœ“ å¿«é€Ÿå›æ»šï¼šå‡ºé—®é¢˜èƒ½å¿«é€Ÿæ¢å¤
```

### 4.2 Dockerå®¹å™¨åŒ–éƒ¨ç½²


**Dockerfileç¤ºä¾‹**
```dockerfile
# Dockerfile for Kafka
FROM openjdk:11-jre-slim

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV KAFKA_VERSION=2.8.1
ENV SCALA_VERSION=2.13
ENV KAFKA_HOME=/opt/kafka

# å®‰è£…ä¾èµ–
RUN apt-get update && apt-get install -y \
    wget \
    bash \
    && rm -rf /var/lib/apt/lists/*

# ä¸‹è½½å¹¶å®‰è£…Kafka
RUN wget -O kafka.tgz "https://archive.apache.org/dist/kafka/${KAFKA_VERSION}/kafka_${SCALA_VERSION}-${KAFKA_VERSION}.tgz" \
    && tar -xzf kafka.tgz -C /opt \
    && mv /opt/kafka_${SCALA_VERSION}-${KAFKA_VERSION} ${KAFKA_HOME} \
    && rm kafka.tgz

# åˆ›å»ºæ•°æ®ç›®å½•
RUN mkdir -p /var/kafka-logs

# å¤åˆ¶é…ç½®æ–‡ä»¶
COPY server.properties ${KAFKA_HOME}/config/
COPY start-kafka.sh /usr/bin/
RUN chmod +x /usr/bin/start-kafka.sh

# æš´éœ²ç«¯å£
EXPOSE 9092

# å¯åŠ¨è„šæœ¬
CMD ["/usr/bin/start-kafka.sh"]
```

**docker-compose.ymlé›†ç¾¤é…ç½®**
```yaml
version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.0.1
    hostname: zookeeper
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    volumes:
      - zk-data:/var/lib/zookeeper/data
      - zk-logs:/var/lib/zookeeper/log

  kafka1:
    image: confluentinc/cp-kafka:7.0.1
    hostname: kafka1
    container_name: kafka1
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 2
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 3
      KAFKA_LOG_RETENTION_HOURS: 168
      KAFKA_LOG_SEGMENT_BYTES: 1073741824
      KAFKA_NUM_PARTITIONS: 6
    volumes:
      - kafka1-data:/var/lib/kafka/data

  kafka2:
    image: confluentinc/cp-kafka:7.0.1
    hostname: kafka2
    container_name: kafka2
    depends_on:
      - zookeeper
    ports:
      - "9093:9093"
    environment:
      KAFKA_BROKER_ID: 2
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9093
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
    volumes:
      - kafka2-data:/var/lib/kafka/data

  kafka3:
    image: confluentinc/cp-kafka:7.0.1
    hostname: kafka3
    container_name: kafka3
    depends_on:
      - zookeeper
    ports:
      - "9094:9094"
    environment:
      KAFKA_BROKER_ID: 3
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9094
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 3
    volumes:
      - kafka3-data:/var/lib/kafka/data

volumes:
  zk-data:
  zk-logs:
  kafka1-data:
  kafka2-data:
  kafka3-data:
```

### 4.3 Ansibleè‡ªåŠ¨åŒ–éƒ¨ç½²


**inventoryæ–‡ä»¶**
```ini
[kafka_cluster]
kafka1 ansible_host=192.168.1.10 broker_id=1
kafka2 ansible_host=192.168.1.11 broker_id=2  
kafka3 ansible_host=192.168.1.12 broker_id=3

[zookeeper]
zk1 ansible_host=192.168.1.13 zk_id=1
zk2 ansible_host=192.168.1.14 zk_id=2
zk3 ansible_host=192.168.1.15 zk_id=3

[all:vars]
ansible_user=kafka
ansible_ssh_private_key_file=~/.ssh/id_rsa
```

**playbookç¤ºä¾‹**
```yaml
# kafka-deploy.yml
---
- name: éƒ¨ç½²Kafkaé›†ç¾¤
  hosts: kafka_cluster
  become: yes
  vars:
    kafka_version: "2.8.1"
    scala_version: "2.13"
    kafka_home: "/opt/kafka"
    kafka_user: "kafka"
    kafka_group: "kafka"
  
  tasks:
    - name: åˆ›å»ºkafkaç”¨æˆ·
      user:
        name: "{{ kafka_user }}"
        group: "{{ kafka_group }}"
        system: yes
        shell: /bin/bash
        home: /home/kafka

    - name: å®‰è£…Java
      package:
        name: openjdk-11-jre-headless
        state: present

    - name: ä¸‹è½½Kafka
      get_url:
        url: "https://archive.apache.org/dist/kafka/{{ kafka_version }}/kafka_{{ scala_version }}-{{ kafka_version }}.tgz"
        dest: "/tmp/kafka.tgz"

    - name: è§£å‹Kafka
      unarchive:
        src: "/tmp/kafka.tgz"
        dest: "/opt"
        remote_src: yes
        owner: "{{ kafka_user }}"
        group: "{{ kafka_group }}"

    - name: åˆ›å»ºè½¯é“¾æ¥
      file:
        src: "/opt/kafka_{{ scala_version }}-{{ kafka_version }}"
        dest: "{{ kafka_home }}"
        state: link
        owner: "{{ kafka_user }}"
        group: "{{ kafka_group }}"

    - name: åˆ›å»ºæ•°æ®ç›®å½•
      file:
        path: "/var/kafka-logs"
        state: directory
        owner: "{{ kafka_user }}"
        group: "{{ kafka_group }}"
        mode: '0755'

    - name: ç”Ÿæˆserver.properties
      template:
        src: server.properties.j2
        dest: "{{ kafka_home }}/config/server.properties"
        owner: "{{ kafka_user }}"
        group: "{{ kafka_group }}"
      notify: restart kafka

    - name: åˆ›å»ºsystemdæœåŠ¡æ–‡ä»¶
      template:
        src: kafka.service.j2
        dest: /etc/systemd/system/kafka.service
      notify: 
        - reload systemd
        - restart kafka

    - name: å¯åŠ¨å¹¶å¯ç”¨KafkaæœåŠ¡
      systemd:
        name: kafka
        state: started
        enabled: yes
        daemon_reload: yes

  handlers:
    - name: reload systemd
      systemd:
        daemon_reload: yes

    - name: restart kafka
      systemd:
        name: kafka
        state: restarted
```

---

## 5. ğŸ“Š ç›‘æ§å‘Šè­¦è„šæœ¬


### 5.1 ç›‘æ§æŒ‡æ ‡ä½“ç³»


**æ ¸å¿ƒç›‘æ§ç»´åº¦**
```
ç³»ç»Ÿå±‚é¢ï¼š
â”œâ”€â”€ CPUä½¿ç”¨ç‡          â† é˜²æ­¢CPUè¿‡è½½
â”œâ”€â”€ å†…å­˜ä½¿ç”¨ç‡          â† é˜²æ­¢OOM
â”œâ”€â”€ ç£ç›˜ä½¿ç”¨ç‡          â† é˜²æ­¢ç£ç›˜æ»¡
â””â”€â”€ ç½‘ç»œIO             â† æ£€æµ‹ç½‘ç»œç“¶é¢ˆ

Kafkaå±‚é¢ï¼š
â”œâ”€â”€ BrokerçŠ¶æ€          â† èŠ‚ç‚¹æ˜¯å¦åœ¨çº¿
â”œâ”€â”€ TopicçŠ¶æ€           â† ä¸»é¢˜æ˜¯å¦å¯ç”¨
â”œâ”€â”€ åˆ†åŒºå‰¯æœ¬çŠ¶æ€        â† æ•°æ®æ˜¯å¦å®Œæ•´
â””â”€â”€ æ¶ˆè´¹å»¶è¿Ÿ           â† æ˜¯å¦æœ‰ç§¯å‹

ä¸šåŠ¡å±‚é¢ï¼š
â”œâ”€â”€ æ¶ˆæ¯ç”Ÿäº§é€Ÿç‡        â† TPSç›‘æ§
â”œâ”€â”€ æ¶ˆæ¯æ¶ˆè´¹é€Ÿç‡        â† å¤„ç†èƒ½åŠ›
â”œâ”€â”€ é”™è¯¯ç‡             â† è´¨é‡ç›‘æ§
â””â”€â”€ å»¶è¿Ÿåˆ†å¸ƒ           â† æ€§èƒ½ç›‘æ§
```

### 5.2 Prometheus + Grafanaç›‘æ§


**JMXå¯¼å‡ºå™¨é…ç½®**
```yaml
# jmx-exporter-config.yml
rules:
  # BrokeræŒ‡æ ‡
  - pattern: kafka.server<type=BrokerTopicMetrics, name=MessagesInPerSec><>Count
    name: kafka_messages_in_total
    type: COUNTER
    labels:
      topic: "$1"

  - pattern: kafka.server<type=BrokerTopicMetrics, name=BytesInPerSec><>Count  
    name: kafka_bytes_in_total
    type: COUNTER

  # ConsumeræŒ‡æ ‡
  - pattern: kafka.consumer<type=consumer-fetch-manager-metrics, client-id=(.+)><>records-lag-max
    name: kafka_consumer_lag_max
    type: GAUGE
    labels:
      client_id: "$1"

  # ProduceræŒ‡æ ‡
  - pattern: kafka.producer<type=producer-metrics, client-id=(.+)><>record-send-rate
    name: kafka_producer_send_rate
    type: GAUGE
    labels:
      client_id: "$1"
```

**å‘Šè­¦è§„åˆ™é…ç½®**
```yaml
# kafka-alerts.yml
groups:
  - name: kafka-alerts
    rules:
      - alert: KafkaBrokerDown
        expr: up{job="kafka-jmx"} == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "Kafka Brokerå®•æœº"
          description: "Broker {{ $labels.instance }} å·²å®•æœºè¶…è¿‡1åˆ†é’Ÿ"

      - alert: KafkaConsumerLagHigh
        expr: kafka_consumer_lag_max > 10000
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "æ¶ˆè´¹å»¶è¿Ÿè¿‡é«˜"
          description: "æ¶ˆè´¹è€… {{ $labels.client_id }} å»¶è¿Ÿè¶…è¿‡10000æ¡æ¶ˆæ¯"

      - alert: KafkaDiskSpaceHigh
        expr: (node_filesystem_size_bytes - node_filesystem_free_bytes) / node_filesystem_size_bytes > 0.85
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "ç£ç›˜ç©ºé—´ä¸è¶³"
          description: "{{ $labels.instance }} ç£ç›˜ä½¿ç”¨ç‡è¶…è¿‡85%"
```

### 5.3 è‡ªå®šä¹‰å‘Šè­¦è„šæœ¬


**é’‰é’‰å‘Šè­¦è„šæœ¬**
```python
#!/usr/bin/env python3
# dingtalk_alert.py - é’‰é’‰å‘Šè­¦è„šæœ¬

import json
import requests
import time
from datetime import datetime

class DingTalkAlert:
    def __init__(self, webhook_url, secret=None):
        self.webhook_url = webhook_url
        self.secret = secret
    
    def send_message(self, title, content, at_mobiles=None, is_at_all=False):
        """å‘é€é’‰é’‰æ¶ˆæ¯"""
        headers = {'Content-Type': 'application/json'}
        
        data = {
            "msgtype": "markdown",
            "markdown": {
                "title": title,
                "text": content
            },
            "at": {
                "atMobiles": at_mobiles or [],
                "isAtAll": is_at_all
            }
        }
        
        try:
            response = requests.post(self.webhook_url, 
                                   headers=headers, 
                                   json=data, 
                                   timeout=10)
            
            if response.status_code == 200:
                result = response.json()
                if result.get('errcode') == 0:
                    print("æ¶ˆæ¯å‘é€æˆåŠŸ")
                    return True
                else:
                    print(f"æ¶ˆæ¯å‘é€å¤±è´¥: {result.get('errmsg')}")
            else:
                print(f"HTTPè¯·æ±‚å¤±è´¥: {response.status_code}")
                
        except Exception as e:
            print(f"å‘é€æ¶ˆæ¯å¼‚å¸¸: {e}")
        
        return False
    
    def format_kafka_alert(self, alert_type, instance, details):
        """æ ¼å¼åŒ–Kafkaå‘Šè­¦æ¶ˆæ¯"""
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # æ ¹æ®å‘Šè­¦ç±»å‹è®¾ç½®ä¸åŒçš„é¢œè‰²å’Œå›¾æ ‡
        if alert_type == "CRITICAL":
            emoji = "ğŸ”´"
            color_tag = "<font color=#ff0000>"
        elif alert_type == "WARNING": 
            emoji = "ğŸŸ¡"
            color_tag = "<font color=#ffa500>"
        else:
            emoji = "ğŸŸ¢"
            color_tag = "<font color=#00ff00>"
        
        content = f"""
{emoji} **Kafkaé›†ç¾¤å‘Šè­¦**

{color_tag}**å‘Šè­¦çº§åˆ«:** {alert_type}</font>

**å‘Šè­¦æ—¶é—´:** {timestamp}

**é—®é¢˜å®ä¾‹:** {instance}

**è¯¦ç»†ä¿¡æ¯:**
{details}

**å¤„ç†å»ºè®®:**
- ç«‹å³æ£€æŸ¥ç›¸å…³æœåŠ¡çŠ¶æ€
- æŸ¥çœ‹æ—¥å¿—ç¡®è®¤å…·ä½“åŸå›   
- å¿…è¦æ—¶è”ç³»è¿ç»´å›¢é˜Ÿ

---
*æœ¬æ¶ˆæ¯ç”±Kafkaç›‘æ§ç³»ç»Ÿè‡ªåŠ¨å‘é€*
        """
        
        return content

# ä½¿ç”¨ç¤ºä¾‹
def send_kafka_alert(alert_type, instance, details):
    webhook_url = "https://oapi.dingtalk.com/robot/send?access_token=YOUR_TOKEN"
    
    alert = DingTalkAlert(webhook_url)
    
    title = f"Kafkaå‘Šè­¦-{alert_type}"
    content = alert.format_kafka_alert(alert_type, instance, details)
    
    # å…³é”®å‘Šè­¦æ—¶@æ‰€æœ‰äºº
    is_at_all = alert_type == "CRITICAL"
    
    alert.send_message(title, content, is_at_all=is_at_all)

# é›†æˆåˆ°ç›‘æ§è„šæœ¬ä¸­
if __name__ == "__main__":
    # æ¨¡æ‹Ÿå‘Šè­¦
    send_kafka_alert(
        "CRITICAL", 
        "kafka-broker-1", 
        "Brokerè¿›ç¨‹å·²åœæ­¢ï¼Œæ— æ³•å¤„ç†æ–°çš„æ¶ˆæ¯"
    )
```

---

## 6. ğŸ’¾ å¤‡ä»½æ¢å¤è„šæœ¬


### 6.1 ä¸ºä»€ä¹ˆéœ€è¦å¤‡ä»½ï¼Ÿ


**æ•°æ®ä¸¢å¤±åœºæ™¯**
```
ç¡¬ä»¶æ•…éšœï¼šç£ç›˜åäº†ï¼Œæ•°æ®å…¨æ²¡äº†
è¯¯æ“ä½œï¼šåˆ é”™Topicï¼Œé‡è¦æ•°æ®ä¸¢å¤±
é›†ç¾¤æ•…éšœï¼šæ•´ä¸ªé›†ç¾¤æŒ‚äº†ï¼Œä¸šåŠ¡ä¸­æ–­
ç‰ˆæœ¬å‡çº§ï¼šå‡çº§å¤±è´¥ï¼Œéœ€è¦å›æ»šæ•°æ®

å¤‡ä»½ç­–ç•¥ï¼š
â€¢ å®šæœŸå¤‡ä»½ï¼šæ¯å¤©è‡ªåŠ¨å¤‡ä»½é‡è¦Topic
â€¢ å¢é‡å¤‡ä»½ï¼šåªå¤‡ä»½æ–°å¢çš„æ•°æ®
â€¢ å¼‚åœ°å¤‡ä»½ï¼šå¤‡ä»½åˆ°ä¸åŒæœºæˆ¿æˆ–äº‘ä¸Š
â€¢ å¤šç‰ˆæœ¬ä¿ç•™ï¼šä¿ç•™å¤šä¸ªå†å²ç‰ˆæœ¬
```

### 6.2 Topicæ•°æ®å¤‡ä»½è„šæœ¬


**MirrorMakerå¤‡ä»½è„šæœ¬**
```bash
#!/bin/bash
# kafka_backup.sh - Kafkaæ•°æ®å¤‡ä»½è„šæœ¬

# é…ç½®å‚æ•°
SOURCE_CLUSTER="localhost:9092"
BACKUP_CLUSTER="backup-kafka:9092"
BACKUP_LOG="/var/log/kafka/backup.log"
CONFIG_FILE="/etc/kafka/mirror-maker.properties"

# éœ€è¦å¤‡ä»½çš„Topicåˆ—è¡¨
TOPICS_TO_BACKUP=(
    "user-events"
    "order-events"
    "payment-events"
    "important-logs"
)

# æ—¥å¿—å‡½æ•°
log_message() {
    echo "$(date '+%Y-%m-%d %H:%M:%S') - $1" | tee -a $BACKUP_LOG
}

# åˆ›å»ºMirrorMakeré…ç½®
create_mirror_config() {
    cat > $CONFIG_FILE << EOF
# MirrorMakeré…ç½®æ–‡ä»¶
bootstrap.servers=$BACKUP_CLUSTER
acks=all
retries=3
batch.size=16384
linger.ms=5
buffer.memory=33554432

# æ¶ˆè´¹è€…é…ç½®
auto.offset.reset=earliest
enable.auto.commit=false
group.id=mirror-maker-backup
EOF
}

# æ£€æŸ¥Topicæ˜¯å¦å­˜åœ¨
check_topic_exists() {
    local cluster=$1
    local topic=$2
    
    kafka-topics.sh --bootstrap-server $cluster \
        --list | grep -q "^$topic$"
    return $?
}

# åˆ›å»ºå¤‡ä»½Topic
create_backup_topic() {
    local topic=$1
    local backup_topic="backup-$topic"
    
    log_message "åˆ›å»ºå¤‡ä»½Topic: $backup_topic"
    
    # è·å–æºTopicé…ç½®
    local source_info=$(kafka-topics.sh --bootstrap-server $SOURCE_CLUSTER \
                       --describe --topic $topic)
    
    # è§£æåˆ†åŒºæ•°å’Œå‰¯æœ¬æ•°
    local partitions=$(echo "$source_info" | head -2 | tail -1 | awk '{print $2}')
    local replicas=$(echo "$source_info" | head -2 | tail -1 | awk '{print $4}')
    
    # åœ¨å¤‡ä»½é›†ç¾¤åˆ›å»ºTopic
    kafka-topics.sh --bootstrap-server $BACKUP_CLUSTER \
        --create \
        --topic $backup_topic \
        --partitions $partitions \
        --replication-factor $replicas \
        --if-not-exists
}

# å¯åŠ¨æ•°æ®åŒæ­¥
start_mirror_maker() {
    local topic=$1
    local backup_topic="backup-$topic"
    
    log_message "å¼€å§‹åŒæ­¥æ•°æ®: $topic -> $backup_topic"
    
    # ä½¿ç”¨MirrorMakerè¿›è¡Œæ•°æ®åŒæ­¥
    nohup kafka-mirror-maker.sh \
        --consumer.config $CONFIG_FILE \
        --producer.config $CONFIG_FILE \
        --whitelist $topic \
        --new.consumer \
        > /var/log/kafka/mirror-$topic.log 2>&1 &
    
    local pid=$!
    echo $pid > /var/run/kafka-mirror-$topic.pid
    
    log_message "MirrorMakerå¯åŠ¨æˆåŠŸ, PID: $pid"
}

# æ£€æŸ¥åŒæ­¥çŠ¶æ€
check_sync_status() {
    local topic=$1
    local backup_topic="backup-$topic"
    
    # è·å–æºTopicçš„æœ€æ–°offset
    local source_offset=$(kafka-run-class.sh kafka.tools.GetOffsetShell \
                         --broker-list $SOURCE_CLUSTER \
                         --topic $topic --time -1 | \
                         awk -F: '{sum+=$3} END {print sum}')
    
    # è·å–å¤‡ä»½Topicçš„æœ€æ–°offset  
    local backup_offset=$(kafka-run-class.sh kafka.tools.GetOffsetShell \
                         --broker-list $BACKUP_CLUSTER \
                         --topic $backup_topic --time -1 | \
                         awk -F: '{sum+=$3} END {print sum}')
    
    local lag=$((source_offset - backup_offset))
    
    log_message "Topic: $topic, æºoffset: $source_offset, å¤‡ä»½offset: $backup_offset, å»¶è¿Ÿ: $lag"
    
    if [ $lag -gt 1000 ]; then
        log_message "è­¦å‘Š: Topic $topic å¤‡ä»½å»¶è¿Ÿè¿‡é«˜: $lag"
        return 1
    fi
    
    return 0
}

# ä¸»å‡½æ•°
main() {
    log_message "å¼€å§‹Kafkaæ•°æ®å¤‡ä»½..."
    
    # åˆ›å»ºé…ç½®æ–‡ä»¶
    create_mirror_config
    
    # å¤„ç†æ¯ä¸ªéœ€è¦å¤‡ä»½çš„Topic
    for topic in "${TOPICS_TO_BACKUP[@]}"; do
        log_message "å¤„ç†Topic: $topic"
        
        # æ£€æŸ¥æºTopicæ˜¯å¦å­˜åœ¨
        if ! check_topic_exists $SOURCE_CLUSTER $topic; then
            log_message "é”™è¯¯: æºTopic $topic ä¸å­˜åœ¨"
            continue
        fi
        
        # åˆ›å»ºå¤‡ä»½Topic
        create_backup_topic $topic
        
        # å¯åŠ¨æ•°æ®åŒæ­¥
        start_mirror_maker $topic
        
        sleep 5  # ç­‰å¾…å¯åŠ¨å®Œæˆ
        
        # æ£€æŸ¥åŒæ­¥çŠ¶æ€
        check_sync_status $topic
    done
    
    log_message "å¤‡ä»½ä»»åŠ¡å¯åŠ¨å®Œæˆ"
}

# åœæ­¢æ‰€æœ‰å¤‡ä»½ä»»åŠ¡
stop_backup() {
    log_message "åœæ­¢æ‰€æœ‰å¤‡ä»½ä»»åŠ¡..."
    
    for topic in "${TOPICS_TO_BACKUP[@]}"; do
        local pid_file="/var/run/kafka-mirror-$topic.pid"
        
        if [ -f $pid_file ]; then
            local pid=$(cat $pid_file)
            if kill -0 $pid 2>/dev/null; then
                kill $pid
                log_message "å·²åœæ­¢Topic $topic çš„å¤‡ä»½ä»»åŠ¡, PID: $pid"
            fi
            rm -f $pid_file
        fi
    done
}

# æ ¹æ®å‚æ•°æ‰§è¡Œå¯¹åº”æ“ä½œ
case "$1" in
    "start")
        main
        ;;
    "stop")
        stop_backup
        ;;
    "status")
        for topic in "${TOPICS_TO_BACKUP[@]}"; do
            check_sync_status $topic
        done
        ;;
    *)
        echo "ç”¨æ³•: $0 {start|stop|status}"
        exit 1
        ;;
esac
```

### 6.3 æ•°æ®æ¢å¤è„šæœ¬


**æ¢å¤è„šæœ¬**
```python
#!/usr/bin/env python3
# kafka_restore.py - Kafkaæ•°æ®æ¢å¤è„šæœ¬

import argparse
import logging
import json
from datetime import datetime
from kafka import KafkaAdminClient, KafkaProducer, KafkaConsumer
from kafka.admin import NewTopic
from kafka.errors import KafkaError

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO,
                   format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class KafkaRestore:
    def __init__(self, source_servers, target_servers):
        self.source_servers = source_servers
        self.target_servers = target_servers
        
        # åˆ›å»ºç®¡ç†å®¢æˆ·ç«¯
        self.source_admin = KafkaAdminClient(bootstrap_servers=source_servers)
        self.target_admin = KafkaAdminClient(bootstrap_servers=target_servers)
    
    def get_topic_config(self, topic_name, admin_client):
        """è·å–Topicé…ç½®ä¿¡æ¯"""
        try:
            # è·å–Topicå…ƒæ•°æ®
            topic_metadata = admin_client.describe_topics([topic_name])
            topic_info = topic_metadata[topic_name]
            
            return {
                'partitions': len(topic_info.partitions),
                'replication_factor': len(topic_info.partitions[0].replicas)
            }
        except Exception as e:
            logger.error(f"è·å–Topicé…ç½®å¤±è´¥: {e}")
            return None
    
    def create_target_topic(self, topic_name, config):
        """åœ¨ç›®æ ‡é›†ç¾¤åˆ›å»ºTopic"""
        try:
            new_topic = NewTopic(
                name=topic_name,
                num_partitions=config['partitions'],
                replication_factor=config['replication_factor']
            )
            
            result = self.target_admin.create_topics([new_topic])
            result.all().get(timeout=30)
            
            logger.info(f"æˆåŠŸåˆ›å»ºTopic: {topic_name}")
            return True
        except Exception as e:
            logger.error(f"åˆ›å»ºTopicå¤±è´¥: {e}")
            return False
    
    def restore_topic_data(self, source_topic, target_topic, start_time=None, end_time=None):
        """æ¢å¤Topicæ•°æ®"""
        logger.info(f"å¼€å§‹æ¢å¤æ•°æ®: {source_topic} -> {target_topic}")
        
        # åˆ›å»ºæ¶ˆè´¹è€…å’Œç”Ÿäº§è€…
        consumer = KafkaConsumer(
            source_topic,
            bootstrap_servers=self.source_servers,
            auto_offset_reset='earliest',
            enable_auto_commit=False,
            group_id=f'restore-{source_topic}',
            value_deserializer=lambda x: x,  # ä¿æŒåŸå§‹å­—èŠ‚
            key_deserializer=lambda x: x if x else None
        )
        
        producer = KafkaProducer(
            bootstrap_servers=self.target_servers,
            acks='all',
            retries=3,
            value_serializer=lambda x: x,  # ä¿æŒåŸå§‹å­—èŠ‚
            key_serializer=lambda x: x if x else None
        )
        
        restored_count = 0
        error_count = 0
        
        try:
            for message in consumer:
                try:
                    # æ—¶é—´è¿‡æ»¤ï¼ˆå¦‚æœæŒ‡å®šäº†æ—¶é—´èŒƒå›´ï¼‰
                    msg_timestamp = message.timestamp
                    if start_time and msg_timestamp < start_time:
                        continue
                    if end_time and msg_timestamp > end_time:
                        break
                    
                    # å‘é€æ¶ˆæ¯åˆ°ç›®æ ‡Topic
                    future = producer.send(
                        target_topic,
                        key=message.key,
                        value=message.value,
                        partition=message.partition,
                        timestamp_ms=message.timestamp
                    )
                    
                    # ç­‰å¾…å‘é€ç¡®è®¤
                    future.get(timeout=10)
                    restored_count += 1
                    
                    # æ¯1000æ¡è®°å½•æ‰“å°è¿›åº¦
                    if restored_count % 1000 == 0:
                        logger.info(f"å·²æ¢å¤ {restored_count} æ¡è®°å½•")
                        
                except Exception as e:
                    error_count += 1
                    logger.error(f"æ¢å¤æ¶ˆæ¯å¤±è´¥: {e}")
                    
                    # å¦‚æœé”™è¯¯è¿‡å¤šï¼Œåœæ­¢æ¢å¤
                    if error_count > 100:
                        logger.error("é”™è¯¯æ•°é‡è¿‡å¤šï¼Œåœæ­¢æ¢å¤")
                        break
        
        finally:
            consumer.close()
            producer.flush()
            producer.close()
            
            logger.info(f"æ¢å¤å®Œæˆ - æˆåŠŸ: {restored_count}, å¤±è´¥: {error_count}")
        
        return restored_count, error_count
    
    def restore_multiple_topics(self, topic_mapping, start_time=None, end_time=None):
        """æ‰¹é‡æ¢å¤å¤šä¸ªTopic"""
        results = {}
        
        for source_topic, target_topic in topic_mapping.items():
            logger.info(f"å¼€å§‹æ¢å¤Topic: {source_topic}")
            
            try:
                # è·å–æºTopicé…ç½®
                config = self.get_topic_config(source_topic, self.source_admin)
                if not config:
                    results[source_topic] = {'status': 'failed', 'reason': 'è·å–é…ç½®å¤±è´¥'}
                    continue
                
                # åˆ›å»ºç›®æ ‡Topic
                if not self.create_target_topic(target_topic, config):
                    results[source_topic] = {'status': 'failed', 'reason': 'åˆ›å»ºTopicå¤±è´¥'}
                    continue
                
                # æ¢å¤æ•°æ®
                success_count, error_count = self.restore_topic_data(
                    source_topic, target_topic, start_time, end_time
                )
                
                results[source_topic] = {
                    'status': 'success',
                    'restored_count': success_count,
                    'error_count': error_count
                }
                
            except Exception as e:
                logger.error(f"æ¢å¤Topic {source_topic} å¤±è´¥: {e}")
                results[source_topic] = {'status': 'failed', 'reason': str(e)}
        
        return results

# å‘½ä»¤è¡Œå·¥å…·
def main():
    parser = argparse.ArgumentParser(description='Kafkaæ•°æ®æ¢å¤å·¥å…·')
    parser.add_argument('--source', required=True, help='æºé›†ç¾¤åœ°å€')
    parser.add_argument('--target', required=True, help='ç›®æ ‡é›†ç¾¤åœ°å€')
    parser.add_argument('--topic-mapping', required=True, help='Topicæ˜ å°„JSONæ–‡ä»¶')
    parser.add_argument('--start-time', type=int, help='å¼€å§‹æ—¶é—´æˆ³(æ¯«ç§’)')
    parser.add_argument('--end-time', type=int, help='ç»“æŸæ—¶é—´æˆ³(æ¯«ç§’)')
    
    args = parser.parse_args()
    
    # è¯»å–Topicæ˜ å°„é…ç½®
    with open(args.topic_mapping, 'r') as f:
        topic_mapping = json.load(f)
    
    # åˆ›å»ºæ¢å¤å™¨
    restore = KafkaRestore(
        source_servers=args.source.split(','),
        target_servers=args.target.split(',')
    )
    
    # æ‰§è¡Œæ¢å¤
    results = restore.restore_multiple_topics(
        topic_mapping, 
        args.start_time, 
        args.end_time
    )
    
    # æ‰“å°ç»“æœ
    logger.info("æ¢å¤ç»“æœ:")
    for topic, result in results.items():
        logger.info(f"  {topic}: {result}")

if __name__ == '__main__':
    main()
```

**ä½¿ç”¨ç¤ºä¾‹**
```json
// topic-mapping.json
{
    "backup-user-events": "user-events",
    "backup-order-events": "order-events", 
    "backup-payment-events": "payment-events"
}
```

```bash
# æ‰§è¡Œæ¢å¤
python3 kafka_restore.py \
    --source backup-kafka:9092 \
    --target localhost:9092 \
    --topic-mapping topic-mapping.json \
    --start-time 1640995200000 \
    --end-time 1641081600000
```

---

## 7. ğŸš¨ æ•…éšœå¤„ç†è„šæœ¬


### 7.1 å¸¸è§æ•…éšœåœºæ™¯


**æ•…éšœåˆ†ç±»ä½“ç³»**
```
æœåŠ¡æ•…éšœï¼š
â”œâ”€â”€ Brokerå®•æœº        â† å•èŠ‚ç‚¹æ•…éšœ
â”œâ”€â”€ ZooKeeperæ•…éšœ     â† åè°ƒæœåŠ¡å¼‚å¸¸
â”œâ”€â”€ ç½‘ç»œåˆ†åŒº         â† é›†ç¾¤é€šä¿¡ä¸­æ–­
â””â”€â”€ ç£ç›˜ç©ºé—´ä¸è¶³      â† å­˜å‚¨èµ„æºè€—å°½

æ€§èƒ½æ•…éšœï¼š
â”œâ”€â”€ æ¶ˆè´¹å»¶è¿Ÿé«˜        â† å¤„ç†è·Ÿä¸ä¸Š
â”œâ”€â”€ ç”Ÿäº§é€Ÿç‡ä¸‹é™      â† å†™å…¥æ€§èƒ½å·®
â”œâ”€â”€ å†…å­˜æ³„æ¼         â† JVMå †å†…å­˜é—®é¢˜
â””â”€â”€ CPUè´Ÿè½½è¿‡é«˜      â† è®¡ç®—èµ„æºä¸è¶³

æ•°æ®æ•…éšœï¼š
â”œâ”€â”€ æ¶ˆæ¯ä¸¢å¤±         â† æ•°æ®å®Œæ•´æ€§é—®é¢˜
â”œâ”€â”€ é‡å¤æ¶ˆè´¹         â† å¹‚ç­‰æ€§å¤±æ•ˆ
â”œâ”€â”€ æ¶ˆæ¯ä¹±åº         â† é¡ºåºæ€§é—®é¢˜
â””â”€â”€ TopicæŸå        â† å…ƒæ•°æ®å¼‚å¸¸
```

### 7.2 è‡ªåŠ¨æ•…éšœæ¢å¤è„šæœ¬


**Brokeræ•…éšœè‡ªæ„ˆè„šæœ¬**
```bash
#!/bin/bash
# kafka_auto_recovery.sh - Kafkaè‡ªåŠ¨æ•…éšœæ¢å¤è„šæœ¬

# é…ç½®å‚æ•°
KAFKA_HOME="/opt/kafka"
KAFKA_CONFIG="/opt/kafka/config/server.properties"
ZOOKEEPER_HOSTS="zk1:2181,zk2:2181,zk3:2181"
BROKER_LIST="localhost:9092,localhost:9093,localhost:9094"
LOG_FILE="/var/log/kafka/auto_recovery.log"
MAX_RECOVERY_ATTEMPTS=3
HEALTH_CHECK_INTERVAL=30

# é’‰é’‰å‘Šè­¦é…ç½®
DINGTALK_WEBHOOK="https://oapi.dingtalk.com/robot/send?access_token=YOUR_TOKEN"

# æ—¥å¿—å‡½æ•°
log_message() {
    local level=$1
    local message=$2
    echo "$(date '+%Y-%m-%d %H:%M:%S') [$level] $message" | tee -a $LOG_FILE
}

# å‘é€é’‰é’‰å‘Šè­¦
send_alert() {
    local title=$1
    local content=$2
    local level=${3:-"INFO"}
    
    local emoji="â„¹ï¸"
    case $level in
        "ERROR") emoji="ğŸ”´" ;;
        "WARN") emoji="ğŸŸ¡" ;;
        "INFO") emoji="ğŸŸ¢" ;;
    esac
    
    local json_data=$(cat << EOF
{
    "msgtype": "markdown",
    "markdown": {
        "title": "$title",
        "text": "$emoji **$title**\n\n$content\n\n$(date '+%Y-%m-%d %H:%M:%S')"
    }
}
EOF
)
    
    curl -s -X POST "$DINGTALK_WEBHOOK" \
         -H "Content-Type: application/json" \
         -d "$json_data" > /dev/null
}

# æ£€æŸ¥Kafkaè¿›ç¨‹çŠ¶æ€
check_kafka_process() {
    pgrep -f "kafka.Kafka" > /dev/null
    return $?
}

# æ£€æŸ¥Kafkaç«¯å£ç›‘å¬
check_kafka_port() {
    local port=${1:-9092}
    netstat -tln | grep ":$port " > /dev/null
    return $?
}

# æ£€æŸ¥ZooKeeperè¿æ¥
check_zookeeper_connection() {
    $KAFKA_HOME/bin/kafka-broker-api-versions.sh \
        --bootstrap-server $BROKER_LIST > /dev/null 2>&1
    return $?
}

# æ£€æŸ¥TopicçŠ¶æ€
check_topics_health() {
    local failed_topics=0
    
    # è·å–å…³é”®ä¸šåŠ¡Topicåˆ—è¡¨
    local critical_topics=("user-events" "order-events" "payment-events")
    
    for topic in "${critical_topics[@]}"; do
        $KAFKA_HOME/bin/kafka-topics.sh \
            --bootstrap-server $BROKER_LIST \
            --describe --topic "$topic" > /dev/null 2>&1
        
        if [ $? -ne 0 ]; then
            log_message "ERROR" "å…³é”®Topic $topic çŠ¶æ€å¼‚å¸¸"
            ((failed_topics++))
        fi
    done
    
    return $failed_topics
}

# æ£€æŸ¥æ¶ˆè´¹è€…å»¶è¿Ÿ
check_consumer_lag() {
    local high_lag_groups=()
    
    # è·å–æ‰€æœ‰æ¶ˆè´¹ç»„
    local groups=$($KAFKA_HOME/bin/kafka-consumer-groups.sh \
                   --bootstrap-server $BROKER_LIST --list 2>/dev/null)
    
    for group in $groups; do
        if [ -n "$group" ]; then
            local max_lag=$($KAFKA_HOME/bin/kafka-consumer-groups.sh \
                           --bootstrap-server $BROKER_LIST \
                           --describe --group "$group" 2>/dev/null | \
                           grep -v "Consumer group" | grep -v "TOPIC" | \
                           awk '{print $5}' | grep -E '^[0-9]+$' | \
                           sort -nr | head -1)
            
            if [ -n "$max_lag" ] && [ "$max_lag" -gt 50000 ]; then
                high_lag_groups+=("$group:$max_lag")
            fi
        fi
    done
    
    if [ ${#high_lag_groups[@]} -gt 0 ]; then
        log_message "WARN" "å‘ç°é«˜å»¶è¿Ÿæ¶ˆè´¹ç»„: ${high_lag_groups[*]}"
        return 1
    fi
    
    return 0
}

# é‡å¯KafkaæœåŠ¡
restart_kafka_service() {
    log_message "INFO" "å¼€å§‹é‡å¯KafkaæœåŠ¡..."
    
    # ä¼˜é›…åœæ­¢
    $KAFKA_HOME/bin/kafka-server-stop.sh
    sleep 10
    
    # å¼ºåˆ¶æ€æ­»è¿›ç¨‹ï¼ˆå¦‚æœè¿˜åœ¨è¿è¡Œï¼‰
    if check_kafka_process; then
        log_message "WARN" "ä¼˜é›…åœæ­¢å¤±è´¥ï¼Œå¼ºåˆ¶æ€æ­»è¿›ç¨‹"
        pkill -f "kafka.Kafka"
        sleep 5
    fi
    
    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    rm -f /tmp/kafka-logs/.lock
    rm -f /tmp/zookeeper/myid.lock
    
    # å¯åŠ¨æœåŠ¡
    nohup $KAFKA_HOME/bin/kafka-server-start.sh $KAFKA_CONFIG > /dev/null 2>&1 &
    
    # ç­‰å¾…å¯åŠ¨å®Œæˆ
    local count=0
    while ! check_kafka_process && [ $count -lt 60 ]; do
        sleep 1
        ((count++))
    done
    
    if check_kafka_process; then
        log_message "INFO" "Kafkaé‡å¯æˆåŠŸ"
        send_alert "KafkaæœåŠ¡æ¢å¤" "KafkaæœåŠ¡å·²æˆåŠŸé‡å¯å¹¶è¿è¡Œæ­£å¸¸" "INFO"
        return 0
    else
        log_message "ERROR" "Kafkaé‡å¯å¤±è´¥"
        send_alert "Kafkaé‡å¯å¤±è´¥" "æœåŠ¡é‡å¯åä»æ— æ³•æ­£å¸¸è¿è¡Œï¼Œéœ€è¦äººå·¥å¹²é¢„" "ERROR"
        return 1
    fi
}

# æ¸…ç†ç£ç›˜ç©ºé—´
cleanup_disk_space() {
    log_message "INFO" "å¼€å§‹æ¸…ç†ç£ç›˜ç©ºé—´..."
    
    # æ¸…ç†æ—§æ—¥å¿—æ–‡ä»¶
    find /var/log/kafka -name "*.log" -mtime +7 -delete
    find /var/log/kafka -name "*.log.*" -mtime +3 -delete
    
    # æ¸…ç†Kafkaä¸´æ—¶æ–‡ä»¶
    find /tmp -name "kafka-*" -mtime +1 -delete
    
    # å‹ç¼©æ—§çš„Kafkaæ—¥å¿—æ®µ
    find /var/kafka-logs -name "*.log" -mtime +1 -size +100M -exec gzip {} \;
    
    # è·å–æ¸…ç†åçš„ç£ç›˜ä½¿ç”¨ç‡
    local disk_usage=$(df /var/kafka-logs | tail -1 | awk '{print $5}' | sed 's/%//')
    log_message "INFO" "ç£ç›˜æ¸…ç†å®Œæˆï¼Œå½“å‰ä½¿ç”¨ç‡: ${disk_usage}%"
    
    if [ $disk_usage -lt 85 ]; then
        return 0
    else
        return 1
    fi
}

# ä¿®å¤Topicå‰¯æœ¬
repair_topic_replicas() {
    local topic=$1
    log_message "INFO" "å¼€å§‹ä¿®å¤Topicå‰¯æœ¬: $topic"
    
    # è·å–Topicåˆ†åŒºä¿¡æ¯
    local partitions_info=$($KAFKA_HOME/bin/kafka-topics.sh \
                           --bootstrap-server $BROKER_LIST \
                           --describe --topic "$topic" 2>/dev/null)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰åˆ†åŒºå‰¯æœ¬ä¸è¶³
    local under_replicated=$(echo "$partitions_info" | grep "Replicas:" | \
                             awk -F'Replicas: ' '{print $2}' | \
                             awk -F' Isr:' '{print $1}' | \
                             tr ',' '\n' | sort | uniq -c | \
                             awk '$1 < 3 {print $2}')
    
    if [ -n "$under_replicated" ]; then
        log_message "WARN" "å‘ç°å‰¯æœ¬ä¸è¶³çš„åˆ†åŒºï¼Œå°è¯•è§¦å‘é‡å¹³è¡¡"
        
        # ç”Ÿæˆé‡åˆ†é…è®¡åˆ’
        cat > /tmp/reassign-$topic.json << EOF
{
    "version": 1,
    "partitions": [
EOF
        
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…åº”è¯¥æ ¹æ®å…·ä½“åˆ†åŒºä¿¡æ¯ç”Ÿæˆ
        echo '    ]' >> /tmp/reassign-$topic.json
        echo '}' >> /tmp/reassign-$topic.json
        
        # æ‰§è¡Œé‡åˆ†é…ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
        $KAFKA_HOME/bin/kafka-reassign-partitions.sh \
            --bootstrap-server $BROKER_LIST \
            --reassignment-json-file /tmp/reassign-$topic.json \
            --execute
        
        return $?
    fi
    
    return 0
}

# ä¸»å¥åº·æ£€æŸ¥å‡½æ•°
perform_health_check() {
    local issues_found=0
    local recovery_actions=()
    
    log_message "INFO" "å¼€å§‹æ‰§è¡Œå¥åº·æ£€æŸ¥..."
    
    # æ£€æŸ¥Kafkaè¿›ç¨‹
    if ! check_kafka_process; then
        log_message "ERROR" "Kafkaè¿›ç¨‹æœªè¿è¡Œ"
        recovery_actions+=("restart_kafka")
        ((issues_found++))
    fi
    
    # æ£€æŸ¥ç«¯å£ç›‘å¬
    if ! check_kafka_port; then
        log_message "ERROR" "Kafkaç«¯å£æœªç›‘å¬"
        recovery_actions+=("restart_kafka")
        ((issues_found++))
    fi
    
    # æ£€æŸ¥ZooKeeperè¿æ¥
    if ! check_zookeeper_connection; then
        log_message "ERROR" "ZooKeeperè¿æ¥å¤±è´¥"
        recovery_actions+=("check_zookeeper")
        ((issues_found++))
    fi
    
    # æ£€æŸ¥Topicå¥åº·çŠ¶æ€
    check_topics_health
    local failed_topics=$?
    if [ $failed_topics -gt 0 ]; then
        log_message "WARN" "å‘ç° $failed_topics ä¸ªå¼‚å¸¸Topic"
        recovery_actions+=("repair_topics")
        ((issues_found++))
    fi
    
    # æ£€æŸ¥æ¶ˆè´¹è€…å»¶è¿Ÿ
    if ! check_consumer_lag; then
        log_message "WARN" "å‘ç°æ¶ˆè´¹è€…å»¶è¿Ÿè¿‡é«˜"
        recovery_actions+=("alert_high_lag")
        ((issues_found++))
    fi
    
    # æ£€æŸ¥ç£ç›˜ç©ºé—´
    local disk_usage=$(df /var/kafka-logs | tail -1 | awk '{print $5}' | sed 's/%//')
    if [ $disk_usage -gt 90 ]; then
        log_message "ERROR" "ç£ç›˜ç©ºé—´ä¸è¶³: ${disk_usage}%"
        recovery_actions+=("cleanup_disk")
        ((issues_found++))
    elif [ $disk_usage -gt 80 ]; then
        log_message "WARN" "ç£ç›˜ç©ºé—´ç´§å¼ : ${disk_usage}%"
        recovery_actions+=("cleanup_disk")
    fi
    
    return $issues_found
}

# æ‰§è¡Œæ¢å¤æ“ä½œ
execute_recovery_actions() {
    local actions=("$@")
    local recovery_success=0
    
    for action in "${actions[@]}"; do
        case $action in
            "restart_kafka")
                if restart_kafka_service; then
                    ((recovery_success++))
                fi
                ;;
            "cleanup_disk")
                if cleanup_disk_space; then
                    ((recovery_success++))
                fi
                ;;
            "repair_topics")
                # ä¿®å¤å…³é”®Topic
                for topic in "user-events" "order-events" "payment-events"; do
                    repair_topic_replicas "$topic"
                done
                ((recovery_success++))
                ;;
            "alert_high_lag")
                send_alert "æ¶ˆè´¹å»¶è¿Ÿå‘Šè­¦" "æ£€æµ‹åˆ°æ¶ˆè´¹è€…å»¶è¿Ÿè¿‡é«˜ï¼Œè¯·æ£€æŸ¥æ¶ˆè´¹è€…çŠ¶æ€" "WARN"
                ;;
            *)
                log_message "WARN" "æœªçŸ¥çš„æ¢å¤æ“ä½œ: $action"
                ;;
        esac
    done
    
    return $recovery_success
}

# ä¸»å¾ªç¯
main_loop() {
    local consecutive_failures=0
    
    while true; do
        perform_health_check
        local issues_count=$?
        
        if [ $issues_count -eq 0 ]; then
            log_message "INFO" "å¥åº·æ£€æŸ¥é€šè¿‡ï¼Œç³»ç»Ÿè¿è¡Œæ­£å¸¸"
            consecutive_failures=0
        else
            log_message "WARN" "å‘ç° $issues_count ä¸ªé—®é¢˜ï¼Œå°è¯•è‡ªåŠ¨æ¢å¤"
            ((consecutive_failures++))
            
            # æ‰§è¡Œæ¢å¤æ“ä½œ
            local recovery_actions=()
            
            # æ ¹æ®é—®é¢˜ç±»å‹å†³å®šæ¢å¤ç­–ç•¥
            if ! check_kafka_process || ! check_kafka_port; then
                recovery_actions+=("restart_kafka")
            fi
            
            local disk_usage=$(df /var/kafka-logs | tail -1 | awk '{print $5}' | sed 's/%//')
            if [ $disk_usage -gt 80 ]; then
                recovery_actions+=("cleanup_disk")
            fi
            
            if [ ${#recovery_actions[@]} -gt 0 ]; then
                execute_recovery_actions "${recovery_actions[@]}"
            fi
            
            # å¦‚æœè¿ç»­å¤±è´¥æ¬¡æ•°è¿‡å¤šï¼Œå‘é€ä¸¥é‡å‘Šè­¦
            if [ $consecutive_failures -ge $MAX_RECOVERY_ATTEMPTS ]; then
                send_alert "Kafkaé›†ç¾¤ä¸¥é‡æ•…éšœ" \
                          "è‡ªåŠ¨æ¢å¤å¤±è´¥ $consecutive_failures æ¬¡ï¼Œéœ€è¦äººå·¥å¹²é¢„" "ERROR"
                
                # é‡ç½®è®¡æ•°å™¨ï¼Œé¿å…é‡å¤å‘Šè­¦
                consecutive_failures=0
                
                # æš‚åœä¸€æ®µæ—¶é—´å†ç»§ç»­
                sleep 300
            fi
        fi
        
        # ç­‰å¾…ä¸‹æ¬¡æ£€æŸ¥
        sleep $HEALTH_CHECK_INTERVAL
    done
}

# å‘½ä»¤è¡Œå‚æ•°å¤„ç†
case "${1:-check}" in
    "start")
        log_message "INFO" "å¯åŠ¨Kafkaè‡ªåŠ¨æ¢å¤æœåŠ¡"
        send_alert "è‡ªåŠ¨æ¢å¤æœåŠ¡å¯åŠ¨" "Kafkaè‡ªåŠ¨æ•…éšœæ¢å¤æœåŠ¡å·²å¯åŠ¨" "INFO"
        main_loop
        ;;
    "check")
        perform_health_check
        ;;
    "restart")
        restart_kafka_service
        ;;
    "cleanup")
        cleanup_disk_space
        ;;
    *)
        echo "ç”¨æ³•: $0 {start|check|restart|cleanup}"
        echo "  start   - å¯åŠ¨æŒç»­ç›‘æ§å’Œè‡ªåŠ¨æ¢å¤"
        echo "  check   - æ‰§è¡Œä¸€æ¬¡å¥åº·æ£€æŸ¥"
        echo "  restart - é‡å¯KafkaæœåŠ¡"
        echo "  cleanup - æ¸…ç†ç£ç›˜ç©ºé—´"
        exit 1
        ;;
esac
```

**å¿«é€Ÿè¯Šæ–­è„šæœ¬**
```bash
#!/bin/bash
# kafka_diagnosis.sh - Kafkaå¿«é€Ÿæ•…éšœè¯Šæ–­è„šæœ¬

KAFKA_HOME="/opt/kafka"
BROKER_LIST="localhost:9092,localhost:9093,localhost:9094"
DIAGNOSIS_LOG="/var/log/kafka/diagnosis.log"

# é¢œè‰²è¾“å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
NC='\033[0m' # No Color

print_result() {
    local status=$1
    local message=$2
    
    case $status in
        "PASS")
            echo -e "${GREEN}[âœ“]${NC} $message"
            ;;
        "FAIL")
            echo -e "${RED}[âœ—]${NC} $message"
            ;;
        "WARN")
            echo -e "${YELLOW}[!]${NC} $message"
            ;;
        "INFO")
            echo -e "${NC}[i]${NC} $message"
            ;;
    esac
}

# ç³»ç»Ÿèµ„æºæ£€æŸ¥
check_system_resources() {
    echo "=== ç³»ç»Ÿèµ„æºæ£€æŸ¥ ==="
    
    # CPUä½¿ç”¨ç‡
    local cpu_usage=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | awk -F'%' '{print $1}')
    if (( $(echo "$cpu_usage > 80" | bc -l) )); then
        print_result "WARN" "CPUä½¿ç”¨ç‡è¿‡é«˜: ${cpu_usage}%"
    else
        print_result "PASS" "CPUä½¿ç”¨ç‡æ­£å¸¸: ${cpu_usage}%"
    fi
    
    # å†…å­˜ä½¿ç”¨ç‡
    local mem_usage=$(free | grep Mem | awk '{printf "%.1f", ($3-$6-$7)/$2 * 100.0}')
    if (( $(echo "$mem_usage > 85" | bc -l) )); then
        print_result "WARN" "å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜: ${mem_usage}%"
    else
        print_result "PASS" "å†…å­˜ä½¿ç”¨ç‡æ­£å¸¸: ${mem_usage}%"
    fi
    
    # ç£ç›˜ä½¿ç”¨ç‡
    local disk_usage=$(df /var/kafka-logs | tail -1 | awk '{print $5}' | sed 's/%//')
    if [ $disk_usage -gt 90 ]; then
        print_result "FAIL" "ç£ç›˜ç©ºé—´ä¸è¶³: ${disk_usage}%"
    elif [ $disk_usage -gt 80 ]; then
        print_result "WARN" "ç£ç›˜ç©ºé—´ç´§å¼ : ${disk_usage}%"
    else
        print_result "PASS" "ç£ç›˜ç©ºé—´å……è¶³: ${disk_usage}%"
    fi
    
    # æ–‡ä»¶æè¿°ç¬¦
    local fd_usage=$(lsof | wc -l)
    local fd_limit=$(ulimit -n)
    local fd_percent=$((fd_usage * 100 / fd_limit))
    
    if [ $fd_percent -gt 80 ]; then
        print_result "WARN" "æ–‡ä»¶æè¿°ç¬¦ä½¿ç”¨ç‡é«˜: ${fd_percent}% (${fd_usage}/${fd_limit})"
    else
        print_result "PASS" "æ–‡ä»¶æè¿°ç¬¦ä½¿ç”¨æ­£å¸¸: ${fd_percent}% (${fd_usage}/${fd_limit})"
    fi
}

# KafkaæœåŠ¡æ£€æŸ¥
check_kafka_service() {
    echo -e "\n=== KafkaæœåŠ¡æ£€æŸ¥ ==="
    
    # è¿›ç¨‹æ£€æŸ¥
    if pgrep -f "kafka.Kafka" > /dev/null; then
        local pid=$(pgrep -f "kafka.Kafka")
        print_result "PASS" "Kafkaè¿›ç¨‹è¿è¡Œæ­£å¸¸ (PID: $pid)"
    else
        print_result "FAIL" "Kafkaè¿›ç¨‹æœªè¿è¡Œ"
        return 1
    fi
    
    # ç«¯å£æ£€æŸ¥
    for port in 9092 9093 9094; do
        if netstat -tln | grep ":$port " > /dev/null; then
            print_result "PASS" "ç«¯å£ $port ç›‘å¬æ­£å¸¸"
        else
            print_result "FAIL" "ç«¯å£ $port æœªç›‘å¬"
        fi
    done
    
    # JVMå †å†…å­˜æ£€æŸ¥
    local kafka_pid=$(pgrep -f "kafka.Kafka")
    if [ -n "$kafka_pid" ]; then
        local heap_info=$(jstat -gc $kafka_pid | tail -1)
        if [ -n "$heap_info" ]; then
            print_result "INFO" "JVMå †å†…å­˜ä¿¡æ¯å¯è·å–"
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´è¯¦ç»†çš„å†…å­˜åˆ†æ
        fi
    fi
}

# è¿æ¥æ€§æ£€æŸ¥
check_connectivity() {
    echo -e "\n=== è¿æ¥æ€§æ£€æŸ¥ ==="
    
    # ZooKeeperè¿æ¥
    if $KAFKA_HOME/bin/kafka-broker-api-versions.sh \
        --bootstrap-server $BROKER_LIST > /dev/null 2>&1; then
        print_result "PASS" "ZooKeeperè¿æ¥æ­£å¸¸"
    else
        print_result "FAIL" "ZooKeeperè¿æ¥å¤±è´¥"
    fi
    
    # é›†ç¾¤å†…éƒ¨é€šä¿¡
    local broker_count=$(echo $BROKER_LIST | tr ',' '\n' | wc -l)
    local active_brokers=$($KAFKA_HOME/bin/kafka-broker-api-versions.sh \
                          --bootstrap-server $BROKER_LIST 2>/dev/null | \
                          grep -c "Broker API")
    
    if [ $active_brokers -eq $broker_count ]; then
        print_result "PASS" "æ‰€æœ‰Brokeré€šä¿¡æ­£å¸¸ ($active_brokers/$broker_count)"
    elif [ $active_brokers -gt 0 ]; then
        print_result "WARN" "éƒ¨åˆ†Brokeré€šä¿¡å¼‚å¸¸ ($active_brokers/$broker_count)"
    else
        print_result "FAIL" "Brokeré›†ç¾¤é€šä¿¡å¤±è´¥"
    fi
}

# Topicå¥åº·æ£€æŸ¥
check_topics() {
    echo -e "\n=== Topicå¥åº·æ£€æŸ¥ ==="
    
    # è·å–Topicåˆ—è¡¨
    local topics=$($KAFKA_HOME/bin/kafka-topics.sh \
                   --bootstrap-server $BROKER_LIST --list 2>/dev/null)
    
    if [ $? -eq 0 ]; then
        local topic_count=$(echo "$topics" | wc -l)
        print_result "PASS" "Topicåˆ—è¡¨è·å–æˆåŠŸ (å…± $topic_count ä¸ª)"
        
        # æ£€æŸ¥å…³é”®Topic
        local critical_topics=("user-events" "order-events" "payment-events")
        for topic in "${critical_topics[@]}"; do
            if echo "$topics" | grep -q "^$topic$"; then
                # æ£€æŸ¥Topicè¯¦æƒ…
                local topic_info=$($KAFKA_HOME/bin/kafka-topics.sh \
                                  --bootstrap-server $BROKER_LIST \
                                  --describe --topic "$topic" 2>/dev/null)
                
                if [ $? -eq 0 ]; then
                    local partitions=$(echo "$topic_info" | grep "PartitionCount" | \
                                     awk -F'PartitionCount: ' '{print $2}' | \
                                     awk '{print $1}')
                    local replicas=$(echo "$topic_info" | grep "ReplicationFactor" | \
                                   awk -F'ReplicationFactor: ' '{print $2}' | \
                                   awk '{print $1}')
                    
                    print_result "PASS" "Topic $topic æ­£å¸¸ (åˆ†åŒº:$partitions, å‰¯æœ¬:$replicas)"
                else
                    print_result "FAIL" "Topic $topic çŠ¶æ€å¼‚å¸¸"
                fi
            else
                print_result "WARN" "å…³é”®Topic $topic ä¸å­˜åœ¨"
            fi
        done
    else
        print_result "FAIL" "æ— æ³•è·å–Topicåˆ—è¡¨"
    fi
}

# æ¶ˆè´¹ç»„æ£€æŸ¥
check_consumer_groups() {
    echo -e "\n=== æ¶ˆè´¹ç»„æ£€æŸ¥ ==="
    
    local groups=$($KAFKA_HOME/bin/kafka-consumer-groups.sh \
                   --bootstrap-server $BROKER_LIST --list 2>/dev/null)
    
    if [ $? -eq 0 ]; then
        local group_count=$(echo "$groups" | wc -l)
        print_result "INFO" "å‘ç° $group_count ä¸ªæ¶ˆè´¹ç»„"
        
        # æ£€æŸ¥æ¶ˆè´¹å»¶è¿Ÿ
        local high_lag_count=0
        while read -r group; do
            if [ -n "$group" ]; then
                local lag_info=$($KAFKA_HOME/bin/kafka-consumer-groups.sh \
                               --bootstrap-server $BROKER_LIST \
                               --describe --group "$group" 2>/dev/null)
                
                local max_lag=$(echo "$lag_info" | grep -v "Consumer group" | \
                              grep -v "TOPIC" | awk '{print $5}' | \
                              grep -E '^[0-9]+ | sort -nr | head -1)
                
                if [ -n "$max_lag" ]; then
                    if [ $max_lag -gt 10000 ]; then
                        print_result "WARN" "æ¶ˆè´¹ç»„ $group å»¶è¿Ÿè¿‡é«˜: $max_lag"
                        ((high_lag_count++))
                    elif [ $max_lag -gt 1000 ]; then
                        print_result "INFO" "æ¶ˆè´¹ç»„ $group æœ‰å»¶è¿Ÿ: $max_lag"
                    fi
                fi
            fi
        done <<< "$groups"
        
        if [ $high_lag_count -eq 0 ]; then
            print_result "PASS" "æ‰€æœ‰æ¶ˆè´¹ç»„å»¶è¿Ÿæ­£å¸¸"
        fi
    else
        print_result "FAIL" "æ— æ³•è·å–æ¶ˆè´¹ç»„ä¿¡æ¯"
    fi
}

# æ—¥å¿—é”™è¯¯æ£€æŸ¥
check_logs() {
    echo -e "\n=== æ—¥å¿—é”™è¯¯æ£€æŸ¥ ==="
    
    local log_files=("/var/log/kafka/server.log" "/var/log/kafka/controller.log")
    
    for log_file in "${log_files[@]}"; do
        if [ -f "$log_file" ]; then
            # æ£€æŸ¥æœ€è¿‘çš„é”™è¯¯
            local recent_errors=$(tail -1000 "$log_file" | grep -i "error\|exception\|failed" | wc -l)
            
            if [ $recent_errors -gt 50 ]; then
                print_result "WARN" "$(basename $log_file) æœ€è¿‘æœ‰è¾ƒå¤šé”™è¯¯: $recent_errors æ¡"
            elif [ $recent_errors -gt 0 ]; then
                print_result "INFO" "$(basename $log_file) æœ€è¿‘æœ‰å°‘é‡é”™è¯¯: $recent_errors æ¡"
            else
                print_result "PASS" "$(basename $log_file) æ— é”™è¯¯æ—¥å¿—"
            fi
        else
            print_result "WARN" "æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: $log_file"
        fi
    done
}

# æ€§èƒ½æŒ‡æ ‡æ£€æŸ¥
check_performance() {
    echo -e "\n=== æ€§èƒ½æŒ‡æ ‡æ£€æŸ¥ ==="
    
    # è¿™é‡Œå¯ä»¥é›†æˆJMXæŒ‡æ ‡æ£€æŸ¥
    local kafka_pid=$(pgrep -f "kafka.Kafka")
    
    if [ -n "$kafka_pid" ]; then
        # GCé¢‘ç‡æ£€æŸ¥
        local gc_info=$(jstat -gc $kafka_pid 2>/dev/null)
        if [ $? -eq 0 ]; then
            print_result "PASS" "JVM GCä¿¡æ¯å¯è·å–"
        else
            print_result "WARN" "æ— æ³•è·å–JVM GCä¿¡æ¯"
        fi
        
        # çº¿ç¨‹æ•°æ£€æŸ¥
        local thread_count=$(ps -T -p $kafka_pid | wc -l)
        if [ $thread_count -gt 1000 ]; then
            print_result "WARN" "çº¿ç¨‹æ•°è¾ƒå¤š: $thread_count"
        else
            print_result "PASS" "çº¿ç¨‹æ•°æ­£å¸¸: $thread_count"
        fi
    fi
}

# ç”Ÿæˆè¯Šæ–­æŠ¥å‘Š
generate_report() {
    local timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    
    cat > $DIAGNOSIS_LOG << EOF
=== Kafkaé›†ç¾¤è¯Šæ–­æŠ¥å‘Š ===
ç”Ÿæˆæ—¶é—´: $timestamp
ä¸»æœºå: $(hostname)
Kafkaç‰ˆæœ¬: $(find $KAFKA_HOME -name "kafka_*.jar" | head -1 | sed 's/.*kafka_\(.*\)\.jar/\1/')

EOF
    
    echo "è¯Šæ–­æŠ¥å‘Šå·²ä¿å­˜åˆ°: $DIAGNOSIS_LOG"
}

# ä¸»å‡½æ•°
main() {
    echo "å¼€å§‹Kafkaé›†ç¾¤è¯Šæ–­..."
    echo "================================"
    
    check_system_resources
    check_kafka_service
    check_connectivity  
    check_topics
    check_consumer_groups
    check_logs
    check_performance
    
    echo -e "\n================================"
    echo "è¯Šæ–­å®Œæˆï¼"
    
    generate_report
}

# æ‰§è¡Œè¯Šæ–­
main "$@"
```

---

## 8. ğŸ“‹ æ ¸å¿ƒè¦ç‚¹æ€»ç»“


### 8.1 å¿…é¡»æŒæ¡çš„è¿ç»´å·¥å…·


**ğŸ”§ å‘½ä»¤è¡Œå·¥å…·ç²¾å**
```
Topicç®¡ç†ä¸‰å‰‘å®¢ï¼š
â€¢ kafka-topics.sh      â† åˆ›å»ºã€æŸ¥çœ‹ã€é…ç½®Topic
â€¢ kafka-configs.sh     â† åŠ¨æ€ä¿®æ”¹é…ç½®
â€¢ kafka-log-dirs.sh    â† æŸ¥çœ‹æ—¥å¿—ç›®å½•çŠ¶æ€

æ•°æ®æ“ä½œåŒå­æ˜Ÿï¼š
â€¢ kafka-console-producer.sh  â† å‘½ä»¤è¡Œç”Ÿäº§æ•°æ®
â€¢ kafka-console-consumer.sh  â† å‘½ä»¤è¡Œæ¶ˆè´¹æ•°æ®

è¿ç»´å¿…å¤‡å·¥å…·ï¼š
â€¢ kafka-consumer-groups.sh   â† æ¶ˆè´¹ç»„ç®¡ç†
â€¢ kafka-broker-api-versions.sh â† é›†ç¾¤çŠ¶æ€æ£€æŸ¥
â€¢ kafka-dump-log.sh          â† æ—¥å¿—æ–‡ä»¶åˆ†æ
```

### 8.2 AdminClientç¼–ç¨‹è¦ç‚¹


**æ ¸å¿ƒåº”ç”¨åœºæ™¯**
```
è‡ªåŠ¨åŒ–è¿ç»´ï¼š
âœ“ æ‰¹é‡åˆ›å»º/åˆ é™¤Topic
âœ“ åŠ¨æ€è°ƒæ•´åˆ†åŒºæ•°
âœ“ ç›‘æ§é›†ç¾¤çŠ¶æ€
âœ“ é…ç½®ç®¡ç†è‡ªåŠ¨åŒ–

é›†æˆç³»ç»Ÿï¼š
âœ“ è¿ç»´å¹³å°é›†æˆ
âœ“ ç›‘æ§ç³»ç»Ÿé›†æˆ  
âœ“ CI/CDæµæ°´çº¿é›†æˆ
âœ“ æ•…éšœè‡ªæ„ˆç³»ç»Ÿ
```

### 8.3 è¿ç»´è„šæœ¬æœ€ä½³å®è·µ


**è„šæœ¬ç¼–å†™åŸåˆ™**
```
å¯é æ€§ä¼˜å…ˆï¼š
â€¢ å……åˆ†çš„é”™è¯¯å¤„ç†
â€¢ æ“ä½œå‰çš„çŠ¶æ€æ£€æŸ¥
â€¢ å¤±è´¥æ—¶çš„å›æ»šæœºåˆ¶
â€¢ è¯¦ç»†çš„æ—¥å¿—è®°å½•

æ˜“ç”¨æ€§è€ƒè™‘ï¼š
â€¢ æ¸…æ™°çš„å‚æ•°è¯´æ˜
â€¢ å‹å¥½çš„è¾“å‡ºæ ¼å¼
â€¢ è¿›åº¦æç¤ºå’ŒçŠ¶æ€åé¦ˆ
â€¢ æ ‡å‡†åŒ–çš„è¿”å›ç 

å®‰å…¨æ€§ä¿éšœï¼š
â€¢ é‡è¦æ“ä½œéœ€è¦ç¡®è®¤
â€¢ æƒé™æ£€æŸ¥å’ŒéªŒè¯
â€¢ æ•æ„Ÿä¿¡æ¯ä¿æŠ¤
â€¢ æ“ä½œå®¡è®¡æ—¥å¿—
```

### 8.4 ç›‘æ§å‘Šè­¦ä½“ç³»


**ç›‘æ§å±‚æ¬¡ç»“æ„**
```
L1 - åŸºç¡€è®¾æ–½å±‚ï¼š
â€¢ æœåŠ¡å™¨ç¡¬ä»¶çŠ¶æ€
â€¢ æ“ä½œç³»ç»Ÿèµ„æº
â€¢ ç½‘ç»œè¿é€šæ€§
â€¢ å­˜å‚¨ç©ºé—´

L2 - KafkaæœåŠ¡å±‚ï¼š
â€¢ Brokerå­˜æ´»çŠ¶æ€
â€¢ Topicå¥åº·çŠ¶å†µ  
â€¢ åˆ†åŒºå‰¯æœ¬çŠ¶æ€
â€¢ JVMè¿è¡ŒçŠ¶æ€

L3 - ä¸šåŠ¡åº”ç”¨å±‚ï¼š
â€¢ æ¶ˆæ¯ç”Ÿäº§é€Ÿç‡
â€¢ æ¶ˆè´¹å»¶è¿Ÿç›‘æ§
â€¢ é”™è¯¯ç‡ç»Ÿè®¡
â€¢ ä¸šåŠ¡æŒ‡æ ‡å¼‚å¸¸
```

### 8.5 æ•…éšœå¤„ç†ç­–ç•¥


**åˆ†çº§å“åº”æœºåˆ¶**
```
ğŸ”´ P0çº§æ•…éšœï¼ˆç«‹å³å“åº”ï¼‰ï¼š
â€¢ æ•´ä¸ªé›†ç¾¤ä¸å¯ç”¨
â€¢ æ ¸å¿ƒä¸šåŠ¡Topicä¸å¯å†™å…¥
â€¢ æ•°æ®å¤§é‡ä¸¢å¤±
â€¢ å“åº”æ—¶é—´ï¼š5åˆ†é’Ÿå†…

ğŸŸ¡ P1çº§æ•…éšœï¼ˆ1å°æ—¶å†…ï¼‰ï¼š
â€¢ å•ä¸ªBrokeræ•…éšœ
â€¢ éƒ¨åˆ†Topicå¼‚å¸¸
â€¢ æ¶ˆè´¹ä¸¥é‡å»¶è¿Ÿ
â€¢ å“åº”æ—¶é—´ï¼š1å°æ—¶å†…

ğŸŸ¢ P2çº§æ•…éšœï¼ˆå½“æ—¥å¤„ç†ï¼‰ï¼š
â€¢ æ€§èƒ½é™çº§
â€¢ èµ„æºä½¿ç”¨ç‡é«˜
â€¢ éæ ¸å¿ƒåŠŸèƒ½å¼‚å¸¸
â€¢ å“åº”æ—¶é—´ï¼šå½“æ—¥å†…
```

### 8.6 è‡ªåŠ¨åŒ–è¿ç»´ä»·å€¼


**ROIåˆ†æ**
```
äººåŠ›æˆæœ¬èŠ‚çœï¼š
â€¢ å‡å°‘é‡å¤æ‰‹å·¥æ“ä½œ
â€¢ é™ä½äººä¸ºé”™è¯¯ç‡
â€¢ æå‡æ•…éšœå“åº”é€Ÿåº¦
â€¢ é‡Šæ”¾äººåŠ›åšæ›´æœ‰ä»·å€¼çš„äº‹

ç³»ç»Ÿç¨³å®šæ€§æå‡ï¼š
â€¢ æ ‡å‡†åŒ–æ“ä½œæµç¨‹
â€¢ ä¸€è‡´çš„ç¯å¢ƒé…ç½®
â€¢ å¿«é€Ÿæ•…éšœæ¢å¤
â€¢ å†å²ç»éªŒå›ºåŒ–

ä¸šåŠ¡è¿ç»­æ€§ä¿éšœï¼š
â€¢ 7Ã—24å°æ—¶è‡ªåŠ¨ç›‘æ§
â€¢ ç§’çº§æ•…éšœæ£€æµ‹
â€¢ è‡ªåŠ¨æ•…éšœæ¢å¤
â€¢ é¢„é˜²æ€§ç»´æŠ¤
```

**æ ¸å¿ƒè®°å¿†è¦ç‚¹**ï¼š
- å·¥å…·æ˜¯åŸºç¡€ï¼šç†Ÿç»ƒæŒæ¡å‘½ä»¤è¡Œå·¥å…·æ˜¯è¿ç»´çš„åŸºæœ¬åŠŸ
- è„šæœ¬æ˜¯åˆ©å™¨ï¼šæ ‡å‡†åŒ–è„šæœ¬è®©è¿ç»´å·¥ä½œäº‹åŠåŠŸå€
- ç›‘æ§æ˜¯çœ¼ç›ï¼šæ²¡æœ‰ç›‘æ§çš„ç³»ç»Ÿå°±æ˜¯ç›²äººæ‘¸è±¡
- è‡ªåŠ¨åŒ–æ˜¯è¶‹åŠ¿ï¼šä»æ‰‹å·¥è¿ç»´åˆ°æ™ºèƒ½è¿ç»´çš„å¿…ç»ä¹‹è·¯
- å®è·µå‡ºçœŸçŸ¥ï¼šå†å¥½çš„å·¥å…·ä¹Ÿéœ€è¦åœ¨å®æˆ˜ä¸­ç£¨ç»ƒ