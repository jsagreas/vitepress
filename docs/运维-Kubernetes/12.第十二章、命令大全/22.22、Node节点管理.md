---
title: 22、Node节点管理
---
## 📚 目录

1. [什么是Kubernetes节点](#1-什么是Kubernetes节点)
2. [节点查看与状态监控](#2-节点查看与状态监控)
3. [节点调度控制](#3-节点调度控制)
4. [节点标签管理](#4-节点标签管理)
5. [节点污点与容忍](#5-节点污点与容忍)
6. [节点维护实战](#6-节点维护实战)
7. [核心要点总结](#7-核心要点总结)

---

## 1. 🖥️ 什么是Kubernetes节点


### 1.1 节点的本质理解


**🔸 简单来说**
```
节点(Node) = 一台能够运行Pod的机器
可以是物理服务器，也可以是虚拟机

就像工厂里的工作台：
- 每个工作台(节点)可以放置多个产品(Pod)
- 工厂管理员(Kubernetes)决定哪个产品放在哪个工作台上
- 有些工作台可能在维修，暂时不能放产品
```

### 1.2 节点的基本组成


**🏗️ 节点内部结构**
```
一个Kubernetes节点包含：

┌─────────────────────────────┐
│         Node (节点)          │
├─────────────────────────────┤
│  kubelet (节点代理)          │ ← 与控制平面通信
├─────────────────────────────┤
│  kube-proxy (网络代理)       │ ← 处理网络规则
├─────────────────────────────┤
│  Container Runtime          │ ← Docker/containerd
├─────────────────────────────┤
│  Pod1   Pod2   Pod3         │ ← 实际运行的应用
└─────────────────────────────┘

通俗理解：
- kubelet：像工作台管理员，接收指令，管理这个工作台上的产品
- kube-proxy：像网络管理员，确保产品间能正常通信
- Container Runtime：像生产设备，实际制造和运行产品
```

### 1.3 节点的生命周期状态


**📊 节点状态说明**
```
Ready：节点健康，可以接收新Pod
NotReady：节点有问题，不能接收新Pod
Unknown：控制平面无法联系到节点

生活比喻：
Ready = 工作台正常运行中
NotReady = 工作台出故障了
Unknown = 联系不上工作台管理员
```

---

## 2. 🔍 节点查看与状态监控


### 2.1 查看所有节点基本信息


**📋 最基础的节点查看**
```bash
# 查看集群中所有节点
kubectl get nodes

# 输出示例：
NAME           STATUS   ROLES           AGE   VERSION
master-node    Ready    control-plane   10d   v1.28.0
worker-node1   Ready    <none>          10d   v1.28.0  
worker-node2   Ready    <none>          10d   v1.28.0
```

> 💡 **新手理解**：这就像查看工厂里有多少个工作台，每个工作台的状态如何

**🔧 获取更详细信息**
```bash
# 显示更多列信息
kubectl get nodes -o wide

# 会显示：内部IP、外部IP、操作系统、内核版本等
```

### 2.2 查看单个节点详细信息


**🔍 深入了解特定节点**
```bash
# 查看节点详细信息
kubectl describe node worker-node1
```

**📝 输出信息解读**
```
重要信息包括：
✅ 基本信息：名称、角色、创建时间
✅ 系统信息：操作系统、内核版本、容器运行时
✅ 资源分配：CPU、内存的总量和已使用量
✅ 运行的Pod：当前在此节点上运行的所有Pod
✅ 事件记录：最近发生的重要事件

新手关注重点：
- Conditions部分：节点是否健康
- Capacity部分：节点总资源
- Allocatable部分：可分配资源  
- Non-terminated Pods部分：正在运行的Pod
```

### 2.3 实时监控节点资源使用


**📊 查看节点资源使用情况**
```bash
# 查看所有节点的CPU和内存使用率
kubectl top nodes

# 输出示例：
NAME           CPU(cores)   CPU%   MEMORY(bytes)   MEMORY%
master-node    159m         7%     1623Mi          42%
worker-node1   89m          4%     856Mi           22%
worker-node2   124m         6%     1205Mi          31%
```

> 🧠 **理解要点**：
> - **CPU(cores)**：当前使用的CPU核心数，159m表示0.159个核心
> - **CPU%**：CPU使用百分比
> - **MEMORY(bytes)**：内存使用量，Mi表示MB
> - **MEMORY%**：内存使用百分比

---

## 3. ⚙️ 节点调度控制


### 3.1 节点调度的基本概念


**🎯 什么是节点调度**
```
调度 = Kubernetes决定把Pod放在哪个节点上

就像安排座位：
🎪 电影院(集群) 有多个座位(节点)
🎫 观众(Pod) 需要找座位坐下
🎭 售票员(调度器) 决定观众坐哪个座位

有时候需要：
- 预留座位(cordon) - 暂时不让新观众坐
- 清空座位(drain) - 让已有观众换座位
- 重新开放(uncordon) - 允许新观众坐
```

### 3.2 禁止节点接收新Pod


**🚫 标记节点不可调度**
```bash
# 将节点标记为不可调度（现有Pod不受影响）
kubectl cordon worker-node1

# 查看效果
kubectl get nodes
# worker-node1 状态会显示 Ready,SchedulingDisabled
```

**💡 使用场景**
```
什么时候用cordon：
✅ 准备对节点进行维护
✅ 发现节点有潜在问题
✅ 临时减少节点负载
✅ 准备升级节点软件

注意：已经运行的Pod不会被影响，只是不会有新Pod被调度到这个节点
```

### 3.3 恢复节点可调度状态


**✅ 重新开放节点调度**
```bash
# 恢复节点可调度状态
kubectl uncordon worker-node1

# 查看效果
kubectl get nodes
# worker-node1 状态恢复为 Ready
```

### 3.4 安全驱逐节点上的Pod


**🔄 优雅地清空节点**
```bash
# 驱逐节点上的所有Pod（除了DaemonSet）
kubectl drain worker-node1 --ignore-daemonsets

# 如果有不受控制的Pod，需要强制删除
kubectl drain worker-node1 --ignore-daemonsets --delete-emptydir-data --force
```

**⚠️ drain命令参数说明**
```
--ignore-daemonsets：忽略DaemonSet Pod（系统Pod）
--delete-emptydir-data：删除使用emptyDir的Pod
--force：强制删除不受控制器管理的Pod
--grace-period=秒数：设置Pod优雅关闭时间

drain过程：
1. 标记节点不可调度
2. 优雅地关闭节点上的Pod  
3. 在其他节点重新创建这些Pod
```

**🔧 典型维护流程**
```bash
# 第1步：查看节点状态
kubectl get nodes

# 第2步：驱逐Pod到其他节点
kubectl drain worker-node1 --ignore-daemonsets

# 第3步：进行节点维护（重启、更新等）
# ... 在节点上进行维护操作 ...

# 第4步：维护完成后恢复调度
kubectl uncordon worker-node1
```

---

## 4. 🏷️ 节点标签管理


### 4.1 标签的作用和意义


**🎯 节点标签的本质**
```
标签(Label) = 给节点贴标签，方便分类管理

生活比喻：
🏷️ 给衣服贴标签：夏装、冬装、正装、休闲装
🏷️ 给节点贴标签：GPU节点、高内存节点、SSD节点、生产环境节点

用途：
✅ Pod可以选择运行在特定类型的节点上
✅ 方便批量管理相同类型的节点
✅ 实现资源的精确调度
```

### 4.2 查看节点现有标签


**🔍 查看节点标签信息**
```bash
# 查看所有节点的标签
kubectl get nodes --show-labels

# 查看特定节点的标签
kubectl describe node worker-node1 | grep Labels
```

**📝 常见系统标签**
```
kubernetes.io/hostname：节点主机名
kubernetes.io/os：操作系统类型（linux/windows）  
kubernetes.io/arch：CPU架构（amd64/arm64）
node.kubernetes.io/instance-type：云提供商的实例类型

这些是Kubernetes自动添加的，一般不需要手动管理
```

### 4.3 添加和修改节点标签


**🏷️ 给节点添加标签**
```bash
# 添加单个标签
kubectl label node worker-node1 disktype=ssd

# 添加多个标签  
kubectl label node worker-node1 env=production zone=zone-a

# 修改已存在的标签（需要加--overwrite）
kubectl label node worker-node1 env=testing --overwrite
```

**💡 标签命名建议**
```
推荐的标签格式：
✅ disktype=ssd          # 磁盘类型
✅ env=production        # 环境类型
✅ zone=zone-a           # 可用区
✅ gpu=true              # 是否有GPU
✅ memory=high           # 内存规格

避免的格式：
❌ 使用空格或特殊字符
❌ 标签值过长
❌ 没有意义的随机字符
```

### 4.4 删除节点标签


**🗑️ 移除不需要的标签**
```bash
# 删除指定标签（注意标签名后面的减号）
kubectl label node worker-node1 disktype-

# 删除多个标签
kubectl label node worker-node1 env- zone-
```

### 4.5 基于标签选择节点


**🎯 利用标签进行Pod调度**
```yaml
# Pod配置示例：只在SSD节点上运行
apiVersion: v1
kind: Pod
metadata:
  name: fast-app
spec:
  nodeSelector:
    disktype: ssd    # 只选择有这个标签的节点
  containers:
  - name: app
    image: nginx
```

---

## 5. 🛡️ 节点污点与容忍


### 5.1 污点的基本概念


**🤢 什么是节点污点(Taint)**
```
污点 = 节点的"拒绝"标签，让节点排斥某些Pod

生活比喻：
🚫 "员工餐厅，访客禁入" - 餐厅有"访客"污点
🚫 "无烟区，吸烟者禁入" - 区域有"吸烟"污点  
🚫 "VIP专区，普通会员禁入" - 专区有"普通会员"污点

Kubernetes中：
节点可以设置污点，拒绝没有"容忍度"的Pod
只有能"容忍"这个污点的Pod才能调度到该节点
```

### 5.2 污点的三种效果


**⚡ 污点效果类型**
```
NoSchedule：不调度新Pod到此节点（已有Pod不受影响）
PreferNoSchedule：尽量不调度到此节点（软限制）
NoExecute：不仅不调度新Pod，还会驱逐已有Pod（最严格）

餐厅比喻：
NoSchedule = "新客人不能进，但已经在吃的可以继续"
PreferNoSchedule = "新客人最好别来，但实在没地方也行"  
NoExecute = "立即清场，所有人都要离开"
```

### 5.3 添加节点污点


**🔧 给节点添加污点**
```bash
# 添加NoSchedule污点
kubectl taint node worker-node1 maintenance=true:NoSchedule

# 添加NoExecute污点（会驱逐现有Pod）
kubectl taint node worker-node1 hardware-issue=true:NoExecute

# 添加PreferNoSchedule污点（软限制）
kubectl taint node worker-node1 high-load=true:PreferNoSchedule
```

**💡 常见污点使用场景**
```
maintenance=true:NoSchedule    # 节点维护期间
hardware-issue=true:NoExecute  # 硬件故障
dedicated=gpu:NoSchedule       # GPU专用节点
environment=prod:NoSchedule    # 生产环境专用
```

### 5.4 查看节点污点


**🔍 检查节点污点状态**
```bash
# 查看节点详细信息中的Taints部分
kubectl describe node worker-node1

# 输出示例中会有：
# Taints: maintenance=true:NoSchedule
```

### 5.5 删除节点污点


**🧽 移除节点污点**
```bash
# 删除指定污点（注意污点名:效果后面的减号）
kubectl taint node worker-node1 maintenance=true:NoSchedule-

# 删除节点的所有污点
kubectl taint node worker-node1 maintenance:NoSchedule-
kubectl taint node worker-node1 hardware-issue:NoExecute-
```

### 5.6 Pod容忍度配置


**🤝 让Pod容忍节点污点**
```yaml
# Pod配置示例：能容忍maintenance污点的Pod
apiVersion: v1
kind: Pod  
metadata:
  name: tolerant-pod
spec:
  tolerations:
  - key: "maintenance"
    operator: "Equal"
    value: "true"
    effect: "NoSchedule"
  containers:
  - name: app
    image: nginx
```

**📋 容忍度配置说明**
```
tolerations参数：
- key: 污点的键名
- operator: Equal(精确匹配) 或 Exists(存在即可)
- value: 污点的值（operator为Exists时可省略）
- effect: 要容忍的效果类型

特殊容忍度：
- 空的key + Exists操作符 = 容忍所有污点
- 空的effect = 容忍该key的所有效果
```

---

## 6. 🔧 节点维护实战


### 6.1 节点维护的完整流程


**📋 标准维护步骤**
```
节点维护就像汽车保养：

第1步：预检查 🔍
- 查看节点状态和资源使用情况
- 确认节点上运行的重要应用
- 通知相关人员维护计划

第2步：安全隔离 🚫  
- cordon节点（停止新Pod调度）
- drain节点（迁移现有Pod）
- 等待Pod完全迁移完成

第3步：执行维护 🔧
- 重启节点、升级系统、更换硬件等
- 验证维护结果

第4步：恢复服务 ✅
- uncordon节点（恢复调度）
- 验证节点正常工作
- 监控节点状态
```

### 6.2 实际维护命令序列


**🛠️ 完整维护脚本**
```bash
#!/bin/bash
NODE_NAME="worker-node1"

echo "=== 开始维护节点 $NODE_NAME ==="

# 第1步：检查节点当前状态
echo "1. 检查节点状态..."
kubectl get node $NODE_NAME
kubectl top node $NODE_NAME

# 第2步：查看节点上的Pod
echo "2. 查看节点上的Pod..."
kubectl get pods --all-namespaces -o wide --field-selector spec.nodeName=$NODE_NAME

# 第3步：标记节点不可调度
echo "3. 标记节点不可调度..."
kubectl cordon $NODE_NAME

# 第4步：驱逐Pod
echo "4. 驱逐节点上的Pod..."
kubectl drain $NODE_NAME --ignore-daemonsets --delete-emptydir-data

# 第5步：等待用户确认完成维护
echo "5. 请在节点上完成维护操作，完成后按回车继续..."
read

# 第6步：恢复节点调度
echo "6. 恢复节点调度..."
kubectl uncordon $NODE_NAME

# 第7步：验证恢复结果
echo "7. 验证节点状态..."
kubectl get node $NODE_NAME
echo "=== 维护完成 ==="
```

### 6.3 紧急情况处理


**🚨 节点异常处理**
```bash
# 节点失联时的处理
# 1. 强制删除节点上的Pod
kubectl delete pods --all-namespaces --field-selector spec.nodeName=failed-node --force --grace-period=0

# 2. 如果节点完全无法恢复，从集群删除
kubectl delete node failed-node

# 3. 重新加入节点到集群
# 在节点上重新执行 kubeadm join 命令
```

### 6.4 批量节点操作


**📦 同时维护多个节点**
```bash
# 给多个节点添加相同标签
kubectl label nodes worker-node1 worker-node2 worker-node3 maintenance=scheduled

# 基于标签批量操作
kubectl get nodes -l maintenance=scheduled

# 批量添加污点
for node in worker-node1 worker-node2; do
  kubectl taint node $node maintenance=true:NoSchedule
done
```

---

## 7. 📋 核心要点总结


### 7.1 必须掌握的基本概念


```
🔸 节点本质：能够运行Pod的机器，包含kubelet、kube-proxy等组件
🔸 节点状态：Ready(正常)、NotReady(故障)、Unknown(失联)
🔸 调度控制：cordon(禁调度)、drain(驱逐)、uncordon(恢复)
🔸 标签管理：给节点分类，实现精确调度
🔸 污点容忍：节点拒绝机制，只有容忍的Pod才能调度
```

### 7.2 关键命令速查表


| **功能** | **命令** | **用途** |
|---------|---------|----------|
| 🔍 **节点查看** | `kubectl get nodes` | 查看所有节点基本信息 |
| 📊 **资源监控** | `kubectl top nodes` | 查看节点资源使用情况 |
| 📝 **详细信息** | `kubectl describe node [节点名]` | 查看节点详细状态 |
| 🚫 **禁止调度** | `kubectl cordon [节点名]` | 标记节点不可调度 |
| ✅ **恢复调度** | `kubectl uncordon [节点名]` | 恢复节点可调度 |
| 🔄 **驱逐Pod** | `kubectl drain [节点名] --ignore-daemonsets` | 安全迁移节点Pod |
| 🏷️ **添加标签** | `kubectl label node [节点名] [键]=[值]` | 给节点添加标签 |
| 🗑️ **删除标签** | `kubectl label node [节点名] [键]-` | 删除节点标签 |
| 🤢 **添加污点** | `kubectl taint node [节点名] [键]=[值]:[效果]` | 添加节点污点 |
| 🧽 **删除污点** | `kubectl taint node [节点名] [键]:[效果]-` | 删除节点污点 |

### 7.3 实际应用场景


**🎯 日常运维场景**
```
节点维护：
cordon → drain → 维护操作 → uncordon

资源调度：
添加标签 → Pod使用nodeSelector选择节点

专用节点：
添加污点 → Pod配置toleration容忍污点

故障处理：
describe查看问题 → 根据情况drain或重启
```

### 7.4 新手注意事项


**⚠️ 重要提醒**
```
安全操作：
✅ drain前确认重要应用的高可用性
✅ 维护master节点时要格外小心
✅ 生产环境操作前做好备份计划

常见错误：
❌ 忘记uncordon导致节点一直不可调度
❌ drain时忘记--ignore-daemonsets参数
❌ 直接重启节点不做drain操作
❌ 污点配置错误导致Pod无法调度
```

### 7.5 最佳实践建议


```
运维建议：
🔸 建立标准的节点维护流程
🔸 使用有意义的标签和污点命名
🔸 定期监控节点资源使用情况
🔸 为不同类型的节点制定不同的维护策略
🔸 在维护前通知相关团队和用户
```

**核心记忆口诀**：
> 节点管理三步走：查看状态先了解，cordon drain安全走，uncordon恢复莫忘记  
> 标签污点巧运用：标签分类好管理，污点拒绝更精准，容忍配置来适配