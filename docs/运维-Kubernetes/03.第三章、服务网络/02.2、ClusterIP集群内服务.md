---
title: 2、ClusterIP集群内服务
---
## 📚 目录

1. [ClusterIP服务基本概念](#1-ClusterIP服务基本概念)
2. [虚拟IP地址分配机制](#2-虚拟IP地址分配机制)
3. [服务端口映射详解](#3-服务端口映射详解)
4. [Session Affinity会话保持](#4-Session-Affinity会话保持)
5. [服务代理模式深入理解](#5-服务代理模式深入理解)
6. [集群内通信测试实战](#6-集群内通信测试实战)
7. [核心要点总结](#7-核心要点总结)

---

## 1. 🌐 ClusterIP服务基本概念


### 1.1 什么是ClusterIP服务


**🔸 简单理解**
```
想象一下：你有一个餐厅（Pod），但是位置经常变化
顾客（其他Pod）怎么找到你？
ClusterIP就像是一个固定的餐厅电话号码
不管餐厅搬到哪里，顾客都能通过这个号码找到你
```

**💡 专业定义**
- **ClusterIP**是Kubernetes中最基础的服务类型
- 为一组Pod提供**稳定的虚拟IP地址**
- 只能在**集群内部**访问，外部无法直接访问
- 是Pod之间通信的**桥梁**

### 1.2 为什么需要ClusterIP


**🚫 没有Service的问题**
```
Pod的痛点：
┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│   Pod A     │    │   Pod B     │    │   Pod C     │
│ IP:10.1.1.5 │    │ IP:10.1.1.8 │    │ IP:10.1.1.9 │
└─────────────┘    └─────────────┘    └─────────────┘
      ↓                   ↓                   ↓
   Pod重启后          Pod重启后          Pod重启后
      ↓                   ↓                   ↓  
┌─────────────┐    ┌─────────────┐    ┌─────────────┐
│   Pod A     │    │   Pod B     │    │   Pod C     │
│ IP:10.1.2.1 │    │ IP:10.1.2.3 │    │ IP:10.1.2.7 │
└─────────────┘    └─────────────┘    └─────────────┘

问题：IP地址经常变化，其他Pod无法稳定访问
```

**✅ 有了ClusterIP的解决方案**
```
稳定的服务访问：
┌─────────────────────────────────────────┐
│           ClusterIP Service             │
│        稳定IP: 10.96.1.100             │
│           端口: 80                      │
└─────────────────────────────────────────┘
                    │
          自动负载均衡分发请求
                    │
    ┌───────────────┼───────────────┐
    ↓               ↓               ↓
┌─────────────┐ ┌─────────────┐ ┌─────────────┐
│   Pod A     │ │   Pod B     │ │   Pod C     │
│后端应用8080 │ │后端应用8080 │ │后端应用8080 │
└─────────────┘ └─────────────┘ └─────────────┘

优势：无论Pod如何变化，访问地址始终是10.96.1.100:80
```

### 1.3 ClusterIP的工作原理


**🔧 工作机制图解**
```
客户端Pod                ClusterIP Service              后端Pod
    │                         │                          │
    │── ①发起请求 ─────────────→│                          │
    │   (访问10.96.1.100:80)   │                          │
    │                         │── ②选择后端Pod ─────────→│
    │                         │   (负载均衡算法)           │
    │                         │                          │
    │                         │←── ③转发请求 ──────────────│
    │←── ④返回响应 ─────────────│                          │
    
工作步骤：
1. 客户端访问Service的虚拟IP
2. kube-proxy根据算法选择健康的后端Pod
3. 请求被转发到选中的Pod
4. Pod处理请求并返回响应
```

---

## 2. 🏷️ 虚拟IP地址分配机制


### 2.1 什么是虚拟IP


**🔸 虚拟IP的特点**
- **不真实存在**：没有实际的网卡绑定这个IP
- **由系统管理**：Kubernetes自动分配和管理
- **集群范围有效**：只在集群内部可访问
- **持久不变**：Service存在期间IP保持固定

**💭 通俗理解**
```
虚拟IP就像：
🏪 商场的总台电话 (虚拟IP: 10.96.1.100)
├── 实际转接到不同的店铺 (Pod1: 10.1.1.5)
├── 客户不需要知道具体店铺位置 (Pod2: 10.1.1.8)  
└── 店铺可以换位置，但总台号码不变 (Pod3: 10.1.1.9)
```

### 2.2 IP地址分配规则


**📊 IP地址池管理**

| **分配方式** | **说明** | **IP范围示例** | **使用场景** |
|------------|----------|---------------|-------------|
| 🔀 **自动分配** | 系统从预设范围随机分配 | `10.96.0.0/12` | 大多数情况 |
| 🎯 **手动指定** | 用户指定特定IP地址 | `10.96.1.100` | 特殊需求 |
| 🚫 **None模式** | 不分配ClusterIP | - | Headless Service |

**⚙️ IP分配配置示例**
```yaml
# 自动分配ClusterIP
apiVersion: v1
kind: Service
metadata:
  name: web-service
spec:
  selector:
    app: web
  ports:
  - port: 80
    targetPort: 8080
  type: ClusterIP  # 默认类型，可省略

---
# 手动指定ClusterIP  
apiVersion: v1
kind: Service
metadata:
  name: database-service
spec:
  selector:
    app: database
  clusterIP: 10.96.1.200  # 手动指定IP
  ports:
  - port: 3306
    targetPort: 3306
```

### 2.3 IP地址查看与验证


**🔍 查看Service IP信息**
```bash
# 查看所有Service的IP
kubectl get services

# 详细查看某个Service
kubectl describe service web-service

# 查看Service的YAML配置
kubectl get service web-service -o yaml
```

**📋 输出示例解读**
```bash
NAME          TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)   AGE
web-service   ClusterIP   10.96.156.25   <none>        80/TCP    5m

解读说明：
- TYPE: ClusterIP (服务类型)
- CLUSTER-IP: 10.96.156.25 (分配的虚拟IP)
- EXTERNAL-IP: <none> (无外部IP，仅集群内访问)
- PORT(S): 80/TCP (服务端口和协议)
```

---

## 3. 🔌 服务端口映射详解


### 3.1 端口映射基本概念


**🔸 三种端口的区别**
```
端口映射关系图：
客户端                Service                Pod
   │                    │                   │
   │────访问port:80────→│                   │
   │                    │──转发到targetPort:8080─→│
   │                    │                   │
   │                    │  nodePort:30080   │
   │                    │  (NodePort类型才有) │

三种端口说明：
🌐 port: Service对外暴露的端口
🎯 targetPort: Pod应用实际监听的端口  
🚪 nodePort: 节点上开放的端口(可选)
```

**💡 端口映射原理**
- **port**：其他Pod访问Service时使用的端口
- **targetPort**：请求最终到达Pod的端口
- **中间转换**：kube-proxy负责端口转换和请求转发

### 3.2 多端口服务配置


**🔧 单端口vs多端口对比**
```yaml
# 单端口服务
apiVersion: v1
kind: Service
metadata:
  name: simple-web
spec:
  selector:
    app: web
  ports:
  - port: 80          # Service端口
    targetPort: 8080  # Pod端口

---
# 多端口服务
apiVersion: v1  
kind: Service
metadata:
  name: complex-app
spec:
  selector:
    app: fullstack-app
  ports:
  - name: web          # 端口名称(多端口时必须)
    port: 80           # HTTP服务端口
    targetPort: 8080   # Web应用端口
  - name: api          
    port: 3000         # API服务端口
    targetPort: 3000   # API应用端口  
  - name: admin
    port: 9090         # 管理端口
    targetPort: 9090   # 管理应用端口
```

### 3.3 端口命名与协议


**📝 端口命名规范**

| **命名模式** | **示例** | **说明** |
|------------|----------|----------|
| 🌐 **按功能命名** | `web`, `api`, `admin` | 根据服务功能命名 |
| 🔢 **按协议命名** | `http`, `https`, `grpc` | 根据协议类型命名 |
| 🎯 **按用途命名** | `frontend`, `backend` | 根据架构层次命名 |

**⚙️ 协议支持**
```yaml
apiVersion: v1
kind: Service  
metadata:
  name: multi-protocol
spec:
  selector:
    app: my-app
  ports:
  - name: http
    port: 80
    targetPort: 8080
    protocol: TCP      # TCP协议(默认)
  - name: metrics  
    port: 9090
    targetPort: 9090
    protocol: TCP
  - name: dns
    port: 53
    targetPort: 53
    protocol: UDP      # UDP协议
```

---

## 4. 🔗 Session Affinity会话保持


### 4.1 什么是会话保持


**🔸 会话保持的需求**
```
无会话保持的问题：
用户请求1 → Service → Pod A (登录成功，存储session)
用户请求2 → Service → Pod B (session丢失，需要重新登录)
用户请求3 → Service → Pod C (session丢失，需要重新登录)

有会话保持的效果：  
用户请求1 → Service → Pod A (登录成功)
用户请求2 → Service → Pod A (同一用户，路由到相同Pod)
用户请求3 → Service → Pod A (会话保持，用户体验好)
```

**💡 适用场景**
- 🔐 **有状态应用**：需要保持用户登录状态
- 🛒 **购物车系统**：购物车数据存储在特定Pod
- 💾 **本地缓存**：应用在Pod内维护缓存数据

### 4.2 会话保持配置


**⚙️ 基于客户端IP的会话保持**
```yaml
apiVersion: v1
kind: Service
metadata:
  name: sticky-service
spec:
  selector:
    app: webapp
  sessionAffinity: ClientIP  # 启用会话保持
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 10800  # 会话超时时间(3小时)
  ports:
  - port: 80
    targetPort: 8080
```

**📊 会话保持类型对比**

| **类型** | **工作原理** | **优势** | **劣势** |
|----------|------------|----------|----------|
| 🌐 **ClientIP** | 根据客户端IP路由 | 简单可靠 | 多用户共享IP时效果差 |
| 🚫 **None** | 完全随机负载均衡 | 分布最均匀 | 无法保持会话 |

### 4.3 会话保持最佳实践


**✅ 推荐做法**
```yaml
# 推荐：结合应用特点配置超时时间
apiVersion: v1
kind: Service
metadata:
  name: web-app
spec:
  selector:
    app: web
  sessionAffinity: ClientIP
  sessionAffinityConfig:
    clientIP:
      timeoutSeconds: 3600   # 1小时超时，适合Web应用
  ports:
  - port: 80
    targetPort: 8080
```

**🚨 注意事项**
- ⚠️ **负载不均**：可能导致某些Pod负载过高
- ⚠️ **扩缩容影响**：Pod重启会丢失会话
- ⚠️ **代理环境**：通过代理访问时效果可能不佳

---

## 5. ⚡ 服务代理模式深入理解


### 5.1 kube-proxy组件作用


**🔸 kube-proxy的职责**
```
kube-proxy的工作流程：
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   API Server    │    │   kube-proxy    │    │   Node网络      │
│                 │    │                 │    │                 │
│  Service定义     │───→│  监听变化        │───→│  更新转发规则    │
│  Endpoint列表   │    │  生成规则        │    │  (iptables/ipvs)│
└─────────────────┘    └─────────────────┘    └─────────────────┘

简单理解：
kube-proxy就像一个智能的交通警察
- 📡 监听Service的变化
- 🚦 制定流量转发规则  
- 🔄 动态更新网络配置
```

### 5.2 三种代理模式详解


**📊 代理模式对比**

| **模式** | **实现方式** | **性能** | **功能** | **适用场景** |
|----------|------------|----------|----------|-------------|
| 🔧 **iptables** | Linux防火墙规则 | 中等 | 基础负载均衡 | 小到中型集群 |
| ⚡ **IPVS** | 内核级负载均衡 | 高 | 多种负载算法 | 大型生产集群 |
| 👥 **userspace** | 用户空间代理 | 低 | 功能完整 | 已废弃 |

### 5.3 iptables模式详解


**🔧 iptables规则示例**
```bash
# 查看Service相关的iptables规则
sudo iptables -t nat -L -n | grep web-service

# 示例输出解读：
KUBE-SVC-XXX  tcp  --  0.0.0.0/0  10.96.156.25  tcp dpt:80
├── 规则链名称：KUBE-SVC-XXX (Service规则)
├── 协议：tcp
├── 目标地址：10.96.156.25 (ClusterIP)
└── 目标端口：80 (Service端口)

KUBE-SEP-XXX  tcp  --  0.0.0.0/0  10.1.1.5      tcp dpt:8080
├── 规则链名称：KUBE-SEP-XXX (Endpoint规则)  
├── 目标地址：10.1.1.5 (Pod IP)
└── 目标端口：8080 (targetPort)
```

**⚖️ iptables模式的负载均衡**
```
负载均衡实现原理：
请求 → ClusterIP:80
          │
      iptables规则
          │
    ┌─────┼─────┐
    │  33%│ 33% │ 34%    (随机概率分发)
    ↓     ↓     ↓
 Pod1   Pod2   Pod3
 8080   8080   8080

特点：
✅ 简单可靠
✅ 内核级别性能好
❌ 只支持随机负载均衡
❌ 大量Service时规则复杂
```

### 5.4 IPVS模式详解


**⚡ IPVS的优势**
```yaml
# 启用IPVS模式的kube-proxy配置
apiVersion: kubeproxy.config.k8s.io/v1alpha1
kind: KubeProxyConfiguration
mode: "ipvs"                    # 使用IPVS模式
ipvs:
  scheduler: "rr"               # 轮询调度算法
  # 其他算法: lc(最少连接), dh(目标哈希)等
```

**🔄 IPVS负载均衡算法**

| **算法** | **英文名** | **工作原理** | **适用场景** |
|----------|-----------|--------------|-------------|
| 🔄 **rr** | Round Robin | 轮流分发请求 | 后端Pod性能相近 |
| ⚖️ **lc** | Least Connection | 选择连接数最少的Pod | 长连接服务 |
| 🎯 **dh** | Destination Hashing | 根据目标地址哈希 | 需要会话保持 |
| 📊 **sh** | Source Hashing | 根据源地址哈希 | 缓存友好场景 |

**📈 IPVS vs iptables性能对比**
```
集群规模      iptables延迟    IPVS延迟     性能提升
1000 Services    100ms         10ms        10倍
5000 Services    500ms         15ms        33倍
10000 Services   1000ms        20ms        50倍

结论：Service数量越多，IPVS优势越明显
```

---

## 6. 🧪 集群内通信测试实战


### 6.1 创建测试环境


**🚀 部署测试应用**
```yaml
# 创建一个简单的Web应用进行测试
apiVersion: apps/v1
kind: Deployment
metadata:
  name: test-web
  labels:
    app: test-web
spec:
  replicas: 3                    # 3个Pod副本
  selector:
    matchLabels:
      app: test-web
  template:
    metadata:
      labels:
        app: test-web
    spec:
      containers:
      - name: web
        image: nginx:1.20        # 使用nginx镜像
        ports:
        - containerPort: 80
        env:
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        # 自定义index页面显示Pod名称
        command: ["/bin/sh"]
        args:
        - -c
        - |
          echo "Hello from Pod: $POD_NAME" > /usr/share/nginx/html/index.html
          nginx -g 'daemon off;'

---
# 创建ClusterIP Service
apiVersion: v1
kind: Service
metadata:
  name: test-web-service
spec:
  selector:
    app: test-web
  ports:
  - name: http
    port: 80                     # Service端口
    targetPort: 80              # Pod端口
  type: ClusterIP              # 明确指定类型
```

### 6.2 基础通信测试


**🔍 测试Service可达性**
```bash
# 部署测试应用
kubectl apply -f test-web.yaml

# 查看Service信息
kubectl get service test-web-service
kubectl get pods -l app=test-web -o wide

# 获取Service的ClusterIP
SERVICE_IP=$(kubectl get service test-web-service -o jsonpath='{.spec.clusterIP}')
echo "Service IP: $SERVICE_IP"
```

**🧪 集群内访问测试**
```bash
# 方法1：使用临时Pod进行测试
kubectl run curl-test --image=curlimages/curl -i --tty --rm -- sh
# 在Pod内执行：
curl http://test-web-service        # 使用Service名称访问
curl http://10.96.156.25           # 使用ClusterIP访问

# 方法2：在已有Pod中测试
kubectl exec -it <任意Pod名称> -- sh
# 执行相同的curl命令

# 方法3：创建测试Job
kubectl create job curl-test --image=curlimages/curl -- \
  curl -s http://test-web-service
kubectl logs job/curl-test
```

### 6.3 负载均衡验证


**⚖️ 负载分布测试**
```bash
# 创建测试脚本验证负载均衡
kubectl run load-test --image=curlimages/curl --rm -i --tty -- sh

# 在Pod内执行多次请求
for i in {1..10}; do
  echo "Request $i:"
  curl -s http://test-web-service | grep "Hello from Pod"
  echo "---"
done
```

**📊 预期输出分析**
```bash
Request 1:
Hello from Pod: test-web-7c6d4f8b9c-abc12
---
Request 2:  
Hello from Pod: test-web-7c6d4f8b9c-def34
---
Request 3:
Hello from Pod: test-web-7c6d4f8b9c-ghi56
---

观察结果：
✅ 请求分发到不同的Pod (负载均衡工作正常)
✅ 每个Pod返回自己的名称 (请求路由正确)
✅ 所有请求都能成功 (Service转发正常)
```

### 6.4 DNS解析测试


**🌐 Service DNS验证**
```bash
# 测试Service的DNS解析
kubectl run dns-test --image=busybox --rm -i --tty -- sh

# 在Pod内测试DNS解析
nslookup test-web-service                    # 短名称解析
nslookup test-web-service.default.svc.cluster.local  # 完整DNS名称

# 测试跨命名空间访问
kubectl create namespace test-ns
# 从default命名空间访问test-ns中的服务
nslookup test-web-service.test-ns.svc.cluster.local
```

**📋 DNS命名规则**
```
Kubernetes Service DNS格式：
<service-name>.<namespace>.svc.<cluster-domain>

示例解析：
test-web-service                              # 同命名空间短名称
test-web-service.default                      # 指定命名空间
test-web-service.default.svc                  # 包含服务类型
test-web-service.default.svc.cluster.local    # 完整FQDN
```

### 6.5 故障排查方法


**🔧 常见问题诊断**
```bash
# 检查Service配置
kubectl describe service test-web-service

# 检查Endpoint是否正确
kubectl get endpoints test-web-service

# 检查Pod标签匹配
kubectl get pods --show-labels
kubectl get service test-web-service -o yaml | grep selector

# 检查kube-proxy状态
kubectl get pods -n kube-system -l k8s-app=kube-proxy

# 查看Service相关事件
kubectl get events --field-selector involvedObject.name=test-web-service
```

**🚨 问题排查清单**

| **问题现象** | **可能原因** | **排查命令** |
|-------------|------------|-------------|
| 🚫 Service无法访问 | 标签选择器不匹配 | `kubectl get pods --show-labels` |
| ❌ DNS解析失败 | CoreDNS问题 | `kubectl get pods -n kube-system` |
| ⚖️ 负载不均衡 | Pod数量或健康状态 | `kubectl get endpoints` |
| 🐛 间歇性失败 | Pod健康检查失败 | `kubectl describe pods` |

---

## 7. 📋 核心要点总结


### 7.1 必须掌握的核心概念


```
🔸 ClusterIP本质：为Pod提供稳定的虚拟IP访问入口
🔸 虚拟IP特性：系统分配，集群内有效，Service生命周期内固定
🔸 端口映射：port(Service) → targetPort(Pod) 的转换机制
🔸 会话保持：基于ClientIP的粘性会话，适用于有状态应用
🔸 代理模式：iptables适合小集群，IPVS适合大规模生产环境
🔸 通信测试：使用curl工具验证Service可达性和负载均衡效果
```

### 7.2 关键理解要点


**🔹 ClusterIP解决的核心问题**
```
Pod IP变化问题：
- Pod重启 → IP地址改变 → 其他服务无法访问
- Service提供 → 固定虚拟IP → 稳定的访问入口

负载均衡问题：  
- 多个Pod副本 → 如何分发请求 → kube-proxy自动负载均衡
- 动态扩缩容 → 自动发现新Pod → Endpoint自动更新
```

**🔹 端口映射的实际意义**
```
灵活性体现：
- 应用监听8080端口 → Service可暴露80端口
- 标准化访问 → 统一的服务接口
- 多端口支持 → 一个Service暴露多个端口

实际应用：
Web应用：port:80 → targetPort:8080 (标准HTTP端口)
数据库：port:3306 → targetPort:3306 (保持数据库端口)
API服务：port:8080 → targetPort:3000 (API标准端口)
```

**🔹 会话保持的使用场景**
```
需要会话保持：
✅ 登录状态保持 → 避免重复登录
✅ 购物车数据 → 保持用户购物体验  
✅ 文件上传 → 分片上传到同一Pod

不需要会话保持：
✅ 无状态API → 完全随机分发性能最优
✅ 静态资源 → 任意Pod都能处理
✅ 微服务架构 → 服务间调用通常无状态
```

### 7.3 生产环境最佳实践


**🚀 性能优化建议**
```
代理模式选择：
小集群(<1000 Service) → iptables模式，配置简单
大集群(>1000 Service) → IPVS模式，性能更好

会话保持配置：
Web应用 → sessionAffinityConfig.timeoutSeconds: 3600 (1小时)
API网关 → sessionAffinity: None (完全随机，性能最优)
数据库连接池 → sessionAffinity: ClientIP (保持连接稳定)
```

**🔧 运维监控要点**
```
关键监控指标：
- Service可用性：定期健康检查
- Endpoint数量：监控Pod扩缩容情况  
- DNS解析延迟：监控CoreDNS性能
- 负载分布：检查Pod负载均衡效果

故障排查流程：
1. 检查Service配置 → kubectl describe service
2. 验证Endpoint状态 → kubectl get endpoints  
3. 测试DNS解析 → nslookup命令
4. 检查网络连通性 → curl测试
5. 查看kube-proxy日志 → 网络规则问题
```

**💡 常见陷阱避免**
```
配置陷阱：
❌ 标签选择器拼写错误 → 导致无法选中Pod
❌ 端口配置错误 → targetPort与Pod端口不匹配  
❌ 过度使用会话保持 → 影响负载均衡效果

网络陷阱：
❌ 防火墙规则冲突 → 阻断Service访问
❌ DNS缓存问题 → Service更新后访问异常
❌ Service端口冲突 → 同一端口被多个Service使用
```

### 7.4 与其他Service类型的关系


**🔄 Service类型演进**
```
集群内访问：ClusterIP (本篇重点)
    ↓ 需要外部访问
节点端口访问：NodePort  
    ↓ 需要负载均衡器
云负载均衡器：LoadBalancer
    ↓ 需要域名访问
外部服务映射：ExternalName

理解：ClusterIP是其他Service类型的基础
NodePort = ClusterIP + 节点端口映射
LoadBalancer = NodePort + 云厂商负载均衡器
```

**核心记忆口诀**：
- ClusterIP虚拟地址稳，Pod变化访问不断
- 端口映射很灵活，内外端口可不同  
- 会话保持看需求，有状态应用才使用
- 代理模式选择好，小用iptables大用IPVS