---
title: 13ã€ç›‘æ§é…ç½®æ–‡ä»¶
---
## ğŸ“š ç›®å½•

1. [ç›‘æ§ç³»ç»Ÿæ¦‚è¿°](#1-ç›‘æ§ç³»ç»Ÿæ¦‚è¿°)
2. [æ ¸å¿ƒç›‘æ§ç»„ä»¶é…ç½®](#2-æ ¸å¿ƒç›‘æ§ç»„ä»¶é…ç½®)
3. [æ—¥å¿—æ”¶é›†ä¸åˆ†æé…ç½®](#3-æ—¥å¿—æ”¶é›†ä¸åˆ†æé…ç½®)
4. [é“¾è·¯è¿½è¸ªé…ç½®](#4-é“¾è·¯è¿½è¸ªé…ç½®)
5. [ç›‘æ§é…ç½®æœ€ä½³å®è·µ](#5-ç›‘æ§é…ç½®æœ€ä½³å®è·µ)
6. [æ ¸å¿ƒè¦ç‚¹æ€»ç»“](#6-æ ¸å¿ƒè¦ç‚¹æ€»ç»“)

---

## 1. ğŸ” ç›‘æ§ç³»ç»Ÿæ¦‚è¿°


### 1.1 ä¸ºä»€ä¹ˆéœ€è¦ç›‘æ§ç³»ç»Ÿ


**ç›‘æ§ç³»ç»Ÿå°±åƒåŒ»é™¢çš„ä½“æ£€è®¾å¤‡**ï¼Œå¸®æˆ‘ä»¬éšæ—¶äº†è§£Kubernetesé›†ç¾¤çš„"å¥åº·çŠ¶å†µ"ï¼š

```
æ²¡æœ‰ç›‘æ§çš„é›†ç¾¤ = ç›²äººæ‘¸è±¡
- ä¸çŸ¥é“Podæ˜¯å¦æ­£å¸¸è¿è¡Œ
- å‘ç°é—®é¢˜æ—¶å·²ç»å¤ªæ™š
- æ— æ³•åˆ†ææ€§èƒ½ç“¶é¢ˆ
- æ•…éšœæ’æŸ¥å›°éš¾

æœ‰ç›‘æ§çš„é›†ç¾¤ = é€æ˜å¯è§†åŒ–
- å®æ—¶æŒæ¡é›†ç¾¤çŠ¶æ€
- æå‰é¢„è­¦æ½œåœ¨é—®é¢˜
- å¿«é€Ÿå®šä½æ•…éšœæ ¹å› 
- ä¼˜åŒ–èµ„æºä½¿ç”¨æ•ˆç‡
```

### 1.2 Kubernetesç›‘æ§ä¸‰å¤§æ”¯æŸ±


**ç›‘æ§ä½“ç³»æ¶æ„å›¾ï¼š**
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    ç›‘æ§æ•°æ®æ”¶é›†å±‚                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  æŒ‡æ ‡ç›‘æ§        â”‚   æ—¥å¿—ç›‘æ§        â”‚   é“¾è·¯è¿½è¸ªç›‘æ§          â”‚
â”‚  (Metrics)      â”‚   (Logs)         â”‚   (Tracing)           â”‚
â”‚                â”‚                  â”‚                       â”‚
â”‚  CPU/å†…å­˜ä½¿ç”¨ç‡   â”‚  åº”ç”¨æ—¥å¿—         â”‚  è¯·æ±‚è°ƒç”¨é“¾            â”‚
â”‚  ç½‘ç»œæµé‡        â”‚  ç³»ç»Ÿæ—¥å¿—         â”‚  æœåŠ¡ä¾èµ–å…³ç³»          â”‚
â”‚  å­˜å‚¨ä½¿ç”¨        â”‚  å®¡è®¡æ—¥å¿—         â”‚  æ€§èƒ½ç“¶é¢ˆåˆ†æ          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    æ•°æ®å­˜å‚¨ä¸å¤„ç†å±‚                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Prometheus     â”‚   Elasticsearch   â”‚   Jaeger/Zipkin       â”‚
â”‚  (æ—¶åºæ•°æ®åº“)     â”‚   (æ—¥å¿—å­˜å‚¨)       â”‚   (è¿½è¸ªå­˜å‚¨)           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                    å¯è§†åŒ–ä¸å‘Šè­¦å±‚                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Grafana        â”‚   Kibana          â”‚   Jaeger UI           â”‚
â”‚  (æŒ‡æ ‡å¯è§†åŒ–)     â”‚   (æ—¥å¿—å¯è§†åŒ–)     â”‚   (è¿½è¸ªå¯è§†åŒ–)         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3 ç›‘æ§ç»„ä»¶å…³ç³»å›¾


**æ ¸å¿ƒç»„ä»¶åä½œå…³ç³»ï¼š**
```
åº”ç”¨Pod
    â”‚
    â”œâ”€â”€â”€ æŒ‡æ ‡æ•°æ® â”€â”€â”€â”€â†’ metrics-server â”€â”€â”€â”€â†’ Prometheus
    â”‚                                          â”‚
    â”œâ”€â”€â”€ æ—¥å¿—æ•°æ® â”€â”€â”€â”€â†’ Fluentd/Promtail â”€â”€â”€â”€â†’ Elasticsearch/Loki
    â”‚                                          â”‚
    â””â”€â”€â”€ è¿½è¸ªæ•°æ® â”€â”€â”€â”€â†’ Jaeger Agent â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Jaeger Collector
                                               â”‚
    Grafana â†â”€â”€ æ•°æ®æŸ¥è¯¢ â”€â”€â”¬â”€â”€ Prometheus       â”‚
       â”‚                  â”œâ”€â”€ Loki            â”‚
       â””â”€â”€ å‘Šè­¦é€šçŸ¥ â”€â”€â”€â”€â†’ Alertmanager         â”‚
                                               â”‚
    Kibana â†â”€â”€â”€â”€ æ—¥å¿—æŸ¥è¯¢ â”€â”€â”€â”€â”€â”€ Elasticsearch  â”‚
                                               â”‚
    Jaeger UI â†â”€â”€ è¿½è¸ªæŸ¥è¯¢ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. âš¡ æ ¸å¿ƒç›‘æ§ç»„ä»¶é…ç½®


### 2.1 Metrics Server - èµ„æºä½¿ç”¨æƒ…å†µç›‘æ§


**Metrics Serveræ˜¯ä»€ä¹ˆï¼Ÿ**
> ğŸ’¡ **ç®€å•ç†è§£**ï¼šMetrics Serverå°±åƒä¸€ä¸ª"èµ„æºä½¿ç”¨æƒ…å†µç»Ÿè®¡å‘˜"ï¼Œä¸“é—¨æ”¶é›†æ¯ä¸ªPodå’ŒNodeçš„CPUã€å†…å­˜ä½¿ç”¨æ•°æ®ï¼Œè®©`kubectl top`å‘½ä»¤èƒ½å¤Ÿæ˜¾ç¤ºèµ„æºä½¿ç”¨æƒ…å†µã€‚

**åŸºç¡€é…ç½®ç¤ºä¾‹ï¼š**
```yaml
# metrics-server.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: metrics-server
  namespace: kube-system
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: metrics-server
  template:
    metadata:
      labels:
        k8s-app: metrics-server
    spec:
      containers:
      - name: metrics-server
        image: k8s.gcr.io/metrics-server/metrics-server:v0.6.4
        args:
        - --cert-dir=/tmp
        - --secure-port=4443
        - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname
        - --kubelet-use-node-status-port
        - --metric-resolution=15s
        # å¼€å‘ç¯å¢ƒå¯æ·»åŠ ä¸å®‰å…¨é€‰é¡¹
        - --kubelet-insecure-tls
        resources:
          requests:
            cpu: 100m
            memory: 200Mi
          limits:
            cpu: 200m
            memory: 400Mi
```

**é…ç½®è¦ç‚¹è¯´æ˜ï¼š**
- `--metric-resolution=15s`ï¼šæ¯15ç§’æ”¶é›†ä¸€æ¬¡æŒ‡æ ‡æ•°æ®
- `--kubelet-insecure-tls`ï¼šè·³è¿‡TLSéªŒè¯ï¼ˆä»…å¼€å‘ç¯å¢ƒä½¿ç”¨ï¼‰
- èµ„æºé™åˆ¶ï¼šç¡®ä¿metrics-serveræœ¬èº«ä¸å ç”¨è¿‡å¤šèµ„æº

### 2.2 Prometheus - æ—¶åºæ•°æ®åº“æ ¸å¿ƒ


**Prometheusæ˜¯ä»€ä¹ˆï¼Ÿ**
> ğŸ’¡ **é€šä¿—è§£é‡Š**ï¼šPrometheuså°±åƒä¸€ä¸ª"æ•°æ®æ”¶é›†ç‹‚é­”"ï¼Œä¸åœåœ°ä»å„ç§åœ°æ–¹æŠ“å–æŒ‡æ ‡æ•°æ®ï¼Œç„¶åå­˜å‚¨èµ·æ¥ä¾›åˆ†æä½¿ç”¨ã€‚å®ƒæ”¯æŒå¼ºå¤§çš„æŸ¥è¯¢è¯­è¨€PromQLã€‚

#### 2.2.1 Prometheusä¸»é…ç½®


```yaml
# prometheus-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: monitoring
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s          # å…¨å±€æŠ“å–é—´éš”
      evaluation_interval: 15s     # è§„åˆ™è¯„ä¼°é—´éš”
    
    # å‘Šè­¦è§„åˆ™æ–‡ä»¶
    rule_files:
      - "alert_rules.yml"
    
    # å‘Šè­¦ç®¡ç†å™¨é…ç½®
    alerting:
      alertmanagers:
      - static_configs:
        - targets:
          - alertmanager:9093
    
    # æŠ“å–é…ç½®
    scrape_configs:
    # æŠ“å–Prometheusè‡ªèº«æŒ‡æ ‡
    - job_name: 'prometheus'
      static_configs:
      - targets: ['localhost:9090']
    
    # æŠ“å–Kubernetes API ServeræŒ‡æ ‡
    - job_name: 'kubernetes-apiservers'
      kubernetes_sd_configs:
      - role: endpoints
      relabel_configs:
      - source_labels: [__meta_kubernetes_namespace, __meta_kubernetes_service_name, __meta_kubernetes_endpoint_port_name]
        action: keep
        regex: default;kubernetes;https
    
    # æŠ“å–NodeæŒ‡æ ‡
    - job_name: 'kubernetes-nodes'
      kubernetes_sd_configs:
      - role: node
      relabel_configs:
      - action: labelmap
        regex: __meta_kubernetes_node_label_(.+)
```

#### 2.2.2 Prometheuséƒ¨ç½²é…ç½®


```yaml
# prometheus-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: prometheus
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: prometheus
  template:
    metadata:
      labels:
        app: prometheus
    spec:
      containers:
      - name: prometheus
        image: prom/prometheus:v2.40.0
        ports:
        - containerPort: 9090
        args:
        - '--config.file=/etc/prometheus/prometheus.yml'
        - '--storage.tsdb.path=/prometheus/'
        - '--web.console.libraries=/etc/prometheus/console_libraries'
        - '--web.console.templates=/etc/prometheus/consoles'
        - '--storage.tsdb.retention.time=15d'  # æ•°æ®ä¿ç•™15å¤©
        - '--web.enable-lifecycle'
        volumeMounts:
        - name: prometheus-config
          mountPath: /etc/prometheus/
        - name: prometheus-storage
          mountPath: /prometheus/
        resources:
          requests:
            cpu: 200m
            memory: 512Mi
          limits:
            cpu: 500m
            memory: 1Gi
      volumes:
      - name: prometheus-config
        configMap:
          name: prometheus-config
      - name: prometheus-storage
        persistentVolumeClaim:
          claimName: prometheus-pvc
```

### 2.3 Node Exporter - èŠ‚ç‚¹ç›‘æ§


**Node Exporteræ˜¯ä»€ä¹ˆï¼Ÿ**
> ğŸ’¡ **ç®€å•ç†è§£**ï¼šNode Exporterå°±åƒæ¯ä¸ªæœåŠ¡å™¨ä¸Šçš„"å¥åº·æ£€æµ‹ä»ª"ï¼Œä¸“é—¨ç›‘æ§æœåŠ¡å™¨ç¡¬ä»¶æŒ‡æ ‡ï¼Œæ¯”å¦‚CPUä½¿ç”¨ç‡ã€å†…å­˜ä½¿ç”¨ã€ç£ç›˜ç©ºé—´ã€ç½‘ç»œæµé‡ç­‰ã€‚

```yaml
# node-exporter-daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: node-exporter
  namespace: monitoring
spec:
  selector:
    matchLabels:
      name: node-exporter
  template:
    metadata:
      labels:
        name: node-exporter
    spec:
      hostNetwork: true    # ä½¿ç”¨å®¿ä¸»æœºç½‘ç»œ
      hostPID: true       # ä½¿ç”¨å®¿ä¸»æœºPID
      containers:
      - name: node-exporter
        image: prom/node-exporter:v1.5.0
        ports:
        - containerPort: 9100
          hostPort: 9100
        args:
        - '--path.procfs=/host/proc'
        - '--path.sysfs=/host/sys'
        - '--collector.filesystem.mount-points-exclude=^/(sys|proc|dev|host|etc)($$|/)'
        volumeMounts:
        - name: proc
          mountPath: /host/proc
          readOnly: true
        - name: sys
          mountPath: /host/sys
          readOnly: true
        - name: root
          mountPath: /rootfs
          readOnly: true
        resources:
          limits:
            cpu: 200m
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 100Mi
      volumes:
      - name: proc
        hostPath:
          path: /proc
      - name: sys
        hostPath:
          path: /sys
      - name: root
        hostPath:
          path: /
```

**DaemonSetä½¿ç”¨è¯´æ˜ï¼š**
- **ä¸ºä»€ä¹ˆç”¨DaemonSetï¼Ÿ** ç¡®ä¿æ¯ä¸ªNodeä¸Šéƒ½è¿è¡Œä¸€ä¸ªNode Exporterå®ä¾‹
- **hostNetwork: trueï¼š** ä½¿ç”¨å®¿ä¸»æœºç½‘ç»œï¼Œç›´æ¥è®¿é—®å®¿ä¸»æœºçš„9100ç«¯å£
- **æŒ‚è½½å®¿ä¸»æœºç›®å½•ï¼š** è·å–çœŸå®çš„ç³»ç»ŸæŒ‡æ ‡æ•°æ®

### 2.4 kube-state-metrics - é›†ç¾¤çŠ¶æ€ç›‘æ§


**kube-state-metricsæ˜¯ä»€ä¹ˆï¼Ÿ**
> ğŸ’¡ **é€šä¿—è§£é‡Š**ï¼škube-state-metricså°±åƒä¸€ä¸ª"é›†ç¾¤çŠ¶æ€æ’­æŠ¥å‘˜"ï¼Œä¸“é—¨æ”¶é›†Kuberneteså¯¹è±¡çš„çŠ¶æ€ä¿¡æ¯ï¼Œæ¯”å¦‚Podæœ‰å¤šå°‘ä¸ªåœ¨è¿è¡Œã€å¤šå°‘ä¸ªå¤±è´¥äº†ã€Deploymentçš„å‰¯æœ¬æ•°é‡ç­‰ã€‚

```yaml
# kube-state-metrics.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kube-state-metrics
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kube-state-metrics
  template:
    metadata:
      labels:
        app: kube-state-metrics
    spec:
      serviceAccountName: kube-state-metrics
      containers:
      - name: kube-state-metrics
        image: k8s.gcr.io/kube-state-metrics/kube-state-metrics:v2.7.0
        ports:
        - name: http-metrics
          containerPort: 8080
        - name: telemetry
          containerPort: 8081
        args:
        - --port=8080
        - --telemetry-port=8081
        - --resources=pods,deployments,services,nodes,replicasets
        resources:
          limits:
            cpu: 200m
            memory: 150Mi
          requests:
            cpu: 100m
            memory: 100Mi

---
# åˆ›å»ºServiceAccountå’Œæƒé™
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kube-state-metrics
  namespace: monitoring

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: kube-state-metrics
rules:
- apiGroups: [""]
  resources: ["nodes", "pods", "services", "endpoints", "secrets", "configmaps"]
  verbs: ["list", "watch"]
- apiGroups: ["apps"]
  resources: ["deployments", "daemonsets", "replicasets", "statefulsets"]
  verbs: ["list", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: kube-state-metrics
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: kube-state-metrics
subjects:
- kind: ServiceAccount
  name: kube-state-metrics
  namespace: monitoring
```

### 2.5 Grafana - æ•°æ®å¯è§†åŒ–


**Grafanaæ˜¯ä»€ä¹ˆï¼Ÿ**
> ğŸ’¡ **å½¢è±¡æ¯”å–»**ï¼šGrafanaå°±åƒä¸€ä¸ª"æ•°æ®ç”»å®¶"ï¼ŒæŠŠPrometheusæ”¶é›†çš„æ¯ç‡¥æ•°å­—è½¬æ¢æˆæ¼‚äº®çš„å›¾è¡¨å’Œä»ªè¡¨ç›˜ï¼Œè®©ç›‘æ§æ•°æ®ä¸€ç›®äº†ç„¶ã€‚

```yaml
# grafana-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: grafana
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: grafana
  template:
    metadata:
      labels:
        app: grafana
    spec:
      containers:
      - name: grafana
        image: grafana/grafana:9.3.0
        ports:
        - containerPort: 3000
        env:
        - name: GF_SECURITY_ADMIN_PASSWORD
          value: "admin123"  # ç”Ÿäº§ç¯å¢ƒåº”ä½¿ç”¨Secret
        - name: GF_INSTALL_PLUGINS
          value: "grafana-kubernetes-app"
        volumeMounts:
        - name: grafana-storage
          mountPath: /var/lib/grafana
        - name: grafana-config
          mountPath: /etc/grafana/grafana.ini
          subPath: grafana.ini
        resources:
          limits:
            cpu: 200m
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 100Mi
      volumes:
      - name: grafana-storage
        persistentVolumeClaim:
          claimName: grafana-pvc
      - name: grafana-config
        configMap:
          name: grafana-config

---
# Grafanaé…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-config
  namespace: monitoring
data:
  grafana.ini: |
    [analytics]
    check_for_updates = true
    
    [grafana_net]
    url = https://grafana.net
    
    [log]
    mode = console
    
    [paths]
    data = /var/lib/grafana/data
    logs = /var/lib/grafana/log
    plugins = /var/lib/grafana/plugins
```

### 2.6 Alertmanager - å‘Šè­¦ç®¡ç†


**Alertmanageræ˜¯ä»€ä¹ˆï¼Ÿ**
> ğŸ’¡ **ç”Ÿæ´»åŒ–ç†è§£**ï¼šAlertmanagerå°±åƒä¸€ä¸ª"æ™ºèƒ½æŠ¥è­¦ç³»ç»Ÿ"ï¼Œå½“Prometheuså‘ç°é—®é¢˜æ—¶ï¼ŒAlertmanagerè´Ÿè´£å†³å®šè¯¥é€šçŸ¥è°ã€é€šè¿‡ä»€ä¹ˆæ–¹å¼é€šçŸ¥ã€é¿å…é‡å¤é€šçŸ¥ç­‰ã€‚

```yaml
# alertmanager-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: alertmanager-config
  namespace: monitoring
data:
  alertmanager.yml: |
    global:
      smtp_smarthost: 'smtp.qq.com:587'
      smtp_from: 'your-email@qq.com'
      smtp_auth_username: 'your-email@qq.com'
      smtp_auth_password: 'your-password'
    
    route:
      group_by: ['alertname']           # æŒ‰å‘Šè­¦åç§°åˆ†ç»„
      group_wait: 10s                   # ç­‰å¾…10ç§’æ”¶é›†åŒç»„å‘Šè­¦
      group_interval: 10s               # åŒç»„å‘Šè­¦å‘é€é—´éš”
      repeat_interval: 1h               # é‡å¤å‘Šè­¦é—´éš”1å°æ—¶
      receiver: 'default-receiver'      # é»˜è®¤æ¥æ”¶è€…
    
    receivers:
    - name: 'default-receiver'
      email_configs:
      - to: 'admin@company.com'
        subject: 'Kubernetes é›†ç¾¤å‘Šè­¦'
        body: |
          å‘Šè­¦è¯¦æƒ…:
          {{ range .Alerts }}
          å‘Šè­¦åç§°: {{ .Annotations.summary }}
          å‘Šè­¦æè¿°: {{ .Annotations.description }}
          é›†ç¾¤: {{ .Labels.cluster }}
          èŠ‚ç‚¹: {{ .Labels.node }}
          æ—¶é—´: {{ .StartsAt }}
          {{ end }}

---
# Alertmanageréƒ¨ç½²
apiVersion: apps/v1
kind: Deployment
metadata:
  name: alertmanager
  namespace: monitoring
spec:
  replicas: 1
  selector:
    matchLabels:
      app: alertmanager
  template:
    metadata:
      labels:
        app: alertmanager
    spec:
      containers:
      - name: alertmanager
        image: prom/alertmanager:v0.25.0
        ports:
        - containerPort: 9093
        args:
        - '--config.file=/etc/alertmanager/alertmanager.yml'
        - '--storage.path=/alertmanager'
        volumeMounts:
        - name: alertmanager-config
          mountPath: /etc/alertmanager
        - name: alertmanager-storage
          mountPath: /alertmanager
        resources:
          limits:
            cpu: 100m
            memory: 128Mi
          requests:
            cpu: 50m
            memory: 64Mi
      volumes:
      - name: alertmanager-config
        configMap:
          name: alertmanager-config
      - name: alertmanager-storage
        emptyDir: {}
```

---

## 3. ğŸ“ æ—¥å¿—æ”¶é›†ä¸åˆ†æé…ç½®


### 3.1 æ—¥å¿—æ”¶é›†æ–¹æ¡ˆå¯¹æ¯”


| æ–¹æ¡ˆç»„åˆ | **é€‚ç”¨åœºæ™¯** | **ä¼˜åŠ¿** | **åŠ£åŠ¿** |
|---------|------------|---------|---------|
| ğŸ”¸ **EFK Stack** | `ä¼ ç»Ÿä¼ä¸šç¯å¢ƒ` | `åŠŸèƒ½å®Œæ•´ï¼Œç”Ÿæ€æˆç†Ÿ` | `èµ„æºæ¶ˆè€—å¤§ï¼Œå¤æ‚åº¦é«˜` |
| ğŸ”¸ **Loki + Promtail** | `äº‘åŸç”Ÿç¯å¢ƒ` | `è½»é‡çº§ï¼Œä¸Prometheusé›†æˆå¥½` | `åŠŸèƒ½ç›¸å¯¹ç®€å•` |
| ğŸ”¸ **Fluentd + ES** | `å¤§è§„æ¨¡æ—¥å¿—å¤„ç†` | `é«˜æ€§èƒ½ï¼Œå¯æ‰©å±•æ€§å¼º` | `é…ç½®å¤æ‚ï¼Œå­¦ä¹ æˆæœ¬é«˜` |

### 3.2 Elasticsearch - æ—¥å¿—å­˜å‚¨


**Elasticsearchæ˜¯ä»€ä¹ˆï¼Ÿ**
> ğŸ’¡ **å½¢è±¡æ¯”å–»**ï¼šElasticsearchå°±åƒä¸€ä¸ª"è¶…çº§å›¾ä¹¦ç®¡ç†å‘˜"ï¼Œèƒ½å¤Ÿå¿«é€Ÿå­˜å‚¨ã€ç´¢å¼•å’Œæœç´¢æµ·é‡çš„æ—¥å¿—æ•°æ®ï¼Œè®©ä½ èƒ½åœ¨å‡ ç§’é’Ÿå†…ä»æ•°ç™¾ä¸‡æ¡æ—¥å¿—ä¸­æ‰¾åˆ°ä½ è¦çš„ä¿¡æ¯ã€‚

```yaml
# elasticsearch.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: elasticsearch
  namespace: logging
spec:
  serviceName: elasticsearch
  replicas: 1
  selector:
    matchLabels:
      app: elasticsearch
  template:
    metadata:
      labels:
        app: elasticsearch
    spec:
      containers:
      - name: elasticsearch
        image: docker.elastic.co/elasticsearch/elasticsearch:7.17.0
        ports:
        - containerPort: 9200
        - containerPort: 9300
        env:
        - name: discovery.type
          value: single-node                    # å•èŠ‚ç‚¹æ¨¡å¼
        - name: ES_JAVA_OPTS
          value: "-Xms512m -Xmx512m"           # JVMå†…å­˜è®¾ç½®
        - name: xpack.security.enabled
          value: "false"                       # ç¦ç”¨å®‰å…¨æ’ä»¶ç®€åŒ–é…ç½®
        volumeMounts:
        - name: elasticsearch-data
          mountPath: /usr/share/elasticsearch/data
        resources:
          limits:
            cpu: 1000m
            memory: 1Gi
          requests:
            cpu: 500m
            memory: 512Mi
  volumeClaimTemplates:
  - metadata:
      name: elasticsearch-data
    spec:
      accessModes: ["ReadWriteOnce"]
      resources:
        requests:
          storage: 10Gi
```

### 3.3 Fluentd - æ—¥å¿—æ”¶é›†


**Fluentdæ˜¯ä»€ä¹ˆï¼Ÿ**
> ğŸ’¡ **ç”Ÿæ´»åŒ–ç†è§£**ï¼šFluentdå°±åƒä¸€ä¸ª"å¿«é€’å‘˜"ï¼Œä¸“é—¨æ”¶é›†å„ä¸ªPodäº§ç”Ÿçš„æ—¥å¿—æ–‡ä»¶ï¼Œç„¶åç»Ÿä¸€é€åˆ°Elasticsearch"ä»“åº“"é‡Œå­˜å‚¨ã€‚

```yaml
# fluentd-daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: fluentd
  namespace: logging
spec:
  selector:
    matchLabels:
      name: fluentd
  template:
    metadata:
      labels:
        name: fluentd
    spec:
      serviceAccountName: fluentd
      containers:
      - name: fluentd
        image: fluent/fluentd-kubernetes-daemonset:v1.15-debian-elasticsearch7-1
        env:
        - name: FLUENT_ELASTICSEARCH_HOST
          value: "elasticsearch.logging.svc.cluster.local"
        - name: FLUENT_ELASTICSEARCH_PORT
          value: "9200"
        - name: FLUENT_ELASTICSEARCH_SCHEME
          value: "http"
        volumeMounts:
        - name: varlog
          mountPath: /var/log
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: fluentd-config
          mountPath: /fluentd/etc/conf.d
        resources:
          limits:
            memory: 200Mi
          requests:
            cpu: 100m
            memory: 200Mi
      volumes:
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: fluentd-config
        configMap:
          name: fluentd-config

---
# Fluentdé…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: fluentd-config
  namespace: logging
data:
  fluent.conf: |
    # æ”¶é›†Kuberneteså®¹å™¨æ—¥å¿—
    <source>
      @type tail
      path /var/log/containers/*.log
      pos_file /var/log/fluentd-containers.log.pos
      tag kubernetes.*
      format json
      read_from_head true
    </source>
    
    # è§£æKuberneteså…ƒæ•°æ®
    <filter kubernetes.**>
      @type kubernetes_metadata
    </filter>
    
    # è¾“å‡ºåˆ°Elasticsearch
    <match **>
      @type elasticsearch
      host "#{ENV['FLUENT_ELASTICSEARCH_HOST']}"
      port "#{ENV['FLUENT_ELASTICSEARCH_PORT']}"
      scheme "#{ENV['FLUENT_ELASTICSEARCH_SCHEME']}"
      index_name fluentd-k8s
      type_name _doc
    </match>
```

### 3.4 Kibana - æ—¥å¿—å¯è§†åŒ–


**Kibanaæ˜¯ä»€ä¹ˆï¼Ÿ**
> ğŸ’¡ **ç®€å•ç†è§£**ï¼šKibanaå°±åƒElasticsearchçš„"å¯è§†åŒ–ç•Œé¢"ï¼Œè®©ä½ é€šè¿‡ç½‘é¡µå°±èƒ½æœç´¢ã€æŸ¥çœ‹å’Œåˆ†ææ—¥å¿—ï¼Œä¸ç”¨æ•²å¤æ‚çš„å‘½ä»¤è¡Œã€‚

```yaml
# kibana.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kibana
  namespace: logging
spec:
  replicas: 1
  selector:
    matchLabels:
      app: kibana
  template:
    metadata:
      labels:
        app: kibana
    spec:
      containers:
      - name: kibana
        image: docker.elastic.co/kibana/kibana:7.17.0
        ports:
        - containerPort: 5601
        env:
        - name: ELASTICSEARCH_HOSTS
          value: "http://elasticsearch:9200"
        - name: SERVER_NAME
          value: "kibana"
        - name: SERVER_HOST
          value: "0.0.0.0"
        resources:
          limits:
            cpu: 500m
            memory: 512Mi
          requests:
            cpu: 200m
            memory: 256Mi

---
apiVersion: v1
kind: Service
metadata:
  name: kibana
  namespace: logging
spec:
  ports:
  - port: 5601
    targetPort: 5601
  selector:
    app: kibana
  type: LoadBalancer  # æˆ–ä½¿ç”¨NodePort/Ingress
```

### 3.5 Loki + Promtail - è½»é‡çº§æ—¥å¿—æ–¹æ¡ˆ


**Lokiæ˜¯ä»€ä¹ˆï¼Ÿ**
> ğŸ’¡ **å¯¹æ¯”ç†è§£**ï¼šå¦‚æœElasticsearchæ˜¯"é‡å‹å¡è½¦"ï¼Œé‚£ä¹ˆLokiå°±æ˜¯"å°æ±½è½¦"ã€‚Lokiä¸“é—¨ä¸ºKubernetesè®¾è®¡ï¼Œæ›´è½»é‡ï¼Œä¸Prometheusç”Ÿæ€é›†æˆæ›´å¥½ã€‚

#### 3.5.1 Lokié…ç½®


```yaml
# loki.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: loki
  namespace: logging
spec:
  replicas: 1
  selector:
    matchLabels:
      app: loki
  template:
    metadata:
      labels:
        app: loki
    spec:
      containers:
      - name: loki
        image: grafana/loki:2.7.0
        ports:
        - containerPort: 3100
        args:
        - -config.file=/etc/loki/local-config.yaml
        volumeMounts:
        - name: loki-config
          mountPath: /etc/loki
        - name: loki-storage
          mountPath: /tmp/loki
        resources:
          limits:
            cpu: 200m
            memory: 256Mi
          requests:
            cpu: 100m
            memory: 128Mi
      volumes:
      - name: loki-config
        configMap:
          name: loki-config
      - name: loki-storage
        emptyDir: {}

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: loki-config
  namespace: logging
data:
  local-config.yaml: |
    auth_enabled: false
    
    server:
      http_listen_port: 3100
    
    ingester:
      lifecycler:
        address: 127.0.0.1
        ring:
          kvstore:
            store: inmemory
          replication_factor: 1
        final_sleep: 0s
    
    schema_config:
      configs:
        - from: 2020-10-24
          store: boltdb-shipper
          object_store: filesystem
          schema: v11
          index:
            prefix: index_
            period: 24h
    
    storage_config:
      boltdb_shipper:
        active_index_directory: /tmp/loki/boltdb-shipper-active
        cache_location: /tmp/loki/boltdb-shipper-cache
      filesystem:
        directory: /tmp/loki/chunks
```

#### 3.5.2 Promtailé…ç½®


**Promtailæ˜¯ä»€ä¹ˆï¼Ÿ**
> ğŸ’¡ **ç®€å•ç†è§£**ï¼šPromtailå°±æ˜¯Lokiçš„"ä¸“ç”¨å¿«é€’å‘˜"ï¼Œä¸“é—¨æ”¶é›†æ—¥å¿—å¹¶å‘é€ç»™Lokiï¼Œæ¯”Fluentdæ›´è½»é‡çº§ã€‚

```yaml
# promtail-daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: promtail
  namespace: logging
spec:
  selector:
    matchLabels:
      name: promtail
  template:
    metadata:
      labels:
        name: promtail
    spec:
      containers:
      - name: promtail
        image: grafana/promtail:2.7.0
        args:
        - -config.file=/etc/promtail/config.yml
        volumeMounts:
        - name: config
          mountPath: /etc/promtail
        - name: varlog
          mountPath: /var/log
          readOnly: true
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        resources:
          limits:
            cpu: 200m
            memory: 128Mi
          requests:
            cpu: 100m
            memory: 64Mi
      volumes:
      - name: config
        configMap:
          name: promtail-config
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: promtail-config
  namespace: logging
data:
  config.yml: |
    server:
      http_listen_port: 9080
      grpc_listen_port: 0
    
    clients:
      - url: http://loki:3100/loki/api/v1/push
    
    scrape_configs:
    - job_name: kubernetes-pods
      kubernetes_sd_configs:
        - role: pod
      relabel_configs:
        - source_labels:
            - __meta_kubernetes_pod_controller_name
          regex: ([0-9a-z-.]+?)(-[0-9a-f]{8,10})?
          target_label: __tmp_controller_name
        - source_labels:
            - __meta_kubernetes_pod_label_app_kubernetes_io_name
            - __meta_kubernetes_pod_label_app
            - __tmp_controller_name
            - __meta_kubernetes_pod_name
          regex: ^;*([^;]+)(;.*)?$
          target_label: app
        - source_labels:
            - __meta_kubernetes_pod_label_app_kubernetes_io_component
            - __meta_kubernetes_pod_label_component
          regex: ^;*([^;]+)(;.*)?$
          target_label: component
```

---

## 4. ğŸ” é“¾è·¯è¿½è¸ªé…ç½®


### 4.1 ä»€ä¹ˆæ˜¯é“¾è·¯è¿½è¸ªï¼Ÿ


**é“¾è·¯è¿½è¸ªçš„ä½œç”¨ï¼š**
> ğŸ’¡ **ç”Ÿæ´»åŒ–æ¯”å–»**ï¼šé“¾è·¯è¿½è¸ªå°±åƒ"å¿«é€’è·Ÿè¸ªç³»ç»Ÿ"ï¼Œå½“ç”¨æˆ·å‘èµ·ä¸€ä¸ªè¯·æ±‚æ—¶ï¼Œè¿™ä¸ªè¯·æ±‚å¯èƒ½è¦ç»è¿‡å¤šä¸ªå¾®æœåŠ¡ï¼Œé“¾è·¯è¿½è¸ªèƒ½å‘Šè¯‰ä½ è¯·æ±‚èµ°è¿‡äº†å“ªäº›æœåŠ¡ã€æ¯ä¸ªç¯èŠ‚èŠ±äº†å¤šé•¿æ—¶é—´ã€åœ¨å“ªé‡Œå‡ºäº†é—®é¢˜ã€‚

**è¯·æ±‚é“¾è·¯ç¤ºä¾‹ï¼š**
```
ç”¨æˆ·è¯·æ±‚æµç¨‹ï¼š
ç”¨æˆ· â†’ ç½‘å…³ â†’ ç”¨æˆ·æœåŠ¡ â†’ æ•°æ®åº“
                â†“
              è®¢å•æœåŠ¡ â†’ æ¶ˆæ¯é˜Ÿåˆ—
                â†“
              åº“å­˜æœåŠ¡ â†’ Redisç¼“å­˜

æ²¡æœ‰é“¾è·¯è¿½è¸ªï¼š
- è¯·æ±‚æ…¢äº†ä¸çŸ¥é“æ˜¯å“ªä¸ªæœåŠ¡çš„é—®é¢˜
- æœåŠ¡ä¹‹é—´è°ƒç”¨å…³ç³»å¤æ‚ï¼Œéš¾ä»¥æ’æŸ¥
- æ€§èƒ½ç“¶é¢ˆæ— æ³•å®šä½

æœ‰äº†é“¾è·¯è¿½è¸ªï¼š
- æ¸…æ¥šçœ‹åˆ°æ¯ä¸ªæœåŠ¡çš„å“åº”æ—¶é—´
- å¿«é€Ÿå®šä½æ€§èƒ½ç“¶é¢ˆ
- ç†è§£æœåŠ¡ä¾èµ–å…³ç³»
```

### 4.2 Jaeger - åˆ†å¸ƒå¼è¿½è¸ªç³»ç»Ÿ


**Jaegeræ˜¯ä»€ä¹ˆï¼Ÿ**
> ğŸ’¡ **ç®€å•ç†è§£**ï¼šJaegerå°±åƒä¸€ä¸ª"è¯·æ±‚ä¾¦æ¢"ï¼Œä¸“é—¨è·Ÿè¸ªæ¯ä¸ªè¯·æ±‚åœ¨å¾®æœåŠ¡ä¹‹é—´çš„"æ—…è¡Œè·¯çº¿"ï¼Œè®°å½•æ¯ä¸€æ­¥çš„è€—æ—¶å’ŒçŠ¶æ€ã€‚

#### 4.2.1 Jaegerä¸€ä½“åŒ–éƒ¨ç½²


```yaml
# jaeger-all-in-one.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jaeger
  namespace: tracing
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jaeger
  template:
    metadata:
      labels:
        app: jaeger
    spec:
      containers:
      - name: jaeger
        image: jaegertracing/all-in-one:1.41
        ports:
        - containerPort: 16686  # Jaeger UI
        - containerPort: 14268  # HTTPæ”¶é›†ç«¯å£
        - containerPort: 14250  # gRPCæ”¶é›†ç«¯å£
        - containerPort: 6831   # UDP Jaeger Agentç«¯å£
        env:
        - name: COLLECTOR_OTLP_ENABLED
          value: "true"
        - name: QUERY_BASE_PATH
          value: "/"
        resources:
          limits:
            cpu: 200m
            memory: 256Mi
          requests:
            cpu: 100m
            memory: 128Mi

---
apiVersion: v1
kind: Service
metadata:
  name: jaeger
  namespace: tracing
spec:
  ports:
  - name: ui
    port: 16686
    targetPort: 16686
  - name: collector-http
    port: 14268
    targetPort: 14268
  - name: collector-grpc
    port: 14250
    targetPort: 14250
  - name: agent
    port: 6831
    targetPort: 6831
    protocol: UDP
  selector:
    app: jaeger
  type: LoadBalancer
```

#### 4.2.2 ç”Ÿäº§çº§Jaegeréƒ¨ç½²


**ç”Ÿäº§ç¯å¢ƒæ¶æ„ï¼š**
```
åº”ç”¨Pod
    â”‚
    â”œâ”€â”€â”€ Jaeger Agent (Sidecar) â”€â”€â”€â”
    â”‚                              â”‚
    â””â”€â”€â”€ Jaeger Agent (DaemonSet) â”€â”¤
                                   â”‚
                                   â–¼
                           Jaeger Collector
                                   â”‚
                                   â–¼
                           Elasticsearch/Cassandra
                                   â”‚
                                   â–¼
                              Jaeger Query
                                   â”‚
                                   â–¼
                              Jaeger UI
```

```yaml
# jaeger-production.yaml
# Jaeger Collector
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jaeger-collector
  namespace: tracing
spec:
  replicas: 2
  selector:
    matchLabels:
      app: jaeger-collector
  template:
    metadata:
      labels:
        app: jaeger-collector
    spec:
      containers:
      - name: jaeger-collector
        image: jaegertracing/jaeger-collector:1.41
        ports:
        - containerPort: 14268
        - containerPort: 14250
        env:
        - name: SPAN_STORAGE_TYPE
          value: elasticsearch
        - name: ES_SERVER_URLS
          value: http://elasticsearch:9200
        - name: ES_NUM_SHARDS
          value: "1"
        - name: ES_NUM_REPLICAS
          value: "0"
        resources:
          limits:
            cpu: 200m
            memory: 256Mi
          requests:
            cpu: 100m
            memory: 128Mi

---
# Jaeger Agent DaemonSet
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: jaeger-agent
  namespace: tracing
spec:
  selector:
    matchLabels:
      app: jaeger-agent
  template:
    metadata:
      labels:
        app: jaeger-agent
    spec:
      containers:
      - name: jaeger-agent
        image: jaegertracing/jaeger-agent:1.41
        ports:
        - containerPort: 6831
          protocol: UDP
        - containerPort: 6832
          protocol: UDP
        - containerPort: 5778
        args:
        - --reporter.grpc.host-port=jaeger-collector:14250
        - --log-level=info
        resources:
          limits:
            cpu: 100m
            memory: 64Mi
          requests:
            cpu: 50m
            memory: 32Mi

---
# Jaeger Query & UI
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jaeger-query
  namespace: tracing
spec:
  replicas: 1
  selector:
    matchLabels:
      app: jaeger-query
  template:
    metadata:
      labels:
        app: jaeger-query
    spec:
      containers:
      - name: jaeger-query
        image: jaegertracing/jaeger-query:1.41
        ports:
        - containerPort: 16686
        env:
        - name: SPAN_STORAGE_TYPE
          value: elasticsearch
        - name: ES_SERVER_URLS
          value: http://elasticsearch:9200
        resources:
          limits:
            cpu: 200m
            memory: 256Mi
          requests:
            cpu: 100m
            memory: 128Mi
```

### 4.3 Zipkin - è½»é‡çº§é“¾è·¯è¿½è¸ª


**Zipkinæ˜¯ä»€ä¹ˆï¼Ÿ**
> ğŸ’¡ **å¯¹æ¯”ç†è§£**ï¼šå¦‚æœJaegeræ˜¯"ä¸“ä¸šæ‘„å½±å¸ˆ"ï¼Œé‚£ä¹ˆZipkinå°±æ˜¯"ç®€æ˜“ç›¸æœº"ï¼ŒåŒæ ·èƒ½æ‹ç…§ï¼ˆé“¾è·¯è¿½è¸ªï¼‰ï¼Œä½†åŠŸèƒ½ç›¸å¯¹ç®€å•ï¼Œé€‚åˆå°å‹é¡¹ç›®ã€‚

```yaml
# zipkin.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: zipkin
  namespace: tracing
spec:
  replicas: 1
  selector:
    matchLabels:
      app: zipkin
  template:
    metadata:
      labels:
        app: zipkin
    spec:
      containers:
      - name: zipkin
        image: openzipkin/zipkin:2.24
        ports:
        - containerPort: 9411
        env:
        - name: STORAGE_TYPE
          value: elasticsearch
        - name: ES_HOSTS
          value: http://elasticsearch:9200
        - name: ES_INDEX
          value: zipkin
        resources:
          limits:
            cpu: 300m
            memory: 512Mi
          requests:
            cpu: 100m
            memory: 256Mi

---
apiVersion: v1
kind: Service
metadata:
  name: zipkin
  namespace: tracing
spec:
  ports:
  - port: 9411
    targetPort: 9411
  selector:
    app: zipkin
  type: LoadBalancer
```

---

## 5. ğŸš€ ç›‘æ§é…ç½®æœ€ä½³å®è·µ


### 5.1 èµ„æºé…ç½®å»ºè®®


**ç›‘æ§ç»„ä»¶èµ„æºéœ€æ±‚è¡¨ï¼š**

| ç»„ä»¶ | **CPUè¯·æ±‚/é™åˆ¶** | **å†…å­˜è¯·æ±‚/é™åˆ¶** | **å­˜å‚¨éœ€æ±‚** | **è¯´æ˜** |
|------|----------------|-----------------|-------------|----------|
| ğŸ”¸ **Prometheus** | `200m/500m` | `512Mi/1Gi` | `10Gi+` | `æ•°æ®é‡å¤§ï¼Œéœ€è¦è¶³å¤Ÿå­˜å‚¨` |
| ğŸ”¸ **Grafana** | `100m/200m` | `100Mi/200Mi` | `1Gi` | `è½»é‡çº§ï¼Œä¸»è¦æ˜¯é…ç½®å­˜å‚¨` |
| ğŸ”¸ **Alertmanager** | `50m/100m` | `64Mi/128Mi` | `1Gi` | `å¾ˆè½»é‡ï¼Œä¸»è¦å¤„ç†å‘Šè­¦` |
| ğŸ”¸ **Node Exporter** | `100m/200m` | `100Mi/200Mi` | `æ— ` | `æ¯ä¸ªèŠ‚ç‚¹éƒ½éœ€è¦` |
| ğŸ”¸ **Elasticsearch** | `500m/1000m` | `512Mi/1Gi` | `10Gi+` | `æ—¥å¿—å­˜å‚¨ï¼Œèµ„æºæ¶ˆè€—å¤§` |
| ğŸ”¸ **Jaeger** | `100m/200m` | `128Mi/256Mi` | `5Gi+` | `è¿½è¸ªæ•°æ®å­˜å‚¨` |

### 5.2 ç½‘ç»œé…ç½®æœ€ä½³å®è·µ


**Serviceæš´éœ²ç­–ç•¥ï¼š**

```yaml
# ç›‘æ§æœåŠ¡æš´éœ²é…ç½®
---
# Grafana - æä¾›Web UIè®¿é—®
apiVersion: v1
kind: Service
metadata:
  name: grafana
  namespace: monitoring
spec:
  type: NodePort        # æˆ–ä½¿ç”¨LoadBalancer/Ingress
  ports:
  - port: 3000
    nodePort: 30300
  selector:
    app: grafana

---
# Prometheus - å†…éƒ¨è®¿é—®ä¸ºä¸»
apiVersion: v1
kind: Service
metadata:
  name: prometheus
  namespace: monitoring
spec:
  type: ClusterIP      # é›†ç¾¤å†…è®¿é—®
  ports:
  - port: 9090
  selector:
    app: prometheus

---
# ä½¿ç”¨Ingressç»Ÿä¸€å…¥å£
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: monitoring-ingress
  namespace: monitoring
spec:
  rules:
  - host: grafana.k8s.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: grafana
            port:
              number: 3000
  - host: prometheus.k8s.local
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: prometheus
            port:
              number: 9090
```

### 5.3 æ•°æ®æŒä¹…åŒ–é…ç½®


**å­˜å‚¨é…ç½®è¦ç‚¹ï¼š**

```yaml
# PVCé…ç½®ç¤ºä¾‹
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: prometheus-pvc
  namespace: monitoring
spec:
  accessModes:
    - ReadWriteOnce
  storageClassName: fast-ssd    # ä½¿ç”¨é«˜æ€§èƒ½å­˜å‚¨
  resources:
    requests:
      storage: 50Gi             # æ ¹æ®æ•°æ®ä¿ç•™æ—¶é—´è®¡ç®—

---
# å¤‡ä»½é…ç½®
apiVersion: batch/v1
kind: CronJob
metadata:
  name: prometheus-backup
  namespace: monitoring
spec:
  schedule: "0 2 * * *"         # æ¯å¤©å‡Œæ™¨2ç‚¹å¤‡ä»½
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: backup
            image: alpine:latest
            command:
            - /bin/sh
            - -c
            - |
              echo "Starting backup..."
              tar -czf /backup/prometheus-$(date +%Y%m%d).tar.gz /prometheus-data/
              # æ¸…ç†30å¤©å‰çš„å¤‡ä»½
              find /backup -name "prometheus-*.tar.gz" -mtime +30 -delete
            volumeMounts:
            - name: prometheus-data
              mountPath: /prometheus-data
              readOnly: true
            - name: backup-storage
              mountPath: /backup
          volumes:
          - name: prometheus-data
            persistentVolumeClaim:
              claimName: prometheus-pvc
          - name: backup-storage
            persistentVolumeClaim:
              claimName: backup-pvc
          restartPolicy: OnFailure
```

### 5.4 å®‰å…¨é…ç½®å»ºè®®


**ç›‘æ§ç³»ç»Ÿå®‰å…¨åŠ å›ºï¼š**

```yaml
# RBACæƒé™æ§åˆ¶
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: monitoring-reader
rules:
- apiGroups: [""]
  resources: ["nodes", "nodes/proxy", "services", "endpoints", "pods"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["extensions"]
  resources: ["ingresses"]
  verbs: ["get", "list", "watch"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]

---
# Grafanaå®‰å…¨é…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: grafana-config
  namespace: monitoring
data:
  grafana.ini: |
    [security]
    admin_user = admin
    admin_password = ${GF_SECURITY_ADMIN_PASSWORD}  # ä»Secretè·å–
    disable_gravatar = true
    cookie_secure = true
    
    [auth]
    disable_login_form = false
    disable_signout_menu = false
    
    [auth.anonymous]
    enabled = false                                 # ç¦ç”¨åŒ¿åè®¿é—®
    
    [users]
    allow_sign_up = false                          # ç¦æ­¢ç”¨æˆ·æ³¨å†Œ
    auto_assign_org = true
    auto_assign_org_role = Viewer                  # é»˜è®¤åªè¯»æƒé™

---
# ä½¿ç”¨Secretå­˜å‚¨æ•æ„Ÿä¿¡æ¯
apiVersion: v1
kind: Secret
metadata:
  name: monitoring-secrets
  namespace: monitoring
type: Opaque
data:
  grafana-admin-password: YWRtaW4xMjM=            # base64ç¼–ç 
  smtp-password: c210cF9wYXNzd29yZA==
  slack-webhook-url: aHR0cHM6Ly9ob29rcy5zbGFjay5jb20v
```

### 5.5 å‘Šè­¦è§„åˆ™é…ç½®


**å¸¸ç”¨å‘Šè­¦è§„åˆ™ç¤ºä¾‹ï¼š**

```yaml
# prometheus-alerts.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-alerts
  namespace: monitoring
data:
  alerts.yml: |
    groups:
    - name: kubernetes-system
      rules:
      # èŠ‚ç‚¹å®•æœºå‘Šè­¦
      - alert: NodeDown
        expr: up{job="kubernetes-nodes"} == 0
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "èŠ‚ç‚¹ {{ $labels.instance }} å·²å®•æœº"
          description: "èŠ‚ç‚¹ {{ $labels.instance }} å·²ç»å®•æœºè¶…è¿‡5åˆ†é’Ÿ"
      
      # CPUä½¿ç”¨ç‡è¿‡é«˜
      - alert: HighCPUUsage
        expr: 100 - (avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "èŠ‚ç‚¹CPUä½¿ç”¨ç‡è¿‡é«˜"
          description: "èŠ‚ç‚¹ {{ $labels.instance }} CPUä½¿ç”¨ç‡å·²è¶…è¿‡80%ï¼Œå½“å‰å€¼ï¼š{{ $value }}%"
      
      # å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜
      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 85
        for: 10m
        labels:
          severity: warning
        annotations:
          summary: "èŠ‚ç‚¹å†…å­˜ä½¿ç”¨ç‡è¿‡é«˜"
          description: "èŠ‚ç‚¹ {{ $labels.instance }} å†…å­˜ä½¿ç”¨ç‡å·²è¶…è¿‡85%ï¼Œå½“å‰å€¼ï¼š{{ $value }}%"
      
      # ç£ç›˜ç©ºé—´ä¸è¶³
      - alert: LowDiskSpace
        expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 90
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "ç£ç›˜ç©ºé—´ä¸è¶³"
          description: "èŠ‚ç‚¹ {{ $labels.instance }} ç£ç›˜ {{ $labels.mountpoint }} ä½¿ç”¨ç‡å·²è¶…è¿‡90%"
      
      # Podé‡å¯è¿‡äºé¢‘ç¹
      - alert: PodCrashLooping
        expr: rate(kube_pod_container_status_restarts_total[15m]) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Podé¢‘ç¹é‡å¯"
          description: "Pod {{ $labels.namespace }}/{{ $labels.pod }} åœ¨è¿‡å»15åˆ†é’Ÿå†…é‡å¯äº† {{ $value }} æ¬¡"
```

---

## 6. ğŸ“‹ æ ¸å¿ƒè¦ç‚¹æ€»ç»“


### 6.1 å¿…é¡»æŒæ¡çš„æ ¸å¿ƒæ¦‚å¿µ


```
ğŸ”¸ ç›‘æ§ä¸‰å¤§æ”¯æŸ±ï¼šæŒ‡æ ‡ç›‘æ§(Metrics) + æ—¥å¿—ç›‘æ§(Logs) + é“¾è·¯è¿½è¸ª(Tracing)
ğŸ”¸ æ ¸å¿ƒç»„ä»¶ä½œç”¨ï¼šPrometheusæ”¶é›†æŒ‡æ ‡ + Grafanaå¯è§†åŒ– + Alertmanagerå‘Šè­¦
ğŸ”¸ æ•°æ®æµå‘ç†è§£ï¼šæ•°æ®æ”¶é›† â†’ å­˜å‚¨å¤„ç† â†’ å¯è§†åŒ–å±•ç¤º â†’ å‘Šè­¦é€šçŸ¥
ğŸ”¸ èµ„æºé…ç½®é‡è¦æ€§ï¼šç›‘æ§ç³»ç»Ÿæœ¬èº«ä¹Ÿéœ€è¦åˆç†çš„èµ„æºåˆ†é…
ğŸ”¸ å®‰å…¨é…ç½®å¿…è¦æ€§ï¼šç›‘æ§æ•°æ®åŒ…å«æ•æ„Ÿä¿¡æ¯ï¼Œéœ€è¦æƒé™æ§åˆ¶
```

### 6.2 å…³é”®ç†è§£è¦ç‚¹


**ğŸ”¹ ä¸ºä»€ä¹ˆéœ€è¦å¤šç§ç›‘æ§å·¥å…·ï¼Ÿ**
```
ä¸åŒå·¥å…·è§£å†³ä¸åŒé—®é¢˜ï¼š
- metrics-serverï¼šåŸºç¡€èµ„æºä½¿ç”¨æƒ…å†µï¼Œæ”¯æŒkubectl top
- Prometheusï¼šè¯¦ç»†çš„æ—¶åºæŒ‡æ ‡æ•°æ®ï¼Œæ”¯æŒå¤æ‚æŸ¥è¯¢
- Node Exporterï¼šç¡¬ä»¶å’Œç³»ç»Ÿçº§æŒ‡æ ‡
- kube-state-metricsï¼šKuberneteså¯¹è±¡çŠ¶æ€æŒ‡æ ‡
- å„å¸å…¶èŒï¼Œäº’ç›¸è¡¥å……
```

**ğŸ”¹ æ—¥å¿—æ–¹æ¡ˆå¦‚ä½•é€‰æ‹©ï¼Ÿ**
```
EFK Stack (Elasticsearch + Fluentd + Kibana)ï¼š
âœ… åŠŸèƒ½å¼ºå¤§ï¼Œæœç´¢èƒ½åŠ›å¼º
âŒ èµ„æºæ¶ˆè€—å¤§ï¼Œé…ç½®å¤æ‚

PLG Stack (Prometheus + Loki + Grafana)ï¼š
âœ… è½»é‡çº§ï¼Œä¸Prometheusé›†æˆå¥½
âŒ æœç´¢åŠŸèƒ½ç›¸å¯¹ç®€å•

é€‰æ‹©åŸåˆ™ï¼šå°é›†ç¾¤ç”¨PLGï¼Œå¤§é›†ç¾¤ç”¨EFK
```

**ğŸ”¹ é“¾è·¯è¿½è¸ªçš„ä»·å€¼ï¼Ÿ**
```
å¾®æœåŠ¡åœºæ™¯ä¸‹çš„å¿…å¤‡å·¥å…·ï¼š
- å¿«é€Ÿå®šä½æ€§èƒ½ç“¶é¢ˆ
- ç†è§£æœåŠ¡ä¾èµ–å…³ç³»
- åˆ†æè¯·æ±‚å¤„ç†æµç¨‹
- æä¾›æ•…éšœæ’æŸ¥çº¿ç´¢
```

### 6.3 é…ç½®å®è·µè¦ç‚¹


**ğŸ”¹ èµ„æºé…ç½®ç­–ç•¥**
```
ç›‘æ§ç³»ç»Ÿèµ„æºè§„åˆ’ï¼š
- é¢„ç•™è¶³å¤Ÿèµ„æºç»™ç›‘æ§ç»„ä»¶
- ä½¿ç”¨é«˜æ€§èƒ½å­˜å‚¨å­˜å‚¨æ—¶åºæ•°æ®
- åˆç†é…ç½®æ•°æ®ä¿ç•™æ—¶é—´
- å®šæœŸå¤‡ä»½é‡è¦ç›‘æ§æ•°æ®
```

**ğŸ”¹ å‘Šè­¦é…ç½®åŸåˆ™**
```
é¿å…å‘Šè­¦ç–²åŠ³ï¼š
- è®¾ç½®åˆç†çš„é˜ˆå€¼
- é…ç½®å‘Šè­¦åˆ†çº§
- é¿å…é‡å¤å‘Šè­¦
- åŠæ—¶å¤„ç†å’Œç¡®è®¤å‘Šè­¦
```

**ğŸ”¹ ç½‘ç»œé…ç½®è€ƒè™‘**
```
æœåŠ¡æš´éœ²ç­–ç•¥ï¼š
- å†…éƒ¨ç»„ä»¶ä½¿ç”¨ClusterIP
- ç”¨æˆ·ç•Œé¢ä½¿ç”¨NodePort/LoadBalancer/Ingress
- é…ç½®ç½‘ç»œç­–ç•¥é™åˆ¶è®¿é—®
- ä½¿ç”¨TLSåŠ å¯†æ•æ„Ÿé€šä¿¡
```

### 6.4 å®é™…åº”ç”¨ä»·å€¼


- **è¿ç»´å¯è§†åŒ–**ï¼šé€šè¿‡Grafanaä»ªè¡¨ç›˜ç›´è§‚äº†è§£é›†ç¾¤çŠ¶æ€
- **é—®é¢˜é¢„è­¦**ï¼šAlertmanagerä¸»åŠ¨å‘ç°å’Œé€šçŸ¥æ½œåœ¨é—®é¢˜
- **æ•…éšœæ’æŸ¥**ï¼šé€šè¿‡æ—¥å¿—å’Œé“¾è·¯è¿½è¸ªå¿«é€Ÿå®šä½é—®é¢˜æ ¹å› 
- **æ€§èƒ½ä¼˜åŒ–**ï¼šåŸºäºç›‘æ§æ•°æ®è¿›è¡Œèµ„æºä¼˜åŒ–å’Œæ€§èƒ½è°ƒä¼˜
- **å®¹é‡è§„åˆ’**ï¼šæ ¹æ®å†å²æ•°æ®é¢„æµ‹èµ„æºéœ€æ±‚è¶‹åŠ¿

### 6.5 å­¦ä¹ è·¯å¾„å»ºè®®


```
å…¥é—¨é˜¶æ®µï¼š
1ï¸âƒ£ éƒ¨ç½²metrics-serverï¼Œç†è§£åŸºç¡€èµ„æºç›‘æ§
2ï¸âƒ£ æ­å»ºPrometheus + Grafanaï¼Œå­¦ä¹ æŒ‡æ ‡ç›‘æ§
3ï¸âƒ£ é…ç½®åŸºç¡€å‘Šè­¦è§„åˆ™ï¼Œä½“éªŒå‘Šè­¦æœºåˆ¶

è¿›é˜¶é˜¶æ®µï¼š  
4ï¸âƒ£ éƒ¨ç½²EFKæˆ–PLGæ—¥å¿—ç³»ç»Ÿï¼Œå­¦ä¹ æ—¥å¿—ç›‘æ§
5ï¸âƒ£ æ­å»ºJaegeré“¾è·¯è¿½è¸ªï¼Œç†è§£åˆ†å¸ƒå¼è¿½è¸ª
6ï¸âƒ£ ä¼˜åŒ–é…ç½®ï¼ŒåŠ å¼ºå®‰å…¨å’Œæ€§èƒ½è°ƒä¼˜

é«˜çº§é˜¶æ®µï¼š
7ï¸âƒ£ è‡ªå®šä¹‰ç›‘æ§æŒ‡æ ‡å’Œå‘Šè­¦è§„åˆ™
8ï¸âƒ£ é›†æˆCI/CDï¼Œå®ç°ç›‘æ§è‡ªåŠ¨åŒ–
9ï¸âƒ£ å¤šé›†ç¾¤ç›‘æ§ï¼Œè·¨åŒºåŸŸæ•°æ®èšåˆ
```

**æ ¸å¿ƒè®°å¿†**ï¼š
- ç›‘æ§æ˜¯Kubernetesè¿ç»´çš„"çœ¼ç›"ï¼Œæ²¡æœ‰ç›‘æ§å°±æ˜¯"ç›²äººæ‘¸è±¡"
- æŒ‡æ ‡ã€æ—¥å¿—ã€è¿½è¸ªä¸‰ä½ä¸€ä½“ï¼Œç¼ºä¸€ä¸å¯
- å·¥å…·é€‰æ‹©è¦ç»“åˆå®é™…åœºæ™¯ï¼Œä¸æ˜¯è¶Šå¤æ‚è¶Šå¥½
- é…ç½®è¦è€ƒè™‘èµ„æºã€å®‰å…¨ã€ç½‘ç»œç­‰å¤šä¸ªæ–¹é¢
- ç›‘æ§ç³»ç»Ÿæœ¬èº«ä¹Ÿéœ€è¦ç›‘æ§å’Œç»´æŠ¤