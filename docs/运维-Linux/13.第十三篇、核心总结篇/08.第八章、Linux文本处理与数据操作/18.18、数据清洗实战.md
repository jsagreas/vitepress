---
title: 18、数据清洗实战
---
## 📚 目录

1. [数据清洗基础概念](#1-数据清洗基础概念)
2. [数据格式标准化处理](#2-数据格式标准化处理)
3. [重复数据识别与清理](#3-重复数据识别与清理)
4. [异常数据过滤方法](#4-异常数据过滤方法)
5. [数据完整性验证](#5-数据完整性验证)
6. [缺失数据处理策略](#6-缺失数据处理策略)
7. [数据类型转换处理](#7-数据类型转换处理)
8. [清洗结果质量评估](#8-清洗结果质量评估)
9. [清洗过程可追溯性](#9-清洗过程可追溯性)
10. [核心要点总结](#10-核心要点总结)

---

## 1. 🧹 数据清洗基础概念


### 1.1 什么是数据清洗


**💡 通俗理解**：数据清洗就像整理一个乱糟糟的房间，要把有用的东西分类整理，把没用的垃圾扔掉，让整个房间变得干净整洁。

```
原始杂乱数据          →         清洗后整洁数据
张三,25,男,北京            张三,25,男,北京
李四,abc,女,上海    →      李四,,女,上海
王五,30,,              王五,30,未知,
重复记录...             (已去除重复)
```

**🔸 数据清洗的本质**：
- **发现问题**：找出数据中的错误、重复、缺失等问题
- **制定规则**：确定如何处理这些问题的标准
- **执行清理**：按照规则自动化处理数据
- **验证结果**：确保清洗后的数据质量符合要求

### 1.2 为什么需要数据清洗


**🎯 现实场景问题**：
```
电商网站用户数据：
- 同一用户多次注册 → 重复数据
- 年龄填写"二十五" → 格式不统一  
- 手机号少写一位 → 数据错误
- 地址栏留空 → 缺失数据
```

**💰 数据质量的价值**：
- **决策准确性**：干净的数据 → 准确的分析 → 正确的决策
- **系统稳定性**：避免程序因异常数据崩溃
- **成本节约**：减少人工排查和修复的时间
- **合规要求**：满足数据质量相关的法规要求

### 1.3 Linux数据清洗的优势


**⚡ 为什么选择Linux工具**：

| 特点 | **传统方式** | **Linux工具** |
|------|-------------|--------------|
| 🔄 **处理速度** | `Excel处理万行数据卡顿` | `sed/awk秒级处理百万行` |
| 💾 **内存占用** | `加载全部数据到内存` | `流式处理，内存占用极小` |
| 🤖 **自动化** | `手动操作，重复性高` | `脚本化，一键执行` |
| 🔗 **集成性** | `工具间切换繁琐` | `管道组合，无缝衔接` |

---

## 2. 📐 数据格式标准化处理


### 2.1 文本格式统一


**🔸 大小写标准化**

不同系统可能产生不同大小写的数据，需要统一处理：

```bash
# 将所有英文转为小写
echo "Beijing SHANGHAI guangzhou" | tr 'A-Z' 'a-z'
# 输出：beijing shanghai guangzhou

# 首字母大写（标准化城市名）
echo "beijing shanghai" | sed 's/\b\w/\U&/g'
# 输出：Beijing Shanghai
```

**📞 电话号码格式统一**

```bash
# 原始数据：多种格式的手机号
cat phones.txt
138-1234-5678
13812345678  
138 1234 5678
+86-138-1234-5678

# 统一格式：保留数字，统一为11位
sed 's/[^0-9]//g' phones.txt | sed 's/^86//' | grep '^[0-9]\{11\}$'
# 输出：13812345678
```

**💡 理解要点**：
- `tr`命令专门用于字符替换和转换
- `sed`的替换功能可以处理复杂的格式转换
- 正则表达式帮助识别和提取特定格式的数据

### 2.2 日期时间格式标准化


**📅 多种日期格式统一**

```bash
# 原始数据：不同格式的日期
cat dates.txt
2025-01-19
01/19/2025
19-Jan-2025
2025年1月19日

# 统一转换为 YYYY-MM-DD 格式
awk '{
    if($0 ~ /^[0-9]{4}-[0-9]{2}-[0-9]{2}$/) print $0
    else if($0 ~ /^[0-9]{2}\/[0-9]{2}\/[0-9]{4}$/) {
        split($0, arr, "/")
        printf "%s-%s-%s\n", arr[3], arr[1], arr[2]
    }
}' dates.txt
```

**⏰ 时间格式处理技巧**：
- 使用`date`命令进行格式转换
- `awk`的条件判断处理不同输入格式
- 正则表达式识别日期格式模式

### 2.3 数值格式标准化


**💰 金额格式处理**

```bash
# 处理带货币符号和千分位的金额
echo "￥1,234.56 $2,345.67 €3,456.78" | \
sed 's/[￥$€,]//g' | \
awk '{for(i=1;i<=NF;i++) printf "%.2f ", $i; print ""}'
# 输出：1234.56 2345.67 3456.78
```

**📊 百分比处理**

```bash
# 将百分比转换为小数
echo "85.5% 90.2% 67.8%" | \
sed 's/%//g' | \
awk '{for(i=1;i<=NF;i++) printf "%.3f ", $i/100; print ""}'
# 输出：0.855 0.902 0.678
```

---

## 3. 🔍 重复数据识别与清理


### 3.1 完全重复数据处理


**🎯 什么是完全重复**：两条记录的所有字段内容都完全相同。

```bash
# 示例数据文件 users.csv
cat > users.csv << EOF
张三,25,男,北京
李四,30,女,上海  
张三,25,男,北京
王五,28,男,广州
李四,30,女,上海
EOF

# 方法1: 使用sort + uniq去重
sort users.csv | uniq > users_clean.csv

# 方法2: 使用awk去重（保持原有顺序）
awk '!seen[$0]++' users.csv > users_clean.csv
```

**💡 去重原理解释**：
- `sort | uniq`：先排序再去重，会改变数据顺序
- `awk '!seen[$0]++'`：使用关联数组记录已见过的行，保持原有顺序
- `seen[$0]++`：每次见到一行，计数器加1
- `!seen[$0]`：只有第一次见到时为真，后续重复的为假

### 3.2 部分重复数据识别


**🔸 基于关键字段去重**

比如用户表中，我们认为手机号相同的就是重复用户：

```bash
# 用户数据：姓名可能不同，但手机号相同
cat > users_with_phone.csv << EOF
张三,13812345678,北京
张山,13812345678,北京
李四,13987654321,上海
李思,13987654321,上海  
EOF

# 基于手机号去重（保留第一条记录）
awk -F',' '!seen[$2]++' users_with_phone.csv
# 输出：张三,13812345678,北京
#       李四,13987654321,上海
```

**🔢 统计重复情况**

```bash
# 查看重复数据统计
awk -F',' '{count[$2]++} END {
    for(phone in count) 
        if(count[phone] > 1) 
            print phone, count[phone]
}' users_with_phone.csv
```

### 3.3 模糊重复数据处理


**🎯 处理相似但不完全相同的数据**

```bash
# 姓名中可能有多余空格的情况
cat > names.csv << EOF
张 三,25
张三,25
 张三 ,25
李四,30
EOF

# 清理空格后去重
sed 's/ //g' names.csv | sort | uniq
```

**📊 重复数据清理策略表**

| 重复类型 | **识别方法** | **处理策略** | **适用场景** |
|---------|-------------|-------------|-------------|
| 🎯 **完全重复** | `sort \| uniq` | `直接删除重复行` | `日志文件、导入数据` |
| 🔑 **关键字段重复** | `awk基于字段去重` | `保留首次出现的记录` | `用户数据、产品信息` |
| 🔍 **模糊重复** | `预处理+去重` | `标准化后去重` | `手工录入数据` |

---

## 4. ⚠️ 异常数据过滤方法


### 4.1 数值异常检测


**📊 年龄数据异常检测**

```bash
# 用户年龄数据，需要过滤异常值
cat > ages.txt << EOF
25
30
-5
999
abc
150
28
EOF

# 过滤合理年龄范围（1-120岁）
awk '$1 ~ /^[0-9]+$/ && $1 >= 1 && $1 <= 120' ages.txt
# 输出：25, 30, 28
```

**💰 价格异常检测**

```bash
# 商品价格异常检测
echo "10.50 0.99 -15.00 999999 abc 25.30" | \
awk '{for(i=1;i<=NF;i++) {
    if($i ~ /^[0-9]+\.?[0-9]*$/ && $i > 0 && $i < 10000)
        print $i
}}'
```

**💡 异常检测原理**：
- 使用正则表达式验证数据格式
- 设置合理的数值范围边界
- 结合业务逻辑判断数据合理性

### 4.2 文本异常检测


**📧 邮箱格式验证**

```bash
# 邮箱地址验证
cat > emails.txt << EOF
user@example.com
invalid-email
test@domain
user@example.org
@invalid.com
user@.com
EOF

# 简单邮箱格式验证
grep '^[a-zA-Z0-9._-]\+@[a-zA-Z0-9.-]\+\.[a-zA-Z]\{2,\}$' emails.txt
# 输出：user@example.com, user@example.org
```

**🆔 身份证号验证**

```bash
# 18位身份证号格式检查
echo "110101199001011234 12345 41032419900101123X" | \
awk '{for(i=1;i<=NF;i++) {
    if($i ~ /^[0-9]{17}[0-9X]$/)
        print $i " - 格式正确"
    else
        print $i " - 格式错误"
}}'
```

### 4.3 统计异常检测


**📈 基于统计的异常检测**

```bash
# 计算平均值和标准差，识别异常值
calculate_outliers() {
    awk '{
        values[NR] = $1
        sum += $1
        count++
    }
    END {
        mean = sum / count
        # 计算标准差
        for(i=1; i<=count; i++) {
            variance += (values[i] - mean) ^ 2
        }
        std = sqrt(variance / count)
        
        print "平均值:", mean
        print "标准差:", std
        print "异常值（超出2个标准差）:"
        
        for(i=1; i<=count; i++) {
            if(values[i] < mean - 2*std || values[i] > mean + 2*std)
                print values[i]
        }
    }'
}

# 使用示例
echo -e "10\n12\n11\n100\n13\n9\n15" | calculate_outliers
```

---

## 5. ✅ 数据完整性验证


### 5.1 必填字段检查


**🔍 检查关键字段是否为空**

```bash
# CSV文件完整性检查
check_required_fields() {
    local file=$1
    local required_fields="1,2,3"  # 第1,2,3列为必填
    
    awk -F',' -v fields="$required_fields" '
    BEGIN {
        split(fields, required, ",")
    }
    {
        line_errors = 0
        for(i in required) {
            field_num = required[i]
            if($field_num == "" || $field_num ~ /^[ \t]*$/) {
                print "第" NR "行第" field_num "列为空: " $0
                line_errors++
            }
        }
        if(line_errors == 0) valid_lines++
        total_lines++
    }
    END {
        print "总行数:", total_lines
        print "有效行数:", valid_lines
        print "错误行数:", total_lines - valid_lines
    }' "$file"
}
```

### 5.2 数据关联性验证


**🔗 外键关联检查**

```bash
# 检查订单中的用户ID是否在用户表中存在
cat > users.csv << EOF
1,张三
2,李四
3,王五
EOF

cat > orders.csv << EOF
101,1,商品A
102,2,商品B  
103,4,商品C
EOF

# 检查订单中的用户ID是否有效
awk -F',' '
FNR==NR {users[$1]=1; next}
{
    if(!($2 in users))
        print "订单" $1 "的用户ID" $2 "不存在"
    else
        print "订单" $1 "验证通过"
}' users.csv orders.csv
```

### 5.3 数据范围验证


**📊 数据范围合规性检查**

```bash
# 综合数据验证脚本
validate_user_data() {
    awk -F',' '
    {
        errors = 0
        # 检查姓名（非空，长度2-10）
        if($1 == "" || length($1) < 2 || length($1) > 10) {
            print "第" NR "行：姓名格式错误 - " $1
            errors++
        }
        
        # 检查年龄（1-120）
        if($2 !~ /^[0-9]+$/ || $2 < 1 || $2 > 120) {
            print "第" NR "行：年龄格式错误 - " $2  
            errors++
        }
        
        # 检查性别（男/女）
        if($3 != "男" && $3 != "女") {
            print "第" NR "行：性别格式错误 - " $3
            errors++
        }
        
        if(errors == 0) {
            valid_count++
            print $0 > "valid_users.csv"
        } else {
            invalid_count++  
            print $0 > "invalid_users.csv"
        }
        total_count++
    }
    END {
        print "验证完成："
        print "总记录数：" total_count
        print "有效记录：" valid_count  
        print "无效记录：" invalid_count
    }'
}
```

---

## 6. 🔧 缺失数据处理策略


### 6.1 缺失数据识别


**🎯 什么算缺失数据**：
- **完全为空**：字段没有任何内容
- **空白字符**：只有空格、制表符等
- **特殊标记**：如"NULL"、"N/A"、"-"等

```bash
# 识别各种形式的缺失数据
identify_missing() {
    awk -F',' '{
        for(i=1; i<=NF; i++) {
            # 检查空值、空白、常见缺失标记
            if($i == "" || $i ~ /^[ \t]*$/ || 
               $i ~ /^(NULL|null|N\/A|n\/a|-)$/) {
                missing[NR,i] = 1
                printf "第%d行第%d列缺失: %s\n", NR, i, $i
            }
        }
    }' "$1"
}
```

### 6.2 缺失数据补填策略


**📊 数值数据补填**

```bash
# 使用平均值补填缺失的年龄
fill_missing_age() {
    awk -F',' '
    # 第一遍：计算平均值
    NR==FNR && $2 != "" && $2 ~ /^[0-9]+$/ {
        sum += $2; count++; next
    }
    NR==FNR {next}
    
    # 第二遍：补填缺失值
    {
        if($2 == "" || $2 !~ /^[0-9]+$/) {
            avg = int(sum/count)
            $2 = avg
            print $0 " (年龄已补填为平均值" avg ")"
        } else {
            print $0
        }
    }' users.csv users.csv
}
```

**🏷️ 文本数据补填**

```bash
# 使用默认值补填缺失的城市信息  
fill_missing_city() {
    awk -F',' '{
        if($4 == "" || $4 ~ /^[ \t]*$/) {
            $4 = "未知"
            print $0 " (城市已补填)"
        } else {
            print $0
        }
    }' users.csv
}
```

### 6.3 智能补填策略


**🎯 基于规则的补填**

```bash
# 根据相关字段推断缺失值
smart_fill() {
    awk -F',' '{
        # 如果手机号以138开头，推断为北京用户
        if($4 == "" && $3 ~ /^138/) {
            $4 = "北京(推断)"
        }
        # 如果姓氏为"李"且城市为空，推断为上海
        else if($4 == "" && $1 ~ /^李/) {
            $4 = "上海(推断)"  
        }
        # 否则使用默认值
        else if($4 == "") {
            $4 = "未知"
        }
        print $0
    }' users.csv
}
```

**📈 缺失数据处理策略对比**

| 策略 | **适用场景** | **优点** | **缺点** |
|------|-------------|---------|---------|
| 🗑️ **删除记录** | `缺失字段较多` | `简单直接` | `数据量减少` |
| 📊 **平均值填充** | `数值型字段` | `保持统计特性` | `可能不准确` |
| 🏷️ **默认值填充** | `分类字段` | `保持记录完整` | `增加偏差` |
| 🧠 **智能推断** | `有规律可循` | `准确性较高` | `规则复杂` |

---

## 7. 🔄 数据类型转换处理


### 7.1 数值类型转换


**🔢 字符串转数值**

```bash
# 处理带有非数字字符的数值数据
convert_to_number() {
    echo "价格: ￥123.45元 数量: 10个 折扣: 85%" | \
    awk '{
        # 提取价格数字
        if(match($0, /￥([0-9.]+)/, price_arr))
            print "价格:", price_arr[1]
            
        # 提取数量数字
        if(match($0, /([0-9]+)个/, qty_arr))
            print "数量:", qty_arr[1]
            
        # 提取百分比并转为小数
        if(match($0, /([0-9.]+)%/, discount_arr))
            print "折扣:", discount_arr[1]/100
    }'
}
```

**💰 金额格式化**

```bash
# 标准化金额显示
format_money() {
    awk '{
        # 添加千分位分隔符
        gsub(/[^0-9.]/, "", $1)  # 清理非数字字符
        num = $1
        
        # 分离整数和小数部分
        if(match(num, /\./, pos)) {
            integer = substr(num, 1, pos-1)  
            decimal = substr(num, pos)
        } else {
            integer = num
            decimal = ""
        }
        
        # 添加千分位逗号
        while(match(integer, /^([0-9]+)([0-9]{3})/, arr)) {
            integer = arr[1] "," arr[2]
        }
        
        print integer decimal
    }'
}

# 使用示例
echo "1234567.89" | format_money
# 输出: 1,234,567.89
```

### 7.2 日期时间转换


**📅 日期格式统一转换**

```bash
# 多种日期格式转换为标准格式
standardize_date() {
    awk '{
        date_str = $1
        
        # YYYY-MM-DD 格式（已标准）
        if(date_str ~ /^[0-9]{4}-[0-9]{2}-[0-9]{2}$/) {
            print date_str
        }
        # MM/DD/YYYY 格式
        else if(match(date_str, /^([0-9]{2})\/([0-9]{2})\/([0-9]{4})$/, arr)) {
            printf "%s-%s-%s\n", arr[3], arr[1], arr[2]
        }
        # DD-MM-YYYY 格式  
        else if(match(date_str, /^([0-9]{2})-([0-9]{2})-([0-9]{4})$/, arr)) {
            printf "%s-%s-%s\n", arr[3], arr[2], arr[1]
        }
        else {
            print "格式错误: " date_str
        }
    }'
}
```

**⏰ 时间戳转换**

```bash
# Unix时间戳转可读日期
convert_timestamp() {
    awk '{
        if($1 ~ /^[0-9]{10}$/) {  # 10位Unix时间戳
            cmd = "date -d @" $1 " +\"%Y-%m-%d %H:%M:%S\""
            cmd | getline readable_date
            close(cmd)
            print $1 " -> " readable_date
        } else {
            print "无效时间戳: " $1
        }
    }'
}

# 使用示例
echo "1705737021" | convert_timestamp
```

### 7.3 文本编码转换


**🔤 字符编码处理**

```bash
# 检测并转换文件编码
detect_and_convert_encoding() {
    local file=$1
    
    # 检测当前编码
    encoding=$(file -bi "$file" | grep -o 'charset=[^[:space:]]*' | cut -d= -f2)
    echo "当前编码: $encoding"
    
    # 如果不是UTF-8，则转换
    if [ "$encoding" != "utf-8" ]; then
        iconv -f "$encoding" -t utf-8 "$file" > "${file}.utf8"
        echo "已转换为UTF-8编码: ${file}.utf8"
    fi
}
```

---

## 8. 📊 清洗结果质量评估


### 8.1 数据质量指标


**📈 完整性评估**

```bash
# 计算数据完整性指标
assess_completeness() {
    awk -F',' '
    {
        total_fields += NF
        for(i=1; i<=NF; i++) {
            if($i != "" && $i !~ /^[ \t]*$/)
                filled_fields++
        }
        total_records++
    }
    END {
        completeness = filled_fields / total_fields * 100
        printf "数据完整性: %.2f%%\n", completeness
        printf "总记录数: %d\n", total_records  
        printf "总字段数: %d\n", total_fields
        printf "已填充字段: %d\n", filled_fields
        printf "缺失字段: %d\n", total_fields - filled_fields
    }' "$1"
}
```

**🎯 准确性评估**

```bash
# 评估数据格式准确性
assess_accuracy() {
    awk -F',' '
    {
        total_records++
        field_errors = 0
        
        # 检查姓名格式（中文字符）
        if($1 !~ /^[\u4e00-\u9fa5]{2,4}$/) field_errors++
        
        # 检查年龄范围
        if($2 !~ /^[0-9]{1,3}$/ || $2 < 1 || $2 > 120) field_errors++
        
        # 检查手机号格式
        if($3 !~ /^1[3-9][0-9]{9}$/) field_errors++
        
        if(field_errors == 0) accurate_records++
    }
    END {
        accuracy = accurate_records / total_records * 100
        printf "数据准确性: %.2f%%\n", accuracy
        printf "准确记录: %d/%d\n", accurate_records, total_records
    }' "$1"
}
```

### 8.2 清洗效果对比


**📊 前后对比报告**

```bash
# 生成清洗前后对比报告
generate_cleaning_report() {
    local original_file=$1
    local cleaned_file=$2
    
    echo "==================== 数据清洗报告 ===================="
    echo "清洗时间: $(date)"
    echo ""
    
    echo "📋 基本统计:"
    echo "原始数据行数: $(wc -l < "$original_file")"
    echo "清洗后行数: $(wc -l < "$cleaned_file")"
    echo "删除重复行数: $(($(wc -l < "$original_file") - $(wc -l < "$cleaned_file")))"
    echo ""
    
    echo "📊 数据质量对比:"
    echo "--- 原始数据质量 ---"
    assess_completeness "$original_file"
    assess_accuracy "$original_file"
    
    echo ""
    echo "--- 清洗后数据质量 ---"  
    assess_completeness "$cleaned_file"
    assess_accuracy "$cleaned_file"
    
    echo ""
    echo "✅ 清洗改进效果:"
    # 这里可以添加更多对比分析
}
```

### 8.3 异常数据统计


**⚠️ 异常模式分析**

```bash
# 统计和分析异常数据模式
analyze_anomalies() {
    awk -F',' '
    {
        # 统计各种异常情况
        if($1 == "") empty_name++
        if($2 !~ /^[0-9]+$/) invalid_age++
        if($3 != "男" && $3 != "女") invalid_gender++
        if($4 == "") empty_city++
        
        total++
    }
    END {
        print "🔍 异常数据统计报告:"
        printf "姓名为空: %d (%.1f%%)\n", empty_name, empty_name/total*100
        printf "年龄格式错误: %d (%.1f%%)\n", invalid_age, invalid_age/total*100  
        printf "性别格式错误: %d (%.1f%%)\n", invalid_gender, invalid_gender/total*100
        printf "城市为空: %d (%.1f%%)\n", empty_city, empty_city/total*100
    }' "$1"
}
```

---

## 9. 📝 清洗过程可追溯性


### 9.1 操作日志记录


**📋 详细日志记录**

```bash
# 创建可追溯的数据清洗流程
traceable_cleaning() {
    local input_file=$1
    local output_file=$2
    local log_file="cleaning_$(date +%Y%m%d_%H%M%S).log"
    
    echo "开始数据清洗流程 - $(date)" | tee "$log_file"
    echo "输入文件: $input_file" | tee -a "$log_file"
    echo "输出文件: $output_file" | tee -a "$log_file"
    echo "===========================================" | tee -a "$log_file"
    
    # 步骤1: 去重
    echo "步骤1: 去除重复数据" | tee -a "$log_file"
    local before_dedup=$(wc -l < "$input_file")
    sort "$input_file" | uniq > temp_dedup.csv
    local after_dedup=$(wc -l < temp_dedup.csv)
    echo "删除重复记录: $((before_dedup - after_dedup)) 条" | tee -a "$log_file"
    
    # 步骤2: 格式标准化  
    echo "步骤2: 数据格式标准化" | tee -a "$log_file"
    awk -F',' '{
        gsub(/^[ \t]+|[ \t]+$/, "", $1)  # 清理姓名前后空格
        gsub(/[^0-9]/, "", $2)           # 清理年龄非数字字符
        print $0
    }' temp_dedup.csv > temp_format.csv
    echo "格式标准化完成" | tee -a "$log_file"
    
    # 步骤3: 异常数据过滤
    echo "步骤3: 过滤异常数据" | tee -a "$log_file"
    local before_filter=$(wc -l < temp_format.csv)
    awk -F',' '$2 >= 1 && $2 <= 120' temp_format.csv > "$output_file"
    local after_filter=$(wc -l < "$output_file")
    echo "过滤异常记录: $((before_filter - after_filter)) 条" | tee -a "$log_file"
    
    # 清理临时文件
    rm -f temp_dedup.csv temp_format.csv
    
    echo "===========================================" | tee -a "$log_file"
    echo "数据清洗完成 - $(date)" | tee -a "$log_file"
    echo "最终输出: $after_filter 条有效记录" | tee -a "$log_file"
    echo "日志文件: $log_file"
}
```

### 9.2 版本控制管理


**🗂️ 数据版本管理**

```bash
# 数据清洗版本管理系统
manage_data_versions() {
    local base_name=$(basename "$1" .csv)
    local version_dir="versions/${base_name}"
    
    # 创建版本目录
    mkdir -p "$version_dir"
    
    # 生成版本号
    local version="v$(date +%Y%m%d_%H%M%S)"
    
    # 保存原始数据
    cp "$1" "${version_dir}/original_${version}.csv"
    
    # 执行清洗（这里调用实际的清洗函数）
    traceable_cleaning "$1" "${version_dir}/cleaned_${version}.csv"
    
    # 创建版本信息文件
    cat > "${version_dir}/version_${version}.info" << EOF
版本: $version
创建时间: $(date)
原始文件: $1
清洗后文件: cleaned_${version}.csv
操作日志: cleaning_*.log
清洗规则: 去重 + 格式化 + 异常过滤
EOF
    
    echo "版本 $version 已创建在: $version_dir"
}
```

### 9.3 回滚和恢复机制


**🔄 数据恢复功能**

```bash
# 数据回滚到指定版本
rollback_to_version() {
    local data_name=$1
    local target_version=$2
    local version_dir="versions/${data_name}"
    
    if [ -f "${version_dir}/cleaned_${target_version}.csv" ]; then
        cp "${version_dir}/cleaned_${target_version}.csv" "${data_name}_current.csv"
        echo "✅ 已回滚到版本: $target_version"
        echo "📁 当前数据文件: ${data_name}_current.csv"
    else
        echo "❌ 版本 $target_version 不存在"
        echo "📋 可用版本:"
        ls -1 "${version_dir}"/cleaned_v*.csv 2>/dev/null | \
        sed 's/.*cleaned_\(v[^.]*\).csv/\1/' || echo "无可用版本"
    fi
}

# 查看版本历史
show_version_history() {
    local data_name=$1
    local version_dir="versions/${data_name}"
    
    echo "📊 数据版本历史:"
    echo "=================="
    
    for info_file in "${version_dir}"/version_v*.info; do
        if [ -f "$info_file" ]; then
            echo ""
            cat "$info_file"
            echo "=================="
        fi
    done
}
```

---

## 10. 📋 核心要点总结


### 10.1 必须掌握的核心概念


```
🔸 数据清洗本质：发现问题 → 制定规则 → 执行清理 → 验证结果
🔸 格式标准化：统一大小写、日期格式、数值格式等
🔸 重复数据处理：完全重复、部分重复、模糊重复的不同策略  
🔸 异常数据过滤：数值异常、文本异常、统计异常检测
🔸 缺失数据处理：识别缺失 → 选择策略 → 智能补填
🔸 数据类型转换：数值、日期、编码格式的标准化转换
🔸 质量评估：完整性、准确性、一致性指标
🔸 过程可追溯：操作日志、版本管理、回滚机制
```

### 10.2 关键理解要点


**🔹 数据清洗的核心思维**
```
问题导向：先识别数据中存在的具体问题
规则制定：根据业务需求制定清洗规则
自动化执行：用脚本批量处理，避免手工操作
质量验证：清洗后要验证数据质量是否达标
```

**🔹 Linux工具的选择原则**
```
简单问题：sed、grep、tr等基础工具
复杂逻辑：awk编程处理
批量操作：shell脚本自动化
大数据量：流式处理，避免内存不足
```

**🔹 清洗策略的平衡**
```
完整性 vs 准确性：有时需要删除不完整的记录
自动化 vs 人工判断：复杂情况可能需要人工介入
标准化 vs 原始性：保留原始数据，创建标准化版本
```

### 10.3 实际应用指导


**🎯 数据清洗实施步骤**

1️⃣ **数据探索**：先了解数据结构和质量现状
2️⃣ **问题识别**：找出重复、缺失、异常、格式问题
3️⃣ **规则制定**：根据业务需求确定处理标准
4️⃣ **脚本开发**：编写可复用的清洗脚本
5️⃣ **测试验证**：小样本测试，验证清洗效果
6️⃣ **批量执行**：对全量数据执行清洗
7️⃣ **质量评估**：评估清洗后的数据质量
8️⃣ **文档记录**：记录清洗过程和结果

**⚠️ 常见陷阱避免**

| 陷阱类型 | **错误做法** | **正确做法** |
|---------|-------------|-------------|
| 🗑️ **过度删除** | `一发现异常就删除记录` | `先尝试修复，再考虑删除` |
| 🔄 **盲目替换** | `全局替换特定字符` | `基于上下文智能替换` |
| 📊 **忽视验证** | `清洗后不检查结果` | `每步都验证处理效果` |
| 💾 **不备份原始数据** | `直接修改原始文件` | `保留原始数据，创建新版本` |

### 10.4 进阶技能发展


**🚀 提升清洗效率**
```
自动化脚本：开发可复用的清洗模板
正则表达式：掌握复杂模式匹配
性能优化：处理大文件时的内存和速度优化
错误处理：脚本中加入异常处理逻辑
```

**📊 质量管理进阶**
```
统计分析：深入分析数据分布和异常模式
业务理解：结合具体业务场景制定清洗规则
持续改进：根据反馈不断优化清洗流程
标准化管理：建立数据清洗的标准作业流程
```

**核心记忆口诀**：
- 数据清洗有章法，识别问题是关键
- 格式统一除重复，异常过滤补缺失  
- Linux工具很强大，awk sed grep组合用
- 质量评估不可少，过程记录便追溯