---
title: 2、uniq去重工具精通
---
## 📚 目录

1. [uniq基础概念与工作原理](#1-uniq基础概念与工作原理)
2. [基础去重语法详解](#2-基础去重语法详解)
3. [核心参数功能掌握](#3-核心参数功能掌握)
4. [uniq与sort组合实战](#4-uniq与sort组合实战)
5. [高级应用技巧](#5-高级应用技巧)
6. [性能优化与最佳实践](#6-性能优化与最佳实践)
7. [核心要点总结](#7-核心要点总结)

---

## 1. 🔧 uniq基础概念与工作原理


### 1.1 什么是uniq工具


**📋 核心定义**
uniq是Linux系统中专门用来**去除重复行**的文本处理工具。它的名字来自"unique"（唯一），作用就是让文本中的重复行变得唯一。

**💡 通俗理解**
想象你有一个班级名单，里面有些同学的名字重复写了好几遍。uniq就像一个细心的老师，帮你把重复的名字找出来或删掉，让名单变得整洁。

### 1.2 uniq的工作机制


**🔸 重要特点：只能处理相邻的重复行**

```
原理解释：
uniq就像一个"近视眼"，只能看到紧挨着的两行
如果重复的行不相邻，uniq看不出来它们是重复的

例子说明：
apple     ← 第1行
apple     ← 第2行（与第1行相邻且重复，会被处理）
banana    ← 第3行
apple     ← 第4行（与第3行不重复，不会被处理）

结果：uniq只会去掉第2行的apple，第4行的apple保留
```

**⚠️ 关键理解**
这就是为什么uniq通常要和sort配合使用 - **先排序让相同行聚在一起，再去重**。

### 1.3 基本语法结构


```bash
uniq [选项] [输入文件] [输出文件]
```

**参数说明：**
- `选项`：控制uniq的具体行为（后面详细讲）
- `输入文件`：要处理的文件，不写就从键盘输入
- `输出文件`：结果保存的文件，不写就显示在屏幕上

---

## 2. 📝 基础去重语法详解


### 2.1 最简单的去重操作


**🔸 默认行为：删除重复行**

```bash
# 准备测试数据
echo -e "apple\napple\nbanana\nbanana\ncherry" > fruits.txt

# 查看原文件
cat fruits.txt
```

输出：
```
apple
apple
banana
banana
cherry
```

**执行去重：**
```bash
uniq fruits.txt
```

输出：
```
apple
banana
cherry
```

**📖 解释**：uniq默认保留每组重复行的第一行，删除后面的重复行。

### 2.2 从标准输入处理数据


**实际场景示例：**

```bash
# 处理命令输出的重复行
ps aux | grep python | awk '{print $11}' | uniq
```

这个例子的含义：
- `ps aux`：列出所有进程
- `grep python`：筛选包含python的行
- `awk '{print $11}'`：提取第11列（通常是命令名）
- `uniq`：去除重复的命令名

### 2.3 输出到文件


```bash
# 将去重结果保存到新文件
uniq fruits.txt unique_fruits.txt

# 或者使用重定向
uniq fruits.txt > unique_fruits.txt
```

---

## 3. ⚙️ 核心参数功能掌握


### 3.1 计数统计功能 (-c)


**🔸 作用**：显示每行出现的次数

```bash
# 统计重复行出现次数
uniq -c fruits.txt
```

输出：
```
   2 apple
   2 banana  
   1 cherry
```

**📊 输出格式解释**：
- 第一列：该行出现的次数
- 第二列：具体内容

**💡 实际应用场景**：

```bash
# 统计访问日志中最频繁的IP
cat access.log | awk '{print $1}' | sort | uniq -c | sort -nr
```

这个命令链的作用：
1. 提取日志中的IP地址（第1列）
2. 排序让相同IP聚集
3. 统计每个IP出现次数
4. 按次数倒序排列

### 3.2 仅显示重复行 (-d)


**🔸 作用**：只显示那些出现多次的行

```bash
uniq -d fruits.txt
```

输出：
```
apple
banana
```

**🎯 使用场景**：
- 查找配置文件中的重复项
- 检测数据中的异常重复
- 质量检查时发现重复数据

**实际示例**：
```bash
# 检查用户配置文件中的重复用户名
cut -d: -f1 /etc/passwd | sort | uniq -d
```

### 3.3 仅显示唯一行 (-u)


**🔸 作用**：只显示那些只出现一次的行

```bash
uniq -u fruits.txt
```

输出：
```
cherry
```

**💼 实际应用**：

```bash
# 找出两个文件中各自独有的行
cat file1.txt file2.txt | sort | uniq -u
```

### 3.4 忽略大小写 (-i)


**🔸 作用**：比较时不区分大小写

准备测试数据：
```bash
echo -e "Apple\napple\nAPPLE\nBanana\nbanana" > mixed_case.txt
```

```bash
# 忽略大小写去重
uniq -i mixed_case.txt
```

输出：
```
Apple
Banana
```

**📋 注意**：保留的是每组中第一次出现的行（保持原始大小写）。

### 3.5 跳过字段比较 (-f)


**🔸 作用**：忽略行首的指定字段数，从后面的字段开始比较

准备数据：
```bash
echo -e "1 apple\n2 apple\n3 banana\n4 banana" > numbered_fruits.txt
```

```bash
# 跳过第一个字段（数字），只比较水果名
uniq -f 1 numbered_fruits.txt
```

输出：
```
1 apple
3 banana
```

**🔍 解释**：跳过第1个字段后，"1 apple"和"2 apple"的后面部分都是"apple"，所以被认为是重复的。

### 3.6 指定比较字符数 (-w)


**🔸 作用**：只比较每行的前N个字符

```bash
echo -e "apple123\napple456\nbanana789" > prefixed_fruits.txt

# 只比较前5个字符
uniq -w 5 prefixed_fruits.txt
```

输出：
```
apple123
banana789
```

**📖 解释**：只比较前5个字符，"apple123"和"apple456"的前5个字符都是"apple"，所以第二行被去掉。

---

## 4. 🔄 uniq与sort组合实战


### 4.1 为什么需要组合使用


**⚠️ 关键问题**：uniq只能处理相邻的重复行

创建测试文件：
```bash
echo -e "apple\nbanana\napple\ncherry\nbanana\napple" > scattered.txt
```

**❌ 单独使用uniq的问题**：
```bash
uniq scattered.txt
```

输出（重复未完全去除）：
```
apple
banana
apple
cherry
banana
apple
```

**✅ 正确的组合方式**：
```bash
sort scattered.txt | uniq
```

输出（完全去重）：
```
apple
banana
cherry
```

### 4.2 经典组合模式


**🔸 模式1：完全去重**
```bash
sort file.txt | uniq
```

**🔸 模式2：去重并统计**
```bash
sort file.txt | uniq -c
```

**🔸 模式3：找出重复项**
```bash
sort file.txt | uniq -d
```

**🔸 模式4：按频率排序**
```bash
sort file.txt | uniq -c | sort -nr
```

### 4.3 实际应用案例


**📊 案例1：分析网站访问日志**

```bash
# 统计访问最多的页面
awk '{print $7}' access.log | sort | uniq -c | sort -nr | head -10
```

命令解释：
- `awk '{print $7}'`：提取URL字段
- `sort`：排序聚合相同URL
- `uniq -c`：统计每个URL访问次数
- `sort -nr`：按次数倒序排序
- `head -10`：显示前10个

**📈 案例2：分析系统进程**

```bash
# 统计正在运行的进程类型
ps aux | awk '{print $11}' | sort | uniq -c | sort -nr
```

---

## 5. 🎯 高级应用技巧


### 5.1 参数组合使用


**🔸 组合1：统计时忽略大小写**
```bash
sort file.txt | uniq -ic
```

**🔸 组合2：跳过字段并统计**
```bash
sort file.txt | uniq -f 1 -c
```

**🔸 组合3：只显示重复行并统计**
```bash
sort file.txt | uniq -dc
```

### 5.2 与其他命令的协作


**🔧 与grep配合**：
```bash
# 查找配置文件中重复的设置项
grep "^[^#]" config.conf | sort | uniq -d
```

**🔧 与cut配合**：
```bash
# 提取CSV文件的某列并去重
cut -d',' -f2 data.csv | sort | uniq
```

**🔧 与awk配合**：
```bash
# 统计日志中错误类型的分布
awk '/ERROR/ {print $3}' logfile | sort | uniq -c
```

### 5.3 处理复杂数据格式


**📊 CSV文件去重**：

```bash
# 基于某一列去重（保留标题行）
head -1 data.csv > unique_data.csv
tail -n +2 data.csv | sort -t',' -k2,2 | uniq -f 1 >> unique_data.csv
```

**📋 配置文件清理**：

```bash
# 清理配置文件中的重复行和空行
grep -v '^$' config.txt | grep -v '^#' | sort | uniq
```

---

## 6. 🚀 性能优化与最佳实践


### 6.1 处理大文件的策略


**💾 内存考虑**：

对于超大文件，sort可能消耗大量内存：

```bash
# 使用临时目录进行外部排序
TMPDIR=/tmp/large_sort sort -T /tmp/large_sort large_file.txt | uniq
```

**🔧 分块处理**：

```bash
# 将大文件分割处理
split -l 100000 large_file.txt chunk_
for chunk in chunk_*; do
    sort "$chunk" | uniq > "unique_$chunk"
done
# 合并结果
sort unique_chunk_* | uniq > final_result.txt
```

### 6.2 输出格式控制


**📊 美化统计输出**：

```bash
# 格式化uniq -c的输出
sort file.txt | uniq -c | awk '{printf "%8d %s\n", $1, $2}'
```

**📈 添加百分比统计**：

```bash
# 计算每项占总数的百分比
sort file.txt | uniq -c | awk '
{
    count[$2] = $1
    total += $1
}
END {
    for (item in count) {
        printf "%s: %d (%.2f%%)\n", item, count[item], count[item]/total*100
    }
}'
```

### 6.3 错误处理和调试


**🔍 常见错误场景**：

| 问题现象 | **可能原因** | **解决方案** |
|---------|------------|-------------|
| 重复行没有去除 | `没有先排序` | `先用sort排序` |
| 统计结果不准确 | `文件编码问题` | `检查文件编码格式` |
| 内存不足 | `文件过大` | `使用分块处理` |
| 结果与预期不符 | `字段分隔符问题` | `检查数据格式` |

**🛠️ 调试技巧**：

```bash
# 查看中间步骤的结果
sort file.txt | tee sorted.txt | uniq -c | tee counted.txt
```

---

## 7. 📋 核心要点总结


### 7.1 必须掌握的基础知识


```
🔸 工作原理：只能处理相邻重复行，需要配合sort使用
🔸 基本语法：uniq [选项] [输入文件] [输出文件]  
🔸 核心参数：-c统计 -d重复行 -u唯一行 -i忽略大小写
🔸 字段控制：-f跳过字段 -w指定字符数
🔸 最佳组合：sort + uniq 是标准模式
```

### 7.2 关键理解要点


**🔹 uniq的"局限性"实际是设计哲学**
```
设计思路：
- 专注处理相邻行，保持高效
- 与sort配合实现完整功能
- 符合Unix"做好一件事"的原则
```

**🔹 参数选择的实用指南**
```
日常去重：sort file | uniq
查找重复：sort file | uniq -d  
统计频次：sort file | uniq -c
分析数据：sort file | uniq -c | sort -nr
```

### 7.3 实际应用记忆要点


**📊 数据分析场景**：
- **日志分析**：提取字段 → 排序 → 去重统计
- **配置清理**：去除重复配置项
- **数据质检**：发现异常重复数据
- **报表生成**：统计各类数据分布

**🔧 命令组合模板**：
- `sort | uniq`：标准去重
- `sort | uniq -c | sort -nr`：频次分析
- `sort | uniq -d`：重复项检查
- `sort | uniq -u`：独有项查找

### 7.4 性能优化要点


**💡 效率提升**：
- 大文件使用分块处理
- 合理设置临时目录
- 预先过滤无关数据
- 选择合适的字段比较方式

**⚠️ 常见陷阱**：
- 忘记排序导致去重不完全
- 忽略文件编码导致结果错误  
- 字段分隔符不明确影响比较
- 大小写敏感性设置不当

**核心记忆口诀**：
```
uniq去重要排序，相邻重复才能除
-c统计-d重复，-u唯一-i忽略
字段跳过用-f，字符限制靠-w
sort组合是标配，大文件分块处理好
```