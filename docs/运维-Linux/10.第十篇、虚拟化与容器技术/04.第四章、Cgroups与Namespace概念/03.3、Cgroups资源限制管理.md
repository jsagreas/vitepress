---
title: 3、Cgroups资源限制管理
---
## 📚 目录

1. [Cgroups基本概念](#1-cgroups基本概念)
2. [CPU资源限制管理](#2-cpu资源限制管理)
3. [内存资源限制控制](#3-内存资源限制控制)
4. [磁盘I/O限制配置](#4-磁盘io限制配置)
5. [网络带宽限制机制](#5-网络带宽限制机制)
6. [进程数量限制控制](#6-进程数量限制控制)
7. [设备访问权限控制](#7-设备访问权限控制)
8. [资源监控与统计](#8-资源监控与统计)
9. [资源限制策略设计](#9-资源限制策略设计)
10. [核心要点总结](#10-核心要点总结)

---

## 1. 🔧 Cgroups基本概念


### 1.1 什么是Cgroups


**🔸 核心定义**
```
Cgroups（Control Groups）：Linux内核提供的资源控制机制
作用：限制、记录和隔离进程组所使用的系统资源
目标：防止某个进程组消耗过多系统资源，影响其他进程
```

**💡 通俗理解**
想象你管理一个公司的办公室：
- **传统方式**：员工想用多少资源就用多少（CPU、内存、网络）
- **Cgroups方式**：给每个部门分配固定的资源配额，不能超出

### 1.2 Cgroups的工作原理


**🏗️ 基本架构**
```
系统资源池
    ├── CPU资源控制
    ├── 内存资源控制  
    ├── 磁盘I/O控制
    ├── 网络带宽控制
    ├── 进程数量控制
    └── 设备访问控制
```

**⚙️ 工作机制**
- **分组管理**：将进程按需求分组，每组有独立的资源限制
- **层次结构**：支持父子关系，子组继承父组的限制
- **实时控制**：动态调整资源分配，立即生效
- **精确统计**：实时监控各组资源使用情况

### 1.3 Cgroups文件系统结构


**📂 目录结构展示**
```
/sys/fs/cgroup/
├── blkio/          ← 磁盘I/O控制
├── cpu/            ← CPU使用控制
├── cpuset/         ← CPU和内存节点分配
├── devices/        ← 设备访问控制
├── freezer/        ← 进程冻结控制
├── memory/         ← 内存使用控制
├── net_cls/        ← 网络分类标记
├── net_prio/       ← 网络优先级控制
└── pids/           ← 进程数量限制
```

**🔍 每个子系统的作用**

| 子系统 | **作用说明** | **控制资源** |
|--------|-------------|-------------|
| `cpu` | 限制CPU使用时间 | CPU时间片分配 |
| `memory` | 限制内存使用量 | 物理内存、交换分区 |
| `blkio` | 限制磁盘I/O | 读写速度、IOPS |
| `devices` | 控制设备访问 | 字符设备、块设备 |
| `pids` | 限制进程数量 | 进程和线程总数 |
| `net_cls` | 网络流量分类 | 网络带宽控制 |

---

## 2. ⚡ CPU资源限制管理


### 2.1 CPU控制基本概念


**🎯 CPU限制的两种方式**
```
权重分配模式（cpu.shares）：
- 相对权重：不是绝对限制，而是相对优先级
- 竞争环境：只在CPU繁忙时生效
- 适用场景：多个应用公平竞争CPU资源

配额限制模式（cpu.cfs_quota_us）：
- 绝对限制：严格限制CPU使用时间
- 固定上限：无论系统负载如何都不能超出
- 适用场景：需要精确控制CPU使用量
```

### 2.2 CPU权重分配（cpu.shares）


**🔸 基本原理**
cpu.shares就像分蛋糕的权重：
- 进程组A设置1024，进程组B设置512
- 当CPU繁忙时，A获得2/3的CPU时间，B获得1/3
- 当CPU空闲时，任何组都可以使用全部CPU

**⚙️ 配置示例**
```bash
# 创建两个CPU控制组
mkdir /sys/fs/cgroup/cpu/group_high
mkdir /sys/fs/cgroup/cpu/group_low

# 设置不同的CPU权重
echo 1024 > /sys/fs/cgroup/cpu/group_high/cpu.shares  # 高优先级
echo 512 > /sys/fs/cgroup/cpu/group_low/cpu.shares    # 低优先级

# 将进程添加到对应组
echo $PID > /sys/fs/cgroup/cpu/group_high/cgroup.procs
```

**📊 权重分配计算**
```
实际分配比例 = 本组权重 / 所有组权重总和

示例：
组A权重：1024
组B权重：512  
组C权重：256
总权重：1792

组A获得：1024/1792 = 57.1% CPU时间
组B获得：512/1792 = 28.6% CPU时间  
组C获得：256/1792 = 14.3% CPU时间
```

### 2.3 CPU配额限制（CFS配额）


**🔸 CFS配额机制**
```
cpu.cfs_period_us：调度周期（微秒），默认100000（100毫秒）
cpu.cfs_quota_us：在一个周期内允许使用的CPU时间

计算公式：
CPU使用率 = cpu.cfs_quota_us / cpu.cfs_period_us
```

**💡 配额设置实例**
```bash
# 限制某个组只能使用50%的CPU
echo 100000 > /sys/fs/cgroup/cpu/myapp/cpu.cfs_period_us  # 100ms周期
echo 50000 > /sys/fs/cgroup/cpu/myapp/cpu.cfs_quota_us    # 50ms配额

# 限制使用1.5个CPU核心
echo 100000 > /sys/fs/cgroup/cpu/myapp/cpu.cfs_period_us  # 100ms周期  
echo 150000 > /sys/fs/cgroup/cpu/myapp/cpu.cfs_quota_us   # 150ms配额
```

**⚠️ 配额限制的特点**
- **严格限制**：即使系统CPU空闲，也不能超过配额
- **精确控制**：可以精确到微秒级别的控制
- **多核支持**：配额可以超过100000，表示使用多个CPU核心

### 2.4 CPU亲和性控制（cpuset）


**🔸 CPU绑定功能**
```bash
# 只允许使用CPU 0和1
echo 0-1 > /sys/fs/cgroup/cpuset/myapp/cpuset.cpus

# 只允许使用CPU 0,2,4（偶数核心）  
echo 0,2,4 > /sys/fs/cgroup/cpuset/myapp/cpuset.cpus

# 绑定到特定的内存节点（NUMA系统）
echo 0 > /sys/fs/cgroup/cpuset/myapp/cpuset.mems
```

---

## 3. 💾 内存资源限制控制


### 3.1 内存限制基本概念


**🔸 内存控制的重要性**
内存不足的后果比CPU不足更严重：
- **CPU不足**：程序变慢，但还能运行
- **内存不足**：程序崩溃，或者触发OOM杀手

**📋 内存限制的类型**
```
物理内存限制（memory.limit_in_bytes）：
- 限制进程组能使用的物理内存总量
- 包括匿名页面和页缓存
- 超出限制会触发内存回收或OOM

交换分区限制（memory.memsw.limit_in_bytes）：
- 限制物理内存+交换分区的总和
- 防止进程无限使用交换分区
- 需要同时设置物理内存限制
```

### 3.2 物理内存限制设置


**💡 基本配置方法**
```bash
# 限制内存使用为512MB
echo 536870912 > /sys/fs/cgroup/memory/myapp/memory.limit_in_bytes

# 使用便捷单位（推荐方式）
echo 512M > /sys/fs/cgroup/memory/myapp/memory.limit_in_bytes
echo 2G > /sys/fs/cgroup/memory/myapp/memory.limit_in_bytes

# 查看当前内存使用情况
cat /sys/fs/cgroup/memory/myapp/memory.usage_in_bytes
```

**🔍 内存单位换算**
| 单位 | **字节数** | **说明** |
|------|-----------|---------|
| `K` | 1024 | 1KB = 1024字节 |
| `M` | 1024K | 1MB = 1024KB |  
| `G` | 1024M | 1GB = 1024MB |
| `T` | 1024G | 1TB = 1024GB |

### 3.3 内存+交换分区限制


**⚙️ 交换分区控制**
```bash
# 设置物理内存限制为1GB
echo 1G > /sys/fs/cgroup/memory/myapp/memory.limit_in_bytes

# 设置内存+交换总限制为1.5GB（即交换分区最多用0.5GB）
echo 1536M > /sys/fs/cgroup/memory/myapp/memory.memsw.limit_in_bytes
```

**📊 内存使用策略**
```
策略1：禁用交换分区
memory.limit_in_bytes = 1G
memory.memsw.limit_in_bytes = 1G
结果：只能使用1GB物理内存，不能使用交换分区

策略2：允许适量交换
memory.limit_in_bytes = 1G  
memory.memsw.limit_in_bytes = 1.5G
结果：物理内存1GB，交换分区最多0.5GB

策略3：无限制交换（不推荐）
memory.limit_in_bytes = 1G
memory.memsw.limit_in_bytes = 不设置
结果：物理内存1GB，交换分区无限制
```

### 3.4 OOM控制和内存回收


**⚠️ OOM控制机制**
```bash
# 控制OOM行为
echo 0 > /sys/fs/cgroup/memory/myapp/memory.oom_control  # 启用OOM killer
echo 1 > /sys/fs/cgroup/memory/myapp/memory.oom_control  # 禁用OOM killer

# 查看OOM事件统计
cat /sys/fs/cgroup/memory/myapp/memory.oom_control
```

**🔄 内存回收策略**
当内存接近限制时，系统的处理顺序：
1. **页面回收**：清理页缓存和不常用页面
2. **交换写入**：将部分内存写入交换分区
3. **OOM终止**：如果还是不够，终止进程

---

## 4. 💽 磁盘I/O限制配置


### 4.1 磁盘I/O控制基本概念


**🔸 为什么需要I/O控制**
磁盘I/O是系统的瓶颈资源：
- 一个进程的大量I/O操作会影响整个系统性能
- 需要公平分配I/O带宽，避免I/O饥饿
- 对不同优先级的应用提供差异化服务

**📋 I/O控制的两个维度**
```
带宽限制：
- 控制每秒读写的字节数
- 适用于连续I/O场景（如文件备份）

IOPS限制：
- 控制每秒I/O操作次数  
- 适用于随机I/O场景（如数据库）
```

### 4.2 I/O权重分配


**⚙️ 权重控制配置**
```bash
# 设置I/O权重（100-1000，默认500）
echo 800 > /sys/fs/cgroup/blkio/high_priority/blkio.weight        # 高优先级
echo 200 > /sys/fs/cgroup/blkio/low_priority/blkio.weight         # 低优先级

# 针对特定设备设置权重
echo "8:0 600" > /sys/fs/cgroup/blkio/myapp/blkio.weight_device  # 对sda设置权重600
```

**🔍 设备号识别方法**
```bash
# 查看设备的主次设备号
ls -l /dev/sda  # 结果：brw-rw---- 1 root disk 8, 0 ...
                # 8是主设备号，0是次设备号

# 或者查看所有块设备
lsblk
cat /proc/partitions
```

### 4.3 I/O速度限制


**🚀 读写速度限制**
```bash
# 限制读速度为10MB/s（设备8:0即/dev/sda）
echo "8:0 10485760" > /sys/fs/cgroup/blkio/myapp/blkio.throttle.read_bps_device

# 限制写速度为5MB/s  
echo "8:0 5242880" > /sys/fs/cgroup/blkio/myapp/blkio.throttle.write_bps_device

# 限制读IOPS为1000次/秒
echo "8:0 1000" > /sys/fs/cgroup/blkio/myapp/blkio.throttle.read_iops_device

# 限制写IOPS为500次/秒
echo "8:0 500" > /sys/fs/cgroup/blkio/myapp/blkio.throttle.write_iops_device
```

**📊 I/O限制类型对比**

| 限制类型 | **适用场景** | **配置文件** | **单位** |
|----------|-------------|-------------|---------|
| 读带宽 | 大文件读取 | `blkio.throttle.read_bps_device` | 字节/秒 |
| 写带宽 | 文件备份写入 | `blkio.throttle.write_bps_device` | 字节/秒 |
| 读IOPS | 数据库查询 | `blkio.throttle.read_iops_device` | 操作/秒 |
| 写IOPS | 日志写入 | `blkio.throttle.write_iops_device` | 操作/秒 |

### 4.4 I/O统计监控


**📈 I/O使用统计**
```bash
# 查看各设备的I/O统计
cat /sys/fs/cgroup/blkio/myapp/blkio.io_service_bytes
# 8:0 Read 1048576    # 读了1MB数据
# 8:0 Write 2097152   # 写了2MB数据

# 查看I/O操作次数统计  
cat /sys/fs/cgroup/blkio/myapp/blkio.io_serviced
# 8:0 Read 256        # 执行了256次读操作
# 8:0 Write 128       # 执行了128次写操作
```

---

## 5. 🌐 网络带宽限制机制


### 5.1 网络控制基本原理


**🔸 网络控制的挑战**
Cgroups本身不直接限制网络带宽，而是配合其他工具：
- **流量分类**：Cgroups标记网络流量
- **带宽控制**：tc工具执行实际的带宽限制
- **协同工作**：两者结合实现网络资源控制

**🏗️ 网络控制架构**
```
应用程序 → Cgroups标记 → 网络栈 → tc流量控制 → 网络接口
```

### 5.2 网络流量分类


**⚙️ 网络分类标记**
```bash
# 创建网络分类组
mkdir /sys/fs/cgroup/net_cls/high_bandwidth
mkdir /sys/fs/cgroup/net_cls/low_bandwidth  

# 设置分类标记（格式：major:minor）
echo 0x100001 > /sys/fs/cgroup/net_cls/high_bandwidth/net_cls.classid
echo 0x100002 > /sys/fs/cgroup/net_cls/low_bandwidth/net_cls.classid

# 将进程分配到对应组
echo $PID > /sys/fs/cgroup/net_cls/high_bandwidth/cgroup.procs
```

### 5.3 配合TC进行带宽控制


**🚦 TC带宽限制配置**
```bash
# 创建队列规则
tc qdisc add dev eth0 root handle 1: htb default 2

# 创建分类（1Mbps带宽）
tc class add dev eth0 parent 1: classid 1:1 htb rate 1mbit
tc class add dev eth0 parent 1:1 classid 1:2 htb rate 500kbit  # 低优先级
tc class add dev eth0 parent 1:1 classid 1:3 htb rate 500kbit  # 高优先级

# 绑定Cgroups分类标记到TC分类
tc filter add dev eth0 protocol ip parent 1: handle 0x100001 fw classid 1:3
tc filter add dev eth0 protocol ip parent 1: handle 0x100002 fw classid 1:2
```

**📊 网络优先级控制**
```bash
# 设置网络优先级（0-7，越大优先级越高）
echo 5 > /sys/fs/cgroup/net_prio/high_priority/net_prio.prioidx
echo 1 > /sys/fs/cgroup/net_prio/low_priority/net_prio.prioidx
```

---

## 6. 👥 进程数量限制控制


### 6.1 进程数量控制的必要性


**⚠️ 为什么要限制进程数量**
进程数量失控的危害：
- **系统资源耗尽**：每个进程都要消耗内存和文件描述符
- **调度开销增大**：过多进程导致上下文切换频繁
- **fork炸弹攻击**：恶意程序无限创建进程使系统崩溃

**💡 真实场景举例**
```
问题场景：
某个Web应用有bug，每个请求都会创建子进程但不回收
1小时后：系统有10000个僵尸进程
结果：系统无法创建新进程，SSH都连不上

解决方案：
使用pids控制器限制该应用最多创建100个进程
超出限制时新的fork()调用会失败，保护系统
```

### 6.2 进程数量限制配置


**⚙️ 基本配置方法**
```bash
# 创建进程控制组
mkdir /sys/fs/cgroup/pids/myapp

# 限制最多创建50个进程/线程
echo 50 > /sys/fs/cgroup/pids/myapp/pids.max

# 查看当前进程数量
cat /sys/fs/cgroup/pids/myapp/pids.current

# 将进程添加到控制组
echo $PID > /sys/fs/cgroup/pids/myapp/cgroup.procs
```

**📊 进程限制状态监控**
```bash
# 查看进程数量统计
cat /sys/fs/cgroup/pids/myapp/pids.current  # 当前进程数
cat /sys/fs/cgroup/pids/myapp/pids.max      # 最大限制数

# 查看超限事件统计
cat /sys/fs/cgroup/pids/myapp/pids.events
# max 0    # 触发限制的次数
```

### 6.3 进程限制的继承关系


**🏗️ 层次化进程控制**
```
父组限制：100个进程
├── 子组A限制：60个进程
├── 子组B限制：50个进程  
└── 子组C限制：30个进程

实际限制：
- 子组A最多用60个进程（受父组100个限制）
- 子组B最多用50个进程  
- 子组C最多用30个进程
- 三个子组总和不能超过父组的100个限制
```

---

## 7. 🔒 设备访问权限控制


### 7.1 设备控制基本概念


**🔸 设备访问控制的作用**
在多用户或容器环境中，需要控制进程对设备的访问：
- **安全隔离**：防止恶意程序访问敏感设备
- **权限管理**：不同进程组有不同的设备访问权限  
- **资源保护**：避免多个进程同时访问同一设备造成冲突

**📋 设备类型分类**
```
字符设备（c）：
- 键盘、鼠标、串口等
- 按字符流方式访问
- 例：/dev/tty, /dev/random

块设备（b）：  
- 硬盘、光驱、U盘等
- 按数据块方式访问
- 例：/dev/sda, /dev/cdrom
```

### 7.2 设备访问权限配置


**⚙️ 设备权限设置**
```bash
# 查看设备的设备号
ls -l /dev/sda1  # brw-rw---- 1 root disk 8, 1
# 8是主设备号，1是次设备号，b表示块设备

# 允许访问特定设备（格式：类型 主设备号:次设备号 权限）
echo "c 1:3 rwm" > /sys/fs/cgroup/devices/myapp/devices.allow
# c=字符设备，1:3是/dev/null，rwm=读写创建权限

# 禁止访问特定设备
echo "b 8:0 rwm" > /sys/fs/cgroup/devices/myapp/devices.deny
# 禁止访问/dev/sda（主硬盘）
```

**🔍 权限字符说明**
| 权限 | **含义** | **说明** |
|------|---------|---------|
| `r` | 读权限 | 可以读取设备数据 |
| `w` | 写权限 | 可以写入设备数据 |
| `m` | 创建权限 | 可以创建设备节点 |
| `rwm` | 全部权限 | 读写创建权限都有 |

### 7.3 设备白名单机制


**🛡️ 白名单控制策略**
```bash
# 设置为白名单模式（默认拒绝所有设备访问）
echo 'a' > /sys/fs/cgroup/devices/myapp/devices.deny

# 然后只允许必要的设备
echo "c 1:3 rwm" > /sys/fs/cgroup/devices/myapp/devices.allow   # /dev/null
echo "c 1:5 rwm" > /sys/fs/cgroup/devices/myapp/devices.allow   # /dev/zero  
echo "c 1:7 rwm" > /sys/fs/cgroup/devices/myapp/devices.allow   # /dev/full
echo "c 5:0 rwm" > /sys/fs/cgroup/devices/myapp/devices.allow   # /dev/tty
echo "c 1:8 rwm" > /sys/fs/cgroup/devices/myapp/devices.allow   # /dev/random
echo "c 1:9 rwm" > /sys/fs/cgroup/devices/myapp/devices.allow   # /dev/urandom
```

**📋 常用设备权限模板**

| 设备路径 | **设备号** | **类型** | **用途** |
|---------|-----------|---------|---------|
| `/dev/null` | c 1:3 | 字符 | 空设备，丢弃数据 |
| `/dev/zero` | c 1:5 | 字符 | 零设备，产生零字节 |
| `/dev/random` | c 1:8 | 字符 | 随机数生成器 |
| `/dev/tty` | c 5:0 | 字符 | 控制终端 |
| `/dev/sda` | b 8:0 | 块 | 第一块硬盘 |

---

## 8. 📊 资源监控与统计


### 8.1 统计数据的重要性


**🎯 为什么要监控资源使用**
- **容量规划**：了解应用的真实资源需求
- **性能调优**：发现资源瓶颈，优化配置
- **故障诊断**：通过异常指标定位问题
- **成本控制**：合理分配资源，避免浪费

### 8.2 CPU使用统计


**📈 CPU统计信息**
```bash
# CPU使用时间统计（纳秒为单位）
cat /sys/fs/cgroup/cpu/myapp/cpuacct.usage
# 结果：1234567890123  (表示使用了1234秒的CPU时间)

# 分核心CPU使用统计
cat /sys/fs/cgroup/cpu/myapp/cpuacct.usage_percpu  
# 结果：500000000000 600000000000 400000000000 300000000000
# 表示4个CPU核心分别使用了500s、600s、400s、300s

# 用户态和系统态时间统计
cat /sys/fs/cgroup/cpu/myapp/cpuacct.stat
# user 50000      # 用户态CPU时间（单位：USER_HZ）
# system 30000    # 内核态CPU时间
```

**💡 CPU使用率计算方法**
```bash
# 计算CPU使用率的步骤
# 1. 记录开始时间和CPU时间
start_time=$(date +%s)
start_cpu=$(cat /sys/fs/cgroup/cpu/myapp/cpuacct.usage)

# 2. 等待一段时间
sleep 10  

# 3. 记录结束时间和CPU时间
end_time=$(date +%s)
end_cpu=$(cat /sys/fs/cgroup/cpu/myapp/cpuacct.usage)

# 4. 计算使用率
time_diff=$((end_time - start_time))
cpu_diff=$((end_cpu - start_cpu))
cpu_usage=$((cpu_diff / 1000000000 * 100 / time_diff))
echo "CPU使用率: ${cpu_usage}%"
```

### 8.3 内存使用统计


**💾 内存统计详细信息**
```bash
# 内存使用详细统计
cat /sys/fs/cgroup/memory/myapp/memory.stat

# 主要指标解释：
# cache 104857600              # 页缓存使用量（100MB）
# rss 52428800                 # 物理内存使用量（50MB）  
# swap 10485760                # 交换分区使用量（10MB）
# pgfault 1000                 # 页错误次数
# pgmajfault 50                # 主页错误次数（需要磁盘I/O）

# 当前内存使用量
cat /sys/fs/cgroup/memory/myapp/memory.usage_in_bytes

# 内存使用峰值
cat /sys/fs/cgroup/memory/myapp/memory.max_usage_in_bytes
```

**📊 内存统计指标含义**

| 指标 | **含义** | **说明** |
|------|---------|---------|
| `rss` | 物理内存 | 进程实际占用的物理内存 |
| `cache` | 页缓存 | 文件系统缓存占用的内存 |
| `swap` | 交换分区 | 被交换到磁盘的内存量 |
| `pgfault` | 页错误 | 访问未在内存中的页面次数 |
| `oom_kill` | OOM终止 | 因内存不足被终止的次数 |

### 8.4 I/O使用统计


**💽 磁盘I/O统计信息**
```bash
# I/O字节数统计
cat /sys/fs/cgroup/blkio/myapp/blkio.io_service_bytes_recursive
# 8:0 Read 1073741824    # 从/dev/sda读取了1GB数据
# 8:0 Write 536870912    # 向/dev/sda写入了512MB数据
# Total Read 1073741824  # 总读取量
# Total Write 536870912  # 总写入量

# I/O操作次数统计  
cat /sys/fs/cgroup/blkio/myapp/blkio.io_serviced_recursive
# 8:0 Read 262144        # 执行了262144次读操作
# 8:0 Write 131072       # 执行了131072次写操作

# I/O等待时间统计
cat /sys/fs/cgroup/blkio/myapp/blkio.io_wait_time_recursive
# 8:0 Read 5000000000    # 读操作等待了5秒
# 8:0 Write 2000000000   # 写操作等待了2秒
```

---

## 9. 🎯 资源限制策略设计


### 9.1 策略设计基本原则


**🔸 设计策略的考虑因素**
- **业务优先级**：核心业务获得更多资源保障
- **资源特性**：不同资源有不同的影响和限制方式  
- **系统容量**：总资源量和预留量的平衡
- **突发处理**：如何处理资源需求的临时峰值

**💡 分层策略设计**
```
系统级别（顶层）：
├── 核心服务组（60%资源）
│   ├── 数据库服务（30%）
│   ├── Web服务（20%）  
│   └── 缓存服务（10%）
├── 普通服务组（30%资源）
│   ├── 后台任务（15%）
│   └── 监控服务（15%）
└── 开发测试组（10%资源）
    ├── 开发环境（5%）
    └── 测试环境（5%）
```

### 9.2 不同应用类型的限制策略


**🔧 Web应用限制策略**
```bash
# Web应用特点：CPU中等，内存稳定，I/O较少，需要快速响应
mkdir /sys/fs/cgroup/{cpu,memory,pids}/webapp

# CPU：使用权重分配，允许突发
echo 1024 > /sys/fs/cgroup/cpu/webapp/cpu.shares

# 内存：设置合理限制，预留20%buffer
echo 1G > /sys/fs/cgroup/memory/webapp/memory.limit_in_bytes
echo 1G > /sys/fs/cgroup/memory/webapp/memory.memsw.limit_in_bytes  # 禁用swap

# 进程：限制进程数防止fork炸弹
echo 100 > /sys/fs/cgroup/pids/webapp/pids.max
```

**🗄️ 数据库限制策略**
```bash
# 数据库特点：CPU高，内存大，I/O密集，稳定性要求高
mkdir /sys/fs/cgroup/{cpu,memory,blkio}/database

# CPU：预留专用CPU核心
echo 2-5 > /sys/fs/cgroup/cpuset/database/cpuset.cpus  # 绑定CPU 2-5

# 内存：分配大内存，启用swap缓冲
echo 4G > /sys/fs/cgroup/memory/database/memory.limit_in_bytes
echo 5G > /sys/fs/cgroup/memory/database/memory.memsw.limit_in_bytes

# I/O：高权重保证I/O性能
echo 800 > /sys/fs/cgroup/blkio/database/blkio.weight
```

**🔄 批处理任务限制策略**
```bash
# 批处理特点：资源需求大但不紧急，可以在空闲时运行
mkdir /sys/fs/cgroup/{cpu,memory,blkio}/batch

# CPU：低权重，不影响其他服务
echo 200 > /sys/fs/cgroup/cpu/batch/cpu.shares

# 内存：限制使用量，允许使用swap
echo 2G > /sys/fs/cgroup/memory/batch/memory.limit_in_bytes  
echo 4G > /sys/fs/cgroup/memory/batch/memory.memsw.limit_in_bytes

# I/O：限制带宽，避免影响其他应用
echo "8:0 50000000" > /sys/fs/cgroup/blkio/batch/blkio.throttle.read_bps_device   # 50MB/s
echo "8:0 30000000" > /sys/fs/cgroup/blkio/batch/blkio.throttle.write_bps_device  # 30MB/s
```

### 9.3 动态调整策略


**⚙️ 基于时间的动态调整**
```bash
#!/bin/bash
# 白天增加Web服务资源，晚上增加批处理资源

current_hour=$(date +%H)

if [ $current_hour -ge 9 ] && [ $current_hour -le 18 ]; then
    # 工作时间：Web服务优先
    echo 2048 > /sys/fs/cgroup/cpu/webapp/cpu.shares      # Web高权重
    echo 200 > /sys/fs/cgroup/cpu/batch/cpu.shares        # 批处理低权重
    echo "调整为白天模式：Web服务优先"
else  
    # 非工作时间：批处理优先
    echo 512 > /sys/fs/cgroup/cpu/webapp/cpu.shares       # Web低权重  
    echo 1024 > /sys/fs/cgroup/cpu/batch/cpu.shares       # 批处理高权重
    echo "调整为夜间模式：批处理优先"
fi
```

**📊 基于负载的自动调整**
```bash
#!/bin/bash
# 根据系统负载动态调整资源限制

load_avg=$(uptime | awk '{print $10}' | sed 's/,//')
load_num=$(echo $load_avg | awk -F. '{print $1}')

if [ $load_num -gt 4 ]; then
    # 高负载：严格限制资源
    echo 1G > /sys/fs/cgroup/memory/webapp/memory.limit_in_bytes
    echo 30000000 > /sys/fs/cgroup/blkio/batch/blkio.throttle.write_bps_device  # 30MB/s
    echo "高负载模式：严格限制资源"
elif [ $load_num -lt 2 ]; then  
    # 低负载：放松资源限制
    echo 2G > /sys/fs/cgroup/memory/webapp/memory.limit_in_bytes
    echo 100000000 > /sys/fs/cgroup/blkio/batch/blkio.throttle.write_bps_device  # 100MB/s
    echo "低负载模式：放松资源限制"
fi
```

### 9.4 限制策略最佳实践


**✅ 策略设计要点**

1. **渐进式限制**
   ```
   不要一开始就设置很严格的限制
   从宽松开始 → 监控使用情况 → 逐步收紧 → 找到最优配置
   ```

2. **预留系统资源**
   ```
   总是为系统进程预留足够资源
   应用资源分配不要超过总量的80-90%
   预留资源用于系统管理和紧急处理
   ```

3. **监控和告警**
   ```
   设置资源使用率告警（如内存使用>80%）
   记录资源限制触发事件
   定期分析资源使用趋势，调整策略
   ```

4. **容错和恢复**
   ```
   设计资源不足时的降级策略
   准备资源限制的快速回滚方案
   测试极限场景下的系统行为
   ```

---

## 10. 📋 核心要点总结


### 10.1 必须掌握的核心概念


```
🔸 Cgroups本质：Linux内核提供的资源控制和隔离机制
🔸 控制方式：通过文件系统接口进行配置和监控  
🔸 资源类型：CPU、内存、I/O、网络、进程数、设备访问
🔸 层次结构：支持父子组关系，资源限制可以继承
🔸 实时生效：配置修改立即生效，无需重启进程
```

### 10.2 关键理解要点


**🔹 CPU控制的两种模式**
```
权重模式（cpu.shares）：
- 相对优先级，适合资源竞争场景
- 系统空闲时可以使用全部CPU
- 类似"按需分配，有多余就给更多"

配额模式（cpu.cfs_quota_us）：
- 绝对限制，严格控制CPU使用量  
- 即使系统空闲也不能超过限制
- 类似"每月给你固定工资，花完就没了"
```

**🔹 内存控制的重要性**
```
内存不足比CPU不足更危险：
- CPU不足：程序变慢但能运行
- 内存不足：程序崩溃或系统死机
- 必须合理设置内存限制和OOM策略
```

**🔹 I/O控制的复杂性**
```
带宽 vs IOPS：
- 带宽控制：适合大文件顺序读写（备份、日志）
- IOPS控制：适合小文件随机访问（数据库、缓存）
- 需要根据应用特点选择合适的控制方式
```

### 10.3 实际应用价值


**💼 容器技术基础**
- Docker和Kubernetes底层就是基于Cgroups实现资源限制
- 理解Cgroups有助于优化容器性能和故障排查
- 掌握手工配置有助于理解容器编排的原理

**🏢 多租户系统管理**  
- 在共享服务器上为不同用户/应用分配资源
- 防止单个应用消耗过多资源影响其他应用
- 实现按需付费和资源配额管理

**⚡ 性能调优和故障诊断**
- 通过资源统计数据分析系统瓶颈
- 设计合理的资源限制策略提升系统稳定性
- 在系统负载过高时快速定位问题应用

### 10.4 学习要点建议


**🎯 动手实践**
```bash
# 建议的学习步骤
1. 创建测试cgroup组，熟悉文件系统接口
2. 编写消耗资源的测试程序，观察限制效果
3. 使用监控工具查看资源使用统计
4. 设计不同场景的资源限制策略
5. 结合Docker等工具理解实际应用
```

**📚 延伸学习**
- **Systemd集成**：现代Linux发行版通过systemd管理cgroups
- **cgroups v2**：新版本的改进和差异
- **容器编排**：Kubernetes如何使用cgroups进行资源管理
- **性能监控**：结合Prometheus等工具监控cgroups指标

**核心记忆口诀**：
- Cgroups控资源，CPU内存I/O全管
- 权重配额两模式，按需分配或固定限
- 层次继承父子组，监控统计助调优
- 容器基础必须懂，多租管理离不开