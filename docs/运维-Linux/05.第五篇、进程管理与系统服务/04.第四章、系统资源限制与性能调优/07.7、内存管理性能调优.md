---
title: 7、内存管理性能调优
---
## 📚 目录

1. [虚拟内存管理概述](#1-虚拟内存管理概述)
2. [交换倾向控制(vm.swappiness)](#2-交换倾向控制vm-swappiness)
3. [脏页回写控制参数](#3-脏页回写控制参数)
4. [OOM Killer行为调整](#4-oom-killer行为调整)
5. [透明大页配置优化](#5-透明大页配置优化)
6. [NUMA内存分配策略](#6-numa内存分配策略)
7. [共享内存段限制参数](#7-共享内存段限制参数)
8. [内存过量分配控制](#8-内存过量分配控制)
9. [核心要点总结](#9-核心要点总结)

---

## 1. 🧠 虚拟内存管理概述


### 1.1 什么是虚拟内存管理


**简单理解**：虚拟内存就像一个"魔术箱"，让系统能够使用比实际物理内存更多的内存空间。

```
物理内存现状：          虚拟内存机制：
实际RAM: 8GB     →     可用空间: 8GB RAM + 交换文件
程序需要: 12GB          程序以为有: 12GB连续内存
结果: 内存不足          结果: 正常运行
```

**核心作用**：
- **内存扩展**：通过交换文件扩大可用内存
- **内存保护**：每个进程都有独立的内存空间
- **内存共享**：相同程序可以共享代码段
- **按需加载**：只加载当前需要的内存页

### 1.2 虚拟内存的工作机制


```
用户程序请求内存
        ↓
    虚拟内存管理器
        ↓
    检查物理内存是否足够
        ↓
   足够？ → 直接分配物理内存
        ↓ 不足够
    将不活跃页面写入交换文件
        ↓
    释放物理内存给新请求
        ↓
    需要时再从交换文件读回
```

### 1.3 查看虚拟内存状态


```bash
# 查看内存使用情况
free -h

# 查看详细虚拟内存统计
cat /proc/meminfo

# 查看当前虚拟内存参数
sysctl -a | grep vm
```

**输出示例解读**：
```bash
$ free -h
              total    used    free  shared  buff/cache   available
Mem:           7.8G    2.1G    1.2G    156M        4.5G        5.2G
Swap:          2.0G      0B    2.0G
```

> 💡 **理解要点**：available比free更重要，它表示真正可用于新程序的内存量

---

## 2. 🔄 交换倾向控制(vm.swappiness)


### 2.1 什么是swappiness


**通俗解释**：swappiness就像系统的"懒惰程度"设置，决定了系统多快会把内存中不常用的数据"搬家"到硬盘上。

```
swappiness = 0    →  极度勤快，绝不轻易使用交换文件
swappiness = 60   →  默认平衡，适度使用交换文件  
swappiness = 100  →  非常懒惰，优先使用交换文件
```

### 2.2 swappiness的影响


**数值含义**：
- **0-10**：尽量不使用swap，优先释放缓存
- **10-60**：平衡使用，根据内存压力调整
- **60-100**：积极使用swap，保留更多缓存

```
高swappiness (90+)：      低swappiness (10-)：
优点: 保留更多文件缓存    优点: 程序运行更快
缺点: 程序响应变慢        缺点: 文件访问可能变慢
适用: 文件服务器          适用: 数据库服务器
```

### 2.3 调优实践


**查看当前值**：
```bash
cat /proc/sys/vm/swappiness
# 通常显示: 60
```

**临时调整**：
```bash
# 设置为较低值，减少swap使用
echo 10 > /proc/sys/vm/swappiness

# 或使用sysctl命令
sysctl vm.swappiness=10
```

**永久设置**：
```bash
# 编辑配置文件
echo 'vm.swappiness = 10' >> /etc/sysctl.conf

# 应用配置
sysctl -p
```

> ⚠️ **调优建议**：
> - **桌面系统**：设置为10-20，保证响应速度
> - **服务器**：设置为5-10，减少磁盘IO
> - **内存充足**：可设置为1，几乎不用swap

### 2.4 不同场景的推荐值


| 使用场景 | 推荐值 | 说明原因 |
|---------|-------|---------|
| **桌面办公** | `10-20` | 保证程序响应速度，用户体验好 |
| **Web服务器** | `5-10` | 减少磁盘IO，提高并发性能 |
| **数据库服务器** | `1-5` | 数据库需要快速内存访问 |
| **文件服务器** | `30-40` | 可以牺牲部分性能保留文件缓存 |
| **内存受限** | `60+` | 物理内存不足时的无奈选择 |

---

## 3. 📄 脏页回写控制参数


### 3.1 什么是脏页


**形象比喻**：脏页就像你在草稿纸上写的笔记，还没有正式写到作业本上。

```
程序写入数据的过程：
1. 程序要保存文件 → 2. 先写入内存(脏页) → 3. 稍后写入磁盘(清洁页)

脏页优点: 写入速度快，程序不用等磁盘
脏页风险: 断电可能丢失，占用内存空间
```

### 3.2 关键参数说明


**vm.dirty_background_ratio**：
```bash
# 含义：后台开始回写脏页的内存百分比阈值
# 默认值通常是10，表示脏页占总内存10%时开始后台写入

# 查看当前值
cat /proc/sys/vm/dirty_background_ratio
```

**vm.dirty_ratio**：
```bash
# 含义：强制同步回写的内存百分比阈值  
# 默认值通常是20，表示脏页占总内存20%时强制等待写入完成

# 查看当前值
cat /proc/sys/vm/dirty_ratio
```

### 3.3 回写机制工作流程


```
内存中脏页增长过程：

脏页比例 < 10% (dirty_background_ratio)
    ↓ 正常状态，程序正常运行
    
脏页比例达到 10%
    ↓ 触发后台回写，但程序仍正常运行
    
脏页比例达到 20% (dirty_ratio)  
    ↓ 强制同步回写，新的写操作必须等待
    
脏页比例降到安全水平
    ↓ 恢复正常运行
```

### 3.4 时间相关参数


**vm.dirty_expire_centisecs**：
```bash
# 含义：脏页多长时间后必须写入磁盘(单位：厘秒)
# 默认：3000 (30秒)
# 作用：防止数据在内存中停留太久

cat /proc/sys/vm/dirty_expire_centisecs
```

**vm.dirty_writeback_centisecs**：
```bash
# 含义：多久检查一次是否有脏页需要回写(单位：厘秒)  
# 默认：500 (5秒)
# 作用：控制检查频率

cat /proc/sys/vm/dirty_writeback_centisecs
```

### 3.5 调优实践案例


**高性能场景优化**：
```bash
# 允许更多脏页积累，减少磁盘IO频率
sysctl vm.dirty_background_ratio=5
sysctl vm.dirty_ratio=15

# 增加检查间隔，减少系统开销
sysctl vm.dirty_writeback_centisecs=1500  # 15秒检查一次
```

**安全性优先场景**：
```bash
# 更积极的回写策略，减少数据丢失风险
sysctl vm.dirty_background_ratio=3
sysctl vm.dirty_ratio=8
sysctl vm.dirty_expire_centisecs=1500     # 15秒必须写入
```

> 🎯 **调优原则**：
> - **高IO性能**：增大比例值，减少回写频率
> - **数据安全**：减小比例值，增加回写频率  
> - **SSD硬盘**：可以设置更激进的回写策略
> - **机械硬盘**：需要平衡性能和磁盘寿命

---

## 4. 💀 OOM Killer行为调整


### 4.1 什么是OOM Killer


**简单理解**：OOM Killer就像系统的"生存游戏裁判"，当内存不够用时，它会选择"牺牲"某些程序来拯救整个系统。

```
内存耗尽情况：
可用内存: ████░░░░░░ (40% 紧张)
可用内存: ██░░░░░░░░ (20% 危险)  
可用内存: █░░░░░░░░░ (10% 极限)
可用内存: ░░░░░░░░░░ (0% 触发OOM)
         ↓
    OOM Killer激活
         ↓
    选择并杀死进程
         ↓
    释放内存，系统继续运行
```

### 4.2 OOM Killer的选择策略


**评分机制**：系统给每个进程打分，分数越高越容易被杀死

```
高分进程(容易被杀)：      低分进程(不容易被杀)：
- 内存占用大的程序        - 系统核心进程  
- 优先级低的进程          - 刚启动的进程
- 运行时间长的进程        - 明确保护的进程
- 普通用户进程            - root用户关键进程

示例：
Web浏览器 (分数高)        systemd (分数极低)
游戏程序 (分数高)         SSH服务 (分数低)
```

### 4.3 控制参数详解


**vm.overcommit_memory**：
```bash
# 查看当前设置
cat /proc/sys/vm/overcommit_memory

# 三种模式：
# 0 = 启发式过量分配(默认)：系统智能判断是否允许分配
# 1 = 总是过量分配：从不拒绝任何内存分配请求
# 2 = 不过量分配：严格按照实际可用内存分配
```

**模式对比**：
```
模式0 (启发式):                模式1 (总是允许):
├─ 智能评估请求               ├─ 从不拒绝分配
├─ 平衡性能和稳定性           ├─ 可能导致OOM
└─ 适合大多数场景             └─ 适合特殊应用

模式2 (严格控制):
├─ 绝不超过实际可用内存
├─ 最大程度避免OOM
└─ 可能拒绝合理的内存请求
```

### 4.4 调整OOM行为


**禁用OOM Killer**：
```bash
# 临时禁用(不推荐)
echo 2 > /proc/sys/vm/panic_on_oom

# 永久设置
echo 'vm.panic_on_oom = 2' >> /etc/sysctl.conf
```

**调整过量分配策略**：
```bash
# 使用严格模式，减少OOM发生
sysctl vm.overcommit_memory=2
sysctl vm.overcommit_ratio=80  # 允许分配物理内存+80%swap
```

**保护重要进程**：
```bash
# 设置进程OOM分数调整值(-1000到1000)
echo -500 > /proc/[PID]/oom_score_adj  # 降低被杀死概率
echo 1000 > /proc/[PID]/oom_score_adj  # 提高被杀死概率
```

### 4.5 监控OOM事件


**查看OOM日志**：
```bash
# 查看系统日志中的OOM记录
dmesg | grep -i "killed process"

# 查看详细的OOM信息
journalctl | grep -i "oom"
```

**示例OOM日志解读**：
```
Out of memory: Kill process 1234 (chrome) score 800 or sacrifice child
Killed process 1234 (chrome) total-vm:2048000kB, anon-rss:1024000kB, file-rss:0kB

解释：
- 进程1234 (chrome浏览器)被杀死
- OOM分数为800 (分数越高越容易被杀)
- 该进程占用了约2GB虚拟内存，1GB物理内存
```

---

## 5. 📈 透明大页配置优化


### 5.1 什么是透明大页(THP)


**通俗解释**：想象内存页面就像书页，正常情况下每页4KB，而大页就像把多页合并成一个大页面。

```
普通页面:               大页面:
页面1: 4KB             大页1: 2MB (相当于512个普通页)
页面2: 4KB             大页2: 2MB  
页面3: 4KB             ...
...                    优势: 管理简单，访问更快
缺点: 管理复杂          缺点: 可能浪费内存
```

### 5.2 THP的优缺点分析


**透明大页优点**：
- **减少页表项**：需要管理的页面数量减少
- **提高TLB效率**：Translation Lookaside Buffer命中率更高
- **减少页错误**：大块内存访问时页错误次数减少

**透明大页缺点**：
- **内存碎片**：可能导致内存浪费
- **分配延迟**：寻找连续大内存块需要时间
- **应用不匹配**：某些应用更适合小页面

### 5.3 THP模式配置


**查看当前状态**：
```bash
cat /sys/kernel/mm/transparent_hugepage/enabled
# 输出示例: [always] madvise never
# [always] = 当前启用状态
```

**三种模式说明**：
```
always:   总是尝试使用大页面
         适用: 大内存应用，科学计算
         
madvise:  只对明确请求的内存使用大页面  
         适用: 混合工作负载，需要精确控制
         
never:    完全禁用透明大页面
         适用: 延迟敏感应用，数据库
```

### 5.4 配置调整实践


**设置不同模式**：
```bash
# 总是启用THP
echo always > /sys/kernel/mm/transparent_hugepage/enabled

# 按需启用THP  
echo madvise > /sys/kernel/mm/transparent_hugepage/enabled

# 完全禁用THP
echo never > /sys/kernel/mm/transparent_hugepage/enabled
```

**控制分配策略**：
```bash
# 查看当前分配策略
cat /sys/kernel/mm/transparent_hugepage/defrag

# 设置分配策略
echo defer > /sys/kernel/mm/transparent_hugepage/defrag  # 延迟整理
echo never > /sys/kernel/mm/transparent_hugepage/defrag  # 从不整理
```

### 5.5 不同应用的THP建议


| 应用类型 | THP设置 | 理由说明 |
|---------|---------|---------|
| **数据库** | `never` | 避免延迟抖动，保证响应时间一致性 |
| **Web服务器** | `madvise` | 平衡性能和灵活性 |
| **科学计算** | `always` | 大量连续内存访问，受益明显 |
| **虚拟化** | `never` | 减少内存管理复杂性 |
| **容器环境** | `madvise` | 避免不必要的内存开销 |

> ⚠️ **重要提醒**：修改THP设置后，建议重启服务以确保设置生效并观察性能变化。

---

## 6. 🏗️ NUMA内存分配策略


### 6.1 什么是NUMA


**简单理解**：NUMA就像一个有多个厨房的大餐厅，每个厨师(CPU)都有自己就近的冰箱(内存)，访问自己的冰箱很快，访问别人的冰箱就比较慢。

```
传统UMA架构:              NUMA架构:
     CPU1  CPU2                 节点0        节点1
        |    |                ┌─────────┐  ┌─────────┐
     ───┴────┴───             │ CPU1    │  │ CPU2    │
        内存                  │ 内存A   │  │ 内存B   │
                             └─────────┘  └─────────┘
                                   │          │
                             快速访问    跨节点访问(慢)
```

### 6.2 检查NUMA状态


**查看NUMA信息**：
```bash
# 检查是否支持NUMA
numactl --hardware

# 查看当前NUMA策略
numactl --show

# 查看内存分布
cat /proc/meminfo | grep -i numa
```

**输出示例解读**：
```bash
$ numactl --hardware
available: 2 nodes (0-1)         # 有2个NUMA节点
node 0 cpus: 0 1 2 3            # 节点0包含CPU 0-3
node 0 size: 8192 MB            # 节点0有8GB内存
node 1 cpus: 4 5 6 7            # 节点1包含CPU 4-7  
node 1 size: 8192 MB            # 节点1有8GB内存
```

### 6.3 NUMA内存分配策略


**四种基本策略**：
```
default (默认):
└─ 优先使用本地节点内存，本地不足时使用远程节点

bind (绑定):  
└─ 只能使用指定节点的内存，内存不足时分配失败

interleave (交替):
└─ 在多个节点间均匀分布内存页面

preferred (偏好):
└─ 首选指定节点，不足时可使用其他节点
```

### 6.4 应用NUMA策略


**为特定程序设置策略**：
```bash
# 将程序绑定到节点0
numactl --cpunodebind=0 --membind=0 ./my_program

# 使用交替策略运行程序
numactl --interleave=all ./my_program  

# 偏好使用节点1的内存
numactl --preferred=1 ./my_program
```

**系统级NUMA调优**：
```bash
# 查看NUMA平衡状态
cat /proc/sys/kernel/numa_balancing

# 启用自动NUMA平衡
echo 1 > /proc/sys/kernel/numa_balancing

# 禁用自动NUMA平衡  
echo 0 > /proc/sys/kernel/numa_balancing
```

### 6.5 NUMA调优实践


**数据库服务器优化**：
```bash
# 禁用NUMA平衡，减少内存迁移开销
echo 0 > /proc/sys/kernel/numa_balancing

# 设置内存分配策略为交替模式
numactl --interleave=all mysqld
```

**Web服务器优化**：
```bash
# 启用NUMA平衡，让系统自动优化
echo 1 > /proc/sys/kernel/numa_balancing

# 根据CPU绑定内存
numactl --cpunodebind=0 --membind=0 nginx
```

> 💡 **调优建议**：
> - **内存密集型应用**：使用bind策略，避免跨节点访问
> - **CPU密集型应用**：使用interleave策略，平衡内存带宽
> - **小型应用**：使用默认策略，让系统自动管理

---

## 7. 🤝 共享内存段限制参数


### 7.1 什么是共享内存


**生活比喻**：共享内存就像几个室友共用的冰箱，大家都可以往里放东西、取东西，这样比每人买一个冰箱更节省空间。

```
传统进程间通信:               共享内存通信:
进程A ─[复制数据]→ 管道 ─[复制数据]→ 进程B    进程A ──┐
                                                  │
缺点: 数据复制，速度慢，占用内存多               ├─ 共享内存区域
                                                  │
                                              进程B ──┘
                                              优点: 直接访问，速度快
```

### 7.2 共享内存相关参数


**kernel.shmmax**：
```bash
# 含义：单个共享内存段的最大大小(字节)
# 默认值通常很小，需要根据应用调整

# 查看当前值
cat /proc/sys/kernel/shmmax
# 或
sysctl kernel.shmmax
```

**kernel.shmall**：
```bash
# 含义：系统中所有共享内存段的总页数限制
# 计算方法：总字节数 ÷ 页面大小(通常4KB)

# 查看当前值  
cat /proc/sys/kernel/shmall
```

**kernel.shmmni**：
```bash
# 含义：系统中共享内存段的最大数量
# 默认值：4096 (通常够用)

# 查看当前值
cat /proc/sys/kernel/shmmni
```

### 7.3 参数计算与调优


**计算合适的值**：
```bash
# 假设你的应用需要2GB共享内存
应用需求: 2GB = 2 * 1024 * 1024 * 1024 = 2147483648 bytes

# 设置shmmax (单个段最大值)
sysctl kernel.shmmax=2147483648

# 计算shmall (总页数，页面大小通常是4KB)
总内存需求: 2GB
页面大小: 4KB = 4096 bytes  
需要页数: 2147483648 ÷ 4096 = 524288 pages

# 设置shmall
sysctl kernel.shmall=524288
```

**数据库场景调优示例**：
```bash
# MySQL/PostgreSQL等数据库通常需要大量共享内存
# 假设系统内存32GB，为数据库分配8GB共享内存

# 设置单个段最大8GB
sysctl kernel.shmmax=8589934592

# 设置总共享内存也是8GB (8GB÷4KB=2097152页)
sysctl kernel.shmall=2097152

# 增加共享内存段数量限制
sysctl kernel.shmmni=8192
```

### 7.4 监控共享内存使用


**查看共享内存使用情况**：
```bash
# 查看当前共享内存段
ipcs -m

# 查看详细信息
ipcs -m -u
```

**输出示例解读**：
```bash
$ ipcs -m
------ Shared Memory Segments --------
key        shmid      owner      perms      bytes      nattch     status
0x00000000 32768      mysql      660        134217728  2          
0x00000000 32769      redis      664        67108864   1          

解释：
- shmid 32768: MySQL使用的共享内存段，128MB大小，2个进程附加
- shmid 32769: Redis使用的共享内存段，64MB大小，1个进程附加
```

### 7.5 常见问题解决


**"无法创建共享内存段"错误**：
```bash
# 错误信息：shmget: Invalid argument 或 Cannot allocate memory

# 解决方法：增加相关限制
sysctl kernel.shmmax=4294967296    # 增加单段最大值
sysctl kernel.shmall=1048576       # 增加总页数限制
sysctl kernel.shmmni=8192          # 增加段数量限制

# 永久保存设置
echo 'kernel.shmmax = 4294967296' >> /etc/sysctl.conf
echo 'kernel.shmall = 1048576' >> /etc/sysctl.conf
echo 'kernel.shmmni = 8192' >> /etc/sysctl.conf

# 应用设置
sysctl -p
```

---

## 8. 💰 内存过量分配控制


### 8.1 什么是内存过量分配


**生活比喻**：内存过量分配就像航空公司的超售机票，明知道可能有乘客不来，就卖出比座位数更多的票。

```
内存分配的现实：
程序申请: 需要1GB内存      实际情况: 程序可能只用500MB
系统响应: 好的，给你1GB    系统策略: 先答应，等真用时再说

过量分配的后果：
所有程序都"按需"申请     →    总申请量超过物理内存
大部分程序不会用完       →    通常能正常运行  
某天所有程序都要用       →    内存不足，触发OOM
```

### 8.2 过量分配的三种模式


**模式0 - 启发式过量分配(默认)**：
```bash
# 系统智能评估每个内存分配请求
# 考虑因素：当前内存使用、可用swap、进程历史等

# 适用场景：大多数普通使用场景
# 优点：平衡了性能和稳定性
# 缺点：可能在边缘情况下判断错误
```

**模式1 - 总是允许过量分配**：
```bash
# 从不拒绝任何内存分配请求  
# 所有malloc()调用都返回成功

# 适用场景：特殊应用，fork()操作频繁的场景
# 优点：程序分配内存从不失败
# 缺点：更容易触发OOM Killer
```

**模式2 - 严格不过量分配**：
```bash
# 严格按照实际可用内存分配
# 可用内存 = 物理内存 + swap空间 × overcommit_ratio%

# 适用场景：关键业务系统，不能容忍OOM
# 优点：基本杜绝OOM，内存使用可预测
# 缺点：可能拒绝合理的内存请求
```

### 8.3 相关参数详解


**vm.overcommit_memory**：
```bash
# 查看当前模式
cat /proc/sys/vm/overcommit_memory

# 修改模式
sysctl vm.overcommit_memory=2  # 使用严格模式
```

**vm.overcommit_ratio**：
```bash
# 仅在模式2下有效，控制可用内存的计算方式
# 可用内存 = 物理内存 + (swap大小 × overcommit_ratio / 100)

# 查看当前值
cat /proc/sys/vm/overcommit_ratio

# 示例计算：
# 物理内存：8GB
# Swap空间：2GB  
# overcommit_ratio：50
# 可用内存 = 8GB + (2GB × 50/100) = 9GB
```

### 8.4 不同场景的配置建议


**高可靠性系统配置**：
```bash
# 使用严格模式，预防OOM
sysctl vm.overcommit_memory=2
sysctl vm.overcommit_ratio=80

# 永久保存
echo 'vm.overcommit_memory = 2' >> /etc/sysctl.conf
echo 'vm.overcommit_ratio = 80' >> /etc/sysctl.conf
```

**高性能计算配置**：
```bash
# 允许更多内存分配，配合大swap使用
sysctl vm.overcommit_memory=1
sysctl vm.swappiness=10          # 但减少实际swap使用
```

**容器环境配置**：
```bash
# 平衡模式，让容器运行时处理资源限制
sysctl vm.overcommit_memory=0    # 使用默认启发式模式
```

### 8.5 监控过量分配状态


**查看内存分配统计**：
```bash
# 查看内存提交情况
grep -E "(Commit|Available)" /proc/meminfo

# 输出示例：
# CommitLimit:     8388608 kB  # 系统允许的最大提交内存
# Committed_AS:    2097152 kB  # 当前已提交的内存总量
```

**计算过量分配比例**：
```bash
# 过量分配比例 = Committed_AS / CommitLimit
# 比例 > 1.0 表示存在过量分配
# 比例越高，OOM风险越大

# 示例脚本监控过量分配
#!/bin/bash
committed=$(grep "Committed_AS" /proc/meminfo | awk '{print $2}')
limit=$(grep "CommitLimit" /proc/meminfo | awk '{print $2}') 
ratio=$(echo "scale=2; $committed / $limit" | bc)
echo "过量分配比例: $ratio"
if (( $(echo "$ratio > 0.8" | bc -l) )); then
    echo "警告：内存过量分配比例过高！"
fi
```

---

## 9. 📋 核心要点总结


### 9.1 必须掌握的核心概念


```
🔸 虚拟内存管理：通过交换机制扩展可用内存空间
🔸 swappiness控制：调节系统使用swap的积极程度(0-100)
🔸 脏页回写：控制内存数据写入磁盘的时机和频率
🔸 OOM Killer：内存不足时系统自动杀死进程的机制
🔸 透明大页：自动使用大内存页面提高性能的技术
🔸 NUMA策略：多处理器系统的内存访问优化
🔸 共享内存：进程间高效数据共享的内存区域
🔸 过量分配：系统允许分配超过物理内存的策略
```

### 9.2 关键参数速查表


| 参数 | 推荐值 | 适用场景 | 作用说明 |
|------|--------|----------|----------|
| `vm.swappiness` | `10-20` | 桌面系统 | 减少swap使用，提高响应速度 |
| `vm.swappiness` | `5-10` | 服务器 | 最小化磁盘IO，保证性能 |
| `vm.dirty_ratio` | `15-20` | 通用场景 | 控制强制同步回写阈值 |
| `vm.dirty_background_ratio` | `5-10` | 通用场景 | 控制后台回写开始阈值 |
| `transparent_hugepage` | `never` | 数据库 | 避免延迟抖动 |
| `transparent_hugepage` | `madvise` | Web服务 | 平衡性能和灵活性 |
| `vm.overcommit_memory` | `2` | 关键系统 | 严格控制内存分配 |
| `kernel.shmmax` | `应用需求×2` | 数据库 | 支持大型共享内存段 |

### 9.3 调优实践要点


**🔹 调优三步法**：
```
第一步：监控现状
- 使用free、top、iotop等工具观察系统状态
- 识别性能瓶颈是在内存、IO还是CPU

第二步：渐进调整  
- 一次只调整一个参数
- 小幅度调整，观察效果
- 记录调整前后的性能数据

第三步：长期观察
- 监控系统稳定性
- 关注异常指标和错误日志
- 根据业务变化及时调整
```

**🔹 不同场景的调优重点**：
```
桌面系统：
- 优先保证响应速度: swappiness=10
- 适度脏页回写: dirty_ratio=15
- 关闭透明大页: never

Web服务器：
- 平衡性能和稳定性: swappiness=10
- 积极脏页回写: dirty_background_ratio=5  
- 按需透明大页: madvise

数据库服务器：
- 最少swap使用: swappiness=1
- 大共享内存: shmmax根据需求设置
- 禁用透明大页: never
- NUMA绑定: 减少跨节点访问

高并发服务：
- 严格内存控制: overcommit_memory=2
- 快速脏页回写: 减小dirty参数
- 优化NUMA: 启用自动平衡
```

### 9.4 故障排查清单


**🚨 内存不足问题排查**：
```
1. 检查物理内存使用: free -h
2. 查看swap使用情况: swapon -s  
3. 检查是否有OOM记录: dmesg | grep -i oom
4. 分析内存大户进程: ps aux --sort=-%mem | head
5. 检查内存参数设置: sysctl -a | grep vm
```

**📊 性能问题排查**：
```
1. 监控IO等待: iotop -a
2. 检查脏页状态: grep dirty /proc/meminfo
3. 观察页面错误: vmstat 1
4. 分析swap活动: sar -W 1
5. 检查NUMA平衡: numastat
```

> 💡 **核心记忆口诀**：
> - swappiness控制换页频，数值越小越少换
> - 脏页回写防数据丢，及时写盘保安全  
> - OOM杀手选进程杀，内存不足它出马
> - 大页透明提性能，数据库用需谨慎
> - NUMA绑定访问快，跨节点慢要避免
> - 共享内存进程通，参数设置要够用
> - 过量分配需小心，严格模式更安全