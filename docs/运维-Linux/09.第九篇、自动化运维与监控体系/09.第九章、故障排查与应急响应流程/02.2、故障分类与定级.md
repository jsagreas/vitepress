---
title: 2、故障分类与定级
---
## 📚 目录

1. [故障分类体系](#1-故障分类体系)
2. [影响范围评估](#2-影响范围评估)
3. [紧急程度定级](#3-紧急程度定级)
4. [SLA/SLO指标体系](#4-SLA/SLO指标体系)
5. [故障升级机制](#5-故障升级机制)
6. [通报机制设计](#6-通报机制设计)
7. [故障责任界定](#7-故障责任界定)
8. [核心要点总结](#8-核心要点总结)

---

## 1. 🔍 故障分类体系


### 1.1 为什么要对故障分类


**实际意义**：就像医生看病要先分科室一样，系统故障也需要分类处理。不同类型的故障有不同的紧急程度、处理方式和专业团队。

> 🤔 **生活类比**
> 
> 家里的问题也需要分类：
> - 水管爆了 → 立即找水管工（紧急故障）
> - 灯泡不亮 → 自己换个灯泡（一般故障）  
> - 房子需要装修 → 慢慢计划（优化需求）

### 1.2 系统故障三大分类


#### 🚨 启动故障（System Boot Failures）


**核心含义**：系统开不了机，或者启动过程中卡住了

```
启动故障的典型表现：
┌─────────────────────────────────┐
│ 开机 → 黑屏/蓝屏/卡在启动界面     │
│ 或者 → 进不了登录界面           │  
│ 或者 → 服务启动失败             │
└─────────────────────────────────┘
```

**常见场景**：
- **硬件故障** - 硬盘坏了、内存条松了、主板问题
- **系统文件损坏** - 关键系统文件被删除或损坏
- **配置文件错误** - 启动配置写错了，系统不知道怎么启动
- **服务依赖问题** - A服务需要B服务先启动，但B启动失败了

**影响程度**：★★★ 最严重，整个系统不可用

#### ⚡ 运行故障（Runtime Failures）


**核心含义**：系统能启动，但运行过程中某些功能不正常

```
运行故障的表现：
系统正常运行 → 突然某个功能出问题
                ↓
        用户无法正常使用部分功能
```

**典型例子**：
- **服务崩溃** - 网站能打开，但购物车功能用不了
- **数据库连接失败** - 系统运行正常，但查不到数据
- **网络通信中断** - 服务器内部正常，但外部访问不了
- **权限异常** - 用户登录正常，但某些操作提示权限不够

**影响程度**：★★ 部分功能不可用，但系统基本可用

#### 🐌 性能故障（Performance Issues）


**核心含义**：系统功能都正常，但是运行很慢，用户体验差

```
性能问题的用户感受：
点击按钮 → 等待很久 → 才有反应
          ↓
    用户抱怨系统太慢
```

**常见表现**：
- **响应慢** - 网页打开要等30秒
- **CPU占用高** - 系统卡顿，风扇狂转
- **内存不足** - 程序运行很慢，经常卡住
- **磁盘IO高** - 文件操作很慢，硬盘灯狂闪

**影响程度**：★ 功能可用但体验差，影响工作效率

### 1.3 故障分类的实际应用


| 故障类型 | **紧急程度** | **处理团队** | **响应时间** | **典型处理方式** |
|---------|-------------|-------------|-------------|-----------------|
| 🚨 **启动故障** | `Critical` | `系统管理员+硬件工程师` | `15分钟内` | `立即响应，现场处理` |
| ⚡ **运行故障** | `High/Medium` | `应用开发团队` | `1小时内` | `远程诊断，代码修复` |
| 🐌 **性能故障** | `Medium/Low` | `性能优化团队` | `4小时内` | `性能分析，逐步优化` |

---

## 2. 📊 影响范围评估


### 2.1 影响范围的重要性


**实际意义**：同样的故障，影响1个人和影响1万个人，处理的紧急程度完全不同。

> 💡 **通俗理解**
>
> 就像停电一样：
> - 你家停电 → 找物业，不算紧急
> - 整个小区停电 → 通知供电局，比较紧急  
> - 全城停电 → 启动应急预案，非常紧急

### 2.2 四个影响范围层级


#### 🖥️ 单机故障（Single Machine）


**含义**：只影响一台服务器，其他服务器正常运行

```
集群架构示意：
┌─────┐ ┌─────┐ ┌─────┐
│ 服务器1│ │ 服务器2│ │ 服务器3│
│  正常 │ │  故障 │ │  正常 │
└─────┘ └─────┘ └─────┘
      ↑       ↑       ↑
    负载均衡会自动分流到正常服务器
```

**典型场景**：
- 某台Web服务器硬盘故障
- 单台数据库从库连接异常
- 个别应用服务进程崩溃

**影响评估**：轻微，用户基本感觉不到

#### 🏢 集群故障（Cluster Failure）


**含义**：整个服务集群都出问题，但其他业务系统正常

```
业务系统架构：
┌─────────┐ ┌─────────┐ ┌─────────┐
│  用户系统 │ │  订单系统 │ │  支付系统 │
│   正常  │ │   故障  │ │   正常  │
└─────────┘ └─────────┘ └─────────┘
            ↑
        整个订单集群不可用
```

**典型场景**：
- 订单服务集群全部宕机
- 数据库主从都连不上
- 某个微服务完全不可用

**影响评估**：严重，特定功能完全不可用

#### 🌐 全网故障（Network-wide）


**含义**：整个系统都出问题，所有用户都受影响

```
全系统影响：
用户A → ❌ 无法访问
用户B → ❌ 无法访问  
用户C → ❌ 无法访问
         ↓
    整个平台不可用
```

**典型场景**：
- 核心网络设备故障
- 主数据中心断电
- DNS解析失败
- 核心认证系统崩溃

**影响评估**：灾难级，业务完全停止

#### 💰 业务影响（Business Impact）


**含义**：从业务角度评估，看对公司收入、声誉的影响

**业务影响分级**：

| 影响程度 | **用户感受** | **业务损失** | **处理优先级** |
|---------|-------------|-------------|---------------|
| 🔥 **严重** | `完全无法使用` | `直接收入损失` | `最高优先级` |
| ⚠️ **中等** | `部分功能异常` | `用户体验下降` | `高优先级` |
| ⚪ **轻微** | `偶尔卡顿` | `基本无影响` | `正常处理` |

### 2.3 影响范围评估方法


**快速评估步骤**：

```
故障发现
    ↓
检查影响用户数量 → 确定影响范围
    ↓
评估业务损失程度 → 确定紧急级别
    ↓
制定响应策略 → 开始处理
```

**评估要考虑的因素**：
- **时间因素** - 工作日比周末影响大
- **用户类型** - VIP用户故障优先级更高  
- **业务关键性** - 支付系统比论坛系统更重要
- **持续时间** - 故障持续时间越长影响越大

---

## 3. ⚡ 紧急程度定级


### 3.1 为什么需要定级


**现实问题**：不是所有故障都需要半夜叫醒工程师处理。合理的定级帮助我们：
- 合理分配人力资源
- 避免"狼来了"效应
- 确保真正紧急的问题得到及时处理

### 3.2 四级定级体系


#### 🔴 Critical（关键级）


**定义**：系统完全不可用，严重影响业务，需要立即处理

**典型场景**：
- 主站完全无法访问
- 支付系统全部宕机  
- 数据库主库崩溃
- 安全事件导致数据泄露

**响应要求**：
```
发现时间 → 15分钟内响应
处理团队 → 核心技术团队全员
工作方式 → 24x7随时待命
升级机制 → 自动通知管理层
```

**判断标准**：
- ✅ 系统完全不可用
- ✅ 影响所有用户  
- ✅ 直接造成收入损失
- ✅ 可能影响公司声誉

#### 🟠 High（高级）


**定义**：核心功能异常，影响大部分用户，需要优先处理

**典型场景**：
- 登录功能异常
- 主要业务模块故障
- 性能严重下降
- 部分集群不可用

**响应要求**：
```
发现时间 → 1小时内响应  
处理团队 → 相关技术团队
工作方式 → 工作时间优先处理
升级机制 → 4小时未解决则升级
```

**判断标准**：
- ✅ 核心功能不可用
- ✅ 影响大量用户
- ✅ 有明显业务影响
- ✅ 用户投诉增多

#### 🟡 Medium（中级）


**定义**：部分功能异常，影响有限，可以计划处理

**典型场景**：
- 非核心功能故障
- 个别服务器异常
- 轻微性能问题  
- 部分用户反馈问题

**响应要求**：
```
发现时间 → 4小时内响应
处理团队 → 对应技术人员
工作方式 → 正常工作时间处理  
升级机制 → 影响扩大时升级
```

#### 🟢 Low（低级）


**定义**：轻微问题，影响很小，可以延后处理

**典型场景**：
- 界面显示问题
- 非关键日志错误
- 监控告警但无实际影响
- 优化需求

**响应要求**：
```
发现时间 → 24小时内确认
处理团队 → 相关开发人员
工作方式 → 计划内处理
升级机制 → 一般不升级
```

### 3.3 定级决策流程


```
故障发现
    ↓
是否影响核心业务？ → 是 → Critical/High
    ↓ 否
影响用户数量多少？ → 大量 → High/Medium  
    ↓ 少量
是否有业务损失？ → 有 → Medium
    ↓ 无
    ↓
   Low级别
```

**定级常见误区**：
- ❌ 按技术复杂度定级（技术简单不代表影响小）
- ❌ 按发现人员级别定级（老板发现的不一定是Critical）
- ❌ 按修复时间长短定级（修复快慢与紧急程度无关）
- ✅ 按实际业务影响定级（这是唯一标准）

---

## 4. 📈 SLA/SLO指标体系


### 4.1 什么是SLA和SLO


**SLA（Service Level Agreement）- 服务等级协议**：
> 🤝 **通俗理解**
> 
> 就像你和快递公司的约定：
> - "24小时内必须送达"
> - "丢件按价值3倍赔偿"
> 
> SLA就是和用户的服务承诺

**SLO（Service Level Objective）- 服务等级目标**：
> 🎯 **通俗理解**  
>
> 公司内部的目标：
> - "我们要做到99.9%的时间可用"
> - "响应时间控制在2秒以内"

### 4.2 核心指标体系


#### 📊 可用性指标（Availability）


**含义**：系统能正常提供服务的时间比例

**计算公式**：
```
可用性 = 正常运行时间 / 总时间 × 100%

例如：
一个月720小时，故障2小时
可用性 = (720-2)/720 × 100% = 99.72%
```

**常见可用性等级**：

| 可用性等级 | **年故障时间** | **月故障时间** | **适用场景** |
|-----------|---------------|---------------|-------------|
| 🟢 **99%** | `87.6小时` | `7.2小时` | `一般业务系统` |
| 🟡 **99.9%** | `8.76小时` | `43.2分钟` | `重要业务系统` |
| 🟠 **99.99%** | `52.6分钟` | `4.32分钟` | `核心业务系统` |
| 🔴 **99.999%** | `5.26分钟` | `25.9秒` | `金融交易系统` |

**实际意义**：
- 99%听起来很高，但一年要停机87小时
- 99.99%意味着一年只能停机不到1小时
- 每增加一个9，成本可能翻倍

#### ⚡ 响应时间指标（Response Time）


**含义**：用户发起请求到收到响应的时间

```
用户体验时间感受：
< 100ms  → 感觉即时响应
< 1秒    → 用户不会感到延迟
< 5秒    → 用户开始感到慢
> 10秒   → 用户可能放弃操作
```

**常用响应时间指标**：
- **平均响应时间** - 所有请求的平均值
- **P95响应时间** - 95%的请求响应时间小于这个值
- **P99响应时间** - 99%的请求响应时间小于这个值

**为什么用P95/P99而不是平均值**：
> 📊 **举例说明**
>
> 100个请求：99个用时1秒，1个用时100秒
> - 平均响应时间：(99×1 + 1×100)/100 = 1.99秒
> - P99响应时间：1秒
> 
> 平均值被极值拉高，P99更能反映用户真实体验

#### 🚀 吞吐量指标（Throughput）


**含义**：系统在单位时间内能处理的请求数量

**常用单位**：
- **QPS（Queries Per Second）** - 每秒查询数
- **TPS（Transactions Per Second）** - 每秒事务数  
- **RPS（Requests Per Second）** - 每秒请求数

**业务场景对应**：
- **电商网站** - 双11期间需要承受10万+QPS
- **社交平台** - 热点事件时需要处理百万级请求
- **支付系统** - 需要保证高并发下的数据一致性

### 4.3 指标设定原则


**SMART原则**：
- **S**pecific（具体）- 指标明确具体
- **M**easurable（可测量）- 能够量化监控
- **A**chievable（可实现）- 技术上可达成
- **R**elevant（相关）- 与业务目标相关
- **T**ime-bound（时限）- 有明确时间限制

**设定建议**：
```
新系统：从低目标开始，逐步提升
成熟系统：基于历史数据设定合理目标
关键系统：预留安全边际，不要设得太极限
```

---

## 5. 📢 故障升级机制


### 5.1 为什么需要升级机制


**现实场景**：
- 一线技术人员2小时没解决问题
- 影响范围在扩大，用户投诉增多
- 需要更高级别的资源和决策权

**升级的核心作用**：
- 🎯 确保问题得到足够重视
- 🔧 调动更多技术资源
- 💰 获得更高决策权限
- 📢 扩大信息传播范围

### 5.2 三种升级路径


#### 🔧 内部升级（Technical Escalation）


**含义**：在技术团队内部升级，寻求更高技术能力支持

```
升级路径：
一线工程师 → 高级工程师 → 技术专家 → 架构师
     ↓           ↓           ↓         ↓
 基础排查    深入分析    系统重构    架构决策
```

**升级条件**：
- ✅ 问题超出当前人员技术能力
- ✅ 需要更深层的系统知识
- ✅ 涉及多个系统间的复杂交互
- ✅ 需要架构级别的决策

**典型场景**：
- 复杂的性能问题需要专家分析
- 跨系统的故障需要架构师协调
- 底层系统问题需要内核专家

#### 🏢 外部升级（External Escalation）


**含义**：升级到合作伙伴、供应商或外部专家

```
外部升级场景：
┌────────────┐    升级    ┌────────────┐
│  我们的系统  │ ────────→ │  云服务商   │
│    故障     │          │  技术支持   │
└────────────┘          └────────────┘
```

**升级条件**：
- ✅ 问题出现在第三方服务
- ✅ 需要供应商的内部信息
- ✅ 涉及硬件保修和更换
- ✅ 需要专业厂商技术支持

**注意事项**：
- 📋 准备详细的问题描述和日志
- ⏰ 了解供应商的支持时间
- 💰 确认是否涉及服务费用
- 🔒 注意信息安全和保密要求

#### 👔 管理层升级（Management Escalation）


**含义**：升级到管理层，寻求资源调配和决策支持

```
管理升级场景：
技术问题 → 需要跨部门协调 → 管理层介入
           需要额外预算投入
           影响重要客户关系
```

**升级条件**：
- ✅ 影响重要客户或合作伙伴
- ✅ 需要跨部门资源协调
- ✅ 可能产生法律或合规风险
- ✅ 需要对外正式沟通

### 5.3 升级决策矩阵


| 故障级别 | **技术升级时间** | **外部升级条件** | **管理升级条件** |
|---------|----------------|-----------------|-----------------|
| 🔴 **Critical** | `30分钟` | `涉及第三方立即升级` | `影响重要客户立即升级` |
| 🟠 **High** | `2小时` | `确认第三方问题升级` | `跨部门协调时升级` |
| 🟡 **Medium** | `8小时` | `需要厂商支持时升级` | `影响扩大时升级` |
| 🟢 **Low** | `24小时` | `常规技术咨询` | `一般不升级` |

---

## 6. 📢 通报机制设计


### 6.1 通报的重要性


**为什么要通报**：
- 让相关人员了解情况，做好准备
- 避免重复报告和处理
- 建立透明的沟通机制
- 为后续分析提供信息记录

> 🏥 **医院类比**
> 
> 急诊科发现重大事故：
> - 立即通知相关科室准备
> - 通知家属了解情况  
> - 向院方报告严重程度
> - 必要时通知媒体澄清

### 6.2 三类通报机制


#### 🔒 内部通报（Internal Notification）


**目标受众**：公司内部相关人员

**通报对象**：
- **技术团队** - 开发、运维、测试人员
- **业务团队** - 产品、运营、客服人员  
- **管理层** - 技术总监、业务负责人
- **支持团队** - 法务、公关、HR等

**通报内容模板**：
```
📋 内部故障通报

故障时间：2025-09-17 14:30
故障级别：High
影响范围：用户登录功能
影响用户：约5000人
当前状态：处理中
预计修复：16:00
负责团队：后端开发组
联系人：张工程师 (tel:13800138000)

详细信息：
登录服务因数据库连接池满导致无法处理新请求
已采取措施：重启服务，扩容连接池
风险评估：可能再次发生，需要优化配置
```

**通报频率**：
- Critical级别：每30分钟更新
- High级别：每1小时更新  
- Medium级别：每4小时更新
- 解决后：发送最终通报

#### 👥 客户通报（Customer Communication）


**目标受众**：受影响的用户和客户

**通报原则**：
- **及时性** - 确认影响后尽快通知
- **真实性** - 不夸大也不隐瞒问题
- **解决方案导向** - 重点说明正在做什么
- **避免技术术语** - 用用户能理解的语言

**通报渠道**：
- **官方网站** - 发布服务状态公告
- **APP推送** - 直接通知受影响用户
- **邮件通知** - 发送给注册用户
- **客服热线** - 提供人工咨询服务
- **社交媒体** - 微博、微信公众号等

**客户通报模板**：
```
🔧 服务状态通知

亲爱的用户：

我们发现您可能在登录时遇到问题，对此深表歉意。

问题原因：服务器负载过高导致响应缓慢
影响范围：部分用户登录功能
我们正在：紧急修复中，预计30分钟内恢复
临时方案：您可以稍后重试，或联系客服

我们会持续更新进展，感谢您的耐心等待。

客服热线：400-xxx-xxxx
```

#### 📰 公告发布（Public Announcement）


**适用场景**：影响重大，需要向公众说明的情况

**发布考虑因素**：
- 影响用户数量超过一定阈值
- 故障持续时间较长
- 涉及数据安全或隐私问题  
- 媒体或公众已经关注
- 法规要求必须公开

**公告发布流程**：
```
技术确认故障 → 评估公众影响 → 法务审核内容
      ↓              ↓              ↓
起草公告内容 → 管理层批准 → 多渠道发布
      ↓              ↓              ↓
监控舆情反馈 → 准备后续应对 → 问题解决后发布恢复公告
```

### 6.3 通报时机和频率


**首次通报时机**：
- Critical：发现后15分钟内
- High：发现后1小时内
- Medium：发现后4小时内

**后续更新频率**：
- 有重大进展时立即更新
- 无明显进展时定期更新
- 解决后24小时内发布总结

**通报避免的问题**：
- ❌ 信息不一致（不同渠道说法不同）
- ❌ 过度承诺（承诺无法达成的修复时间）
- ❌ 推卸责任（把问题推给第三方）
- ❌ 技术细节过多（用户不关心技术实现）

---

## 7. ⚖️ 故障责任界定


### 7.1 责任界定的意义


**为什么要界定责任**：
- 🎯 不是为了处罚，而是为了改进
- 📚 总结经验教训，避免重复发生
- 🔧 优化流程和制度
- 💰 合理分配改进资源

> 🚗 **交通事故类比**
> 
> 交通事故后要判定责任：
> - 不是为了处罚司机
> - 而是为了保险理赔
> - 总结事故原因
> - 改进交通设施和规则

### 7.2 三类责任分类


#### ⚙️ 系统责任（System-Related）


**含义**：由技术系统本身缺陷导致的故障

**典型场景**：
- **设计缺陷** - 系统架构不合理，无法承受正常负载
- **代码Bug** - 程序逻辑错误导致功能异常
- **配置错误** - 系统参数设置不当
- **容量不足** - 系统资源无法满足业务需求
- **依赖故障** - 第三方服务故障影响自身系统

**责任归属**：技术团队和产品设计团队

**典型例子**：
```
电商网站在双11期间：
问题：服务器CPU使用率达到100%，网站访问缓慢
原因：系统设计时未考虑到双11的流量峰值
责任：系统架构设计不足
改进：增加服务器，优化架构，做好容量规划
```

**改进措施**：
- 🔍 代码审查和测试流程优化
- 📊 容量规划和性能测试
- 🏗️ 系统架构优化
- 📋 配置管理规范化

#### 👤 人为责任（Human Error）


**含义**：由人员操作失误导致的故障

**典型场景**：
- **误操作** - 删除了重要文件或数据
- **配置错误** - 修改配置时写错了参数
- **流程违规** - 未按规定流程操作
- **权限滥用** - 使用了不该使用的权限
- **沟通不当** - 信息传递错误导致问题

**责任归属**：具体操作人员和相关管理人员

**典型例子**：
```
数据库维护操作：
问题：运维人员执行SQL时误删了生产环境数据
原因：操作时连错了数据库环境
责任：操作人员未按规范确认环境
改进：建立操作检查清单，加强环境区分标识
```

**预防措施**：
- 📋 建立操作检查清单
- 🔒 完善权限管理制度
- 📚 加强人员培训
- 🤝 建立双人确认机制

#### 🌍 外部责任（External Factors）


**含义**：由外部不可控因素导致的故障

**典型场景**：
- **硬件故障** - 服务器、网络设备物理损坏
- **第三方服务** - 云服务提供商故障
- **网络问题** - ISP网络中断
- **自然灾害** - 地震、火灾等不可抗力
- **恶意攻击** - DDoS攻击、黑客入侵

**责任归属**：外部供应商或不可抗力

**典型例子**：
```
云服务故障：
问题：网站无法访问，用户无法下单
原因：云服务提供商数据中心网络故障
责任：第三方服务商问题
改进：建立多云架构，避免单点故障
```

**应对策略**：
- 🔄 建立备份和冗余机制
- 📋 完善供应商管理制度
- 🛡️ 加强安全防护措施
- 📞 建立应急联系机制

### 7.3 责任界定流程


```
故障发生
    ↓
故障处理（先解决问题）
    ↓
收集相关证据和日志
    ↓
技术分析故障原因
    ↓
确定主要责任类别
    ↓
制定改进措施
    ↓
总结经验教训
```

### 7.4 责任界定原则


**核心原则**：
- **事实导向** - 基于客观事实，不凭主观判断
- **改进导向** - 目的是改进，不是处罚
- **系统思考** - 考虑整个系统和流程
- **公正透明** - 过程公开，结论公正

**避免的误区**：
- ❌ 找替罪羊（单纯找人背锅）
- ❌ 推卸责任（各部门互相推诿）  
- ❌ 重复犯错（不总结经验教训）
- ❌ 处罚导向（重点放在处罚而非改进）

**责任界定矩阵**：

| 责任类型 | **主要原因** | **责任归属** | **改进重点** | **预防措施** |
|---------|-------------|-------------|-------------|-------------|
| ⚙️ **系统** | `技术缺陷` | `技术团队` | `系统优化` | `设计审查` |
| 👤 **人为** | `操作失误` | `操作人员` | `流程完善` | `培训规范` |
| 🌍 **外部** | `外部因素` | `供应商等` | `冗余设计` | `风险管控` |

---

## 8. 📋 核心要点总结


### 8.1 必须掌握的核心概念


```
🔸 故障分类：启动故障（系统起不来）、运行故障（功能有问题）、性能故障（运行太慢）
🔸 影响范围：单机→集群→全网→业务，影响面越大越紧急
🔸 紧急程度：Critical（立即处理）→High（优先处理）→Medium（计划处理）→Low（延后处理）
🔸 SLA指标：可用性（能用多长时间）、响应时间（多快响应）、吞吐量（处理多少请求）
🔸 升级机制：内部升级（找技术专家）、外部升级（找供应商）、管理层升级（找领导）
🔸 通报机制：内部通报（告诉同事）、客户通报（告诉用户）、公告发布（告诉公众）
🔸 责任界定：系统问题（技术缺陷）、人为问题（操作失误）、外部问题（不可抗力）
```

### 8.2 关键理解要点


**🔹 故障分类的实用价值**：
```
不同类型故障处理方式完全不同：
- 启动故障：重点检查硬件和系统文件
- 运行故障：重点检查应用服务和配置  
- 性能故障：重点检查资源使用和优化空间
```

**🔹 定级不是技术问题而是业务问题**：
```
定级标准：
✅ 看影响多少用户（业务影响）
✅ 看造成多大损失（经济影响）
❌ 不看技术复杂度（修复难度）
❌ 不看发现人级别（汇报层级）
```

**🔹 升级是为了解决问题而不是推卸责任**：
```
升级的目的：
- 调动更多资源
- 获得更高权限
- 寻求专业支持
- 加快问题解决
```

### 8.3 实际应用指导


**📊 快速定级决策表**：

| 判断维度 | **Critical** | **High** | **Medium** | **Low** |
|---------|-------------|----------|-----------|---------|
| 🎯 **业务影响** | `核心业务停止` | `重要功能异常` | `一般功能问题` | `轻微影响` |
| 👥 **影响用户** | `全部用户` | `大量用户` | `部分用户` | `个别用户` |
| ⏰ **响应时间** | `15分钟` | `1小时` | `4小时` | `24小时` |
| 📢 **通报范围** | `全员+管理层` | `相关团队` | `直接负责人` | `技术人员` |

**🔧 实践操作建议**：
- **建立故障等级预案** - 每个级别都有对应的处理流程
- **定期演练升级机制** - 确保关键时刻流程顺畅
- **完善监控告警系统** - 及时发现不同类型的故障
- **建立知识库** - 记录典型故障的分类和处理方法

**核心记忆口诀**：
- 故障分类定级别，影响范围定紧急
- 升级机制要顺畅，通报及时又准确  
- 责任界定为改进，经验总结防重复
- 系统人为和外部，对症下药是关键