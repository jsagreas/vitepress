---
title: 6、监控规则与告警配置
---
## 📚 目录

1. [告警规则基础概念](#1-告警规则基础概念)
2. [告警规则文件配置](#2-告警规则文件配置)
3. [告警表达式编写技巧](#3-告警表达式编写技巧)
4. [告警严重级别与分类](#4-告警严重级别与分类)
5. [告警触发与持续时间](#5-告警触发与持续时间)
6. [告警标签和注解详解](#6-告警标签和注解详解)
7. [告警规则测试验证](#7-告警规则测试验证)
8. [告警最佳实践](#8-告警最佳实践)
9. [核心要点总结](#9-核心要点总结)

---

## 1. 🎯 告警规则基础概念


### 1.1 什么是告警规则


**告警规则**就是告诉监控系统"在什么情况下需要通知我们有问题"的配置文件。

想象一下，你家里装了一个烟雾报警器：
- **监控指标** = 空气中的烟雾浓度
- **告警规则** = 当烟雾浓度超过某个值时就响警报
- **告警触发** = 警报器开始响铃通知你

```
现实场景对比：

家庭烟雾报警器：
烟雾浓度 > 阈值 → 响铃通知

服务器监控告警：
CPU使用率 > 80% → 发送邮件/短信通知运维人员
```

### 1.2 告警规则的作用


🎯 **核心价值**
- **提前预警**：在问题严重前就通知我们
- **减少故障**：及时发现异常避免服务中断  
- **节省人力**：不需要人工时刻盯着监控面板
- **问题定位**：快速知道哪里出了什么问题

### 1.3 告警工作流程


```
数据收集 → 规则评估 → 触发判断 → 发送通知 → 处理响应

详细流程：
1. Prometheus收集各种监控数据
2. 每隔一段时间检查告警规则
3. 发现指标满足告警条件
4. 等待持续时间确认不是瞬间抖动
5. 发送告警通知到相关人员
6. 运维人员收到通知处理问题
```

---

## 2. 📋 告警规则文件配置


### 2.1 规则文件基本结构


告警规则使用 **YAML格式** 编写，就像写配置清单一样简单。

```yaml
# alert_rules.yml - 告警规则文件
groups:                          # 规则组
  - name: "服务器基础监控"          # 组名称
    rules:                       # 具体规则列表
      - alert: "CPU使用率过高"      # 告警名称
        expr: cpu_usage > 80       # 触发条件
        for: 2m                    # 持续时间
        labels:                    # 标签
          severity: warning
        annotations:               # 注解说明
          summary: "服务器CPU使用率超过80%"
```

### 2.2 完整配置示例


```yaml
# /etc/prometheus/rules/server_alerts.yml
groups:
  - name: "系统资源监控"
    interval: 30s                 # 评估间隔
    rules:
      # CPU告警规则
      - alert: HighCpuUsage
        expr: 100 - (avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100) > 80
        for: 2m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "服务器 {{ $labels.instance }} CPU使用率过高"
          description: "CPU使用率已达到 {{ $value }}%，超过80%阈值"
          
      # 内存告警规则  
      - alert: HighMemoryUsage
        expr: (1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100 > 90
        for: 1m
        labels:
          severity: critical
          team: infrastructure
        annotations:
          summary: "服务器 {{ $labels.instance }} 内存使用率过高"
          description: "内存使用率 {{ $value }}%，剩余可用内存不足10%"

      # 磁盘空间告警
      - alert: DiskSpaceLow
        expr: (1 - (node_filesystem_avail_bytes / node_filesystem_size_bytes)) * 100 > 85
        for: 5m
        labels:
          severity: warning
          team: infrastructure
        annotations:
          summary: "磁盘空间不足"
          description: "{{ $labels.instance }} 的 {{ $labels.mountpoint }} 磁盘使用率 {{ $value }}%"
```

### 2.3 规则文件加载配置


在 `prometheus.yml` 中指定规则文件位置：

```yaml
# prometheus.yml 主配置文件
global:
  scrape_interval: 15s
  evaluation_interval: 15s       # 告警规则评估间隔

rule_files:                      # 告警规则文件路径
  - "/etc/prometheus/rules/*.yml"
  - "/etc/prometheus/rules/alerts/*.yml"

alerting:                        # 告警管理器配置
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093
```

---

## 3. 🔍 告警表达式编写技巧


### 3.1 基础表达式语法


告警表达式使用 **PromQL**（Prometheus查询语言）编写，就像写数学公式一样。

**基本语法结构**：
```
指标名称{标签过滤条件} 运算符 阈值
```

### 3.2 常用表达式模式


| 监控项目 | 表达式示例 | 说明 |
|---------|-----------|------|
| **CPU使用率** | `100 - avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100 > 80` | CPU使用率超过80% |
| **内存使用率** | `(1 - node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes) * 100 > 90` | 内存使用率超过90% |
| **磁盘使用率** | `(1 - node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 > 85` | 磁盘使用率超过85% |
| **服务状态** | `up == 0` | 服务下线 |
| **网络连接** | `node_netstat_Tcp_CurrEstab > 1000` | TCP连接数过多 |

### 3.3 表达式编写技巧


**💡 实用技巧**

```yaml
# 1. 使用rate()函数计算速率
- alert: HighRequestRate
  expr: rate(http_requests_total[5m]) > 100

# 2. 使用avg_over_time()平滑数据
- alert: HighAverageLoad
  expr: avg_over_time(node_load1[10m]) > 4

# 3. 使用absent()检测指标缺失
- alert: MetricMissing
  expr: absent(up{job="web-server"})

# 4. 使用predict_linear()预测趋势
- alert: DiskWillFull
  expr: predict_linear(node_filesystem_free_bytes[6h], 24*3600) < 0
```

**⚠️ 常见错误避免**
- **不要用瞬时值**：`cpu_usage > 80` ❌
- **应该用平均值**：`avg_over_time(cpu_usage[5m]) > 80` ✅
- **避免过于敏感**：设置合理的时间窗口和阈值

---

## 4. 🏷️ 告警严重级别与分类


### 4.1 告警级别定义


根据问题的紧急程度，告警通常分为几个级别：

```
🔥 Critical (严重)：需要立即处理的紧急问题
⚠️  Warning (警告)：需要关注但不紧急的问题  
ℹ️  Info (信息)：仅供了解的状态信息
```

### 4.2 级别划分标准


| 级别 | 影响范围 | 响应时间 | 典型场景 |
|------|---------|---------|----------|
| **🔥 Critical** | `服务完全不可用` | `立即响应(5分钟内)` | 服务宕机、数据库连接失败、磁盘满 |
| **⚠️ Warning** | `性能下降但可用` | `1小时内响应` | CPU使用率高、内存使用率高 |
| **ℹ️ Info** | `状态变化通知` | `工作时间处理` | 服务重启、配置变更 |

### 4.3 级别配置示例


```yaml
groups:
  - name: "分级告警示例"
    rules:
      # 严重级别 - 服务完全不可用
      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
          team: sre
        annotations:
          summary: "🔥 服务完全宕机"
          
      # 警告级别 - 性能问题但服务可用  
      - alert: HighCPU
        expr: cpu_usage > 80
        for: 5m
        labels:
          severity: warning
          team: ops
        annotations:
          summary: "⚠️ CPU使用率过高"
          
      # 信息级别 - 状态变化通知
      - alert: ServiceRestart
        expr: changes(process_start_time_seconds[10m]) > 0
        labels:
          severity: info
          team: dev
        annotations:
          summary: "ℹ️ 服务重启通知"
```

---

## 5. ⏰ 告警触发与持续时间


### 5.1 持续时间的作用


**for** 参数指定告警条件需要持续多长时间才真正触发告警，这样可以：
- **避免误报**：防止短暂的数据波动触发告警
- **确认问题**：确保问题确实存在而非瞬间抖动
- **减少干扰**：减少不必要的告警通知

```
时间轴示例：
秒    0    30   60   90   120  150  180
CPU  95%  92%  88%  85%  82%  79%  76%
告警  ×    ×    ×    ×    √   (触发) (恢复)

for: 2m 表示CPU超过80%持续2分钟才发送告警
```

### 5.2 持续时间配置策略


```yaml
# 不同场景的持续时间建议
rules:
  # 严重问题 - 短持续时间
  - alert: ServiceDown
    expr: up == 0
    for: 30s              # 服务宕机30秒就告警
    
  # 性能问题 - 中等持续时间  
  - alert: HighCPU
    expr: cpu_usage > 80
    for: 2m               # CPU高2分钟才告警
    
  # 趋势问题 - 长持续时间
  - alert: DiskSpaceGrowing
    expr: disk_usage > 85
    for: 10m              # 磁盘空间问题10分钟才告警
```

### 5.3 持续时间选择原则


**🎯 选择建议**

- **紧急问题**：`30s - 1m` （如服务宕机）
- **性能问题**：`2m - 5m` （如CPU、内存高）
- **资源问题**：`5m - 10m` （如磁盘空间）
- **趋势问题**：`10m - 30m` （如增长趋势）

---

## 6. 🏷️ 告警标签和注解详解


### 6.1 标签（Labels）的作用


**标签**用于给告警分类和路由，就像给邮件打标签一样：

```yaml
labels:
  severity: critical      # 严重程度
  team: infrastructure   # 负责团队  
  service: web-api       # 相关服务
  environment: production # 环境标识
```

标签的主要用途：
- **告警分组**：相同标签的告警会被分组处理
- **路由规则**：根据标签决定发送给谁
- **过滤筛选**：在告警管理器中过滤特定告警

### 6.2 注解（Annotations）的作用


**注解**提供告警的详细描述信息，帮助快速理解和处理问题：

```yaml
annotations:
  summary: "简短的问题描述"
  description: "详细的问题说明和上下文"
  runbook_url: "处理手册链接"
  dashboard_url: "相关监控面板链接"
```

### 6.3 完整标签和注解示例


```yaml
- alert: DatabaseConnectionFailed
  expr: mysql_up == 0
  for: 1m
  labels:
    severity: critical
    team: database
    service: mysql
    environment: "{{ $labels.env }}"
    component: database
  annotations:
    summary: "数据库 {{ $labels.instance }} 连接失败"
    description: |
      数据库服务器 {{ $labels.instance }} 无法连接，这将影响以下服务：
      - 用户认证服务
      - 订单处理系统
      - 数据分析服务
      
      当前状态: {{ $value }}
      持续时间: {{ $labels.for }}
    runbook_url: "https://wiki.company.com/database-troubleshooting"
    dashboard_url: "https://grafana.company.com/db-dashboard"
    impact: "用户无法登录和下单"
    action: "立即检查数据库服务状态并重启"
```

### 6.4 模板变量使用


在注解中可以使用模板变量获取动态信息：

| 变量 | 含义 | 示例 |
|------|------|------|
| `{{ $labels.instance }}` | 实例标识 | `192.168.1.10:9100` |
| `{{ $labels.job }}` | 任务名称 | `node-exporter` |
| `{{ $value }}` | 当前指标值 | `85.6` |
| `{{ $labels.mountpoint }}` | 挂载点 | `/home` |

---

## 7. 🧪 告警规则测试验证


### 7.1 语法检查


在部署规则前，先检查语法是否正确：

```bash
# 检查规则文件语法
promtool check rules /path/to/alert_rules.yml

# 输出示例
SUCCESS: 3 rules found

# 如果有错误会显示详细信息
FAILED: error parsing /path/to/alert_rules.yml: 
  yaml: line 10: did not find expected key
```

### 7.2 表达式测试


在Prometheus Web界面测试表达式：

```
访问步骤：
1. 打开 http://prometheus-server:9090
2. 进入 Graph 标签页
3. 在查询框输入表达式
4. 点击 Execute 查看结果
5. 观察是否符合预期

测试示例表达式：
100 - (avg(irate(node_cpu_seconds_total{mode="idle"}[5m])) * 100)
```

### 7.3 告警状态查看


```
告警状态检查：
1. 进入 Alerts 标签页
2. 查看告警规则状态：
   - Inactive: 条件不满足
   - Pending: 满足条件但未达到持续时间
   - Firing: 正在触发告警
```

### 7.4 规则重载


修改规则文件后需要重载配置：

```bash
# 重载配置（不重启服务）
curl -X POST http://localhost:9090/-/reload

# 或者发送信号
sudo kill -HUP $(pgrep prometheus)

# 查看日志确认重载成功
tail -f /var/log/prometheus/prometheus.log
```

---

## 8. 🎯 告警最佳实践


### 8.1 规则设计原则


**📋 关键原则**

```
✅ 问题导向：告警应该反映真实的业务问题
✅ 可操作性：收到告警后知道该怎么处理
✅ 避免噪音：减少无用的告警干扰  
✅ 分层监控：不同层次设置不同告警
✅ 文档完善：每个告警都有处理文档
```

### 8.2 阈值设置技巧


**合理阈值的设定方法**：

```yaml
# 基于历史数据设定阈值
- alert: HighResponseTime
  # 响应时间超过历史95分位数的1.5倍
  expr: histogram_quantile(0.95, http_request_duration_seconds) > 0.5
  
# 基于业务影响设定阈值  
- alert: HighErrorRate
  # 错误率超过1%影响用户体验
  expr: rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) > 0.01

# 基于资源容量设定阈值
- alert: DiskAlmostFull  
  # 磁盘使用超过90%需要扩容
  expr: (1 - node_filesystem_avail_bytes / node_filesystem_size_bytes) * 100 > 90
```

### 8.3 告警分组策略


```yaml
# 按服务分组
groups:
  - name: "web-service-alerts"
    rules: [...]
    
  - name: "database-alerts" 
    rules: [...]

# 按环境分组
groups:
  - name: "production-alerts"
    rules: [...]
    
  - name: "staging-alerts"
    rules: [...]
```

### 8.4 避免告警疲劳


**防止告警过多的策略**：

- **设置合理阈值**：不要过于敏感
- **使用抑制规则**：相关告警只发送主要的
- **告警降噪**：过滤掉已知的临时问题
- **定期评估**：删除不再需要的告警规则

```yaml
# 示例：只在业务时间告警非紧急问题
- alert: NonCriticalIssue
  expr: some_metric > threshold
  for: 5m
  labels:
    severity: warning
    # 添加时间标签用于路由
    business_hours: "{{ if and (ge (hour) 9) (le (hour) 18) }}yes{{ else }}no{{ end }}"
```

### 8.5 规则文件组织


**推荐的文件结构**：

```
/etc/prometheus/rules/
├── infrastructure/
│   ├── server-basic.yml      # 服务器基础监控
│   ├── network.yml           # 网络监控
│   └── storage.yml           # 存储监控
├── applications/
│   ├── web-service.yml       # Web服务监控
│   ├── database.yml          # 数据库监控
│   └── cache.yml             # 缓存监控
└── business/
    ├── sla-alerts.yml        # SLA相关告警
    └── business-metrics.yml   # 业务指标告警
```

---

## 9. 📋 核心要点总结


### 9.1 必须掌握的核心概念


```
🎯 告警规则本质：定义在什么条件下需要通知相关人员
📋 配置文件结构：groups -> rules -> alert/expr/for/labels/annotations
🔍 表达式编写：使用PromQL定义触发条件，注意时间窗口和阈值
⏰ 持续时间设置：避免误报，确保问题真实存在
🏷️ 标签和注解：用于分类路由和提供详细信息
```

### 9.2 关键理解要点


**🔹 告警规则的核心价值**
```
提前预警：在问题严重前就发现
减少故障：及时处理避免服务中断  
节省人力：自动化监控减少人工盯守
快速定位：明确告知问题位置和原因
```

**🔹 配置文件的设计思路**
```
分组管理：相关规则放在同一组便于管理
层次分明：从基础设施到应用到业务层层监控
标签路由：通过标签决定告警发送给谁处理
详细注解：提供足够信息帮助快速解决问题
```

**🔹 表达式编写要点**
```
避免瞬时值：使用时间窗口平滑数据波动
合理阈值：基于历史数据和业务影响设定
预测趋势：不仅监控当前状态也关注发展趋势
缺失检测：监控指标采集是否正常
```

### 9.3 实际应用指导


**✅ 告警规则设计清单**
- [x] 规则名称清晰易懂
- [x] 表达式经过测试验证  
- [x] 持续时间设置合理
- [x] 严重级别准确分类
- [x] 标签信息完整
- [x] 注解描述详细
- [x] 处理文档链接
- [x] 语法检查通过

**⚠️ 常见错误避免**
- ❌ 阈值设置过于敏感导致误报
- ❌ 持续时间过短产生告警风暴  
- ❌ 缺少标签导致无法正确路由
- ❌ 注解信息不足影响问题处理
- ❌ 规则过多导致告警疲劳

### 9.4 进阶学习方向


**🚀 深入主题**
- **Alertmanager配置**：学习告警路由、分组、抑制规则
- **告警模板定制**：自定义告警消息格式和样式
- **多渠道通知**：邮件、短信、钉钉、微信等集成
- **告警自动处理**：结合自动化运维工具自动修复
- **SLI/SLO监控**：基于服务等级目标的高级监控

**核心记忆口诀**：
- 规则配置有条理，分组管理更清晰
- 表达式写要仔细，阈值时间需合理  
- 标签注解信息全，问题处理有指南
- 测试验证不可少，告警质量才会好