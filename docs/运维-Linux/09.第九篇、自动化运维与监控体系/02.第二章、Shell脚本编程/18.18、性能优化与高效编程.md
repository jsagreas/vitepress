---
title: 18、性能优化与高效编程
---
## 📚 目录

1. [脚本性能分析与profiling](#1-脚本性能分析与profiling)
2. [算法复杂度优化技巧](#2-算法复杂度优化技巧)
3. [内存使用优化策略](#3-内存使用优化策略)
4. [I/O操作效率提升](#4-io操作效率提升)
5. [外部命令调用优化](#5-外部命令调用优化)
6. [并发处理性能调优](#6-并发处理性能调优)
7. [缓存机制实现与应用](#7-缓存机制实现与应用)
8. [代码重构与优化实践](#8-代码重构与优化实践)
9. [核心要点总结](#9-核心要点总结)

---

## 1. 🔍 脚本性能分析与profiling


### 1.1 性能分析的核心概念


**什么是Shell脚本性能分析？**
性能分析就是找出脚本运行慢的原因，就像体检一样检查脚本的"健康状况"。我们需要知道脚本把时间花在哪里，哪些操作最耗时。

**📊 性能分析的关键指标**
- **执行时间** - 脚本跑完需要多长时间
- **CPU使用率** - 脚本占用了多少处理器资源
- **内存消耗** - 脚本使用了多少内存
- **I/O操作** - 文件读写和网络访问的频率

### 1.2 内置性能分析工具


**🕒 time命令 - 最基础的性能测量**

`time`命令能告诉我们脚本运行的基本时间信息：

```bash
# 测量脚本执行时间
time ./my_script.sh

# 输出结果解释：
# real    0m2.150s  # 实际运行时间（墙上时钟时间）
# user    0m1.200s  # CPU用户态时间
# sys     0m0.800s  # CPU内核态时间
```

**💡 理解时间数据的含义**
- **real时间** - 从开始到结束的实际时间，包括等待I/O的时间
- **user时间** - CPU执行用户代码的时间
- **sys时间** - CPU执行系统调用的时间

**⚡ set -x 调试模式**

启用调试模式可以看到每条命令的执行过程：

```bash
#!/bin/bash
set -x  # 开启调试模式

# 脚本内容
for file in *.txt; do
    wc -l "$file"
done

set +x  # 关闭调试模式
```

### 1.3 高级性能分析技术


**📈 使用strace追踪系统调用**

`strace`可以显示脚本调用了哪些系统函数，帮助找出性能瓶颈：

```bash
# 追踪脚本的系统调用
strace -c ./my_script.sh 2>trace.log

# -c 参数会生成调用统计报告
# 显示哪些系统调用用时最多
```

**🎯 自定义性能监控**

在脚本中添加时间戳来监控关键部分：

```bash
#!/bin/bash

# 记录开始时间
start_time=$(date +%s.%N)

# 执行要监控的代码块
process_large_file() {
    local file="$1"
    section_start=$(date +%s.%N)
    
    # 实际处理逻辑
    grep "pattern" "$file" | sort | uniq -c
    
    section_end=$(date +%s.%N)
    section_duration=$(echo "$section_end - $section_start" | bc)
    echo "处理文件 $file 耗时: ${section_duration}秒" >&2
}

# 计算总耗时
end_time=$(date +%s.%N)
total_duration=$(echo "$end_time - $start_time" | bc)
echo "脚本总耗时: ${total_duration}秒" >&2
```

---

## 2. 🧠 算法复杂度优化技巧


### 2.1 理解算法复杂度


**什么是算法复杂度？**
算法复杂度就是衡量算法效率的标准。想象一下在图书馆找书：
- **线性查找** - 一本一本找，数据越多越慢
- **二分查找** - 按分类快速定位，效率高很多

**⏱️ 常见复杂度级别**

| 复杂度 | 性能表现 | 实际含义 | 示例场景 |
|--------|----------|----------|----------|
| `O(1)` | ⭐⭐⭐⭐⭐ | 固定时间 | 数组索引访问 |
| `O(log n)` | ⭐⭐⭐⭐ | 很快 | 二分查找 |
| `O(n)` | ⭐⭐⭐ | 线性增长 | 遍历文件 |
| `O(n²)` | ⭐⭐ | 慢 | 嵌套循环 |
| `O(2ⁿ)` | ⭐ | 非常慢 | 递归斐波那契 |

### 2.2 避免嵌套循环陷阱


**❌ 低效的嵌套循环**

```bash
# 这种写法对于大数据集会很慢 O(n²)
#!/bin/bash
for user in $(cat users.txt); do
    for log_line in $(cat access.log); do
        if [[ "$log_line" == *"$user"* ]]; then
            echo "找到用户 $user 的访问记录"
        fi
    done
done
```

**✅ 优化后的单次遍历**

```bash
# 更高效的方法 O(n)
#!/bin/bash
# 先准备用户列表
declare -A users
while IFS= read -r user; do
    users["$user"]=1
done < users.txt

# 只遍历一次日志文件
while IFS= read -r log_line; do
    for user in "${!users[@]}"; do
        if [[ "$log_line" == *"$user"* ]]; then
            echo "找到用户 $user 的访问记录: $log_line"
            break  # 找到后立即跳出
        fi
    done
done < access.log
```

### 2.3 使用关联数组提升查找效率


**🔍 关联数组的威力**

关联数组（哈希表）查找速度是O(1)，比线性查找快得多：

```bash
#!/bin/bash

# 创建IP白名单的关联数组
declare -A whitelist_ips
whitelist_ips["192.168.1.100"]=1
whitelist_ips["192.168.1.101"]=1
whitelist_ips["10.0.0.50"]=1

# 高效检查IP是否在白名单中
check_ip() {
    local ip="$1"
    if [[ -n "${whitelist_ips[$ip]}" ]]; then
        echo "IP $ip 在白名单中"
        return 0
    else
        echo "IP $ip 不在白名单中"
        return 1
    fi
}

# 处理访问日志时，查找速度很快
while IFS= read -r log_line; do
    ip=$(echo "$log_line" | awk '{print $1}')
    check_ip "$ip"
done < access.log
```

---

## 3. 💾 内存使用优化策略


### 3.1 理解Shell的内存使用模式


**Shell脚本如何使用内存？**
Shell脚本的内存主要用于：
- **变量存储** - 字符串和数组数据
- **命令替换** - `$(command)`的输出缓存
- **管道缓冲** - 命令间数据传输的临时存储

### 3.2 避免大文件一次性读取


**❌ 内存杀手：一次性读取大文件**

```bash
# 这样做会把整个文件加载到内存中
large_file_content=$(cat huge_file.log)
# 如果文件有几GB，系统内存可能不够用
```

**✅ 流式处理大文件**

```bash
# 逐行处理，内存占用固定
process_large_file() {
    local filename="$1"
    while IFS= read -r line; do
        # 处理每一行
        if [[ "$line" == *"ERROR"* ]]; then
            echo "发现错误: $line"
        fi
    done < "$filename"
}
```

### 3.3 变量管理最佳实践


**🧹 及时清理不需要的变量**

```bash
#!/bin/bash

process_temp_data() {
    local temp_array=()
    
    # 临时处理大量数据
    for i in {1..10000}; do
        temp_array+=("data_$i")
    done
    
    # 处理完成后主动清理
    unset temp_array
    
    # 对于特别大的变量，可以显式清理
    large_string=""
}
```

**📊 监控脚本内存使用**

```bash
# 在脚本中监控内存使用情况
monitor_memory() {
    local process_id=$$
    ps -o pid,vsz,rss -p $process_id
    echo "VSZ: 虚拟内存大小"
    echo "RSS: 实际物理内存使用"
}

# 在关键位置调用监控
echo "开始处理前:"
monitor_memory

# 执行内存密集操作
# ...

echo "处理完成后:"
monitor_memory
```

---

## 4. 📁 I/O操作效率提升


### 4.1 理解I/O操作的成本


**什么是I/O操作？**
I/O（Input/Output）就是输入输出操作，包括：
- **文件读写** - 从磁盘读取或写入文件
- **网络通信** - 网络数据传输
- **设备交互** - 与硬件设备通信

**💡 为什么I/O操作慢？**
I/O操作比内存操作慢几千倍，就像：
- 内存操作：在桌上拿东西（毫秒级）
- 磁盘操作：跑到仓库取东西（毫秒级，但比内存慢很多）
- 网络操作：让别人邮寄东西（秒级）

### 4.2 文件I/O优化技巧


**📂 批量处理 vs 频繁访问**

```bash
# ❌ 低效：频繁打开关闭文件
log_message() {
    local message="$1"
    echo "$(date): $message" >> app.log
}

# 每次调用都要打开关闭文件
for i in {1..1000}; do
    log_message "处理第 $i 条记录"
done
```

```bash
# ✅ 高效：批量写入
{
    for i in {1..1000}; do
        echo "$(date): 处理第 $i 条记录"
    done
} >> app.log
```

**🔄 使用内存缓冲**

```bash
#!/bin/bash

# 在内存中累积数据，定期批量写入
log_buffer=""
buffer_size=0
max_buffer_size=100

buffered_log() {
    local message="$1"
    log_buffer+="$(date): $message"$'\n'
    ((buffer_size++))
    
    # 缓冲区满了就写入文件
    if ((buffer_size >= max_buffer_size)); then
        echo -n "$log_buffer" >> app.log
        log_buffer=""
        buffer_size=0
    fi
}

# 脚本结束前确保缓冲区内容被写入
cleanup_buffer() {
    if [[ -n "$log_buffer" ]]; then
        echo -n "$log_buffer" >> app.log
    fi
}

trap cleanup_buffer EXIT
```

### 4.3 管道优化技巧


**⚡ 减少管道阶段**

```bash
# ❌ 多个管道阶段，多次I/O
cat large_file.txt | grep "ERROR" | sort | uniq -c | head -10

# ✅ 合并操作，减少中间步骤
awk '/ERROR/ {print $0}' large_file.txt | sort | uniq -c | head -10
```

**📋 使用进程替换避免临时文件**

```bash
# ❌ 使用临时文件
grep "pattern1" file.txt > temp1.txt
grep "pattern2" file.txt > temp2.txt
diff temp1.txt temp2.txt
rm temp1.txt temp2.txt

# ✅ 使用进程替换
diff <(grep "pattern1" file.txt) <(grep "pattern2" file.txt)
```

---

## 5. ⚙️ 外部命令调用优化


### 5.1 理解外部命令的开销


**什么是外部命令调用？**
外部命令就是调用系统中的其他程序，如`grep`、`sed`、`awk`等。每次调用都需要：
1. **创建新进程** - 系统要为命令分配资源
2. **加载程序** - 从磁盘读取程序到内存
3. **执行命令** - 运行实际的处理逻辑
4. **清理资源** - 命令结束后释放资源

### 5.2 减少不必要的外部命令调用


**🔧 使用Shell内置功能替代外部命令**

```bash
# ❌ 频繁调用外部命令
count=0
for file in *.txt; do
    lines=$(wc -l < "$file")  # 每次都调用wc命令
    count=$((count + lines))
done

# ✅ 使用Shell内置计数
count=0
for file in *.txt; do
    while IFS= read -r line; do
        ((count++))
    done < "$file"
done
```

**📝 批量处理外部命令**

```bash
# ❌ 多次调用sed
for file in *.txt; do
    sed -i 's/old/new/g' "$file"
    sed -i 's/foo/bar/g' "$file"
    sed -i 's/hello/hi/g' "$file"
done

# ✅ 一次调用处理多个替换
for file in *.txt; do
    sed -i -e 's/old/new/g' -e 's/foo/bar/g' -e 's/hello/hi/g' "$file"
done
```

### 5.3 命令选择的性能考量


**🏃 选择更快的替代命令**

| 场景 | 慢的选择 | 快的选择 | 原因 |
|------|----------|----------|------|
| 文本搜索 | `grep \| head -1` | `grep -m1` | 找到第一个就停止 |
| 文件测试 | `ls file \| wc -l` | `[[ -f file ]]` | 内置测试更快 |
| 字符串长度 | `echo "$str" \| wc -c` | `${#str}` | 变量展开更直接 |
| 随机数 | `shuf -i 1-100 -n1` | `$RANDOM` | 内置变量更快 |

**💡 实际优化示例**

```bash
# ❌ 慢的实现
check_file_exists() {
    local file="$1"
    if [[ $(ls "$file" 2>/dev/null | wc -l) -gt 0 ]]; then
        return 0
    else
        return 1
    fi
}

# ✅ 快的实现
check_file_exists() {
    local file="$1"
    [[ -e "$file" ]]
}
```

---

## 6. 🚀 并发处理性能调优


### 6.1 理解并发处理的优势


**什么是并发处理？**
并发就是同时做多件事情，就像餐厅里：
- **串行处理** - 一个厨师按顺序做菜
- **并发处理** - 多个厨师同时做不同的菜

**⚡ 何时使用并发？**
- 处理大量独立的任务
- I/O密集型操作（文件读写、网络请求）
- CPU多核可以并行的计算任务

### 6.2 后台进程并发


**🔄 基本并发模式**

```bash
#!/bin/bash

# 要处理的文件列表
files=(file1.txt file2.txt file3.txt file4.txt)

process_file() {
    local file="$1"
    echo "开始处理 $file"
    
    # 模拟处理过程
    grep "pattern" "$file" > "processed_$file"
    
    echo "完成处理 $file"
}

# ❌ 串行处理
for file in "${files[@]}"; do
    process_file "$file"
done

# ✅ 并发处理
for file in "${files[@]}"; do
    process_file "$file" &  # & 表示后台运行
done

# 等待所有后台进程完成
wait
echo "所有文件处理完成"
```

### 6.3 控制并发数量


**🎛️ 限制并发进程数**

不控制并发数量可能会：
- 创建太多进程，系统资源耗尽
- 磁盘I/O竞争，反而变慢

```bash
#!/bin/bash

max_jobs=4  # 最大并发数
current_jobs=0

process_with_limit() {
    local item="$1"
    
    # 如果达到最大并发数，等待
    while ((current_jobs >= max_jobs)); do
        sleep 0.1
        # 检查完成的进程
        wait -n  # 等待任意一个后台进程完成
        ((current_jobs--))
    done
    
    # 启动新的后台进程
    {
        echo "处理 $item"
        sleep 2  # 模拟处理时间
        echo "$item 处理完成"
    } &
    
    ((current_jobs++))
}

# 处理大量项目
for i in {1..20}; do
    process_with_limit "item_$i"
done

# 等待剩余进程完成
wait
echo "全部完成"
```

### 6.4 使用xargs并行处理


**⚙️ xargs的并行模式**

`xargs`命令自带并行处理能力：

```bash
# 并行处理多个文件
find . -name "*.log" | xargs -P 4 -I {} gzip {}
# -P 4: 最多4个并行进程
# -I {}: 用{}代表每个输入项

# 并行下载多个文件
echo -e "url1\nurl2\nurl3\nurl4" | xargs -P 2 -I {} wget {}
```

---

## 7. 💾 缓存机制实现与应用


### 7.1 理解缓存的价值


**什么是缓存？**
缓存就是把计算结果暂时保存起来，下次需要时直接使用，不用重新计算。就像：
- **没有缓存** - 每次都重新计算，费时费力
- **有缓存** - 第一次计算后保存结果，之后直接用

**🎯 适合缓存的场景**
- 计算结果不经常变化
- 计算过程比较耗时
- 相同的计算会重复执行

### 7.2 简单文件缓存实现


**📁 基于文件的缓存系统**

```bash
#!/bin/bash

# 缓存目录
CACHE_DIR="/tmp/script_cache"
mkdir -p "$CACHE_DIR"

# 生成缓存键（基于参数的哈希值）
get_cache_key() {
    local params="$*"
    echo -n "$params" | md5sum | cut -d' ' -f1
}

# 检查缓存是否存在且有效
is_cache_valid() {
    local cache_file="$1"
    local max_age_seconds="$2"
    
    if [[ -f "$cache_file" ]]; then
        local file_age=$(($(date +%s) - $(stat -c %Y "$cache_file")))
        if ((file_age < max_age_seconds)); then
            return 0  # 缓存有效
        fi
    fi
    return 1  # 缓存无效或不存在
}

# 耗时的计算函数（示例）
expensive_calculation() {
    local input="$1"
    
    # 生成缓存文件路径
    local cache_key=$(get_cache_key "expensive_calc" "$input")
    local cache_file="$CACHE_DIR/$cache_key"
    
    # 检查缓存（有效期5分钟）
    if is_cache_valid "$cache_file" 300; then
        echo "从缓存获取结果" >&2
        cat "$cache_file"
        return 0
    fi
    
    # 缓存不存在或过期，重新计算
    echo "重新计算结果" >&2
    local result
    result=$(echo "$input" | rev | tr '[:lower:]' '[:upper:]')
    
    # 保存到缓存
    echo "$result" > "$cache_file"
    echo "$result"
}

# 使用示例
echo "第一次调用:"
expensive_calculation "hello world"

echo "第二次调用（应该使用缓存）:"
expensive_calculation "hello world"
```

### 7.3 内存缓存实现


**🧠 关联数组内存缓存**

```bash
#!/bin/bash

# 内存缓存实现
declare -A memory_cache
declare -A cache_timestamps

# 内存缓存函数
cached_function() {
    local func_name="$1"
    local cache_key="$2"
    local max_age="$3"
    shift 3  # 移除前3个参数，剩下的是实际函数参数
    
    local full_key="${func_name}_${cache_key}"
    local current_time=$(date +%s)
    
    # 检查缓存是否存在且有效
    if [[ -n "${memory_cache[$full_key]}" ]]; then
        local cache_time="${cache_timestamps[$full_key]}"
        if ((current_time - cache_time < max_age)); then
            echo "💾 缓存命中: $cache_key" >&2
            echo "${memory_cache[$full_key]}"
            return 0
        fi
    fi
    
    # 执行实际计算
    echo "🔄 计算中: $cache_key" >&2
    local result
    case "$func_name" in
        "get_system_info")
            result=$(uname -a)
            ;;
        "count_processes")
            result=$(ps aux | wc -l)
            ;;
        *)
            result="Unknown function: $func_name"
            ;;
    esac
    
    # 保存到内存缓存
    memory_cache[$full_key]="$result"
    cache_timestamps[$full_key]="$current_time"
    
    echo "$result"
}

# 使用示例
echo "=== 系统信息缓存测试 ==="
cached_function "get_system_info" "system" 60  # 缓存60秒
sleep 1
cached_function "get_system_info" "system" 60  # 应该使用缓存

echo "=== 进程数量缓存测试 ==="
cached_function "count_processes" "procs" 30   # 缓存30秒
cached_function "count_processes" "procs" 30   # 应该使用缓存
```

---

## 8. 🔧 代码重构与优化实践


### 8.1 识别性能瓶颈的方法


**🔍 性能问题的常见症状**
- 脚本运行时间明显过长
- 系统资源使用率过高
- 用户体验不佳

**📊 性能分析检查清单**

```bash
#!/bin/bash
# 性能检查脚本

performance_audit() {
    echo "=== Shell脚本性能审计 ==="
    
    echo "1️⃣ 检查循环嵌套深度"
    echo "   - 避免O(n²)及以上复杂度的嵌套循环"
    
    echo "2️⃣ 检查外部命令调用频率"
    grep -c 'grep\|sed\|awk\|cut' "$1" 2>/dev/null || echo "   - 统计外部命令使用次数"
    
    echo "3️⃣ 检查文件操作模式"
    echo "   - 是否有频繁的文件打开/关闭操作"
    
    echo "4️⃣ 检查变量使用"
    echo "   - 大数组和字符串是否及时清理"
    
    echo "5️⃣ 检查并发可能性"
    echo "   - 独立任务是否可以并行处理"
}
```

### 8.2 重构实例：日志分析脚本


**❌ 重构前：低效版本**

```bash
#!/bin/bash
# 低效的日志分析脚本

analyze_log() {
    local logfile="$1"
    
    # 问题1：多次读取同一文件
    total_lines=$(wc -l < "$logfile")
    error_count=$(grep "ERROR" "$logfile" | wc -l)
    warning_count=$(grep "WARN" "$logfile" | wc -l)
    
    # 问题2：嵌套循环，O(n²)复杂度
    declare -a unique_ips
    while IFS= read -r line; do
        ip=$(echo "$line" | awk '{print $1}')
        found=0
        for existing_ip in "${unique_ips[@]}"; do
            if [[ "$existing_ip" == "$ip" ]]; then
                found=1
                break
            fi
        done
        if ((found == 0)); then
            unique_ips+=("$ip")
        fi
    done < "$logfile"
    
    echo "总行数: $total_lines"
    echo "错误数: $error_count"
    echo "警告数: $warning_count"
    echo "唯一IP数: ${#unique_ips[@]}"
}
```

**✅ 重构后：高效版本**

```bash
#!/bin/bash
# 高效的日志分析脚本

analyze_log_optimized() {
    local logfile="$1"
    
    # 一次遍历完成所有统计
    local total_lines=0
    local error_count=0
    local warning_count=0
    declare -A unique_ips
    
    while IFS= read -r line; do
        ((total_lines++))
        
        # 检查错误和警告
        if [[ "$line" == *"ERROR"* ]]; then
            ((error_count++))
        elif [[ "$line" == *"WARN"* ]]; then
            ((warning_count++))
        fi
        
        # 提取IP并去重（使用关联数组）
        local ip=$(echo "$line" | awk '{print $1}')
        unique_ips["$ip"]=1
        
    done < "$logfile"
    
    echo "📊 日志分析结果："
    echo "   总行数: $total_lines"
    echo "   错误数: $error_count"
    echo "   警告数: $warning_count"
    echo "   唯一IP数: ${#unique_ips[@]}"
}
```

### 8.3 重构最佳实践


**🎯 重构的黄金法则**

1. **单一职责原则** - 每个函数只做一件事
2. **避免重复计算** - 相同的计算只执行一次
3. **批量处理** - 合并相似的操作
4. **早期退出** - 找到结果后立即返回
5. **使用合适的数据结构** - 关联数组比线性搜索快

**🔄 重构步骤建议**

```bash
# 重构检查清单
refactoring_checklist() {
    echo "🔧 重构检查清单："
    echo ""
    echo "✅ 1. 测量当前性能基线"
    echo "   time ./script.sh > baseline.log"
    echo ""
    echo "✅ 2. 识别最慢的部分"
    echo "   添加计时代码，找出瓶颈"
    echo ""
    echo "✅ 3. 逐个优化瓶颈"
    echo "   一次优化一个问题"
    echo ""
    echo "✅ 4. 验证改进效果"
    echo "   重新测量性能"
    echo ""
    echo "✅ 5. 确保功能正确性"
    echo "   对比优化前后的输出结果"
}
```

---

## 9. 📋 核心要点总结


### 9.1 必须掌握的关键概念


**🎯 性能优化的核心原则**
- **测量优先** - 先测量性能，找出真正的瓶颈
- **局部优化** - 重点优化最慢的部分，不要过度优化
- **权衡取舍** - 性能vs可读性vs维护性的平衡
- **持续监控** - 性能优化是个持续过程

**📊 关键性能指标**
```
⏱️ 执行时间    - 脚本完成所需的总时间
🧠 内存使用    - 峰值内存占用和持续占用
💽 I/O操作     - 文件读写和网络访问频率
🔄 CPU利用率   - 处理器资源的使用效率
```

### 9.2 性能优化技术总览


| 优化领域 | 关键技术 | 效果等级 | 适用场景 |
|----------|----------|----------|----------|
| **算法复杂度** | 关联数组、避免嵌套循环 | 🔥🔥🔥🔥🔥 | 大数据处理 |
| **I/O优化** | 批量操作、流式处理 | 🔥🔥🔥🔥 | 文件密集型 |
| **并发处理** | 后台进程、xargs -P | 🔥🔥🔥🔥 | 独立任务多 |
| **缓存机制** | 文件/内存缓存 | 🔥🔥🔥 | 重复计算多 |
| **命令优化** | 内置功能、命令选择 | 🔥🔥 | 通用优化 |

### 9.3 实战优化流程


**🔍 Step 1: 性能分析**
```bash
# 建立性能基线
time ./script.sh
strace -c ./script.sh
```

**🎯 Step 2: 识别瓶颈**
```bash
# 添加分段计时
start=$(date +%s.%N)
# ... 代码段 ...
end=$(date +%s.%N)
echo "耗时: $(echo "$end - $start" | bc)秒"
```

**⚡ Step 3: 针对性优化**
- 算法瓶颈 → 降低复杂度
- I/O瓶颈 → 批量处理
- CPU瓶颈 → 并发处理
- 重复计算 → 添加缓存

**✅ Step 4: 验证效果**
```bash
# 对比优化前后的性能
echo "优化前:" && time ./old_script.sh
echo "优化后:" && time ./new_script.sh
```

### 9.4 常见性能陷阱


**❌ 需要避免的反模式**

```bash
# 🚫 频繁调用外部命令
for file in *.txt; do
    lines=$(wc -l < "$file")  # 每次都启动wc进程
done

# 🚫 嵌套循环暴力搜索
for item1 in "${array1[@]}"; do
    for item2 in "${array2[@]}"; do
        [[ "$item1" == "$item2" ]] && echo "找到匹配"
    done
done

# 🚫 重复读取同一文件
grep "pattern1" large_file.txt
grep "pattern2" large_file.txt
grep "pattern3" large_file.txt
```

### 9.5 性能优化的实用建议


**💡 优化优先级指南**

1. **高优先级** 🔥🔥🔥
   - 算法复杂度优化（O(n²) → O(n)）
   - 去除不必要的嵌套循环
   - 大文件的流式处理

2. **中优先级** 🔥🔥
   - 批量I/O操作
   - 减少外部命令调用
   - 适当使用并发处理

3. **低优先级** 🔥
   - 变量命名优化
   - 代码结构调整
   - 注释和文档完善

**🎯 记忆要点**
- 性能优化要**测量驱动**，不要凭感觉
- **20/80法则** - 20%的代码占用80%的执行时间
- 优化要**循序渐进** - 一次解决一个问题
- **可读性很重要** - 不要为了性能牺牲代码清晰度
- **适度优化** - 过度优化可能得不偿失

**核心理念**: Shell脚本性能优化的本质是**减少不必要的工作**，让计算机用最少的资源做最多的事情。就像整理房间一样，先找出最乱的地方重点整理，而不是把每个角落都擦得一尘不染。