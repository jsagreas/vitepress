---
title: 4、时序数据库选型与设计
---
## 📚 目录

1. [时序数据库基础概念](#1-时序数据库基础概念)
2. [主流时序数据库对比分析](#2-主流时序数据库对比分析)
3. [数据存储策略与分片设计](#3-数据存储策略与分片设计)
4. [数据保留策略与降采样](#4-数据保留策略与降采样)
5. [查询性能优化与索引设计](#5-查询性能优化与索引设计)
6. [数据备份与恢复策略](#6-数据备份与恢复策略)
7. [存储容量规划与扩容](#7-存储容量规划与扩容)
8. [数据压缩算法选择](#8-数据压缩算法选择)
9. [高可用集群部署](#9-高可用集群部署)
10. [核心要点总结](#10-核心要点总结)

---

## 1. ⏰ 时序数据库基础概念


### 1.1 什么是时序数据库

**时序数据库（Time Series Database，TSDB）** 是专门用来处理时间序列数据的数据库系统。

> **💭 生活类比**
> 
> 想象你每天记录体重变化：
> ```
> 2024-01-01 9:00  体重:70.2kg
> 2024-01-01 18:00 体重:70.5kg
> 2024-01-02 9:00  体重:70.1kg
> ```
> 这就是典型的时序数据 - 按时间顺序记录的测量值

### 1.2 时序数据的特点


**核心特征**：
- **时间戳**：每条记录都有准确的时间标记
- **只增不改**：通常只插入新数据，很少修改历史数据
- **大量写入**：监控系统每秒可能产生数百万个数据点
- **查询模式固定**：主要按时间范围查询和聚合

```
时序数据结构示意：
时间戳        标签(Tags)           数值(Value)
2024-01-01T10:00:00Z  server=web01,cpu=user    85.2
2024-01-01T10:00:00Z  server=web01,cpu=system  12.1
2024-01-01T10:00:00Z  server=web02,cpu=user    76.8
```

### 1.3 为什么需要专门的时序数据库


**传统关系数据库的问题**：
- **写入性能差**：频繁插入导致索引维护开销大
- **查询效率低**：时间范围查询需要全表扫描
- **存储空间大**：无法有效压缩时序数据
- **扩展困难**：难以应对海量数据写入

**时序数据库的优势**：
```
📊 性能对比
           传统MySQL    时序数据库
写入性能   1万条/秒     100万条/秒  
查询速度   秒级         毫秒级
存储空间   100%        20-30%
扩展性     垂直扩展      水平扩展
```

---

## 2. 🔍 主流时序数据库对比分析


### 2.1 Prometheus - 监控领域的王者


**核心特点**：
- **拉取模式**：主动从目标抓取指标数据
- **PromQL**：强大的查询语言，专为时序数据设计
- **生态丰富**：与Kubernetes、Grafana完美集成

```yaml
# Prometheus配置示例
global:
  scrape_interval: 15s  # 每15秒采集一次数据

scrape_configs:
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['localhost:9100']
    scrape_interval: 10s
```

**适用场景**：
- ✅ Kubernetes监控
- ✅ 应用性能监控（APM）
- ✅ 告警系统
- ❌ 大规模历史数据存储（单机版本）

**优缺点对比**：

| 优点 | 缺点 |
|------|------|
| 🎯 监控生态完整 | 📊 单机存储限制 |
| 🔍 PromQL查询强大 | 💾 数据保留期短 |
| 🚀 部署简单 | 🔄 集群方案复杂 |
| 📈 告警功能完善 | 📝 写入API功能弱 |

### 2.2 InfluxDB - 通用时序数据库


**核心特点**：
- **推送模式**：应用主动写入数据
- **SQL兼容**：类SQL查询语法，学习成本低
- **多协议支持**：支持HTTP、UDP、TCP多种写入方式

```sql
-- InfluxQL查询示例
SELECT mean("cpu_usage") 
FROM "system" 
WHERE time > now() - 1h 
GROUP BY time(5m), "hostname"
```

**数据模型对比**：
```
InfluxDB数据结构：
measurement,tag1=value1,tag2=value2 field1=value1,field2=value2 timestamp

示例：
cpu,host=server01,region=us-west usage_idle=85.2,usage_user=10.1 1577836800000
```

**适用场景**：
- ✅ IoT数据收集
- ✅ 业务指标存储
- ✅ 日志数据分析
- ✅ 金融数据处理

### 2.3 TimescaleDB - PostgreSQL的时序扩展


**核心特点**：
- **完整SQL支持**：100%兼容PostgreSQL
- **自动分区**：基于时间的智能分片
- **混合负载**：既支持时序查询，也支持关系查询

```sql
-- TimescaleDB自动分区
SELECT create_hypertable('conditions', 'time');

-- 标准SQL查询
SELECT time_bucket('1 hour', time) as hour,
       avg(temperature) as avg_temp
FROM conditions 
WHERE time > NOW() - INTERVAL '24 hours'
GROUP BY hour
ORDER BY hour;
```

**适用场景**：
- ✅ 既有PostgreSQL技能的团队
- ✅ 需要复杂SQL查询的场景
- ✅ 混合工作负载（时序+关系数据）
- ✅ 企业级数据仓库

### 2.4 三大数据库详细对比


| 对比维度 | **Prometheus** | **InfluxDB** | **TimescaleDB** |
|---------|----------------|--------------|-----------------|
| **数据模型** | `指标+标签` | `测量+标签+字段` | `关系表+时间列` |
| **查询语言** | `PromQL` | `InfluxQL/Flux` | `标准SQL` |
| **写入性能** | `中等` | `很高` | `高` |
| **存储压缩** | `中等` | `很好` | `好` |
| **集群支持** | `需商业版` | `需商业版` | `开源支持` |
| **学习成本** | `中等` | `低` | `最低` |
| **生态系统** | `监控生态最好` | `IoT生态丰富` | `PostgreSQL生态` |

> **💡 选择建议**
> 
> - **监控告警场景** → 选择Prometheus
> - **IoT数据采集** → 选择InfluxDB  
> - **企业数据仓库** → 选择TimescaleDB
> - **混合业务场景** → TimescaleDB最灵活

---

## 3. 💾 数据存储策略与分片设计


### 3.1 时间分片策略


**为什么需要分片**：
时序数据具有天然的时间属性，按时间分片是最自然的方式。

```
时间分片示意图：
┌─ 2024-01 ─┐ ┌─ 2024-02 ─┐ ┌─ 2024-03 ─┐
│  分片1     │ │  分片2     │ │  分片3     │
│ 数据块1-100│ │数据块101-200│ │数据块201-300│
└───────────┘ └───────────┘ └───────────┘
```

**常见分片粒度**：

| 数据量级 | 建议分片粒度 | 适用场景 |
|---------|-------------|----------|
| 小型系统 | **按月分片** | 企业内部监控 |
| 中型系统 | **按周分片** | 中等规模IoT |
| 大型系统 | **按天分片** | 大规模监控平台 |
| 海量系统 | **按小时分片** | 金融交易数据 |

### 3.2 水平分片设计


**基于时间+标签的复合分片**：

```yaml
# 分片规则配置示例
sharding_rules:
  - name: "metrics_shard"
    time_column: "timestamp"
    shard_key: ["hostname", "service_name"]
    partitions:
      - time_range: "2024-01-01 to 2024-01-31"
        shards: ["shard_01", "shard_02", "shard_03"]
      - time_range: "2024-02-01 to 2024-02-28"  
        shards: ["shard_04", "shard_05", "shard_06"]
```

**分片优势分析**：
- **查询加速**：只需扫描相关分片
- **维护便利**：可以独立维护每个分片
- **扩容灵活**：新增分片不影响历史数据
- **故障隔离**：单个分片故障不影响其他分片

### 3.3 存储引擎选择


**LSM-Tree存储引擎**：
```
LSM-Tree结构特点：
┌─ 内存表(MemTable) ─┐
│   最新写入的数据    │
└─────────┬─────────┘
          │ 刷盘
┌─ Level 0 ─┐ ┌─ Level 1 ─┐ ┌─ Level N ─┐
│ 不重叠文件 │ │ 重叠文件   │ │   ...    │
└───────────┘ └───────────┘ └─────────┘
```

**适用于时序数据的原因**：
- **写入优化**：顺序写入，性能极高
- **压缩友好**：时序数据压缩比很高
- **读写分离**：写入和查询使用不同的数据结构

---

## 4. 📅 数据保留策略与降采样


### 4.1 分层数据保留策略


**典型的数据生命周期**：

```
数据保留分层策略：
┌─ 原始数据(15s) ─┐  保留7天   ┌─ 1分钟聚合 ─┐  保留30天
│    最高精度     │ ────────→ │   中等精度   │ ────────→
│    频繁查询     │           │   常规查询   │
└─────────────────┘           └──────────────┘
                                      │
                                      ▼
┌─ 5分钟聚合 ─┐  保留1年     ┌─ 1小时聚合 ─┐  长期保存
│   低精度    │ ←──────────  │   趋势分析   │
│   趋势分析  │               │   历史数据   │
└─────────────┘               └──────────────┘
```

### 4.2 自动降采样配置


**Prometheus降采样规则**：
```yaml
# recording_rules.yml
groups:
  - name: cpu_usage_aggregation
    interval: 30s
    rules:
      # 1分钟平均值
      - record: cpu_usage:rate1m
        expr: rate(cpu_usage_seconds_total[1m])
      
      # 5分钟平均值  
      - record: cpu_usage:rate5m
        expr: rate(cpu_usage_seconds_total[5m])
      
      # 1小时平均值
      - record: cpu_usage:rate1h
        expr: rate(cpu_usage_seconds_total[1h])
```

**InfluxDB降采样策略**：
```sql
-- 创建降采样策略
CREATE CONTINUOUS QUERY "cpu_mean_1h" ON "telegraf"
BEGIN
  SELECT mean("usage_idle") AS "mean_usage_idle"
  INTO "telegraf"."one_year"."cpu_1h"
  FROM "telegraf"."one_week"."cpu"
  GROUP BY time(1h), *
END
```

### 4.3 数据保留成本分析


**存储成本计算**：

| 精度级别 | 数据点/秒 | 保留期 | 存储需求 | 月成本 |
|---------|----------|--------|----------|--------|
| **15秒精度** | `4个/分钟` | `7天` | `~100GB` | `$50` |
| **1分钟聚合** | `1个/分钟` | `30天` | `~80GB` | `$40` |  
| **5分钟聚合** | `12个/小时` | `1年` | `~200GB` | `$100` |
| **1小时聚合** | `24个/天` | `3年` | `~50GB` | `$25` |

> **🧠 成本优化技巧**
> 
> 1. **合理规划精度**：不是所有指标都需要秒级精度
> 2. **及时清理**：定期删除不需要的历史数据
> 3. **压缩优化**：选择合适的压缩算法
> 4. **分级存储**：热数据SSD，冷数据HDD

---

## 5. 🚀 查询性能优化与索引设计


### 5.1 时序数据查询特点


**常见查询模式**：
```sql
-- 模式1：时间范围查询
SELECT * FROM metrics 
WHERE timestamp BETWEEN '2024-01-01' AND '2024-01-02'
  AND hostname = 'web01';

-- 模式2：聚合查询
SELECT time_bucket('5 minutes', timestamp) as time_period,
       avg(cpu_usage) as avg_cpu
FROM metrics 
WHERE timestamp > now() - interval '24 hours'
GROUP BY time_period
ORDER BY time_period;

-- 模式3：多维度查询
SELECT hostname, service_name, max(memory_usage)
FROM metrics
WHERE timestamp > now() - interval '1 hour'
  AND region = 'us-west'
GROUP BY hostname, service_name;
```

### 5.2 索引设计原则


**时序数据索引策略**：

```
复合索引设计：
┌─ 时间戳(timestamp) ─┐  主键，自然排序
├─ 主机名(hostname)  ─┤  高基数标签  
├─ 服务名(service)   ─┤  中基数标签
└─ 地区(region)      ─┘  低基数标签
```

**索引创建建议**：
- **时间列必须是主要索引**：所有查询都包含时间过滤
- **高基数标签优先**：hostname比region更适合作为索引
- **避免过多索引**：每个额外索引都会影响写入性能

### 5.3 查询优化技巧


**分页查询优化**：
```sql
-- ❌ 低效的OFFSET分页
SELECT * FROM metrics 
ORDER BY timestamp DESC 
LIMIT 1000 OFFSET 50000;  -- 会扫描前50000行

-- ✅ 基于时间戳的游标分页
SELECT * FROM metrics 
WHERE timestamp < '2024-01-01T10:00:00Z'
ORDER BY timestamp DESC 
LIMIT 1000;
```

**预聚合查询优化**：
```yaml
# 使用物化视图预计算常用聚合
CREATE MATERIALIZED VIEW cpu_usage_hourly AS
SELECT time_bucket('1 hour', timestamp) as hour,
       hostname,
       avg(cpu_usage) as avg_cpu,
       max(cpu_usage) as max_cpu,
       min(cpu_usage) as min_cpu
FROM cpu_metrics
GROUP BY hour, hostname;

# 刷新策略
REFRESH MATERIALIZED VIEW CONCURRENTLY cpu_usage_hourly;
```

**查询性能监控**：

| 查询类型 | 目标响应时间 | 监控指标 |
|---------|-------------|----------|
| **实时查询** | `< 100ms` | P95延迟 |
| **历史聚合** | `< 1s` | P99延迟 |
| **长时间范围** | `< 5s` | 平均延迟 |
| **复杂分析** | `< 30s` | 查询成功率 |

---

## 6. 💼 数据备份与恢复策略


### 6.1 备份策略设计


**分层备份方案**：

```
备份层次结构：
┌─ 实时备份(Streaming) ─┐  延迟: < 1秒
│   主从同步复制         │  RTO: < 5分钟
└─ 热备份 ──────────────┘  RPO: < 1秒
          │
          ▼
┌─ 定期快照(Snapshot)  ─┐  延迟: 每小时
│   增量备份            │  RTO: < 30分钟  
└─ 温备份 ──────────────┘  RPO: < 1小时
          │
          ▼
┌─ 长期归档(Archive)   ─┐  延迟: 每天
│   压缩存储            │  RTO: < 4小时
└─ 冷备份 ──────────────┘  RPO: < 1天
```

### 6.2 Prometheus备份实践


**文件系统级备份**：
```bash
#!/bin/bash
# Prometheus备份脚本

PROMETHEUS_DATA="/var/lib/prometheus"
BACKUP_DIR="/backup/prometheus"
DATE=$(date +%Y%m%d_%H%M%S)

# 创建快照
curl -XPOST http://localhost:9090/api/v1/admin/tsdb/snapshot

# 获取快照目录
SNAPSHOT_DIR=$(ls -t ${PROMETHEUS_DATA}/snapshots/ | head -1)

# 压缩备份
tar -czf "${BACKUP_DIR}/prometheus_${DATE}.tar.gz" \
    -C "${PROMETHEUS_DATA}/snapshots/" "${SNAPSHOT_DIR}"

# 清理旧快照
find ${PROMETHEUS_DATA}/snapshots/ -type d -mtime +7 -exec rm -rf {} \;

echo "备份完成: prometheus_${DATE}.tar.gz"
```

### 6.3 InfluxDB备份与恢复


**在线备份命令**：
```bash
# 全量备份
influxd backup -portable /backup/influx_full_$(date +%Y%m%d)

# 增量备份
influxd backup -portable -start 2024-01-01T00:00:00Z \
               -end 2024-01-02T00:00:00Z \
               /backup/influx_incremental_$(date +%Y%m%d)

# 恢复数据
influxd restore -portable /backup/influx_full_20240101
```

### 6.4 备份验证与测试


**备份完整性检查**：
```bash
#!/bin/bash
# 备份验证脚本

BACKUP_FILE="/backup/prometheus_20240101_120000.tar.gz"

# 1. 文件完整性检查
echo "检查备份文件完整性..."
if tar -tzf "$BACKUP_FILE" >/dev/null 2>&1; then
    echo "✅ 备份文件完整"
else
    echo "❌ 备份文件损坏"
    exit 1
fi

# 2. 数据一致性检查  
echo "检查数据一致性..."
TEMP_DIR="/tmp/backup_verify"
mkdir -p "$TEMP_DIR"
tar -xzf "$BACKUP_FILE" -C "$TEMP_DIR"

# 检查关键文件
if [[ -f "$TEMP_DIR/meta.json" && -d "$TEMP_DIR/chunks" ]]; then
    echo "✅ 关键数据文件存在"
else
    echo "❌ 关键数据文件缺失"
    exit 1
fi

rm -rf "$TEMP_DIR"
echo "🎉 备份验证通过"
```

---

## 7. 📈 存储容量规划与扩容


### 7.1 容量需求评估


**数据量计算公式**：

> **💡 容量计算**
> 
> **日数据量** = 指标数 × 采集频率 × 数据点大小 × 86400秒
> 
> 示例：
> - 指标数量：10,000个
> - 采集频率：15秒一次
> - 数据点大小：24字节（时间戳8字节+值8字节+标签8字节）
> 
> 日数据量 = 10,000 × (86400/15) × 24 = 1.38GB/天

**不同规模的容量规划**：

| 系统规模 | 指标数量 | 数据保留期 | 存储需求 | 建议配置 |
|---------|----------|-----------|----------|----------|
| **小型** | `< 1万` | `30天` | `~50GB` | `单机部署` |
| **中型** | `1-10万` | `90天` | `~500GB` | `主从架构` |
| **大型** | `10-100万` | `1年` | `~5TB` | `分片集群` |
| **超大型** | `> 100万` | `2年` | `~50TB` | `多级存储` |

### 7.2 存储层次设计


**热温冷数据分层**：

```
存储分层架构：
┌─ 热数据(Hot) ────────┐  近7天数据
│ • NVMe SSD存储       │  • 高IOPS需求
│ • 内存缓存           │  • 实时查询
│ • 响应 < 100ms       │
└──────────┬───────────┘
           │
┌─ 温数据(Warm) ───────┐  7-90天数据  
│ • SATA SSD存储       │  • 中等IOPS
│ • 定期查询           │  • 历史分析
│ • 响应 < 1s          │
└──────────┬───────────┘
           │
┌─ 冷数据(Cold) ───────┐  > 90天数据
│ • 机械硬盘/对象存储   │  • 低IOPS
│ • 归档压缩           │  • 长期保存
│ • 响应 < 10s         │
└─────────────────────┘
```

### 7.3 自动扩容策略


**基于监控指标的扩容**：
```yaml
# 自动扩容规则配置
autoscaling:
  triggers:
    - metric: "disk_usage_percent"
      threshold: 80
      action: "add_storage"
      
    - metric: "write_ops_per_second"  
      threshold: 10000
      action: "add_write_node"
      
    - metric: "query_latency_p99"
      threshold: 5000  # 5秒
      action: "add_read_replica"

  actions:
    add_storage:
      type: "expand_volume"
      increment: "500GB"
      
    add_write_node:
      type: "scale_out"
      node_spec: "4core_16gb"
      
    add_read_replica:
      type: "add_replica"
      replica_count: 1
```

---

## 8. 🗜️ 数据压缩算法选择


### 8.1 时序数据压缩原理


**时序数据的特点决定了高压缩比**：
- **时间戳规律性**：等间隔采集，可以差分压缩
- **数值相关性**：相邻数值变化不大，适合增量编码
- **模式重复**：标签组合有限，可以字典压缩

```
压缩前后对比：
原始数据：
timestamp:1704067200, value:85.2, labels:"host=web01,cpu=user"
timestamp:1704067215, value:85.4, labels:"host=web01,cpu=user"  
timestamp:1704067230, value:85.1, labels:"host=web01,cpu=user"

压缩后：
base_timestamp:1704067200, delta:[0,15,30]
base_value:85.2, delta:[0,0.2,-0.3]  
label_dict:{0:"host=web01,cpu=user"}, refs:[0,0,0]
```

### 8.2 主流压缩算法对比


**算法性能对比**：

| 压缩算法 | 压缩比 | 压缩速度 | 解压速度 | CPU开销 | 适用场景 |
|---------|--------|----------|----------|---------|----------|
| **Snappy** | `3-5倍` | `很快` | `很快` | `低` | `实时写入` |
| **LZ4** | `3-6倍` | `极快` | `极快` | `极低` | `高并发写入` |
| **ZSTD** | `5-15倍` | `中等` | `快` | `中等` | `存储优化` |
| **Gorilla** | `10-20倍` | `快` | `快` | `低` | `时序专用` |

### 8.3 Facebook Gorilla算法详解


**Gorilla算法特点**：
- **专为时序数据设计**：利用时序数据的特殊性质
- **双重压缩**：时间戳压缩 + 数值压缩
- **在线压缩**：边写入边压缩，不影响查询

```
Gorilla时间戳压缩：
第一个时间戳：存储完整值
后续时间戳：存储与预期值的差异

示例：
T1: 1704067200 (完整存储)
T2: 1704067215 (预期+15，实际+15，差异0，存储0)  
T3: 1704067230 (预期+15，实际+15，差异0，存储0)
T4: 1704067247 (预期+15，实际+17，差异+2，存储2)
```

### 8.4 压缩策略配置


**InfluxDB压缩配置**：
```toml
# influxdb.conf
[data]
  # 启用TSM压缩
  cache-max-memory-size = "1g"
  compact-full-write-cold-duration = "4h"
  
  # 压缩算法选择
  tsm-use-madv-willneed = true
  
  # 压缩级别
  max-series-per-database = 1000000
  max-values-per-tag = 100000
```

---

## 9. 🏗️ 高可用集群部署


### 9.1 高可用架构设计


**三层高可用架构**：

```
高可用集群架构：
                    ┌─ Load Balancer ─┐
                    │   (Nginx/HAProxy) │
                    └─────────┬─────────┘
                              │
              ┌───────────────┼───────────────┐
              │               │               │
    ┌─ Write Node1 ─┐ ┌─ Write Node2 ─┐ ┌─ Write Node3 ─┐
    │   Primary      │ │   Secondary   │ │   Secondary   │
    │   写入主节点    │ │   写入备节点   │ │   写入备节点   │
    └────────┬──────┘ └───────┬───────┘ └───────┬───────┘
             │                │                 │
             └────────┬───────┴─────────────────┘
                      │ 数据同步
              ┌───────┼───────┐
              │       │       │
    ┌─ Read Replica1─┐ ┌─ Read Replica2─┐ ┌─ Read Replica3─┐
    │   只读节点1    │ │   只读节点2    │ │   只读节点3    │  
    │   查询负载均衡  │ │   查询负载均衡  │ │   查询负载均衡  │
    └───────────────┘ └───────────────┘ └───────────────┘
```

### 9.2 Prometheus高可用部署


**Prometheus联邦架构**：
```yaml
# 全局Prometheus配置
global:
  scrape_interval: 15s
  external_labels:
    cluster: 'global'
    replica: '1'

scrape_configs:
  # 从本地Prometheus联邦
  - job_name: 'federate'
    scrape_interval: 15s
    honor_labels: true
    metrics_path: '/federate'
    params:
      'match[]':
        - '{job="prometheus"}'
        - '{__name__=~"job:.*"}'
    static_configs:
      - targets:
        - 'prometheus-shard1:9090'
        - 'prometheus-shard2:9090'
        - 'prometheus-shard3:9090'
```

**Thanos长期存储方案**：
```yaml
# Thanos Sidecar配置
apiVersion: apps/v1
kind: Deployment
metadata:
  name: thanos-sidecar
spec:
  template:
    spec:
      containers:
      - name: thanos-sidecar
        image: thanosio/thanos:v0.31.0
        args:
        - sidecar
        - --tsdb.path=/var/prometheus
        - --prometheus.url=http://localhost:9090
        - --objstore.config-file=/etc/thanos/bucket.yml
        - --shipper.upload-compacted
```

### 9.3 InfluxDB企业集群


**InfluxDB集群架构**：
```
InfluxDB Enterprise集群：
┌─ Data Nodes ──────────────┐
│  ┌─ Node1 ─┐ ┌─ Node2 ─┐  │  存储数据
│  │ Shard1  │ │ Shard2  │  │  处理写入
│  │ Shard3  │ │ Shard4  │  │  执行查询
│  └─────────┘ └─────────┘  │
└───────────────────────────┘
             │
         协调管理
             │
┌─ Meta Nodes ─────────────┐
│  ┌─ Meta1 ─┐ ┌─ Meta2 ─┐ │  管理集群
│  │ Leader  │ │Follower │ │  分片协调
│  └─────────┘ └─────────┘ │  一致性保证
└───────────────────────────┘
```

### 9.4 故障转移机制


**自动故障检测与切换**：
```bash
#!/bin/bash
# 健康检查脚本

PROMETHEUS_ENDPOINT="http://localhost:9090/-/healthy"
BACKUP_ENDPOINT="http://backup-prometheus:9090"

# 检查主节点健康状态
check_health() {
    curl -s -o /dev/null -w "%{http_code}" "$PROMETHEUS_ENDPOINT"
}

# 执行故障转移
failover() {
    echo "检测到主节点故障，执行故障转移..."
    
    # 更新负载均衡器配置
    consul-template -template="nginx.conf.tpl:nginx.conf:nginx -s reload"
    
    # 通知告警系统
    curl -X POST "$ALERT_WEBHOOK" \
         -d '{"text":"Prometheus failover executed"}'
    
    echo "故障转移完成"
}

# 主检查逻辑
if [[ $(check_health) != "200" ]]; then
    failover
fi
```

**容灾级别定义**：

| 容灾级别 | RTO目标 | RPO目标 | 方案特点 |
|---------|---------|---------|----------|
| **基础级** | `< 30分钟` | `< 5分钟` | `主从热备` |
| **标准级** | `< 5分钟` | `< 1分钟` | `自动故障转移` |
| **高级** | `< 30秒` | `< 10秒` | `多活架构` |
| **金融级** | `< 10秒` | `< 3秒` | `同城双活` |

---

## 10. 📋 核心要点总结


### 10.1 时序数据库选型决策


> **📌 选型核心原则**
> 
> **监控告警场景** → **Prometheus**
> - 生态最完善，与Kubernetes集成最好
> - PromQL查询语言强大，告警功能完备
> 
> **IoT数据采集** → **InfluxDB**  
> - 写入性能最优，支持多种协议
> - 压缩比高，适合大规模数据存储
> 
> **企业数据仓库** → **TimescaleDB**
> - SQL兼容性最好，学习成本最低
> - 支持复杂查询，可处理混合负载

### 10.2 架构设计关键要素


**🔧 存储设计**：
- **时间分片**：按业务量级选择合适的分片粒度
- **数据分层**：热温冷数据使用不同存储介质
- **压缩策略**：选择适合的压缩算法平衡性能和空间

**🚀 性能优化**：
- **索引设计**：时间戳主索引，高基数标签辅助索引
- **查询优化**：使用预聚合和物化视图
- **降采样**：多层次数据保留策略

**🛡️ 可靠性保障**：
- **备份策略**：分层备份，定期验证
- **高可用**：主从架构 + 故障自动转移
- **监控告警**：全方位性能和健康监控

### 10.3 运维最佳实践


**📊 容量规划**：
```yaml
监控指标:
  - 磁盘使用率 > 80% → 扩容告警
  - 写入延迟 P99 > 1s → 性能告警  
  - 查询成功率 < 99% → 可用性告警
  - 备份失败次数 > 0 → 可靠性告警
```

**🔄 日常维护**：
- **定期备份验证**：每周执行恢复测试
- **性能调优**：根据监控数据优化配置
- **容量评估**：月度容量使用情况分析
- **版本升级**：关注安全更新和新特性

**🧠 记忆要点**：
- 时序数据库核心是**时间 + 标签 + 数值**
- 选型关键看**场景匹配度**而非单纯性能
- 架构设计要考虑**现在需求**和**未来扩展**
- 运维重点是**自动化**和**可观测性**

> **💡 最后建议**
> 
> 时序数据库选型没有银弹，要结合具体业务场景：
> - **小规模**：优先考虑简单性和维护成本
> - **大规模**：重点关注性能和可扩展性  
> - **企业级**：需要平衡功能完整性和技术生态
> 
> 记住：**合适的才是最好的**！