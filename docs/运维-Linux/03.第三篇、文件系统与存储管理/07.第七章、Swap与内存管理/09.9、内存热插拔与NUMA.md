---
title: 9、内存热插拔与NUMA
---
## 📚 目录

1. [内存热插拔基本概念](#1-内存热插拔基本概念)
2. [NUMA架构内存管理](#2-NUMA架构内存管理)
3. [numactl工具使用方法](#3-numactl工具使用方法)
4. [内存节点亲和性设置](#4-内存节点亲和性设置)
5. [/proc/zoneinfo内存区域信息](#5-proc-zoneinfo内存区域信息)
6. [内存迁移与平衡机制](#6-内存迁移与平衡机制)
7. [NUMA感知应用优化](#7-NUMA感知应用优化)
8. [大页内存配置使用](#8-大页内存配置使用)
9. [核心要点总结](#9-核心要点总结)

---

## 1. 🔧 内存热插拔基本概念


### 1.1 什么是内存热插拔


> 💡 **概念理解**：内存热插拔就是在系统运行时动态添加或移除内存条，无需关机重启

**传统模式 vs 热插拔模式**：
```
传统模式：关机 → 插拔内存 → 开机 → 系统识别
热插拔模式：在线添加/移除 → 系统实时识别 → 无需重启
```

**核心优势**：
- ✅ **零停机时间** - 业务不中断
- ✅ **动态扩容** - 根据需求实时调整
- ✅ **故障恢复** - 坏内存条可在线更换
- ✅ **资源优化** - 闲置内存可回收利用

### 1.2 热插拔的工作原理


**系统架构支持**：
```
硬件层面：
┌─────────────────┐
│   内存控制器     │ ← 支持热插拔检测
├─────────────────┤
│   内存插槽      │ ← 物理热插拔接口
├─────────────────┤
│   电源管理      │ ← 动态电源控制
└─────────────────┘

软件层面：
内核模块 → ACPI接口 → 设备管理 → 内存管理子系统
```

> ⚠️ **重要提醒**：不是所有服务器都支持内存热插拔，需要硬件和BIOS的专门支持

### 1.3 内存状态管理


**内存块状态分类**：
- **online** - 在线可用状态
- **offline** - 离线不可用状态  
- **going-offline** - 正在下线过程中
- **movable** - 可迁移状态

```bash
# 查看内存块状态
cat /sys/devices/system/memory/memory*/state

# 示例输出
online
online
offline
movable
```

---

## 2. 🏗️ NUMA架构内存管理


### 2.1 NUMA架构是什么


> 📖 **NUMA概念**：Non-Uniform Memory Access，非一致性内存访问架构

**传统SMP vs NUMA对比**：
```
SMP架构（对称多处理器）：
    CPU1    CPU2    CPU3    CPU4
     │       │       │       │
     └───────┼───────┼───────┘
             │
        ┌────▼────┐
        │  共享内存 │ ← 所有CPU访问内存延迟相同
        └─────────┘

NUMA架构：
   Node0              Node1
 ┌─────────┐        ┌─────────┐
 │ CPU0 CPU1│        │ CPU2 CPU3│
 │    │    │        │    │    │
 │ ┌──▼──┐ │        │ ┌──▼──┐ │
 │ │Local│ │←──互连──→│ │Local│ │
 │ │Memory│ │        │ │Memory│ │
 │ └─────┘ │        │ └─────┘ │
 └─────────┘        └─────────┘
   本地内存快         远程内存慢
```

**NUMA的核心思想**：
- **本地内存访问** - CPU访问本节点内存速度快
- **远程内存访问** - CPU访问其他节点内存速度慢  
- **内存亲和性** - 尽量让进程使用本地内存

### 2.2 NUMA节点信息查看


```bash
# 查看NUMA节点拓扑
numactl --hardware

# 示例输出解读
available: 2 nodes (0-1)          # 有2个NUMA节点
node 0 cpus: 0 1 2 3             # 节点0包含CPU 0-3
node 0 size: 16384 MB            # 节点0有16GB内存
node 0 free: 12288 MB            # 节点0空闲12GB
node 1 cpus: 4 5 6 7             # 节点1包含CPU 4-7
node 1 size: 16384 MB            # 节点1有16GB内存
node distances:                   # 节点间距离（访问延迟）
node   0   1
  0:  10  21                     # 本地访问=10，远程访问=21
  1:  21  10
```

> 🔍 **距离含义**：数字越小访问越快，本地通常是10，远程可能是20-30

### 2.3 内存分配策略


**NUMA内存分配策略**：

| 策略名称 | **描述** | **适用场景** |
|---------|---------|-------------|
| **default** | `优先本地节点，不足时使用其他节点` | `一般应用` |
| **bind** | `严格绑定到指定节点` | `性能敏感应用` |
| **interleave** | `轮询分配到多个节点` | `内存密集型应用` |
| **preferred** | `首选指定节点，不足时其他节点` | `大多数生产环境` |

---

## 3. 🛠️ numactl工具使用方法


### 3.1 numactl基础用法


**安装numactl工具**：
```bash
# RHEL/CentOS系列
yum install numactl

# Debian/Ubuntu系列  
apt-get install numactl
```

**基本语法结构**：
```bash
numactl [选项] [命令]
```

### 3.2 常用查看命令


```bash
# 查看NUMA硬件信息
numactl --hardware

# 查看当前进程NUMA策略
numactl --show

# 示例输出
policy: default                   # 当前策略
preferred node: current          # 首选节点
physcpubind: 0 1 2 3 4 5 6 7    # 可用CPU
cpubind: 0 1                     # CPU绑定
nodebind: 0 1                    # 节点绑定
membind: 0 1                     # 内存绑定
```

### 3.3 内存策略设置


**绑定到特定节点**：
```bash
# 将应用绑定到节点0
numactl --membind=0 ./my_app

# 将应用绑定到节点0和1
numactl --membind=0,1 ./my_app

# CPU和内存都绑定到节点0
numactl --cpunodebind=0 --membind=0 ./my_app
```

**交错分配内存**：
```bash
# 在节点0和1之间交错分配内存
numactl --interleave=0,1 ./my_app

# 在所有节点间交错分配
numactl --interleave=all ./my_app
```

> 💡 **实用技巧**：对于内存访问模式不规则的应用，使用interleave策略通常能获得更好的平均性能

### 3.4 进程内存迁移


```bash
# 将正在运行的进程内存迁移到节点1
numactl --physcpubind=4-7 --membind=1 --pid=12345

# 将进程的页面从节点0迁移到节点1  
migratepages 12345 0 1
```

**检查迁移效果**：
```bash
# 查看进程的NUMA内存使用情况
numastat -p 12345
```

---

## 4. ⚖️ 内存节点亲和性设置


### 4.1 什么是内存亲和性


> 📖 **亲和性概念**：让进程优先使用离其运行的CPU最近的内存，减少访问延迟

**亲和性的好处**：
```
本地内存访问：
CPU0 → Node0内存：延迟 ~100ns
          ↓
        性能最优

远程内存访问：  
CPU0 → Node1内存：延迟 ~300ns
          ↓
        性能下降2-3倍
```

### 4.2 设置CPU和内存亲和性


**taskset + numactl组合使用**：
```bash
# 将进程绑定到CPU0-3和对应的内存节点0
taskset -c 0-3 numactl --membind=0 ./my_app

# 更简洁的写法
numactl --cpunodebind=0 --membind=0 ./my_app
```

**动态调整运行中的进程**：
```bash
# 获取进程PID
pgrep my_app

# 调整CPU亲和性
taskset -cp 0-3 12345

# 调整内存亲和性
echo 0 > /proc/12345/numa_maps
```

### 4.3 查看亲和性设置


```bash
# 查看进程CPU亲和性
taskset -p 12345

# 查看进程内存使用分布
numastat -p 12345

# 示例输出
Per-node process memory usage (in MBs) for PID 12345 (my_app)
                           Node 0          Node 1           Total
                  --------------- --------------- ---------------
Huge                         0.00            0.00            0.00
Heap                       256.50           12.25          268.75
Stack                        2.00            0.00            2.00
Private                    512.75           28.50          541.25
```

> 🔍 **解读技巧**：理想情况下，进程应该主要使用一个节点的内存，如果分布很平均可能存在亲和性问题

---

## 5. 📊 /proc/zoneinfo内存区域信息


### 5.1 zoneinfo文件解读


> 📖 **zoneinfo作用**：显示系统内存区域的详细统计信息，帮助理解内存使用情况

```bash
# 查看内存区域信息
cat /proc/zoneinfo
```

**主要内存区域类型**：
```
DMA: Direct Memory Access区域（0-16MB）
├─ 用于: 老式设备的DMA操作
└─ 特点: 地址空间受限

DMA32: 32位DMA区域（0-4GB）  
├─ 用于: 32位设备的DMA操作
└─ 特点: 现代系统的DMA首选区域

Normal: 普通内存区域（>4GB）
├─ 用于: 一般的内存分配
└─ 特点: 系统主要内存区域

Movable: 可移动内存区域
├─ 用于: 支持内存热插拔
└─ 特点: 页面可以迁移
```

### 5.2 关键统计信息


**每个区域的重要字段**：
```bash
# 示例输出片段
Node 0, zone   Normal
  pages free     12345      # 空闲页面数
        min      2048       # 最小水位线
        low      2560       # 低水位线  
        high     3072       # 高水位线
        scanned  0          # 扫描的页面数
        spanned  524288     # 区域总页面数
        present  524288     # 实际可用页面数
        managed  508234     # 受管理的页面数
```

> 💡 **水位线含义**：
> - **min**: 内存极度不足的临界线
> - **low**: 开始后台回收的阈值  
> - **high**: 停止后台回收的阈值

### 5.3 内存压力分析


**通过zoneinfo判断内存压力**：
```bash
# 编写简单的内存监控脚本
#!/bin/bash
grep -E "(free|min|low|high)" /proc/zoneinfo | \
while read zone metric value; do
    if [[ $metric == "free" ]] && [[ $value -lt 1000 ]]; then
        echo "警告: $zone 区域内存不足，仅剩 $value 页"
    fi
done
```

**内存回收活动监控**：
```bash
# 监控页面扫描活动
watch -n 1 'grep scanned /proc/zoneinfo'
```

---

## 6. 🔄 内存迁移与平衡机制


### 6.1 内存迁移的概念


> 📖 **内存迁移**：将内存页面从一个NUMA节点搬移到另一个节点，优化内存访问性能

**迁移触发条件**：
- 🎯 **手动迁移** - 管理员主动调整
- 🔄 **自动平衡** - 系统检测到访问模式变化
- 🚨 **内存不足** - 某个节点内存紧张
- 🔧 **亲和性调整** - 进程绑定策略改变

### 6.2 手动内存迁移


**使用migratepages工具**：
```bash
# 将进程12345的内存从节点0迁移到节点1
migratepages 12345 0 1

# 批量迁移多个节点
migratepages 12345 0,2 1,3
```

**监控迁移进度**：
```bash
# 迁移前后对比内存分布
numastat -p 12345 | tee before.txt
migratepages 12345 0 1
numastat -p 12345 | tee after.txt
diff before.txt after.txt
```

### 6.3 自动内存平衡


**NUMA balancing机制**：
```bash
# 查看自动平衡状态
cat /proc/sys/kernel/numa_balancing

# 启用自动平衡（通常默认启用）
echo 1 > /proc/sys/kernel/numa_balancing
```

**平衡策略调优**：
```bash
# 调整页面扫描间隔（毫秒）
echo 1000 > /proc/sys/kernel/numa_balancing_scan_period_min_ms
echo 60000 > /proc/sys/kernel/numa_balancing_scan_period_max_ms

# 调整扫描页面大小
echo 256 > /proc/sys/kernel/numa_balancing_scan_size_mb
```

> ⚠️ **注意事项**：自动平衡会带来一定的CPU开销，对于延迟敏感的应用建议关闭

### 6.4 内存平衡监控


```bash
# 查看NUMA统计信息
cat /proc/vmstat | grep numa

# 关键指标解释：
numa_hit          # 本地节点内存分配成功次数
numa_miss         # 远程节点内存分配次数  
numa_foreign      # 其他节点在本节点分配次数
numa_pages_migrated # 已迁移的页面数
```

---

## 7. 🚀 NUMA感知应用优化


### 7.1 应用程序优化策略


**数据库应用优化**：
```bash
# MySQL优化示例
numactl --interleave=all mysqld \
  --innodb-numa-interleave=1

# Redis优化示例  
numactl --cpunodebind=0 --membind=0 redis-server
```

**Web服务器优化**：
```bash
# Nginx多进程NUMA绑定
# 进程1绑定到节点0
numactl --cpunodebind=0 --membind=0 nginx -g "worker_processes 4;"

# 进程2绑定到节点1
numactl --cpunodebind=1 --membind=1 nginx -g "worker_processes 4;"
```

### 7.2 编程层面的NUMA优化


**内存分配策略**：
```c
#include <numaif.h>

// 分配本地节点内存
void* local_malloc(size_t size) {
    int node = numa_node_of_cpu(sched_getcpu());
    return numa_alloc_onnode(size, node);
}

// 检查内存访问模式
void check_memory_locality() {
    unsigned long nodes[8];
    get_mempolicy(NULL, nodes, 8, ptr, MPOL_F_ADDR);
}
```

**线程亲和性设置**：
```c
#define _GNU_SOURCE
#include <pthread.h>
#include <sched.h>

void bind_thread_to_node(pthread_t thread, int node) {
    cpu_set_t cpuset;
    CPU_ZERO(&cpuset);
    
    // 将线程绑定到指定节点的CPU
    for (int i = node * 4; i < (node + 1) * 4; i++) {
        CPU_SET(i, &cpuset);
    }
    
    pthread_setaffinity_np(thread, sizeof(cpuset), &cpuset);
}
```

### 7.3 应用性能监控


```bash
# 监控应用的NUMA性能
#!/bin/bash
APP_PID=$(pgrep my_app)

while true; do
    echo "=== $(date) ==="
    
    # CPU使用分布
    taskset -p $APP_PID
    
    # 内存使用分布  
    numastat -p $APP_PID
    
    # 内存访问统计
    grep numa /proc/$APP_PID/numa_maps | head -5
    
    sleep 10
done
```

---

## 8. 📄 大页内存配置使用


### 8.1 大页内存的概念


> 📖 **大页内存**：使用更大的内存页面（通常2MB或1GB），减少页表开销，提高内存访问效率

**普通页面 vs 大页面对比**：
```
普通页面：4KB per page
┌───┬───┬───┬───┐
│ 4K│ 4K│ 4K│ 4K│ ← 1MB需要256个页表项
└───┴───┴───┴───┘

大页面：2MB per page  
┌───────────────────┐
│       2MB         │ ← 1MB只需要1个页表项
└───────────────────┘

优势：减少TLB miss，降低页表内存占用
```

### 8.2 大页内存配置


**查看大页内存状态**：
```bash
# 查看大页配置信息
cat /proc/meminfo | grep -i huge

# 示例输出
HugePages_Total:      512      # 总大页数
HugePages_Free:       256      # 空闲大页数  
HugePages_Rsvd:        64      # 已预留大页数
Hugepagesize:        2048 kB   # 大页大小
```

**配置大页内存**：
```bash
# 临时配置（重启失效）
echo 512 > /proc/sys/vm/nr_hugepages

# 永久配置
echo "vm.nr_hugepages = 512" >> /etc/sysctl.conf
sysctl -p
```

**NUMA环境大页配置**：
```bash
# 为特定NUMA节点分配大页
echo 256 > /sys/devices/system/node/node0/hugepages/hugepages-2048kB/nr_hugepages
echo 256 > /sys/devices/system/node/node1/hugepages/hugepages-2048kB/nr_hugepages
```

### 8.3 应用程序使用大页


**直接内存映射**：
```bash
# 挂载hugetlbfs文件系统
mkdir /mnt/hugepages
mount -t hugetlbfs nodev /mnt/hugepages

# 应用程序通过文件映射使用大页
# (在应用代码中使用mmap映射/mnt/hugepages下的文件)
```

**透明大页设置**：
```bash
# 查看透明大页状态
cat /sys/kernel/mm/transparent_hugepage/enabled

# 设置为always（积极使用）
echo always > /sys/kernel/mm/transparent_hugepage/enabled

# 设置为madvise（按需使用）
echo madvise > /sys/kernel/mm/transparent_hugepage/enabled

# 完全禁用
echo never > /sys/kernel/mm/transparent_hugepage/enabled
```

### 8.4 大页内存监控


**监控大页使用情况**：
```bash
# 查看系统大页统计
cat /proc/meminfo | grep -i huge

# 查看进程大页使用
cat /proc/PID/smaps | grep -i huge

# 监控透明大页统计
cat /proc/vmstat | grep thp
```

**性能对比测试**：
```bash
#!/bin/bash
# 测试大页内存vs普通内存的性能差异

echo "测试普通内存性能..."
time ./memory_test --normal-pages

echo "测试大页内存性能..."  
time ./memory_test --huge-pages

echo "测试完成，对比结果"
```

---

## 9. 📋 核心要点总结


### 9.1 必须掌握的核心概念


```
🔸 内存热插拔：在线添加/移除内存，零停机扩容
🔸 NUMA架构：非一致性内存访问，本地内存快，远程内存慢  
🔸 内存亲和性：让进程使用距离最近的内存，优化访问性能
🔸 内存迁移：页面在节点间搬移，平衡负载和性能
🔸 大页内存：使用大页面减少页表开销，提升访问效率
```

### 9.2 关键理解要点


**🔹 NUMA的本质优势**
```
理解核心：
- CPU访问本地内存延迟低（~100ns）
- CPU访问远程内存延迟高（~300ns）  
- 合理的内存亲和性可提升2-3倍性能
- 不当的内存分配会严重拖累性能
```

**🔹 工具使用场景**
```
numactl: 启动时指定NUMA策略
taskset: 绑定进程到特定CPU  
migratepages: 迁移已运行进程的内存
numastat: 监控内存使用分布
/proc/zoneinfo: 诊断内存压力问题
```

**🔹 优化策略选择**
```
CPU密集型应用：
→ 使用cpunodebind绑定CPU和内存到同一节点

内存密集型应用：  
→ 使用interleave分散内存访问压力

I/O密集型应用：
→ 使用preferred策略，允许跨节点分配
```

### 9.3 实际应用指导


**生产环境最佳实践**：
- ✅ **监控先行**：先用numastat了解当前内存分布
- ✅ **渐进调优**：从默认策略开始，逐步优化
- ✅ **压力测试**：NUMA优化后必须验证性能效果
- ✅ **文档记录**：记录优化策略和效果，便于维护

**常见问题解决思路**：
- 🔧 **内存访问慢** → 检查NUMA亲和性设置
- 🔧 **内存分布不均** → 使用interleave策略
- 🔧 **单节点内存不足** → 考虑迁移或扩容
- 🔧 **应用性能下降** → 对比NUMA优化前后的metrics

**性能优化检查清单**：
- [ ] 确认硬件支持NUMA和内存热插拔
- [ ] 了解应用的内存访问模式  
- [ ] 选择合适的NUMA内存分配策略
- [ ] 配置CPU和内存亲和性
- [ ] 监控内存使用分布和访问延迟
- [ ] 根据监控结果调整优化策略
- [ ] 验证优化效果并建立基线

**核心记忆口诀**：
- NUMA本地快，远程访问慢，亲和性设置很关键
- 热插拔内存，在线就扩容，业务不停机
- 大页减开销，TLB miss少，内存访问效率高
- numactl工具好，监控调优离不了