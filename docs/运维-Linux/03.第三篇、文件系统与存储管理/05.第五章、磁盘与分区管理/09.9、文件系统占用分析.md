---
title: 9、文件系统占用分析
---
## 📚 目录

1. [文件系统占用分析概述](#1-文件系统占用分析概述)
2. [find命令按大小查找文件](#2-find命令按大小查找文件)
3. [日志文件增长监控与轮转](#3-日志文件增长监控与轮转)
4. [临时文件清理策略](#4-临时文件清理策略)
5. [缓存文件识别与管理](#5-缓存文件识别与管理)
6. [重复文件检测与去重](#6-重复文件检测与去重)
7. [文件系统碎片化检测](#7-文件系统碎片化检测)
8. [目录层次深度分析](#8-目录层次深度分析)
9. [文件访问时间统计分析](#9-文件访问时间统计分析)
10. [核心要点总结](#10-核心要点总结)

---

## 1. 💾 文件系统占用分析概述


### 1.1 什么是文件系统占用分析


**简单理解**：就像整理房间一样，我们需要知道哪些东西占地方最多，哪些可以扔掉，哪些需要整理。

```
文件系统占用分析 = 磁盘空间管理 + 性能优化 + 系统维护

核心目的：
• 找出占用空间最大的文件和目录
• 识别可以删除的垃圾文件
• 监控文件增长趋势
• 优化存储使用效率
```

### 1.2 为什么要做占用分析


**实际问题场景**：
```
💥 常见问题：
• 磁盘空间不足，系统运行缓慢
• 日志文件疯狂增长，把硬盘撑爆
• 找不到哪个程序在偷偷占用空间
• 重复文件太多，浪费存储资源
• 系统越用越慢，不知道原因

🎯 分析的价值：
• 及时发现空间问题，避免系统崩溃
• 优化存储使用，节约成本
• 提升系统性能
• 制定合理的清理策略
```

### 1.3 分析工具概览


```
基础工具套装：
┌─────────────┬─────────────┬─────────────────┐
│   工具名    │   主要用途   │     适用场景     │
├─────────────┼─────────────┼─────────────────┤
│ find        │ 查找文件     │ 按条件精确查找   │
│ du          │ 目录占用     │ 查看文件夹大小   │
│ df          │ 磁盘使用     │ 整体空间使用情况 │
│ ls          │ 文件列表     │ 查看文件基本信息 │
│ lsof        │ 打开文件     │ 查看被占用文件   │
└─────────────┴─────────────┴─────────────────┘
```

---

## 2. 🔍 find命令按大小查找文件


### 2.1 find命令基本语法


**核心概念**：`find`就像一个超级侦探，可以根据各种线索找到你要的文件。

```bash
# 基本语法格式
find [搜索路径] [搜索条件] [动作]

# 按大小搜索的核心参数
-size [±]数字[单位]
```

**大小单位说明**：
```
单位对照表：
c = 字节 (bytes)
k = KB (1024字节)  
M = MB (1024KB)
G = GB (1024MB)

符号含义：
+100M = 大于100MB
-100M = 小于100MB  
100M  = 正好100MB (很少用)
```

### 2.2 实用查找示例


**找出系统中的大文件**：
```bash
# 查找大于100MB的文件
find / -type f -size +100M 2>/dev/null

# 查找大于1GB的文件，并显示详细信息
find / -type f -size +1G -ls 2>/dev/null

# 在用户目录下查找大文件
find /home -type f -size +50M -exec ls -lh {} \;
```

> **💡 小技巧**：`2>/dev/null`的作用是把错误信息扔掉，避免看到一堆"权限拒绝"的提示

**按文件类型查找占用空间**：
```bash
# 查找所有视频文件
find /home -name "*.mp4" -o -name "*.avi" -o -name "*.mkv" -size +100M

# 查找所有压缩包
find /home -name "*.zip" -o -name "*.tar*" -o -name "*.rar" -size +10M

# 查找所有图片文件
find /home -name "*.jpg" -o -name "*.png" -o -name "*.bmp" -size +5M
```

### 2.3 高级查找技巧


**组合条件查找**：
```bash
# 查找最近7天内创建的大文件
find /var/log -type f -size +10M -mtime -7

# 查找超过30天未访问的大文件
find /tmp -type f -size +50M -atime +30

# 查找特定用户的大文件
find /home -user www-data -type f -size +20M
```

**统计和排序**：
```bash
# 找出最大的10个文件
find / -type f -exec ls -s {} \; 2>/dev/null | sort -rn | head -10

# 按目录统计大文件数量
find /var -type f -size +100M | cut -d'/' -f1-3 | sort | uniq -c
```

---

## 3. 📊 日志文件增长监控与轮转


### 3.1 日志文件为什么会疯狂增长


**生活类比**：日志文件就像家里的垃圾桶，不定期清理就会堆得满满的。

```
日志增长的常见原因：
┌─ 💭 为什么日志会爆炸式增长 ──┐
│ • 应用程序出错，疯狂写错误日志  │
│ • 调试级别设置过高，记录太详细  │
│ • 网络攻击，产生大量访问日志    │
│ • 系统监控程序，频繁记录状态    │
│ • 定时任务出错，重复写入错误    │
└────────────────────────────────┘
```

### 3.2 监控日志增长


**实时监控脚本**：
```bash
#!/bin/bash
# 日志增长监控脚本

# 定义要监控的日志文件
LOG_FILES=(
    "/var/log/syslog"
    "/var/log/apache2/access.log"
    "/var/log/mysql/error.log"
)

# 监控循环
while true; do
    for log_file in "${LOG_FILES[@]}"; do
        if [ -f "$log_file" ]; then
            size=$(du -m "$log_file" | cut -f1)
            echo "$(date): $log_file 当前大小: ${size}MB"
            
            # 如果超过100MB就发出警告
            if [ $size -gt 100 ]; then
                echo "⚠️  警告: $log_file 已超过100MB!"
            fi
        fi
    done
    sleep 300  # 每5分钟检查一次
done
```

**一行命令监控**：
```bash
# 监控当前目录下日志文件的变化
watch -n 60 'find /var/log -name "*.log" -size +50M -exec ls -lh {} \;'

# 查看日志文件实时增长
tail -f /var/log/syslog | while read line; do
    echo "$(date): 新日志行: $line"
done
```

### 3.3 日志轮转配置


**什么是日志轮转**：就像换垃圾袋一样，当日志文件太大时，自动创建新文件，旧的进行压缩或删除。

```bash
# logrotate配置示例
cat > /etc/logrotate.d/myapp << EOF
/var/log/myapp/*.log {
    daily          # 每天轮转
    rotate 7       # 保留7个备份
    compress       # 压缩旧文件
    delaycompress  # 延迟一天压缩
    missingok      # 文件不存在不报错
    notifempty     # 空文件不轮转
    create 644 www-data www-data  # 创建新文件权限
}
EOF

# 手动测试轮转
logrotate -d /etc/logrotate.d/myapp  # 调试模式
logrotate -f /etc/logrotate.d/myapp  # 强制轮转
```

---

## 4. 🧹 临时文件清理策略


### 4.1 临时文件都藏在哪里


**常见临时文件位置**：
```
临时文件常见位置地图：
/tmp/           ← 系统临时目录，重启会清空
/var/tmp/       ← 持久临时目录，重启不清空
/home/用户/.cache/  ← 用户缓存文件
/var/cache/     ← 系统缓存文件
/var/spool/     ← 队列文件，如邮件、打印
~/.thumbnails/  ← 图片缩略图缓存
```

### 4.2 安全清理策略


**制定清理规则**：
```bash
# 清理脚本示例
#!/bin/bash

# 1. 清理超过7天的临时文件
find /tmp -type f -atime +7 -delete
echo "已清理/tmp下7天前的文件"

# 2. 清理大的缓存文件
find /var/cache -type f -size +100M -mtime +30 -delete
echo "已清理/var/cache下的大文件"

# 3. 清理用户缓存（谨慎操作）
find /home/*/.cache -type f -atime +30 -size +10M -delete
echo "已清理用户缓存中的大文件"

# 4. 清理日志备份
find /var/log -name "*.gz" -mtime +90 -delete
echo "已清理90天前的日志备份"
```

**智能清理函数**：
```bash
# 智能清理函数
clean_temp_files() {
    local target_dir=$1
    local days=$2
    local min_size=$3
    
    echo "正在清理 $target_dir 目录..."
    
    # 统计清理前的大小
    before_size=$(du -sm "$target_dir" 2>/dev/null | cut -f1)
    
    # 执行清理
    find "$target_dir" -type f -atime +$days -size +$min_size -delete 2>/dev/null
    
    # 统计清理后的大小
    after_size=$(du -sm "$target_dir" 2>/dev/null | cut -f1)
    
    # 计算释放的空间
    freed_space=$((before_size - after_size))
    echo "释放了 ${freed_space}MB 空间"
}

# 使用示例
clean_temp_files "/tmp" 7 "1M"
clean_temp_files "/var/cache" 30 "10M"
```

### 4.3 设置定时清理


```bash
# 添加到crontab
crontab -e

# 每天凌晨2点清理临时文件
0 2 * * * /usr/local/bin/clean_temp.sh

# 每周清理一次用户缓存
0 3 * * 0 find /home/*/.cache -type f -atime +7 -delete

# 每月清理大的日志文件
0 4 1 * * find /var/log -name "*.log" -size +500M -delete
```

---

## 5. 💾 缓存文件识别与管理


### 5.1 什么是缓存文件


**简单理解**：缓存就像你的"小抄"，把常用的东西提前准备好，用的时候更快。

```
缓存文件的类型：
┌─ 🗂️ 缓存文件分类 ──────────┐
│ • 浏览器缓存：网页、图片等   │
│ • 软件包缓存：下载的安装包   │
│ • 字体缓存：系统字体信息     │
│ • 图标缓存：应用程序图标     │
│ • 编译缓存：编译产生的中间文件│
│ • 数据库缓存：查询结果缓存   │
└─────────────────────────────┘
```

### 5.2 识别不同类型的缓存


**软件包管理器缓存**：
```bash
# APT包缓存（Ubuntu/Debian）
du -sh /var/cache/apt/archives/
find /var/cache/apt/archives -name "*.deb" -size +50M

# YUM包缓存（CentOS/RHEL）  
du -sh /var/cache/yum/
yum clean all  # 清理所有缓存

# 查看包缓存统计
apt-cache stats  # Debian系
```

**应用程序缓存**：
```bash
# Python pip缓存
du -sh ~/.cache/pip/
pip cache purge  # 清理pip缓存

# Docker镜像缓存
docker system df  # 查看Docker空间使用
docker system prune -a  # 清理无用镜像

# Node.js npm缓存
du -sh ~/.npm/
npm cache clean --force  # 强制清理
```

### 5.3 智能缓存管理


**缓存清理策略**：
```bash
#!/bin/bash
# 智能缓存管理脚本

cache_manager() {
    echo "📊 缓存使用情况分析:"
    
    # 分析各种缓存大小
    echo "软件包缓存:"
    du -sh /var/cache/apt/archives/ 2>/dev/null || echo "  APT缓存不存在"
    du -sh /var/cache/yum/ 2>/dev/null || echo "  YUM缓存不存在"
    
    echo "用户缓存:"
    for user_cache in /home/*/.cache; do
        if [ -d "$user_cache" ]; then
            size=$(du -sh "$user_cache" | cut -f1)
            user=$(basename $(dirname "$user_cache"))
            echo "  $user: $size"
        fi
    done
    
    # 提供清理建议
    echo ""
    echo "🧹 清理建议:"
    
    # 检查大的缓存目录
    large_cache=$(find /var/cache /home/*/.cache -type d -exec du -sm {} \; 2>/dev/null | awk '$1>100' | sort -rn)
    
    if [ -n "$large_cache" ]; then
        echo "以下缓存目录较大，可考虑清理:"
        echo "$large_cache" | while read size dir; do
            echo "  $dir: ${size}MB"
        done
    fi
}

# 执行分析
cache_manager
```

---

## 6. 🔄 重复文件检测与去重


### 6.1 为什么会有重复文件


```
重复文件产生的原因：
┌─ 💭 重复文件怎么来的 ──────┐
│ • 复制粘贴时不小心重复了    │
│ • 不同目录存了同样的备份    │
│ • 下载文件时重名自动改名    │
│ • 程序自动生成相同内容      │
│ • 多人使用同一系统重复保存  │
└────────────────────────────┘
```

### 6.2 使用fdupes工具去重


**安装和基本使用**：
```bash
# 安装fdupes
apt install fdupes  # Ubuntu/Debian
yum install fdupes  # CentOS/RHEL

# 基本语法
fdupes [选项] 目录

# 常用选项
-r    递归搜索子目录
-S    显示重复文件大小
-m    显示MD5值
-d    交互式删除重复文件
-N    不提示，直接处理
```

**实用示例**：
```bash
# 查找用户目录下的重复文件
fdupes -r /home/username/

# 显示重复文件及其大小
fdupes -rS /home/username/Documents/

# 交互式删除重复文件
fdupes -rd /home/username/Downloads/

# 只显示重复文件，不显示唯一文件
fdupes -r --omitfirst /home/username/
```

### 6.3 自制重复文件检测脚本


```bash
#!/bin/bash
# 重复文件检测脚本

find_duplicates() {
    local search_dir=$1
    echo "🔍 在 $search_dir 中查找重复文件..."
    
    # 使用MD5校验和查找重复文件
    find "$search_dir" -type f -exec md5sum {} \; 2>/dev/null | \
    sort | \
    uniq -d -w 32 | \
    while read checksum file; do
        echo "发现重复文件组:"
        # 找出所有具有相同校验和的文件
        find "$search_dir" -type f -exec md5sum {} \; 2>/dev/null | \
        grep "^$checksum" | \
        while read dup_checksum dup_file; do
            echo "  $dup_file"
            ls -lh "$dup_file"
        done
        echo "---"
    done
}

# 使用示例
find_duplicates "/home/user/Documents"
```

**高级去重脚本**：
```bash
#!/bin/bash
# 智能去重脚本

smart_dedup() {
    local target_dir=$1
    local dry_run=${2:-true}  # 默认只是预览，不真的删除
    
    echo "📊 分析 $target_dir 中的重复文件..."
    
    # 创建临时文件存储结果
    temp_file=$(mktemp)
    
    # 查找重复文件
    find "$target_dir" -type f -size +1M -exec md5sum {} \; 2>/dev/null | \
    sort | \
    uniq -d -w 32 > "$temp_file"
    
    if [ -s "$temp_file" ]; then
        echo "🔍 发现以下重复文件:"
        
        while read checksum file; do
            echo "文件组 (MD5: ${checksum:0:8}...):"
            
            # 找出所有重复文件
            duplicates=$(find "$target_dir" -type f -exec md5sum {} \; 2>/dev/null | grep "^$checksum" | cut -d' ' -f2-)
            
            echo "$duplicates" | while read dup_file; do
                size=$(ls -lh "$dup_file" | awk '{print $5}')
                mtime=$(stat -c %Y "$dup_file")
                echo "  📁 $dup_file ($size, 修改时间: $(date -d @$mtime))"
            done
            
            if [ "$dry_run" != "true" ]; then
                # 保留最新的文件，删除其他的
                newest_file=$(echo "$duplicates" | xargs ls -t | head -1)
                echo "  ✅ 保留: $newest_file"
                
                echo "$duplicates" | grep -v "$newest_file" | while read old_file; do
                    echo "  🗑️  删除: $old_file"
                    rm -f "$old_file"
                done
            fi
            
            echo "---"
        done
    else
        echo "✅ 没有发现重复文件"
    fi
    
    rm -f "$temp_file"
}

# 使用示例
smart_dedup "/home/user/Downloads" true   # 预览模式
# smart_dedup "/home/user/Downloads" false  # 实际删除模式
```

---

## 7. 🧩 文件系统碎片化检测


### 7.1 什么是文件系统碎片化


**生活类比**：碎片化就像书架上的书摆放很乱，本来连续的一套书被分散放在不同地方，找起来就很麻烦。

```
碎片化的影响：
┌─ ⚡ 碎片化带来的问题 ──────┐
│ • 文件读写速度变慢          │
│ • 磁盘寻道时间增加          │
│ • 系统整体性能下降          │
│ • 文件操作响应延迟          │
│ • 备份和恢复效率降低        │
└─────────────────────────────┘
```

### 7.2 检测文件系统碎片化


**使用e2fsck检测（ext文件系统）**：
```bash
# 检查文件系统状态（只读模式）
e2fsck -n /dev/sda1

# 查看文件系统详细信息
tune2fs -l /dev/sda1 | grep -i frag

# 使用filefrag查看单个文件碎片
filefrag -v /var/log/syslog
```

**碎片化分析脚本**：
```bash
#!/bin/bash
# 文件系统碎片化分析

analyze_fragmentation() {
    local mount_point=$1
    echo "🔍 分析 $mount_point 的碎片化情况..."
    
    # 检查文件系统类型
    fs_type=$(df -T "$mount_point" | tail -1 | awk '{print $2}')
    echo "文件系统类型: $fs_type"
    
    if [ "$fs_type" = "ext4" ] || [ "$fs_type" = "ext3" ] || [ "$fs_type" = "ext2" ]; then
        # 对于ext文件系统
        device=$(df "$mount_point" | tail -1 | awk '{print $1}')
        echo "设备: $device"
        
        # 检查碎片化程度
        echo "📊 碎片化信息:"
        tune2fs -l "$device" 2>/dev/null | grep -E "(Free blocks|Fragment)"
        
        # 检查大文件的碎片化
        echo "📁 检查大文件碎片化:"
        find "$mount_point" -type f -size +100M 2>/dev/null | head -10 | while read file; do
            fragments=$(filefrag "$file" | awk '{print $2}')
            echo "  $file: $fragments 个片段"
        done
        
    else
        echo "⚠️  $fs_type 文件系统不支持此分析方法"
    fi
}

# 使用示例
analyze_fragmentation "/"
analyze_fragmentation "/home"
```

### 7.3 碎片整理策略


**ext4文件系统整理**：
```bash
# 注意：ext4文件系统通常不需要整理，但如果确实需要：

# 1. 使用e4defrag（需要安装e2fsprogs-extra）
e4defrag -c /home  # 检查碎片化程度
e4defrag /home     # 整理/home目录

# 2. 针对单个文件整理
e4defrag /var/log/large_log_file

# 3. 查看整理效果
e4defrag -c /home
```

**预防碎片化的方法**：
```bash
# 1. 定期清理临时文件
find /tmp -type f -atime +7 -delete

# 2. 合理设置文件大小限制
# 在/etc/logrotate.d/配置中设置合理的日志轮转

# 3. 避免频繁的小文件写入
# 使用缓冲区或批量写入减少碎片化
```

---

## 8. 📏 目录层次深度分析


### 8.1 为什么要关心目录深度


```
目录层次过深的问题：
┌─ 🏗️ 深层目录的影响 ──────┐
│ • 文件路径过长，操作不便   │
│ • 备份和同步效率低下       │
│ • 某些程序有路径长度限制   │
│ • 影响文件系统性能         │
│ • 用户查找文件困难         │
└─────────────────────────────┘
```

### 8.2 分析目录深度


**查找最深的目录**：
```bash
# 找出最深的目录结构
find /home -type d | awk -F/ 'NF > max {max = NF; path = $0} END {print "最深层次:", max-1, "路径:", path}'

# 统计不同深度的目录数量
find /home -type d | awk -F/ '{print NF-1}' | sort -n | uniq -c
```

**目录深度分析脚本**：
```bash
#!/bin/bash
# 目录深度分析工具

analyze_directory_depth() {
    local target_dir=$1
    local max_depth=${2:-20}  # 默认最大深度20层
    
    echo "📊 分析 $target_dir 的目录层次结构..."
    
    # 创建临时文件
    temp_file=$(mktemp)
    
    # 收集所有目录及其深度
    find "$target_dir" -type d 2>/dev/null | while read dir; do
        depth=$(echo "$dir" | awk -F/ '{print NF-1}')
        echo "$depth $dir" >> "$temp_file"
    done
    
    # 统计分析
    echo "📈 深度分布统计:"
    cat "$temp_file" | awk '{print $1}' | sort -n | uniq -c | \
    awk '{printf "  深度 %2d: %4d 个目录\n", $2, $1}'
    
    echo ""
    echo "🔍 最深的10个目录:"
    sort -rn "$temp_file" | head -10 | \
    awk '{printf "  深度 %2d: %s\n", $1, substr($0, index($0, $2))}'
    
    # 找出异常深度的目录
    echo ""
    echo "⚠️  可能有问题的深度目录 (超过${max_depth}层):"
    awk -v max="$max_depth" '$1 > max {printf "  深度 %2d: %s\n", $1, substr($0, index($0, $2))}' "$temp_file"
    
    rm -f "$temp_file"
}

# 使用示例
analyze_directory_depth "/home" 15
analyze_directory_depth "/var" 20
```

### 8.3 优化目录结构


**目录重组建议**：
```bash
# 查找可能需要重组的目录
find_deep_directories() {
    local search_dir=$1
    echo "🔍 查找需要重组的深层目录..."
    
    # 找出深度超过10层且文件数少于5个的目录
    find "$search_dir" -type d 2>/dev/null | while read dir; do
        depth=$(echo "$dir" | awk -F/ '{print NF-1}')
        if [ $depth -gt 10 ]; then
            file_count=$(find "$dir" -maxdepth 1 -type f | wc -l)
            if [ $file_count -lt 5 ]; then
                echo "  📁 $dir (深度: $depth, 文件数: $file_count)"
            fi
        fi
    done
}

# 自动创建扁平化结构建议
suggest_flatten() {
    local deep_dir=$1
    echo "💡 建议的扁平化结构:"
    
    # 分析目录中的文件类型
    find "$deep_dir" -type f -name "*.*" | \
    sed 's/.*\.//' | sort | uniq -c | sort -rn | \
    awk '{printf "  %s文件: %d个\n", $2, $1}' | head -5
}
```

---

## 9. ⏰ 文件访问时间统计分析


### 9.1 理解文件时间戳


**三种时间的含义**：
```
Linux文件的三种时间：
┌─ 🕐 文件时间戳说明 ─────────┐
│ atime (Access Time):        │
│   最后访问时间，读取文件时更新 │
│                             │
│ mtime (Modify Time):        │
│   最后修改时间，文件内容变化时更新│
│                             │  
│ ctime (Change Time):        │
│   最后改变时间，文件属性变化时更新│
└─────────────────────────────────┘
```

**查看文件时间戳**：
```bash
# 查看详细时间信息
stat filename

# 使用ls显示不同时间
ls -l  filename  # 显示mtime
ls -lu filename  # 显示atime  
ls -lc filename  # 显示ctime
```

### 9.2 文件访问模式分析


**查找长时间未访问的文件**：
```bash
# 查找30天内未访问的文件
find /home -type f -atime +30

# 查找超过1年未访问的大文件
find /home -type f -atime +365 -size +100M -exec ls -lhu {} \;

# 统计不同时间段的文件分布
analyze_file_age() {
    local target_dir=$1
    echo "📊 $target_dir 文件年龄分布:"
    
    echo "最近7天访问的文件:"
    find "$target_dir" -type f -atime -7 | wc -l
    
    echo "7-30天前访问的文件:" 
    find "$target_dir" -type f -atime +7 -atime -30 | wc -l
    
    echo "30-365天前访问的文件:"
    find "$target_dir" -type f -atime +30 -atime -365 | wc -l
    
    echo "超过1年未访问的文件:"
    find "$target_dir" -type f -atime +365 | wc -l
}
```

**热文件识别**：
```bash
#!/bin/bash
# 热文件分析脚本

find_hot_files() {
    local target_dir=$1
    echo "🔥 分析 $target_dir 中的热文件..."
    
    # 最近经常访问的文件
    echo "📈 最近7天频繁访问的文件:"
    find "$target_dir" -type f -atime -7 -size +1M | while read file; do
        # 获取文件信息
        size=$(ls -lh "$file" | awk '{print $5}')
        atime=$(stat -c %X "$file")
        echo "  📄 $file ($size, 最后访问: $(date -d @$atime '+%Y-%m-%d %H:%M'))"
    done | head -20
    
    echo ""
    echo "❄️  长期未使用的大文件 (可考虑归档):"
    find "$target_dir" -type f -atime +90 -size +50M | while read file; do
        size=$(ls -lh "$file" | awk '{print $5}')
        atime=$(stat -c %X "$file")
        echo "  📄 $file ($size, 最后访问: $(date -d @$atime '+%Y-%m-%d'))"
    done | head -20
}

# 使用示例
find_hot_files "/home/user"
```

### 9.3 访问模式优化建议


**基于访问时间的自动归档**：
```bash
#!/bin/bash
# 智能归档脚本

smart_archive() {
    local source_dir=$1
    local archive_dir=${2:-"$source_dir/archived"}
    local days=${3:-365}  # 默认1年未访问就归档
    
    echo "🗂️  准备归档 $source_dir 中超过 $days 天未访问的文件..."
    
    # 创建归档目录
    mkdir -p "$archive_dir"
    
    # 查找要归档的文件
    candidates=$(find "$source_dir" -maxdepth 1 -type f -atime +$days -size +10M)
    
    if [ -n "$candidates" ]; then
        echo "📋 将要归档的文件:"
        echo "$candidates" | while read file; do
            size=$(ls -lh "$file" | awk '{print $5}')
            atime=$(stat -c %X "$file")
            echo "  📄 $file ($size, 最后访问: $(date -d @$atime '+%Y-%m-%d'))"
        done
        
        echo ""
        read -p "确认归档这些文件吗? (y/N): " confirm
        
        if [ "$confirm" = "y" ] || [ "$confirm" = "Y" ]; then
            echo "$candidates" | while read file; do
                echo "🚚 归档: $file"
                mv "$file" "$archive_dir/"
            done
            echo "✅ 归档完成"
        else
            echo "❌ 已取消归档"
        fi
    else
        echo "✅ 没有需要归档的文件"
    fi
}

# 使用示例
smart_archive "/home/user/Documents" "/backup/archived" 180
```

---

## 10. 📋 核心要点总结


### 10.1 必须掌握的核心概念


```
🎯 文件系统占用分析的核心价值：
• 及时发现并解决空间问题，避免系统故障
• 优化存储使用效率，提升系统性能
• 建立科学的文件管理和清理机制
• 为系统监控和维护提供数据支持
```

### 10.2 关键命令速查


| 功能 | **核心命令** | **典型用法** | **重要参数** |
|------|-------------|-------------|-------------|
| **按大小查找** | `find` | `find / -size +100M -type f` | `-size +100M` 大于100MB |
| **目录占用** | `du` | `du -sh /var/log` | `-s` 汇总，`-h` 人性化显示 |
| **重复文件** | `fdupes` | `fdupes -r /home` | `-r` 递归，`-d` 删除 |
| **文件碎片** | `filefrag` | `filefrag -v /var/log/syslog` | `-v` 详细信息 |
| **日志轮转** | `logrotate` | `logrotate -f /etc/logrotate.conf` | `-f` 强制轮转 |

### 10.3 实用管理策略


**🔄 定期维护计划**：
```
每日检查：
- 监控关键目录空间使用情况
- 检查日志文件异常增长
- 清理/tmp目录中的临时文件

每周维护：
- 运行重复文件检测和清理
- 分析用户目录占用情况
- 检查缓存文件增长趋势

每月整理：
- 归档长期未访问的大文件
- 分析目录结构优化机会
- 生成存储使用报告
```

**⚠️ 操作安全提醒**：
```
🛡️ 安全操作原则：
• 删除前务必确认，重要文件先备份
• 使用-n或--dry-run参数预览操作结果
• 避免在系统关键目录执行批量删除
• 定期验证备份完整性
• 监控脚本执行结果，及时发现异常
```

### 10.4 故障排查思路


```
🔧 常见问题解决流程：

磁盘空间不足：
1. 使用df -h查看整体使用情况
2. 用du -sh /*找出占用最多的目录  
3. 深入分析大目录，找出具体问题
4. 根据文件类型制定清理策略

系统性能下降：
1. 检查是否存在大量小文件
2. 分析文件访问模式是否合理
3. 确认文件系统碎片化程度
4. 优化目录结构和文件组织方式
```

**🧠 记忆口诀**：
```
"找大文件用find size，清日志要看logrotate
重复文件fdupes除，缓存临时定期清
时间戳里藏玄机，访问模式要分析"
```

### 10.5 进阶学习方向


- **自动化运维**：编写更智能的监控和清理脚本
- **存储优化**：学习LVM、RAID等高级存储技术  
- **性能调优**：深入了解文件系统参数优化
- **监控集成**：将分析结果集成到监控系统
- **容器化环境**：适应Docker等容器化存储管理

**核心价值**：掌握文件系统占用分析，就像拥有了系统健康的"体检报告"，能够及时发现问题、优化性能、预防故障，是Linux系统管理的重要技能。