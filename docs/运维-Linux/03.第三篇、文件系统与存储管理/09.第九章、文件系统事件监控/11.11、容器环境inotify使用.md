---
title: 11、容器环境inotify使用
---
## 📚 目录

1. [容器环境inotify基础](#1-容器环境inotify基础)
2. [Docker容器内inotify限制](#2-Docker容器内inotify限制)
3. [容器监控宿主机文件](#3-容器监控宿主机文件)
4. [卷挂载监控配置](#4-卷挂载监控配置)
5. [容器间文件事件通信](#5-容器间文件事件通信)
6. [Kubernetes中文件监控](#6-Kubernetes中文件监控)
7. [容器编排监控策略](#7-容器编排监控策略)
8. [微服务配置监控](#8-微服务配置监控)
9. [容器化应用日志监控](#9-容器化应用日志监控)
10. [云原生监控最佳实践](#10-云原生监控最佳实践)
11. [核心要点总结](#11-核心要点总结)

---

## 1. 🐳 容器环境inotify基础


### 1.1 什么是容器环境下的inotify


**简单理解**：在Docker、Kubernetes等容器环境中使用Linux的`inotify`文件监控功能

> 💡 **通俗解释**  
> 想象你在一个小房子里（容器）监视某个文件夹的变化，但这个文件夹可能在房子里，也可能在房子外面（宿主机）。inotify就是你的"监控眼"，帮你实时发现文件的变化

```
传统Linux环境：
应用程序 → inotify → 文件系统

容器环境：
容器应用 → inotify → 容器文件系统 or 宿主机文件系统
```

### 1.2 容器环境的特殊性


**与传统环境的区别**：

```
传统环境特点：
┌─────────────────┐
│   应用程序       │ 
├─────────────────┤
│   文件系统       │ ← 直接访问
└─────────────────┘

容器环境特点：
┌─────────────────┐
│   容器应用       │ 
├─────────────────┤
│  容器文件系统    │ ← 隔离层
├─────────────────┤
│  宿主机文件系统  │ ← 可能需要穿透
└─────────────────┘
```

**关键差异**：
- **文件系统隔离**：容器有自己独立的文件系统
- **权限限制**：容器内权限受到限制
- **资源配额**：inotify监控数量可能受限
- **网络隔离**：容器间通信需要特殊配置

---

## 2. 🚫 Docker容器内inotify限制


### 2.1 系统资源限制


**inotify实例数限制**：

> ⚠️ **重要限制**  
> 容器继承宿主机的inotify限制，但多个容器共享这些限制

```bash
# 查看当前inotify限制
cat /proc/sys/fs/inotify/max_user_instances    # 默认128
cat /proc/sys/fs/inotify/max_user_watches      # 默认8192
cat /proc/sys/fs/inotify/max_queued_events     # 默认16384

# 查看当前使用情况
find /proc/*/fd -lname anon_inode:inotify -print 2> /dev/null | wc -l
```

### 2.2 权限限制问题


**容器内权限不足**：

```dockerfile
# 错误示例 - 普通用户无法修改系统参数
FROM ubuntu:20.04
RUN echo 'fs.inotify.max_user_watches=524288' >> /etc/sysctl.conf
# 这在容器内不会生效！
```

**正确的解决方案**：

```bash
# 方法1：在宿主机上调整（推荐）
echo 'fs.inotify.max_user_watches=524288' | sudo tee -a /etc/sysctl.conf
sudo sysctl -p

# 方法2：Docker运行时调整
docker run --sysctl fs.inotify.max_user_watches=524288 myapp
```

### 2.3 文件系统类型限制


**不同文件系统的inotify支持**：

```
支持inotify的文件系统：
✅ ext4, ext3, ext2      ← 完全支持
✅ xfs                   ← 完全支持
✅ btrfs                 ← 完全支持
❌ NFS                   ← 不支持（重要！）
❌ tmpfs                 ← 部分支持
⚠️  overlay2            ← Docker默认存储驱动，支持但有限制
```

**检查容器内文件系统类型**：

```bash
# 在容器内检查
df -T /path/to/monitor

# 检查Docker存储驱动
docker info | grep "Storage Driver"
```

---

## 3. 👁️ 容器监控宿主机文件


### 3.1 卷挂载监控


**基本挂载监控**：

```bash
# 挂载宿主机目录到容器
docker run -v /host/data:/container/data myapp

# 容器内监控挂载的目录
inotifywait -m /container/data -e create,delete,modify
```

**监控脚本示例**：

```bash
#!/bin/bash
# monitor-host-files.sh

WATCH_DIR="/container/data"
LOG_FILE="/var/log/file-monitor.log"

# 检查目录是否存在
if [ ! -d "$WATCH_DIR" ]; then
    echo "Error: Directory $WATCH_DIR not found"
    exit 1
fi

# 开始监控
echo "Starting to monitor $WATCH_DIR"
inotifywait -m "$WATCH_DIR" -e create,delete,modify,move \
    --format '%w%f %e %T' --timefmt '%Y-%m-%d %H:%M:%S' >> "$LOG_FILE" &

# 获取监控进程ID
MONITOR_PID=$!
echo "Monitor started with PID: $MONITOR_PID"

# 优雅退出处理
trap "kill $MONITOR_PID; exit" SIGINT SIGTERM
wait
```

### 3.2 绑定挂载vs卷挂载


**两种挂载方式的inotify表现**：

```bash
# 绑定挂载（推荐用于监控）
docker run -v /host/path:/container/path myapp
# ✅ inotify事件能正常传递

# 命名卷挂载
docker run -v myvolume:/container/path myapp
# ⚠️ 事件传递可能受限制
```

**验证挂载监控**：

```bash
# 测试脚本
#!/bin/bash
# 在宿主机创建测试文件
echo "test" > /host/data/test.txt

# 在容器内检查是否收到事件
docker exec mycontainer inotifywait -t 5 /container/data -e create
```

---

## 4. 📁 卷挂载监控配置


### 4.1 Docker Compose配置


**完整的监控配置**：

```yaml
# docker-compose.yml
version: '3.8'
services:
  file-monitor:
    image: myapp:latest
    volumes:
      # 绑定挂载宿主机目录
      - /host/config:/app/config:ro        # 只读配置
      - /host/data:/app/data:rw           # 读写数据
      - /host/logs:/app/logs:rw           # 日志目录
    sysctls:
      # 调整inotify限制（需要特权模式）
      - fs.inotify.max_user_watches=524288
    privileged: false
    security_opt:
      - no-new-privileges:true
    environment:
      - MONITOR_CONFIG_DIR=/app/config
      - MONITOR_DATA_DIR=/app/data
```

### 4.2 监控配置文件变化


**配置热重载监控**：

```python
#!/usr/bin/env python3
# config-monitor.py

import os
import time
import subprocess
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

class ConfigHandler(FileSystemEventHandler):
    def __init__(self, app_reload_cmd):
        self.app_reload_cmd = app_reload_cmd
        
    def on_modified(self, event):
        if event.is_directory:
            return
            
        # 只监控配置文件
        if event.src_path.endswith(('.conf', '.ini', '.yaml', '.yml')):
            print(f"Config file changed: {event.src_path}")
            self.reload_app()
    
    def reload_app(self):
        """重新加载应用配置"""
        try:
            subprocess.run(self.app_reload_cmd, shell=True, check=True)
            print("Application reloaded successfully")
        except subprocess.CalledProcessError as e:
            print(f"Failed to reload application: {e}")

def main():
    config_dir = os.getenv('MONITOR_CONFIG_DIR', '/app/config')
    reload_cmd = os.getenv('RELOAD_COMMAND', 'kill -HUP 1')
    
    event_handler = ConfigHandler(reload_cmd)
    observer = Observer()
    observer.schedule(event_handler, config_dir, recursive=True)
    
    observer.start()
    print(f"Monitoring config directory: {config_dir}")
    
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        observer.stop()
    observer.join()

if __name__ == "__main__":
    main()
```

### 4.3 多目录监控策略


**分层监控配置**：

```
监控目录结构：
/app/
├── config/          ← 配置文件监控（高优先级）
│   ├── app.conf     
│   └── db.conf      
├── data/            ← 数据文件监控（中优先级）
│   ├── input/       
│   └── output/      
└── logs/            ← 日志监控（低优先级，仅统计）
    └── app.log      
```

```bash
#!/bin/bash
# multi-dir-monitor.sh

# 配置文件监控（立即处理）
inotifywait -m /app/config -e modify,create,delete \
    --format 'CONFIG %w%f %e %T' --timefmt '%H:%M:%S' &

# 数据文件监控（批量处理）
inotifywait -m /app/data -e create,moved_to \
    --format 'DATA %w%f %e %T' --timefmt '%H:%M:%S' &

# 日志文件监控（仅统计）
inotifywait -m /app/logs -e modify \
    --format 'LOG %w%f %e %T' --timefmt '%H:%M:%S' &

# 等待所有后台进程
wait
```

---

## 5. 🔄 容器间文件事件通信


### 5.1 共享卷通信


**通过共享卷传递文件事件**：

```yaml
# docker-compose.yml
version: '3.8'
services:
  producer:
    image: producer-app
    volumes:
      - shared-data:/shared
    environment:
      - ROLE=producer
      
  consumer:
    image: consumer-app
    volumes:
      - shared-data:/shared
    environment:
      - ROLE=consumer
    depends_on:
      - producer

volumes:
  shared-data:
```

**生产者代码示例**：

```python
#!/usr/bin/env python3
# producer.py

import os
import json
import time
from datetime import datetime

def create_event_file(event_data):
    """创建事件文件供其他容器监控"""
    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S_%f')
    event_file = f"/shared/events/event_{timestamp}.json"
    
    # 确保目录存在
    os.makedirs(os.path.dirname(event_file), exist_ok=True)
    
    with open(event_file, 'w') as f:
        json.dump({
            'timestamp': timestamp,
            'event': event_data,
            'producer': os.getenv('HOSTNAME', 'unknown')
        }, f)
    
    print(f"Event created: {event_file}")

def main():
    while True:
        # 模拟业务事件
        event_data = {
            'type': 'data_processed',
            'count': 100,
            'status': 'success'
        }
        create_event_file(event_data)
        time.sleep(10)

if __name__ == "__main__":
    main()
```

**消费者代码示例**：

```python
#!/usr/bin/env python3
# consumer.py

import os
import json
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

class EventHandler(FileSystemEventHandler):
    def on_created(self, event):
        if event.is_directory:
            return
            
        if event.src_path.endswith('.json'):
            self.process_event_file(event.src_path)
    
    def process_event_file(self, file_path):
        """处理事件文件"""
        try:
            with open(file_path, 'r') as f:
                event_data = json.load(f)
            
            print(f"Received event from {event_data.get('producer')}: {event_data}")
            
            # 处理完成后删除事件文件
            os.remove(file_path)
            
        except Exception as e:
            print(f"Error processing event file {file_path}: {e}")

def main():
    events_dir = "/shared/events"
    os.makedirs(events_dir, exist_ok=True)
    
    event_handler = EventHandler()
    observer = Observer()
    observer.schedule(event_handler, events_dir, recursive=False)
    
    observer.start()
    print(f"Monitoring events in: {events_dir}")
    
    try:
        while True:
            import time
            time.sleep(1)
    except KeyboardInterrupt:
        observer.stop()
    observer.join()

if __name__ == "__main__":
    main()
```

### 5.2 信号文件模式


**使用信号文件协调容器**：

```bash
# 创建信号文件的标准格式
create_signal() {
    local signal_type=$1
    local signal_data=$2
    local signal_file="/shared/signals/${signal_type}_$(date +%s).signal"
    
    echo "$signal_data" > "$signal_file"
    echo "Signal created: $signal_file"
}

# 监控信号文件
monitor_signals() {
    inotifywait -m /shared/signals -e create --format '%f' | while read filename
    do
        if [[ $filename == *.signal ]]; then
            echo "Processing signal: $filename"
            process_signal "/shared/signals/$filename"
            rm "/shared/signals/$filename"
        fi
    done
}
```

---

## 6. ⚓ Kubernetes中文件监控


### 6.1 Pod内文件监控


**在Kubernetes Pod中监控文件**：

```yaml
# k8s-file-monitor.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: file-monitor
spec:
  replicas: 1
  selector:
    matchLabels:
      app: file-monitor
  template:
    metadata:
      labels:
        app: file-monitor
    spec:
      containers:
      - name: monitor
        image: file-monitor:latest
        volumeMounts:
        - name: config-volume
          mountPath: /app/config
        - name: data-volume
          mountPath: /app/data
        env:
        - name: MONITOR_CONFIG_DIR
          value: "/app/config"
        - name: MONITOR_DATA_DIR
          value: "/app/data"
        resources:
          limits:
            memory: "256Mi"
            cpu: "200m"
          requests:
            memory: "128Mi"
            cpu: "100m"
      volumes:
      - name: config-volume
        configMap:
          name: app-config
      - name: data-volume
        persistentVolumeClaim:
          claimName: app-data-pvc
```

### 6.2 ConfigMap监控


**监控ConfigMap变化并自动重载**：

```python
#!/usr/bin/env python3
# k8s-config-monitor.py

import os
import time
import subprocess
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

class K8sConfigHandler(FileSystemEventHandler):
    def __init__(self):
        self.reload_command = os.getenv('RELOAD_COMMAND', 'kill -HUP 1')
        
    def on_created(self, event):
        # ConfigMap更新时会创建新的符号链接
        if '..data' in event.src_path:
            print("ConfigMap updated, reloading application...")
            self.reload_application()
    
    def on_moved(self, event):
        # ConfigMap的原子更新操作
        if '..data' in event.dest_path:
            print("ConfigMap atomically updated")
            time.sleep(1)  # 等待文件系统稳定
            self.reload_application()
    
    def reload_application(self):
        try:
            subprocess.run(self.reload_command, shell=True, check=True)
            print("Application reloaded successfully")
        except subprocess.CalledProcessError as e:
            print(f"Failed to reload: {e}")

def main():
    config_dir = os.getenv('MONITOR_CONFIG_DIR', '/app/config')
    
    if not os.path.exists(config_dir):
        print(f"Config directory {config_dir} does not exist")
        return
    
    event_handler = K8sConfigHandler()
    observer = Observer()
    observer.schedule(event_handler, config_dir, recursive=True)
    
    observer.start()
    print(f"Monitoring Kubernetes ConfigMap at: {config_dir}")
    
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        observer.stop()
    observer.join()

if __name__ == "__main__":
    main()
```

### 6.3 Secret文件监控


**安全地监控Secret变化**：

```bash
#!/bin/bash
# secret-monitor.sh

SECRET_DIR="/app/secrets"
LOG_FILE="/var/log/secret-monitor.log"

# 检查Secret目录
if [ ! -d "$SECRET_DIR" ]; then
    echo "Secret directory not found: $SECRET_DIR"
    exit 1
fi

echo "Monitoring secrets at: $SECRET_DIR"

# 监控Secret文件变化（不记录内容）
inotifywait -m "$SECRET_DIR" -e create,delete,move --format '%e %f %T' --timefmt '%Y-%m-%d %H:%M:%S' | while read event filename timestamp
do
    case $event in
        "CREATE" | "MOVED_TO")
            echo "[$timestamp] Secret file created/updated: $filename" >> "$LOG_FILE"
            # 通知应用重新加载Secret
            kill -USR1 $(cat /var/run/app.pid) 2>/dev/null
            ;;
        "DELETE" | "MOVED_FROM")
            echo "[$timestamp] Secret file removed: $filename" >> "$LOG_FILE"
            ;;
    esac
done
```

---

## 7. 🎭 容器编排监控策略


### 7.1 多容器协调监控


**服务发现与文件监控结合**：

```yaml
# docker-compose.yml - 微服务监控编排
version: '3.8'
services:
  config-server:
    image: config-server:latest
    volumes:
      - ./configs:/app/configs:ro
      - shared-events:/events
    command: >
      sh -c "python /app/config-server.py &
             python /app/file-monitor.py"
    
  app-server:
    image: app-server:latest
    volumes:
      - shared-events:/events
    depends_on:
      - config-server
    environment:
      - CONFIG_SERVER_URL=http://config-server:8080
      - MONITOR_EVENTS_DIR=/events
    
  nginx:
    image: nginx:alpine
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf:ro
      - shared-logs:/var/log/nginx
    ports:
      - "80:80"
    depends_on:
      - app-server

volumes:
  shared-events:
  shared-logs:
```

### 7.2 健康检查集成


**将文件监控集成到健康检查**：

```python
#!/usr/bin/env python3
# health-check-with-monitor.py

import os
import time
import json
from datetime import datetime, timedelta

class HealthChecker:
    def __init__(self):
        self.monitor_status_file = "/tmp/file-monitor-status"
        self.last_event_file = "/tmp/last-event-time"
        
    def check_monitor_health(self):
        """检查文件监控服务是否正常工作"""
        try:
            # 检查监控进程状态文件
            if not os.path.exists(self.monitor_status_file):
                return False, "Monitor status file not found"
                
            with open(self.monitor_status_file, 'r') as f:
                status = json.load(f)
                
            # 检查最后更新时间
            last_update = datetime.fromisoformat(status['last_update'])
            if datetime.now() - last_update > timedelta(minutes=5):
                return False, "Monitor not responding"
                
            return True, "Monitor healthy"
            
        except Exception as e:
            return False, f"Health check error: {e}"
    
    def check_file_events(self):
        """检查文件事件是否正常处理"""
        try:
            if not os.path.exists(self.last_event_file):
                return True, "No events yet"  # 初始状态正常
                
            with open(self.last_event_file, 'r') as f:
                last_event_time = datetime.fromisoformat(f.read().strip())
                
            # 如果太久没有事件，可能有问题
            if datetime.now() - last_event_time > timedelta(hours=1):
                return False, "No recent file events"
                
            return True, "File events normal"
            
        except Exception as e:
            return False, f"Event check error: {e}"

def main():
    checker = HealthChecker()
    
    # 综合健康检查
    monitor_ok, monitor_msg = checker.check_monitor_health()
    events_ok, events_msg = checker.check_file_events()
    
    overall_healthy = monitor_ok and events_ok
    
    health_data = {
        'healthy': overall_healthy,
        'timestamp': datetime.now().isoformat(),
        'checks': {
            'monitor': {'status': monitor_ok, 'message': monitor_msg},
            'events': {'status': events_ok, 'message': events_msg}
        }
    }
    
    print(json.dumps(health_data, indent=2))
    
    # 返回适当的退出代码（用于Docker健康检查）
    exit(0 if overall_healthy else 1)

if __name__ == "__main__":
    main()
```

**Dockerfile健康检查配置**：

```dockerfile
FROM python:3.9-slim

# 安装inotify-tools
RUN apt-get update && apt-get install -y inotify-tools && rm -rf /var/lib/apt/lists/*

COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . /app
WORKDIR /app

# 配置健康检查
HEALTHCHECK --interval=30s --timeout=10s --start-period=5s --retries=3 \
    CMD python /app/health-check-with-monitor.py

CMD ["python", "/app/main.py"]
```

---

## 8. ⚙️ 微服务配置监控


### 8.1 配置中心集成


**与配置中心的文件监控集成**：

```python
#!/usr/bin/env python3
# microservice-config-monitor.py

import os
import json
import time
import requests
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

class ConfigCenterHandler(FileSystemEventHandler):
    def __init__(self, config_center_url, service_name):
        self.config_center_url = config_center_url
        self.service_name = service_name
        self.last_config_hash = None
        
    def on_modified(self, event):
        if event.is_directory:
            return
            
        if event.src_path.endswith('.properties') or event.src_path.endswith('.yml'):
            print(f"Config file changed: {event.src_path}")
            self.sync_to_config_center(event.src_path)
    
    def sync_to_config_center(self, file_path):
        """同步配置到配置中心"""
        try:
            with open(file_path, 'r') as f:
                config_content = f.read()
            
            # 计算配置哈希，避免重复同步
            import hashlib
            config_hash = hashlib.md5(config_content.encode()).hexdigest()
            
            if config_hash == self.last_config_hash:
                print("Config content unchanged, skipping sync")
                return
            
            # 上传到配置中心
            response = requests.post(
                f"{self.config_center_url}/api/config/{self.service_name}",
                json={
                    'content': config_content,
                    'version': int(time.time()),
                    'source': 'file-monitor'
                },
                timeout=10
            )
            
            if response.status_code == 200:
                self.last_config_hash = config_hash
                print("Config synced successfully")
            else:
                print(f"Failed to sync config: {response.status_code}")
                
        except Exception as e:
            print(f"Error syncing config: {e}")

def main():
    config_dir = os.getenv('CONFIG_DIR', '/app/config')
    config_center_url = os.getenv('CONFIG_CENTER_URL', 'http://config-server:8080')
    service_name = os.getenv('SERVICE_NAME', 'unknown-service')
    
    if not os.path.exists(config_dir):
        print(f"Config directory not found: {config_dir}")
        return
    
    event_handler = ConfigCenterHandler(config_center_url, service_name)
    observer = Observer()
    observer.schedule(event_handler, config_dir, recursive=True)
    
    observer.start()
    print(f"Monitoring config for service '{service_name}' at: {config_dir}")
    
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        observer.stop()
    observer.join()

if __name__ == "__main__":
    main()
```

### 8.2 动态配置更新


**实现配置的热更新**：

```python
#!/usr/bin/env python3
# dynamic-config-updater.py

import os
import json
import signal
import subprocess
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

class DynamicConfigHandler(FileSystemEventHandler):
    def __init__(self, app_pid_file, reload_endpoint=None):
        self.app_pid_file = app_pid_file
        self.reload_endpoint = reload_endpoint
        
    def on_modified(self, event):
        if event.is_directory:
            return
            
        config_file = os.path.basename(event.src_path)
        print(f"Config file updated: {config_file}")
        
        # 根据配置文件类型选择重载方式
        if config_file.startswith('database'):
            self.reload_database_config()
        elif config_file.startswith('cache'):
            self.reload_cache_config()
        else:
            self.reload_application()
    
    def reload_database_config(self):
        """重载数据库配置"""
        print("Reloading database configuration...")
        if self.reload_endpoint:
            self.http_reload('database')
        else:
            self.signal_reload('USR1')
    
    def reload_cache_config(self):
        """重载缓存配置"""
        print("Reloading cache configuration...")
        if self.reload_endpoint:
            self.http_reload('cache')
        else:
            self.signal_reload('USR2')
    
    def reload_application(self):
        """重载整个应用"""
        print("Reloading entire application...")
        if self.reload_endpoint:
            self.http_reload('application')
        else:
            self.signal_reload('HUP')
    
    def signal_reload(self, signal_name):
        """通过信号重载"""
        try:
            with open(self.app_pid_file, 'r') as f:
                pid = int(f.read().strip())
            
            sig = getattr(signal, f'SIG{signal_name}')
            os.kill(pid, sig)
            print(f"Sent {signal_name} signal to process {pid}")
            
        except Exception as e:
            print(f"Failed to send signal: {e}")
    
    def http_reload(self, config_type):
        """通过HTTP接口重载"""
        try:
            import requests
            response = requests.post(
                f"{self.reload_endpoint}/reload/{config_type}",
                timeout=5
            )
            print(f"HTTP reload response: {response.status_code}")
        except Exception as e:
            print(f"HTTP reload failed: {e}")

def main():
    config_dir = os.getenv('CONFIG_DIR', '/app/config')
    app_pid_file = os.getenv('APP_PID_FILE', '/var/run/app.pid')
    reload_endpoint = os.getenv('RELOAD_ENDPOINT')  # 可选的HTTP重载接口
    
    event_handler = DynamicConfigHandler(app_pid_file, reload_endpoint)
    observer = Observer()
    observer.schedule(event_handler, config_dir, recursive=False)
    
    observer.start()
    print(f"Dynamic config updater started, monitoring: {config_dir}")
    
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        observer.stop()
    observer.join()

if __name__ == "__main__":
    main()
```

---

## 9. 📝 容器化应用日志监控


### 9.1 日志文件轮转监控


**监控日志轮转并处理**：

```python
#!/usr/bin/env python3
# log-rotation-monitor.py

import os
import gzip
import time
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

class LogRotationHandler(FileSystemEventHandler):
    def __init__(self, archive_dir):
        self.archive_dir = archive_dir
        os.makedirs(archive_dir, exist_ok=True)
        
    def on_moved(self, event):
        """处理日志轮转事件"""
        if event.is_directory:
            return
            
        # 检查是否是日志轮转产生的文件
        if '.log.' in event.dest_path and event.dest_path.endswith(('.1', '.2', '.3')):
            print(f"Log rotated: {event.dest_path}")
            self.process_rotated_log(event.dest_path)
    
    def on_created(self, event):
        """处理新日志文件创建"""
        if event.is_directory:
            return
            
        if event.src_path.endswith('.log'):
            print(f"New log file created: {event.src_path}")
            # 确保新日志文件有正确的权限
            os.chmod(event.src_path, 0o644)
    
    def process_rotated_log(self, log_file):
        """处理轮转后的日志文件"""
        try:
            # 压缩旧日志文件
            compressed_file = f"{self.archive_dir}/{os.path.basename(log_file)}.gz"
            
            with open(log_file, 'rb') as f_in:
                with gzip.open(compressed_file, 'wb') as f_out:
                    f_out.writelines(f_in)
            
            # 删除原始文件
            os.remove(log_file)
            print(f"Compressed and archived: {compressed_file}")
            
            # 清理旧的压缩文件（保留最近10个）
            self.cleanup_old_archives()
            
        except Exception as e:
            print(f"Error processing rotated log: {e}")
    
    def cleanup_old_archives(self):
        """清理旧的归档文件"""
        try:
            archive_files = []
            for f in os.listdir(self.archive_dir):
                if f.endswith('.gz'):
                    file_path = os.path.join(self.archive_dir, f)
                    archive_files.append((file_path, os.path.getmtime(file_path)))
            
            # 按修改时间排序，保留最新的10个文件
            archive_files.sort(key=lambda x: x[1], reverse=True)
            
            for file_path, _ in archive_files[10:]:
                os.remove(file_path)
                print(f"Cleaned up old archive: {file_path}")
                
        except Exception as e:
            print(f"Error cleaning up archives: {e}")

def main():
    log_dir = os.getenv('LOG_DIR', '/app/logs')
    archive_dir = os.getenv('ARCHIVE_DIR', '/app/logs/archive')
    
    if not os.path.exists(log_dir):
        print(f"Log directory not found: {log_dir}")
        return
    
    event_handler = LogRotationHandler(archive_dir)
    observer = Observer()
    observer.schedule(event_handler, log_dir, recursive=False)
    
    observer.start()
    print(f"Log rotation monitor started for: {log_dir}")
    
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        observer.stop()
    observer.join()

if __name__ == "__main__":
    main()
```

### 9.2 实时日志分析


**监控日志并实时分析**：

```bash
#!/bin/bash
# real-time-log-analyzer.sh

LOG_DIR="/app/logs"
ALERT_ENDPOINT="http://alertmanager:9093/api/v1/alerts"

# 创建命名管道用于日志处理
PIPE_FILE="/tmp/log-analysis-pipe"
mkfifo "$PIPE_FILE" 2>/dev/null || true

# 错误模式匹配
ERROR_PATTERNS=(
    "ERROR"
    "FATAL"
    "Exception"
    "OutOfMemoryError"
    "Connection refused"
)

# 监控日志文件变化并分析
monitor_logs() {
    inotifywait -m "$LOG_DIR" -e modify --format '%w%f' | while read logfile
    do
        if [[ $logfile == *.log ]]; then
            # 获取新增的日志内容
            tail -n 10 "$logfile" > "$PIPE_FILE" &
            analyze_log_content "$PIPE_FILE" "$logfile"
        fi
    done
}

# 分析日志内容
analyze_log_content() {
    local pipe_file=$1
    local source_file=$2
    
    while read -r line; do
        # 检查错误模式
        for pattern in "${ERROR_PATTERNS[@]}"; do
            if echo "$line" | grep -qi "$pattern"; then
                send_alert "$pattern" "$line" "$source_file"
            fi
        done
        
        # 检查性能问题
        if echo "$line" | grep -qi "slow query\|timeout\|high cpu"; then
            send_performance_alert "$line" "$source_file"
        fi
        
    done < "$pipe_file"
}

# 发送告警
send_alert() {
    local pattern=$1
    local message=$2
    local source=$3
    
    echo "ALERT: Found $pattern in $source: $message"
    
    # 发送到告警系统（示例）
    curl -X POST "$ALERT_ENDPOINT" -H 'Content-Type: application/json' -d "[{
        \"labels\": {
            \"alertname\": \"LogError\",
            \"severity\": \"critical\",
            \"service\": \"$(basename $source .log)\",
            \"pattern\": \"$pattern\"
        },
        \"annotations\": {
            \"summary\": \"Error detected in logs\",
            \"description\": \"$message\"
        }
    }]" 2>/dev/null || echo "Failed to send alert"
}

# 启动监控
echo "Starting real-time log analysis for: $LOG_DIR"
monitor_logs
```

---

## 10. ☁️ 云原生监控最佳实践


### 10.1 Prometheus集成


**将文件监控指标暴露给Prometheus**：

```python
#!/usr/bin/env python3
# prometheus-file-monitor.py

import os
import time
from prometheus_client import Counter, Histogram, Gauge, start_http_server
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler

# Prometheus 指标定义
file_events_total = Counter('file_events_total', 'Total file events', ['event_type', 'file_path'])
file_processing_duration = Histogram('file_processing_seconds', 'File processing duration')
active_watchers = Gauge('active_file_watchers', 'Number of active file watchers')
last_event_timestamp = Gauge('last_file_event_timestamp', 'Timestamp of last file event')

class PrometheusFileHandler(FileSystemEventHandler):
    def __init__(self):
        self.processing_start_time = {}
        
    def on_any_event(self, event):
        if event.is_directory:
            return
            
        # 记录事件指标
        event_type = event.event_type
        file_path = os.path.basename(event.src_path)
        
        file_events_total.labels(event_type=event_type, file_path=file_path).inc()
        last_event_timestamp.set_to_current_time()
        
        print(f"Event: {event_type} - {file_path}")
        
        # 处理特定事件
        if event_type == 'created':
            self.processing_start_time[event.src_path] = time.time()
        elif event_type == 'modified':
            self.process_file_modification(event.src_path)
    
    @file_processing_duration.time()
    def process_file_modification(self, file_path):
        """处理文件修改事件并记录处理时间"""
        try:
            # 模拟文件处理逻辑
            with open(file_path, 'r') as f:
                content = f.read()
            
            # 这里可以添加实际的文件处理逻辑
            time.sleep(0.1)  # 模拟处理时间
            
            print(f"Processed file: {file_path} ({len(content)} bytes)")
            
        except Exception as e:
            print(f"Error processing file {file_path}: {e}")

class FileMonitorMetrics:
    def __init__(self, watch_paths):
        self.watch_paths = watch_paths
        self.observers = []
        
    def start_monitoring(self):
        """启动文件监控"""
        for path in self.watch_paths:
            if os.path.exists(path):
                observer = Observer()
                event_handler = PrometheusFileHandler()
                observer.schedule(event_handler, path, recursive=True)
                observer.start()
                self.observers.append(observer)
                print(f"Started monitoring: {path}")
        
        # 更新活跃监控器数量
        active_watchers.set(len(self.observers))
    
    def stop_monitoring(self):
        """停止文件监控"""
        for observer in self.observers:
            observer.stop()
            observer.join()
        active_watchers.set(0)

def main():
    # 从环境变量获取监控路径
    watch_paths = os.getenv('WATCH_PATHS', '/app/data,/app/config').split(',')
    metrics_port = int(os.getenv('METRICS_PORT', '8000'))
    
    # 启动Prometheus指标服务器
    start_http_server(metrics_port)
    print(f"Prometheus metrics server started on port {metrics_port}")
    
    # 启动文件监控
    monitor = FileMonitorMetrics(watch_paths)
    monitor.start_monitoring()
    
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("Stopping file monitor...")
        monitor.stop_monitoring()

if __name__ == "__main__":
    main()
```

### 10.2 云原生配置管理


**完整的云原生文件监控配置**：

```yaml
# k8s-complete-file-monitor.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: file-monitor-config
data:
  monitor-config.yaml: |
    monitor:
      paths:
        - /app/config
        - /app/data
      events:
        - create
        - modify
        - delete
      batch_size: 100
      flush_interval: 30s
    
    alerts:
      enabled: true
      webhook_url: "http://webhook-service:8080/alerts"
      
    metrics:
      enabled: true
      port: 8000
      
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: file-monitor
  labels:
    app: file-monitor
spec:
  replicas: 1
  selector:
    matchLabels:
      app: file-monitor
  template:
    metadata:
      labels:
        app: file-monitor
      annotations:
        prometheus.io/scrape: "true"
        prometheus.io/port: "8000"
        prometheus.io/path: "/metrics"
    spec:
      containers:
      - name: file-monitor
        image: file-monitor:v1.0.0
        ports:
        - containerPort: 8000
          name: metrics
        env:
        - name: WATCH_PATHS
          value: "/app/config,/app/data"
        - name: METRICS_PORT
          value: "8000"
        - name: LOG_LEVEL
          value: "INFO"
        volumeMounts:
        - name: config-volume
          mountPath: /app/config
        - name: data-volume
          mountPath: /app/data
        - name: monitor-config
          mountPath: /etc/monitor
        resources:
          limits:
            memory: "512Mi"
            cpu: "500m"
          requests:
            memory: "256Mi"
            cpu: "200m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8000
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8000
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
      - name: config-volume
        configMap:
          name: app-config
      - name: data-volume
        persistentVolumeClaim:
          claimName: app-data-pvc
      - name: monitor-config
        configMap:
          name: file-monitor-config

---
apiVersion: v1
kind: Service
metadata:
  name: file-monitor-service
  labels:
    app: file-monitor
spec:
  ports:
  - port: 8000
    targetPort: 8000
    name: metrics
  selector:
    app: file-monitor

---
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: file-monitor
  labels:
    app: file-monitor
spec:
  selector:
    matchLabels:
      app: file-monitor
  endpoints:
  - port: metrics
    path: /metrics
    interval: 30s
```

---

## 11. 📋 核心要点总结


### 11.1 必须掌握的核心概念


> 💡 **核心理解**  
> 容器环境的inotify监控需要考虑文件系统隔离、权限限制和资源配额等特殊性

```
🔸 容器限制：继承宿主机inotify限制，多容器共享资源
🔸 文件系统：支持overlay2等Docker存储驱动，但NFS不支持
🔸 卷挂载：绑定挂载最适合文件监控，事件传递更可靠
🔸 权限管理：容器内无法修改系统参数，需要宿主机配置
🔸 网络通信：容器间通过共享卷或网络进行事件通信
```

### 11.2 关键技术要点


**🔹 Docker环境配置**
```bash
# 宿主机调整inotify限制
echo 'fs.inotify.max_user_watches=524288' >> /etc/sysctl.conf
sysctl -p

# Docker运行时配置
docker run --sysctl fs.inotify.max_user_watches=524288 myapp
```

**🔹 Kubernetes配置管理**
```yaml
# ConfigMap监控重点
volumeMounts:
- name: config-volume
  mountPath: /app/config
# 监控..data符号链接的变化
```

**🔹 监控策略选择**
- **配置文件**：立即处理，触发应用重载
- **数据文件**：批量处理，提高效率
- **日志文件**：仅统计分析，不影响性能

### 11.3 最佳实践总结


**📊 监控架构设计**
```
单体应用：直接使用inotify监控本地文件
微服务：通过配置中心 + 事件总线协调
云原生：集成Prometheus指标 + Kubernetes生命周期
```

**⚠️ 常见问题避免**
- 不要在NFS挂载点使用inotify
- 避免监控过多文件导致资源耗尽
- 容器重启时注意监控状态恢复
- 注意ConfigMap更新的原子性操作

**🎯 性能优化建议**
- 合理设置事件过滤，避免无用事件
- 使用批量处理减少系统调用
- 实施健康检查确保监控服务可用
- 集成监控指标便于问题排查

**核心记忆**：
- 容器环境inotify需要特殊配置和考虑
- 卷挂载是容器文件监控的基础
- 配置热重载是微服务的核心需求
- 云原生监控需要与编排平台深度集成