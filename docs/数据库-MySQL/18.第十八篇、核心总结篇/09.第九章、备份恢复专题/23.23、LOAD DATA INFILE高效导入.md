---
title: 23、LOAD DATA INFILE高效导入
---
## 📚 目录

1. [LOAD DATA基础概念](#1-LOAD-DATA基础概念)
2. [基本语法与参数详解](#2-基本语法与参数详解)
3. [文件格式与分隔符设置](#3-文件格式与分隔符设置)
4. [NULL值与重复数据处理](#4-NULL值与重复数据处理)
5. [本地文件导入(LOCAL)](#5-本地文件导入LOCAL)
6. [性能优化策略](#6-性能优化策略)
7. [大文件处理技巧](#7-大文件处理技巧)
8. [错误处理与监控](#8-错误处理与监控)
9. [核心要点总结](#9-核心要点总结)

---

## 1. 💡 LOAD DATA基础概念


### 1.1 什么是LOAD DATA INFILE


**通俗解释**：想象你有一个Excel表格里的数据，需要快速导入到MySQL数据库中。`LOAD DATA INFILE`就是MySQL提供的一个"超级快速导入工具"，专门用来批量导入文本文件中的数据。

```
简单理解：
普通插入 = 一条一条地往数据库里放数据（慢）
LOAD DATA = 一次性把整个文件的数据倒进数据库（快）

速度对比：
INSERT逐条插入：1万条数据需要几分钟
LOAD DATA批量导入：1万条数据只需要几秒钟
```

### 1.2 为什么要使用LOAD DATA


**核心优势**：
- 🚀 **速度极快**：比INSERT语句快10-100倍
- 📊 **批量处理**：一次处理几万、几十万条数据
- 💾 **内存高效**：不会像逐条插入那样占用大量内存
- 🔄 **事务友好**：可以控制事务提交方式

### 1.3 适用场景


> 💡 **什么时候用LOAD DATA**：
> - 从Excel、CSV文件导入数据到数据库
> - 数据迁移：从其他系统导出的数据文件
> - 日志文件导入：网站访问日志、系统日志等
> - 批量数据初始化：测试数据、基础数据等

---

## 2. 📋 基本语法与参数详解


### 2.1 基础语法结构


```sql
LOAD DATA [LOCAL] INFILE '文件路径'
INTO TABLE 表名
[CHARACTER SET 字符集]
[FIELDS
    [TERMINATED BY '字段分隔符']
    [ENCLOSED BY '字段包围符']
    [ESCAPED BY '转义符']
]
[LINES
    [STARTING BY '行开始符']
    [TERMINATED BY '行结束符']
]
[IGNORE number {LINES | ROWS}]
[重复数据处理方式]
[(字段列表)]
```

### 2.2 最简单的使用示例


**场景**：有一个CSV文件，内容如下：
```
1,张三,25,北京
2,李四,30,上海
3,王五,28,广州
```

**对应的表结构**：
```sql
CREATE TABLE users (
    id INT PRIMARY KEY,
    name VARCHAR(50),
    age INT,
    city VARCHAR(50)
);
```

**最简单的导入命令**：
```sql
LOAD DATA INFILE '/tmp/users.csv'
INTO TABLE users
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n';
```

> 📍 **解释**：
> - `FIELDS TERMINATED BY ','`：告诉MySQL每个字段用逗号分隔
> - `LINES TERMINATED BY '\n'`：告诉MySQL每行用换行符结束

### 2.3 参数详细说明


| 参数 | 含义 | 举例 | 说明 |
|------|------|------|------|
| **TERMINATED BY** | 字段分隔符 | `','` | 逗号分隔（CSV） |
| **ENCLOSED BY** | 字段包围符 | `'"'` | 双引号包围字段 |
| **ESCAPED BY** | 转义符 | `'\\'` | 反斜杠转义特殊字符 |
| **STARTING BY** | 行开始符 | `'#'` | 每行以#开始 |
| **LINES TERMINATED** | 行结束符 | `'\n'` | 换行符结束 |

---

## 3. 📁 文件格式与分隔符设置


### 3.1 常见文件格式处理


#### 🔸 CSV格式（逗号分隔）

```
文件内容示例：
1,"张三",25,"北京市"
2,"李四",30,"上海市"

导入命令：
LOAD DATA INFILE '/tmp/data.csv'
INTO TABLE users
FIELDS TERMINATED BY ',' 
ENCLOSED BY '"'
LINES TERMINATED BY '\n';
```

#### 🔸 TSV格式（制表符分隔）

```
文件内容示例：
1	张三	25	北京市
2	李四	30	上海市

导入命令：
LOAD DATA INFILE '/tmp/data.tsv'
INTO TABLE users
FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n';
```

#### 🔸 自定义分隔符

```
文件内容示例：
1|张三|25|北京市
2|李四|30|上海市

导入命令：
LOAD DATA INFILE '/tmp/data.txt'
INTO TABLE users
FIELDS TERMINATED BY '|'
LINES TERMINATED BY '\n';
```

### 3.2 字符集处理


```sql
-- 指定文件字符集（解决中文乱码）
LOAD DATA INFILE '/tmp/data.csv'
INTO TABLE users
CHARACTER SET utf8mb4
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n';
```

> ⚠️ **注意**：如果文件是GBK编码，但数据库是UTF8，就需要指定字符集转换

### 3.3 处理特殊字符


```sql
-- 处理包含逗号和引号的数据
LOAD DATA INFILE '/tmp/data.csv'
INTO TABLE users
FIELDS TERMINATED BY ',' 
ENCLOSED BY '"'
ESCAPED BY '\\'
LINES TERMINATED BY '\n';
```

**文件内容示例**：
```
1,"张三,工程师",25,"北京市"
2,"李四说:\"你好\"",30,"上海市"
```

---

## 4. 🔄 NULL值与重复数据处理


### 4.1 NULL值处理


**场景**：CSV文件中有空值
```
文件内容：
1,张三,,北京
2,李四,30,
3,,25,广州
```

**处理方式**：
```sql
LOAD DATA INFILE '/tmp/data.csv'
INTO TABLE users
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
(id, name, @age, city)
SET age = NULLIF(@age, '');
```

> 💡 **解释**：
> - `@age`：临时变量接收原始值
> - `NULLIF(@age, '')`：如果@age是空字符串，就设为NULL

### 4.2 重复数据处理策略


#### 🔸 REPLACE方式（替换重复数据）

```sql
LOAD DATA INFILE '/tmp/data.csv'
REPLACE INTO TABLE users
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n';
```

**效果**：如果主键重复，会删除旧记录，插入新记录

#### 🔸 IGNORE方式（忽略重复数据）

```sql
LOAD DATA INFILE '/tmp/data.csv'
IGNORE INTO TABLE users
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n';
```

**效果**：如果主键重复，会跳过新记录，保留旧记录

#### 🔸 默认方式（报错停止）

```sql
-- 不加REPLACE或IGNORE，遇到重复数据会报错停止
LOAD DATA INFILE '/tmp/data.csv'
INTO TABLE users
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n';
```

### 4.3 对比三种处理方式


| 处理方式 | 遇到重复数据 | 适用场景 |
|----------|-------------|----------|
| **默认** | 报错停止 | 数据质量要求高，不允许重复 |
| **REPLACE** | 新数据覆盖旧数据 | 数据更新场景 |
| **IGNORE** | 保留旧数据，跳过新数据 | 增量导入，避免重复 |

---

## 5. 🏠 本地文件导入(LOCAL)


### 5.1 LOCAL关键字的作用


**通俗解释**：
- **不加LOCAL**：文件必须在MySQL服务器的机器上
- **加LOCAL**：文件可以在客户端（你的电脑）上

```
情况1：文件在服务器上
LOAD DATA INFILE '/var/lib/mysql-files/data.csv'
INTO TABLE users;

情况2：文件在你的电脑上
LOAD DATA LOCAL INFILE '/home/user/data.csv'
INTO TABLE users;
```

### 5.2 LOCAL使用示例


```sql
-- 从本地电脑导入文件
LOAD DATA LOCAL INFILE 'C:/Users/用户名/Desktop/data.csv'
INTO TABLE users
FIELDS TERMINATED BY ','
ENCLOSED BY '"'
LINES TERMINATED BY '\r\n';  -- Windows换行符
```

### 5.3 LOCAL的限制与安全


> ⚠️ **安全注意事项**：
> - MySQL默认可能禁用LOCAL功能
> - 需要设置`local_infile=1`参数启用
> - 建议只在可信环境下使用

**启用LOCAL功能**：
```sql
-- 检查是否启用
SHOW VARIABLES LIKE 'local_infile';

-- 启用LOCAL功能
SET GLOBAL local_infile = 1;
```

### 5.4 文件路径注意事项


```
Windows系统：
✅ 正确：'C:/data/users.csv'
✅ 正确：'C:\\data\\users.csv'
❌ 错误：'C:\data\users.csv'  (单反斜杠会被转义)

Linux/Mac系统：
✅ 正确：'/home/user/data.csv'
✅ 正确：'/tmp/users.csv'
```

---

## 6. ⚡ 性能优化策略


### 6.1 基础性能优化


#### 🔸 禁用自动提交

```sql
-- 导入前关闭自动提交
SET autocommit = 0;

LOAD DATA INFILE '/tmp/data.csv'
INTO TABLE users
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n';

-- 手动提交
COMMIT;

-- 恢复自动提交
SET autocommit = 1;
```

**效果**：避免每条记录都提交一次事务，提升速度

#### 🔸 临时禁用索引检查

```sql
-- 导入前禁用
SET unique_checks = 0;
SET foreign_key_checks = 0;

-- 执行导入
LOAD DATA INFILE '/tmp/data.csv' INTO TABLE users;

-- 导入后恢复
SET unique_checks = 1;
SET foreign_key_checks = 1;
```

### 6.2 高级性能优化


#### 🔸 调整缓冲区大小

```sql
-- 增加批量插入缓冲区
SET bulk_insert_buffer_size = 256 * 1024 * 1024;  -- 256MB
```

#### 🔸 优化表结构

```sql
-- 导入前删除非必要索引
ALTER TABLE users DROP INDEX idx_name;

-- 导入完成后重建索引
ALTER TABLE users ADD INDEX idx_name(name);
```

### 6.3 性能测试对比


```
测试数据：100万条记录

方法1：逐条INSERT
INSERT INTO users VALUES (1,'张三',25,'北京');
执行时间：约15分钟

方法2：基础LOAD DATA
LOAD DATA INFILE 'data.csv' INTO TABLE users;
执行时间：约2分钟

方法3：优化后LOAD DATA
-- 关闭自动提交 + 禁用检查
执行时间：约30秒
```

> 🚀 **性能提升总结**：优化后的LOAD DATA比逐条INSERT快约30倍！

---

## 7. 📦 大文件处理技巧


### 7.1 大文件分割策略


**问题**：一个10GB的CSV文件，直接导入可能会：
- 内存不足
- 导入时间过长
- 出错后重新开始很麻烦

**解决方案**：分割成小文件
```bash
# Linux命令分割文件（每100万行一个文件）
split -l 1000000 big_data.csv small_data_

# 生成文件：
# small_data_aa (第1-100万行)
# small_data_ab (第101-200万行)
# small_data_ac (第201-300万行)
```

### 7.2 批量导入脚本


```sql
-- 创建存储过程批量导入
DELIMITER //
CREATE PROCEDURE batch_import()
BEGIN
    DECLARE done INT DEFAULT FALSE;
    DECLARE file_name VARCHAR(255);
    
    -- 文件列表
    DECLARE file_cursor CURSOR FOR 
        SELECT CONCAT('/tmp/split_', suffix, '.csv') 
        FROM file_list;
    
    DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE;
    
    OPEN file_cursor;
    
    import_loop: LOOP
        FETCH file_cursor INTO file_name;
        IF done THEN
            LEAVE import_loop;
        END IF;
        
        -- 执行导入
        SET @sql = CONCAT('LOAD DATA INFILE ''', file_name, ''' INTO TABLE users');
        PREPARE stmt FROM @sql;
        EXECUTE stmt;
        DEALLOCATE PREPARE stmt;
        
    END LOOP;
    
    CLOSE file_cursor;
END //
DELIMITER ;
```

### 7.3 导入进度监控


```sql
-- 查看当前导入状态
SHOW PROCESSLIST;

-- 查看表记录数变化
SELECT COUNT(*) FROM users;

-- 查看表空间大小
SELECT 
    table_name,
    ROUND(data_length/1024/1024, 2) as 'Data Size(MB)',
    ROUND(index_length/1024/1024, 2) as 'Index Size(MB)'
FROM information_schema.tables 
WHERE table_name = 'users';
```

### 7.4 内存监控


```sql
-- 查看MySQL内存使用
SHOW STATUS LIKE 'Innodb_buffer_pool%';

-- 监控连接数
SHOW STATUS LIKE 'Connections';
SHOW STATUS LIKE 'Max_used_connections';
```

---

## 8. 🔍 错误处理与监控


### 8.1 常见错误及解决方案


#### 🔸 文件权限错误

```
错误信息：
ERROR 1045 (28000): Access denied for user

解决方案：
1. 检查文件权限
chmod 644 /tmp/data.csv

2. 将文件放到MySQL允许的目录
SHOW VARIABLES LIKE 'secure_file_priv';
```

#### 🔸 字符集错误

```
错误信息：
Incorrect string value

解决方案：
LOAD DATA INFILE '/tmp/data.csv'
INTO TABLE users
CHARACTER SET utf8mb4  -- 指定正确的字符集
```

#### 🔸 字段数量不匹配

```
错误信息：
Column count doesn't match value count

解决方案：
-- 明确指定字段列表
LOAD DATA INFILE '/tmp/data.csv'
INTO TABLE users
(id, name, age, city)  -- 只导入这4个字段
```

### 8.2 错误日志查看


```sql
-- 查看MySQL错误日志位置
SHOW VARIABLES LIKE 'log_error';

-- 查看警告信息
SHOW WARNINGS;

-- 查看最后的错误
SHOW ERRORS;
```

### 8.3 数据验证脚本


```sql
-- 导入前后数据对比
-- 导入前记录行数
SET @before_count = (SELECT COUNT(*) FROM users);

-- 执行导入
LOAD DATA INFILE '/tmp/data.csv' INTO TABLE users;

-- 导入后记录行数
SET @after_count = (SELECT COUNT(*) FROM users);

-- 计算导入了多少条
SELECT @after_count - @before_count AS imported_rows;
```

### 8.4 回滚机制


```sql
-- 开启事务进行安全导入
START TRANSACTION;

LOAD DATA INFILE '/tmp/data.csv'
INTO TABLE users
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n';

-- 检查数据是否正确
SELECT COUNT(*) FROM users WHERE name IS NULL;

-- 如果数据有问题，回滚
-- ROLLBACK;

-- 如果数据正确，提交
-- COMMIT;
```

---

## 9. 📋 核心要点总结


### 9.1 必须掌握的基本概念


```
🔸 LOAD DATA本质：MySQL的高速批量导入工具
🔸 基本语法：指定文件路径、表名、分隔符
🔸 LOCAL关键字：区分服务器文件和本地文件
🔸 重复数据处理：REPLACE(替换)、IGNORE(忽略)、默认(报错)
🔸 性能优化：关闭自动提交、禁用检查、调整缓冲区
```

### 9.2 关键理解要点


**🔹 什么时候用LOAD DATA**
```
适合场景：
✅ 大量数据导入（1万条以上）
✅ CSV、TSV等结构化文件
✅ 数据迁移和初始化
✅ 日志文件导入

不适合场景：
❌ 少量数据（几百条以内用INSERT更简单）
❌ 复杂的数据转换需求
❌ 需要复杂业务逻辑处理
```

**🔹 性能优化的核心思路**
```
减少事务提交次数：关闭自动提交
减少约束检查：临时禁用索引和外键检查
增加缓冲区：调整相关内存参数
文件分割：大文件拆分成小文件批量处理
```

**🔹 错误处理的最佳实践**
```
预防为主：
- 检查文件格式和权限
- 验证字符集匹配
- 确认字段数量对应

安全导入：
- 使用事务包装
- 备份原始数据
- 分步验证结果
```

### 9.3 实际应用指导


**📊 常用场景模板**

```sql
-- 场景1：标准CSV导入
LOAD DATA LOCAL INFILE 'data.csv'
INTO TABLE users
FIELDS TERMINATED BY ',' ENCLOSED BY '"'
LINES TERMINATED BY '\n'
IGNORE 1 LINES;  -- 跳过表头

-- 场景2：增量数据导入
LOAD DATA LOCAL INFILE 'increment.csv'
IGNORE INTO TABLE users  -- 忽略重复数据
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n';

-- 场景3：大文件高性能导入
SET autocommit = 0;
SET unique_checks = 0;
SET foreign_key_checks = 0;

LOAD DATA INFILE '/tmp/bigdata.csv'
INTO TABLE users
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n';

COMMIT;
SET autocommit = 1;
SET unique_checks = 1;
SET foreign_key_checks = 1;
```

### 9.4 记忆要诀


> 💡 **LOAD DATA记忆口诀**：
> - 文件路径要准确，权限字符集别忘
> - 分隔符要对应，换行符Linux Windows不同
> - LOCAL本地服务器，重复数据三选择
> - 性能优化有套路，关闭检查调参数
> - 大文件要分割，错误处理要周全

**核心价值**：LOAD DATA INFILE是MySQL中最快的批量导入方式，掌握它能让你的数据导入效率提升几十倍，是处理大数据量必备的技能！