---
title: 27ã€BINLOGåœ¨å¤§æ•°æ®ç¯å¢ƒåº”ç”¨
---
## ğŸ“š ç›®å½•

1. [å¤§æ•°æ®ç¯å¢ƒBINLOGæ¦‚è¿°](#1-å¤§æ•°æ®ç¯å¢ƒBINLOGæ¦‚è¿°)
2. [å¤§æ•°æ®é‡BINLOGç®¡ç†ç­–ç•¥](#2-å¤§æ•°æ®é‡BINLOGç®¡ç†ç­–ç•¥)
3. [é«˜å¹¶å‘å†™å…¥ä¼˜åŒ–æ–¹æ¡ˆ](#3-é«˜å¹¶å‘å†™å…¥ä¼˜åŒ–æ–¹æ¡ˆ)
4. [åˆ†å¸ƒå¼BINLOGå¤„ç†æ¶æ„](#4-åˆ†å¸ƒå¼BINLOGå¤„ç†æ¶æ„)
5. [å®æ—¶æ•°æ®åŒæ­¥æœºåˆ¶](#5-å®æ—¶æ•°æ®åŒæ­¥æœºåˆ¶)
6. [ETLæ•°æ®æŠ½å–åº”ç”¨](#6-ETLæ•°æ®æŠ½å–åº”ç”¨)
7. [æ•°æ®æ¹–é›†æˆæ–¹æ¡ˆ](#7-æ•°æ®æ¹–é›†æˆæ–¹æ¡ˆ)
8. [æµå¼æ•°æ®å¤„ç†](#8-æµå¼æ•°æ®å¤„ç†)
9. [CDCå˜æ›´æ•°æ®æ•è·](#9-CDCå˜æ›´æ•°æ®æ•è·)
10. [Kafkaé›†æˆåº”ç”¨](#10-Kafkaé›†æˆåº”ç”¨)
11. [å¤§æ•°æ®æ¶æ„è®¾è®¡](#11-å¤§æ•°æ®æ¶æ„è®¾è®¡)
12. [æ ¸å¿ƒè¦ç‚¹æ€»ç»“](#12-æ ¸å¿ƒè¦ç‚¹æ€»ç»“)

---

## 1. ğŸŒŠ å¤§æ•°æ®ç¯å¢ƒBINLOGæ¦‚è¿°


### 1.1 ä»€ä¹ˆæ˜¯å¤§æ•°æ®ç¯å¢ƒä¸‹çš„BINLOG


**ğŸ”¸ ç®€å•ç†è§£**
```
æƒ³è±¡ä¸€ä¸ªå¤§å‹è¶…å¸‚ï¼š
- æ™®é€šè¶…å¸‚ï¼šæ¯å¤©å‡ åƒç¬”äº¤æ˜“ï¼Œä¸€ä¸ªæ”¶é“¶å‘˜è®°å½•å°±å¤Ÿäº†
- å¤§å‹å•†åœºï¼šæ¯å¤©å‡ ç™¾ä¸‡ç¬”äº¤æ˜“ï¼Œéœ€è¦å¤šä¸ªæ”¶é“¶ç³»ç»ŸååŒå·¥ä½œ

BINLOGåœ¨å¤§æ•°æ®ç¯å¢ƒå°±åƒå•†åœºçš„äº¤æ˜“è®°å½•ç³»ç»Ÿï¼š
â€¢ æ•°æ®é‡ï¼šTBçº§åˆ«çš„æ—¥å¿—æ–‡ä»¶
â€¢ å¹¶å‘é‡ï¼šæ¯ç§’æ•°ä¸‡æ¬¡æ•°æ®å˜æ›´
â€¢ å®æ—¶æ€§ï¼šæ¯«ç§’çº§çš„æ•°æ®åŒæ­¥è¦æ±‚
â€¢ å¤æ‚æ€§ï¼šå¤šç³»ç»Ÿã€å¤šä¸šåŠ¡çº¿åŒæ—¶ä½¿ç”¨
```

**ğŸ’¡ å¤§æ•°æ®ç¯å¢ƒç‰¹å¾**
```
æ•°æ®é‡ç‰¹å¾ï¼š
â€¢ å•æ—¥BINLOGï¼šå‡ åGBåˆ°å‡ TB
â€¢ å†å²æ•°æ®ï¼šPBçº§åˆ«çš„ç´¯ç§¯
â€¢ å³°å€¼å†™å…¥ï¼šæ¯ç§’å‡ ä¸‡åˆ°å‡ åä¸‡äº‹åŠ¡

ä¸šåŠ¡ç‰¹å¾ï¼š
â€¢ 7Ã—24å°æ—¶ä¸é—´æ–­è¿è¡Œ
â€¢ å¤šåœ°åŸŸåˆ†å¸ƒå¼éƒ¨ç½²
â€¢ å®æ—¶æ€§è¦æ±‚æé«˜
â€¢ æ•°æ®ä¸€è‡´æ€§è¦æ±‚ä¸¥æ ¼
```

### 1.2 å¤§æ•°æ®ç¯å¢ƒé¢ä¸´çš„æŒ‘æˆ˜


**âš ï¸ ä¸»è¦æŒ‘æˆ˜**
```
å­˜å‚¨æŒ‘æˆ˜ï¼š
é—®é¢˜ï¼šBINLOGæ–‡ä»¶å¿«é€Ÿå¢é•¿ï¼Œå ç”¨å¤§é‡ç£ç›˜ç©ºé—´
å½±å“ï¼šå­˜å‚¨æˆæœ¬é«˜ï¼Œç®¡ç†å¤æ‚

æ€§èƒ½æŒ‘æˆ˜ï¼š
é—®é¢˜ï¼šé«˜å¹¶å‘å†™å…¥å¯¼è‡´BINLOGå†™å…¥æˆä¸ºç“¶é¢ˆ
å½±å“ï¼šæ•°æ®åº“æ€§èƒ½ä¸‹é™ï¼Œå“åº”æ—¶é—´å¢åŠ 

ä¼ è¾“æŒ‘æˆ˜ï¼š
é—®é¢˜ï¼šå¤§é‡BINLOGæ•°æ®éœ€è¦å®æ—¶ä¼ è¾“åˆ°å…¶ä»–ç³»ç»Ÿ
å½±å“ï¼šç½‘ç»œå¸¦å®½å ç”¨é«˜ï¼Œä¼ è¾“å»¶è¿Ÿ

å¤„ç†æŒ‘æˆ˜ï¼š
é—®é¢˜ï¼šä¸‹æ¸¸ç³»ç»Ÿéœ€è¦å®æ—¶å¤„ç†æµ·é‡BINLOGæ•°æ®
å½±å“ï¼šå¤„ç†èƒ½åŠ›ä¸è¶³ï¼Œæ•°æ®ç§¯å‹
```

### 1.3 è§£å†³æ–¹æ¡ˆæ€è·¯


**ğŸ¯ æ ¸å¿ƒè§£å†³æ€è·¯**
```
åˆ†å±‚å¤„ç†ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ä¸šåŠ¡åº”ç”¨å±‚     â”‚ â† ä¸“æ³¨ä¸šåŠ¡é€»è¾‘
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   æ•°æ®å¤„ç†å±‚     â”‚ â† BINLOGè§£æå¤„ç†
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   æ•°æ®ä¼ è¾“å±‚     â”‚ â† å¯é çš„æ•°æ®ä¼ è¾“
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   æ•°æ®å­˜å‚¨å±‚     â”‚ â† é«˜æ•ˆçš„å­˜å‚¨ç®¡ç†
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ°´å¹³æ‰©å±•ï¼š
ä¸»åº“ â†’ å¤šä¸ªä»åº“ â†’ åˆ†å¸ƒå¼å¤„ç†é›†ç¾¤

å‚ç›´ä¼˜åŒ–ï¼š
ç¡¬ä»¶ä¼˜åŒ– + è½¯ä»¶è°ƒä¼˜ + æ¶æ„è®¾è®¡
```

---

## 2. ğŸ“¦ å¤§æ•°æ®é‡BINLOGç®¡ç†ç­–ç•¥


### 2.1 BINLOGæ–‡ä»¶å¤§å°æ§åˆ¶


**ğŸ”§ æ–‡ä»¶å¤§å°ä¼˜åŒ–é…ç½®**
```sql
-- åˆç†è®¾ç½®å•ä¸ªBINLOGæ–‡ä»¶å¤§å°
SET GLOBAL max_binlog_size = 512M;  -- å»ºè®®512MB-1GB

-- è®¾ç½®BINLOGç¼“å­˜å¤§å°
SET GLOBAL binlog_cache_size = 4M;  -- å•äº‹åŠ¡ç¼“å­˜
SET GLOBAL max_binlog_cache_size = 512M;  -- æœ€å¤§ç¼“å­˜

-- æ§åˆ¶äº‹åŠ¡å¤§å°
SET GLOBAL max_binlog_stmt_cache_size = 1G;
```

**ğŸ’¡ æ–‡ä»¶å¤§å°é€‰æ‹©åŸåˆ™**
```
æ–‡ä»¶è¿‡å°çš„é—®é¢˜ï¼š
â€¢ æ–‡ä»¶æ•°é‡è¿‡å¤šï¼Œç®¡ç†å¤æ‚
â€¢ é¢‘ç¹åˆ‡æ¢æ–‡ä»¶ï¼Œå½±å“æ€§èƒ½
â€¢ å¤‡ä»½å’Œä¼ è¾“æ•ˆç‡ä½

æ–‡ä»¶è¿‡å¤§çš„é—®é¢˜ï¼š
â€¢ å•ä¸ªæ–‡ä»¶æ•…éšœå½±å“èŒƒå›´å¤§
â€¢ æ¢å¤æ—¶é—´é•¿
â€¢ å†…å­˜å ç”¨é«˜

æ¨èé…ç½®ï¼š
æ™®é€šä¸šåŠ¡ï¼š256MB - 512MB
é«˜å¹¶å‘ä¸šåŠ¡ï¼š512MB - 1GB
è¶…å¤§äº‹åŠ¡åœºæ™¯ï¼š1GB - 2GB
```

### 2.2 BINLOGè¿‡æœŸæ¸…ç†ç­–ç•¥


**ğŸ—‘ï¸ è‡ªåŠ¨æ¸…ç†é…ç½®**
```sql
-- è®¾ç½®BINLOGä¿ç•™å¤©æ•°
SET GLOBAL binlog_expire_logs_seconds = 259200;  -- 3å¤©
-- æˆ–è€…ä½¿ç”¨æ—§å‚æ•°ï¼ˆå…¼å®¹æ€§ï¼‰
SET GLOBAL expire_logs_days = 3;

-- æ£€æŸ¥å½“å‰è®¾ç½®
SHOW VARIABLES LIKE '%expire_logs%';

-- æ‰‹åŠ¨æ¸…ç†æŒ‡å®šæ—¶é—´å‰çš„BINLOG
PURGE BINARY LOGS BEFORE DATE_SUB(NOW(), INTERVAL 3 DAY);
```

**ğŸ“‹ åˆ†çº§æ¸…ç†ç­–ç•¥**
```
å®æ—¶æ•°æ®ï¼ˆå½“å¤©ï¼‰ï¼š
ä¿ç•™ç­–ç•¥ï¼šå¿…é¡»ä¿ç•™
å­˜å‚¨ä½ç½®ï¼šé«˜é€ŸSSD
å¤‡ä»½ç­–ç•¥ï¼šå®æ—¶åŒæ­¥å¤‡ä»½

è¿‘æœŸæ•°æ®ï¼ˆ1-7å¤©ï¼‰ï¼š
ä¿ç•™ç­–ç•¥ï¼šæœ¬åœ°ä¿ç•™
å­˜å‚¨ä½ç½®ï¼šæ™®é€šSSD/æœºæ¢°ç¡¬ç›˜
å¤‡ä»½ç­–ç•¥ï¼šå®šæœŸå¤‡ä»½åˆ°å½’æ¡£å­˜å‚¨

å†å²æ•°æ®ï¼ˆ7å¤©ä»¥ä¸Šï¼‰ï¼š
ä¿ç•™ç­–ç•¥ï¼šå½’æ¡£å­˜å‚¨
å­˜å‚¨ä½ç½®ï¼šä½æˆæœ¬å­˜å‚¨ï¼ˆOSS/S3ï¼‰
å¤‡ä»½ç­–ç•¥ï¼šå†·å¤‡ä»½ï¼ŒæŒ‰éœ€æ¢å¤

æ¸…ç†è„šæœ¬ç¤ºä¾‹ï¼š
#!/bin/bash
# æ¸…ç†7å¤©å‰çš„BINLOG
mysql -e "PURGE BINARY LOGS BEFORE DATE_SUB(NOW(), INTERVAL 7 DAY);"
```

### 2.3 åˆ†åŒºå­˜å‚¨ç®¡ç†


**ğŸ’¾ å­˜å‚¨åˆ†åŒºç­–ç•¥**
```
æŒ‰æ—¶é—´åˆ†åŒºï¼š
/data/mysql/binlog/2024/01/15/mysql-bin.000001
/data/mysql/binlog/2024/01/15/mysql-bin.000002
/data/mysql/binlog/2024/01/16/mysql-bin.000003

æŒ‰ä¸šåŠ¡åˆ†åŒºï¼š
/data/mysql/binlog/user/mysql-bin.000001
/data/mysql/binlog/order/mysql-bin.000002
/data/mysql/binlog/payment/mysql-bin.000003

å­˜å‚¨é…ç½®ç¤ºä¾‹ï¼š
# é…ç½®BINLOGç›®å½•
log-bin=/data/binlog/mysql-bin
binlog-format=ROW
sync_binlog=1
```

---

## 3. âš¡ é«˜å¹¶å‘å†™å…¥ä¼˜åŒ–æ–¹æ¡ˆ


### 3.1 BINLOGå†™å…¥æ€§èƒ½ä¼˜åŒ–


**ğŸš€ å…³é”®å‚æ•°è°ƒä¼˜**
```sql
-- åŒæ­¥ç­–ç•¥ä¼˜åŒ–
SET GLOBAL sync_binlog = 1;  -- å¼ºä¸€è‡´æ€§ï¼Œæ¯æ¬¡äº‹åŠ¡åŒæ­¥
-- æˆ–è€…
SET GLOBAL sync_binlog = 100;  -- é«˜æ€§èƒ½ï¼Œæ¯100æ¬¡äº‹åŠ¡åŒæ­¥

-- ç»„æäº¤ä¼˜åŒ–
SET GLOBAL binlog_group_commit_sync_delay = 1000;  -- 1mså»¶è¿Ÿ
SET GLOBAL binlog_group_commit_sync_no_delay_count = 10;  -- 10ä¸ªäº‹åŠ¡ä¸€ç»„

-- IOçº¿ç¨‹ä¼˜åŒ–
SET GLOBAL innodb_flush_log_at_trx_commit = 2;  -- é…åˆä½¿ç”¨
```

**âš–ï¸ æ€§èƒ½ä¸ä¸€è‡´æ€§å¹³è¡¡**
```
å¼ºä¸€è‡´æ€§é…ç½®ï¼ˆé‡‘èçº§ï¼‰ï¼š
sync_binlog = 1
innodb_flush_log_at_trx_commit = 1
ä¼˜ç‚¹ï¼šæ•°æ®ç»å¯¹å®‰å…¨
ç¼ºç‚¹ï¼šæ€§èƒ½è¾ƒä½

å‡†ä¸€è‡´æ€§é…ç½®ï¼ˆä¸€èˆ¬ä¸šåŠ¡ï¼‰ï¼š
sync_binlog = 100  
innodb_flush_log_at_trx_commit = 2
ä¼˜ç‚¹ï¼šæ€§èƒ½è¾ƒå¥½
ç¼ºç‚¹ï¼šå¯èƒ½ä¸¢å¤±å°‘é‡æ•°æ®

é«˜æ€§èƒ½é…ç½®ï¼ˆæ—¥å¿—åˆ†æï¼‰ï¼š
sync_binlog = 1000
innodb_flush_log_at_trx_commit = 0
ä¼˜ç‚¹ï¼šæ€§èƒ½æœ€é«˜
ç¼ºç‚¹ï¼šæ•°æ®å®‰å…¨æ€§è¾ƒä½
```

### 3.2 å¹¶å‘å†™å…¥æ¶æ„ä¼˜åŒ–


**ğŸ”„ åˆ†åº“åˆ†è¡¨ç­–ç•¥**
```
æ°´å¹³åˆ†åº“ï¼š
ç”¨æˆ·åº“ï¼šuser_db_01, user_db_02, user_db_03
è®¢å•åº“ï¼šorder_db_01, order_db_02, order_db_03

æ¯ä¸ªåˆ†åº“ç‹¬ç«‹çš„BINLOGï¼š
ç”¨æˆ·åº“1: user_db_01-bin.000001
ç”¨æˆ·åº“2: user_db_02-bin.000001
è®¢å•åº“1: order_db_01-bin.000001

åˆ†ç‰‡è§„åˆ™ç¤ºä¾‹ï¼š
ç”¨æˆ·åˆ†ç‰‡ï¼šuser_id % 16
è®¢å•åˆ†ç‰‡ï¼šorder_id % 32
```

**ğŸ”€ è¯»å†™åˆ†ç¦»ä¼˜åŒ–**
```
ä¸»ä»æ¶æ„ï¼š
        ä¸»åº“ï¼ˆå†™ï¼‰
         â†“ BINLOG
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”´â”€â”€â”€â”€â”
   ä»åº“1     ä»åº“2     ä»åº“3
  ï¼ˆè¯»ï¼‰    ï¼ˆè¯»ï¼‰    ï¼ˆè¯»ï¼‰

BINLOGä¼ è¾“ä¼˜åŒ–ï¼š
â€¢ å¹¶è¡Œå¤åˆ¶ï¼šå¤šçº¿ç¨‹ä¼ è¾“BINLOG
â€¢ åŠåŒæ­¥å¤åˆ¶ï¼šç¡®ä¿è‡³å°‘ä¸€ä¸ªä»åº“æ”¶åˆ°
â€¢ å»¶è¿Ÿç›‘æ§ï¼šå®æ—¶ç›‘æ§ä¸»ä»å»¶è¿Ÿ
```

### 3.3 æ‰¹é‡å†™å…¥ä¼˜åŒ–


**ğŸ“¦ æ‰¹é‡æ“ä½œç­–ç•¥**
```sql
-- å¼€å¯æ‰¹é‡æ’å…¥ä¼˜åŒ–
SET GLOBAL bulk_insert_buffer_size = 64M;

-- æ‰¹é‡æ“ä½œç¤ºä¾‹
START TRANSACTION;
INSERT INTO user_table VALUES 
(1, 'user1', 'email1'),
(2, 'user2', 'email2'),
(3, 'user3', 'email3'),
-- ... æ‰¹é‡æ•°æ®
(1000, 'user1000', 'email1000');
COMMIT;

-- é¿å…å•è¡Œæ’å…¥
-- ä¸æ¨èï¼š
INSERT INTO table VALUES (1, 'data1');
INSERT INTO table VALUES (2, 'data2');

-- æ¨èï¼š
INSERT INTO table VALUES 
(1, 'data1'),
(2, 'data2'),
(3, 'data3');
```

---

## 4. ğŸ—ï¸ åˆ†å¸ƒå¼BINLOGå¤„ç†æ¶æ„


### 4.1 åˆ†å¸ƒå¼æ¶æ„è®¾è®¡


**ğŸŒ æ•´ä½“æ¶æ„å›¾**
```
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   ä¸šåŠ¡ç³»ç»Ÿ   â”‚
                        â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚     MySQLé›†ç¾¤     â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”   â”‚
                    â”‚  â”‚ä¸»åº“1â”‚ä¸»åº“2â”‚   â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚ BINLOG
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   BINLOGæ”¶é›†å±‚   â”‚
                    â”‚ â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚ â”‚Canalâ”‚Maxwell â”‚ â”‚
                    â”‚ â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    æ¶ˆæ¯é˜Ÿåˆ—å±‚     â”‚
                    â”‚  â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”   â”‚
                    â”‚  â”‚Kafkaâ”‚Pulsarâ”‚  â”‚
                    â”‚  â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   æ•°æ®å¤„ç†å±‚     â”‚
                    â”‚ â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚ â”‚Flinkâ”‚Storm   â”‚ â”‚
                    â”‚ â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   æ•°æ®å­˜å‚¨å±‚     â”‚
                    â”‚ â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
                    â”‚ â”‚ES   â”‚ClickHouseâ”‚â”‚
                    â”‚ â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.2 Canalåˆ†å¸ƒå¼éƒ¨ç½²


**ğŸ› ï¸ Canalé›†ç¾¤é…ç½®**
```yaml
# canal.properties
canal.serverMode = kafka
canal.mq.servers = kafka1:9092,kafka2:9092,kafka3:9092
canal.mq.retries = 3
canal.mq.batchSize = 16384

# instance.properties
canal.instance.mysql.slaveId = 1001
canal.instance.master.address = mysql-master:3306
canal.instance.dbUsername = canal
canal.instance.dbPassword = canal123
canal.instance.defaultDatabaseName = test
canal.instance.connectionCharset = UTF-8

# å¤šå®ä¾‹é…ç½®
instance1: å¤„ç†ç”¨æˆ·ç›¸å…³è¡¨
instance2: å¤„ç†è®¢å•ç›¸å…³è¡¨  
instance3: å¤„ç†æ”¯ä»˜ç›¸å…³è¡¨
```

**ğŸ“Š ç›‘æ§ä¸ç®¡ç†**
```bash
# Canalé›†ç¾¤çŠ¶æ€ç›‘æ§
curl http://canal-admin:8089/api/v1/canal/list

# å®ä¾‹è¿è¡ŒçŠ¶æ€
curl http://canal-server:11110/api/v1/canal/instance/running

# ç›‘æ§å…³é”®æŒ‡æ ‡
- BINLOGä½ç½®å»¶è¿Ÿ
- æ¶ˆæ¯å‘é€æˆåŠŸç‡
- å®ä¾‹è¿è¡ŒçŠ¶æ€
- ç½‘ç»œè¿æ¥çŠ¶æ€
```

### 4.3 é«˜å¯ç”¨æ¶æ„


**ğŸ”„ æ•…éšœè½¬ç§»æœºåˆ¶**
```
ä¸»å¤‡åˆ‡æ¢ï¼š
Canalä¸»èŠ‚ç‚¹ â†â†’ Canalå¤‡èŠ‚ç‚¹
    â†“              â†“
  Kafkaåˆ†åŒº1    Kafkaåˆ†åŒº2

æ•…éšœæ£€æµ‹ï¼š
â€¢ å¿ƒè·³æ£€æµ‹ï¼šæ¯5ç§’æ£€æŸ¥ä¸€æ¬¡
â€¢ å¥åº·æ£€æŸ¥ï¼šæ£€æŸ¥MySQLè¿æ¥çŠ¶æ€
â€¢ è‡ªåŠ¨åˆ‡æ¢ï¼š30ç§’å†…å®Œæˆæ•…éšœè½¬ç§»

é…ç½®ç¤ºä¾‹ï¼š
# å¯ç”¨HAæ¨¡å¼
canal.zkServers = zk1:2181,zk2:2181,zk3:2181
canal.instance.master.heartbeatPeriod = 5000
canal.instance.detecting.enable = true
```

---

## 5. ğŸ”„ å®æ—¶æ•°æ®åŒæ­¥æœºåˆ¶


### 5.1 å‡†å®æ—¶åŒæ­¥æ–¹æ¡ˆ


**â±ï¸ åŒæ­¥å»¶è¿Ÿæ§åˆ¶**
```
å»¶è¿Ÿçº§åˆ«å®šä¹‰ï¼š
å®æ—¶çº§ï¼š< 100msï¼ˆé‡‘èäº¤æ˜“ï¼‰
å‡†å®æ—¶çº§ï¼š< 1sï¼ˆç”¨æˆ·çŠ¶æ€ï¼‰
è¿‘å®æ—¶çº§ï¼š< 10sï¼ˆæ•°æ®åˆ†æï¼‰
æ‰¹å¤„ç†çº§ï¼š< 1å°æ—¶ï¼ˆæŠ¥è¡¨ç»Ÿè®¡ï¼‰

æŠ€æœ¯å®ç°ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MySQL   â”‚â”€â”€â†’â”‚ Canal   â”‚â”€â”€â†’â”‚ Kafka   â”‚
â”‚ BINLOG  â”‚   â”‚ è§£æ    â”‚   â”‚ é˜Ÿåˆ—    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â†“
                          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                          â”‚ æ¶ˆè´¹ç«¯   â”‚
                          â”‚ å¤„ç†    â”‚
                          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ğŸ¯ åŒæ­¥ç­–ç•¥é…ç½®**
```yaml
# å®æ—¶åŒæ­¥é…ç½®
canal.instance.parser.parallelBufferSize = 256
canal.instance.parser.parallelThreadSize = 16
canal.mq.batchSize = 50
canal.mq.lingerMs = 100

# Kafkaæ¶ˆè´¹ç«¯é…ç½®  
batch.size=16384
linger.ms=5
max.poll.records=500
```

### 5.2 æ•°æ®ä¸€è‡´æ€§ä¿éšœ


**âœ… ä¸€è‡´æ€§æ£€æŸ¥æœºåˆ¶**
```sql
-- æ•°æ®ä¸€è‡´æ€§æ ¡éªŒ
-- 1. è¡Œæ•°æ ¡éªŒ
SELECT COUNT(*) FROM source_table;
SELECT COUNT(*) FROM target_table;

-- 2. æ ¡éªŒå’Œæ£€æŸ¥
SELECT SUM(CRC32(CONCAT_WS('', col1, col2, col3))) FROM source_table;
SELECT SUM(CRC32(CONCAT_WS('', col1, col2, col3))) FROM target_table;

-- 3. å¢é‡æ•°æ®æ ¡éªŒ
SELECT MAX(update_time) FROM source_table;
SELECT MAX(update_time) FROM target_table;
```

**ğŸ” æ•°æ®è´¨é‡ç›‘æ§**
```python
# æ•°æ®ä¸€è‡´æ€§ç›‘æ§è„šæœ¬
class DataConsistencyChecker:
    def __init__(self, source_db, target_db):
        self.source_db = source_db
        self.target_db = target_db
    
    def check_row_count(self, table_name):
        """æ£€æŸ¥è¡Œæ•°æ˜¯å¦ä¸€è‡´"""
        source_count = self.source_db.execute(
            f"SELECT COUNT(*) FROM {table_name}"
        ).fetchone()[0]
        
        target_count = self.target_db.execute(
            f"SELECT COUNT(*) FROM {table_name}"
        ).fetchone()[0]
        
        if source_count != target_count:
            self.alert(f"{table_name} è¡Œæ•°ä¸ä¸€è‡´: {source_count} vs {target_count}")
    
    def check_data_freshness(self, table_name):
        """æ£€æŸ¥æ•°æ®æ–°é²œåº¦"""
        max_time_source = self.source_db.execute(
            f"SELECT MAX(update_time) FROM {table_name}"
        ).fetchone()[0]
        
        max_time_target = self.target_db.execute(
            f"SELECT MAX(update_time) FROM {table_name}"
        ).fetchone()[0]
        
        delay = (max_time_source - max_time_target).total_seconds()
        if delay > 300:  # 5åˆ†é’Ÿå»¶è¿Ÿå‘Šè­¦
            self.alert(f"{table_name} æ•°æ®å»¶è¿Ÿ {delay} ç§’")
```

---

## 6. ğŸ”„ ETLæ•°æ®æŠ½å–åº”ç”¨


### 6.1 åŸºäºBINLOGçš„ETLæ¶æ„


**ğŸ­ ETLå¤„ç†æµç¨‹**
```
Extractï¼ˆæŠ½å–ï¼‰     Transformï¼ˆè½¬æ¢ï¼‰      Loadï¼ˆåŠ è½½ï¼‰
     â†“                   â†“                  â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ BINLOG  â”‚   â†’    â”‚ æ•°æ®æ¸…æ´— â”‚   â†’    â”‚ æ•°æ®ä»“åº“ â”‚
â”‚ å®æ—¶æµ   â”‚        â”‚ æ ¼å¼è½¬æ¢ â”‚        â”‚ æ‰¹é‡åŠ è½½ â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è¯¦ç»†æµç¨‹ï¼š
1. BINLOGè§£æ â†’ åŸå§‹å˜æ›´äº‹ä»¶
2. æ•°æ®è¿‡æ»¤ â†’ ç­›é€‰éœ€è¦çš„è¡¨å’Œå­—æ®µ  
3. æ•°æ®è½¬æ¢ â†’ ä¸šåŠ¡é€»è¾‘å¤„ç†
4. æ•°æ®èšåˆ â†’ æŒ‰æ—¶é—´çª—å£èšåˆ
5. æ•°æ®åŠ è½½ â†’ å†™å…¥ç›®æ ‡ç³»ç»Ÿ
```

**ğŸ“‹ ETLé…ç½®ç¤ºä¾‹**
```yaml
# ETLä»»åŠ¡é…ç½®
source:
  type: mysql-binlog
  host: mysql-master
  database: ecommerce
  tables: 
    - users
    - orders  
    - products

transform:
  filters:
    - table: users
      columns: [id, name, email, create_time]
      exclude_operations: [DELETE]
    
    - table: orders  
      columns: [id, user_id, amount, status, create_time]
      conditions: 
        - amount > 0
        - status != 'CANCELLED'

  aggregations:
    - name: user_order_stats
      group_by: [user_id, date(create_time)]
      metrics:
        - sum(amount) as total_amount
        - count(*) as order_count

target:
  type: clickhouse
  host: clickhouse-cluster
  database: analytics
  batch_size: 10000
  flush_interval: 60s
```

### 6.2 å®æ—¶ETLå¤„ç†


**âš¡ Flinkå®æ—¶ETLç¤ºä¾‹**
```java
public class BinlogETLJob {
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = 
            StreamExecutionEnvironment.getExecutionEnvironment();
        
        // 1. ä»Kafkaè¯»å–BINLOGæ•°æ®
        DataStream<String> binlogStream = env
            .addSource(new FlinkKafkaConsumer<>(
                "mysql-binlog", 
                new SimpleStringSchema(), 
                kafkaProps));
        
        // 2. è§£æBINLOGäº‹ä»¶
        DataStream<BinlogEvent> eventStream = binlogStream
            .map(new BinlogParser())
            .filter(event -> event.getTable().equals("orders"));
        
        // 3. æ•°æ®è½¬æ¢
        DataStream<OrderEvent> orderStream = eventStream
            .map(new OrderTransformer())
            .filter(order -> order.getAmount() > 0);
        
        // 4. æ—¶é—´çª—å£èšåˆ
        DataStream<OrderStats> statsStream = orderStream
            .keyBy(OrderEvent::getUserId)
            .window(TumblingProcessingTimeWindows.of(Time.minutes(5)))
            .aggregate(new OrderStatsAggregator());
        
        // 5. è¾“å‡ºåˆ°ClickHouse
        statsStream.addSink(new ClickHouseSink());
        
        env.execute("BINLOG ETL Job");
    }
}
```

### 6.3 æ•°æ®è´¨é‡ä¿éšœ


**ğŸ›¡ï¸ æ•°æ®è´¨é‡æ£€æŸ¥**
```python
class DataQualityChecker:
    def __init__(self):
        self.rules = []
    
    def add_rule(self, table, column, rule_type, params):
        """æ·»åŠ æ•°æ®è´¨é‡è§„åˆ™"""
        self.rules.append({
            'table': table,
            'column': column, 
            'type': rule_type,
            'params': params
        })
    
    def validate_record(self, record):
        """éªŒè¯å•æ¡è®°å½•"""
        for rule in self.rules:
            if record['table'] == rule['table']:
                value = record['data'].get(rule['column'])
                
                if rule['type'] == 'not_null' and value is None:
                    return False, f"{rule['column']} ä¸èƒ½ä¸ºç©º"
                
                if rule['type'] == 'range':
                    min_val, max_val = rule['params']
                    if value < min_val or value > max_val:
                        return False, f"{rule['column']} è¶…å‡ºèŒƒå›´"
                
                if rule['type'] == 'format':
                    pattern = rule['params']['pattern']
                    if not re.match(pattern, str(value)):
                        return False, f"{rule['column']} æ ¼å¼é”™è¯¯"
        
        return True, None

# ä½¿ç”¨ç¤ºä¾‹
checker = DataQualityChecker()
checker.add_rule('orders', 'amount', 'range', (0, 999999))
checker.add_rule('users', 'email', 'format', {'pattern': r'^[^@]+@[^@]+\.[^@]+$'})
```

---

## 7. ğŸŠ æ•°æ®æ¹–é›†æˆæ–¹æ¡ˆ


### 7.1 æ•°æ®æ¹–æ¶æ„è®¾è®¡


**ğŸŒŠ ç°ä»£æ•°æ®æ¹–æ¶æ„**
```
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   ä¸šåŠ¡ç³»ç»Ÿç¾¤    â”‚
                    â”‚ â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”  â”‚
                    â”‚ â”‚ç”µå•† â”‚ç¤¾äº¤ â”‚  â”‚
                    â”‚ â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚ BINLOG
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   å®æ—¶æ‘„å–å±‚    â”‚
                    â”‚ â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”  â”‚
                    â”‚ â”‚Canalâ”‚Debeziumâ”‚ â”‚
                    â”‚ â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   æµå¤„ç†å±‚      â”‚
                    â”‚ â”Œâ”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”  â”‚
                    â”‚ â”‚Kafkaâ”‚Kinesisâ”‚ â”‚
                    â”‚ â””â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”˜  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                    â”‚                    â”‚
        â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   åŸå§‹æ•°æ®å±‚  â”‚    â”‚   æ¸…æ´—æ•°æ®å±‚  â”‚    â”‚   åº”ç”¨æ•°æ®å±‚  â”‚
â”‚  (Raw Data)  â”‚    â”‚(Cleaned Data)â”‚    â”‚(Curated Data)â”‚
â”‚             â”‚    â”‚             â”‚    â”‚             â”‚
â”‚ â€¢ BINLOGåŸæ–‡ â”‚    â”‚ â€¢ ç»“æ„åŒ–æ•°æ® â”‚    â”‚ â€¢ ä¸šåŠ¡ä¸»é¢˜åº“ â”‚
â”‚ â€¢ JSONæ ¼å¼   â”‚    â”‚ â€¢ æ•°æ®è´¨é‡   â”‚    â”‚ â€¢ èšåˆæŒ‡æ ‡  â”‚
â”‚ â€¢ åˆ†åŒºå­˜å‚¨   â”‚    â”‚ â€¢ æ ‡å‡†æ ¼å¼   â”‚    â”‚ â€¢ æœºå™¨å­¦ä¹   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                    â”‚                    â”‚
        â–¼                    â–¼                    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 ç»Ÿä¸€å­˜å‚¨å±‚ (S3/HDFS/OSS)               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.2 BINLOGåˆ°æ•°æ®æ¹–çš„Pipeline


**ğŸš° æ•°æ®æµæ°´çº¿é…ç½®**
```yaml
# æ•°æ®æ¹–æ‘„å–é…ç½®
pipeline:
  name: mysql-to-datalake
  
  source:
    type: mysql-binlog
    connection:
      host: mysql-cluster
      port: 3306
      username: replication_user
    
    tables:
      - database: ecommerce
        tables: [users, orders, products, payments]
        
  processing:
    # åŸå§‹å±‚ï¼šä¿å­˜å®Œæ•´BINLOG
    raw_layer:
      format: json
      compression: gzip
      partition: [year, month, day, hour]
      path: s3://datalake/raw/mysql/{database}/{table}/
      
    # æ¸…æ´—å±‚ï¼šç»“æ„åŒ–æ•°æ®
    cleaned_layer:
      format: parquet
      schema_evolution: true
      partition: [date]
      path: s3://datalake/cleaned/mysql/{database}/{table}/
      
    # åº”ç”¨å±‚ï¼šä¸šåŠ¡ä¸»é¢˜
    curated_layer:
      format: delta
      optimization: true
      partition: [business_date]
      path: s3://datalake/curated/analytics/
      
  quality:
    rules:
      - check: not_null
        columns: [id, create_time]
      - check: data_type
        strict: true
      - check: business_rules
        custom: validate_order_amount
```

### 7.3 æ•°æ®ç‰ˆæœ¬ç®¡ç†


**ğŸ“š Delta Lakeé›†æˆ**
```python
from delta.tables import DeltaTable
from pyspark.sql import SparkSession

class BinlogToDelta:
    def __init__(self, spark_session):
        self.spark = spark_session
    
    def process_binlog_batch(self, binlog_events, table_path):
        """å¤„ç†BINLOGæ‰¹æ¬¡å¹¶å†™å…¥Deltaè¡¨"""
        
        # 1. è§£æBINLOGäº‹ä»¶
        df = self.spark.createDataFrame(binlog_events)
        
        # 2. æŒ‰æ“ä½œç±»å‹åˆ†ç»„å¤„ç†
        inserts = df.filter(df.operation == 'INSERT')
        updates = df.filter(df.operation == 'UPDATE') 
        deletes = df.filter(df.operation == 'DELETE')
        
        # 3. æ£€æŸ¥Deltaè¡¨æ˜¯å¦å­˜åœ¨
        if DeltaTable.isDeltaTable(self.spark, table_path):
            delta_table = DeltaTable.forPath(self.spark, table_path)
            
            # å¤„ç†æ›´æ–°
            if updates.count() > 0:
                delta_table.alias("target").merge(
                    updates.alias("source"),
                    "target.id = source.id"
                ).whenMatchedUpdateAll().execute()
            
            # å¤„ç†åˆ é™¤
            if deletes.count() > 0:
                delta_table.delete(
                    condition="id IN (SELECT id FROM deletes)"
                )
            
            # å¤„ç†æ’å…¥
            if inserts.count() > 0:
                inserts.write.format("delta").mode("append").save(table_path)
        else:
            # é¦–æ¬¡åˆ›å»ºè¡¨
            df.write.format("delta").save(table_path)
    
    def create_time_travel_view(self, table_path):
        """åˆ›å»ºæ—¶é—´æ—…è¡Œè§†å›¾"""
        self.spark.sql(f"""
            CREATE VIEW historical_data AS
            SELECT *, _commit_timestamp as version_time
            FROM delta.`{table_path}`
            VERSION AS OF 10  -- æŸ¥çœ‹ç¬¬10ä¸ªç‰ˆæœ¬
        """)
```

---

## 8. ğŸŒŠ æµå¼æ•°æ®å¤„ç†


### 8.1 å®æ—¶æµå¤„ç†æ¶æ„


**âš¡ æµå¤„ç†æŠ€æœ¯æ ˆ**
```
å®æ—¶æ•°æ®æµï¼š
MySQL BINLOG â†’ Canal â†’ Kafka â†’ Flink â†’ è¾“å‡ºç³»ç»Ÿ

å¤„ç†æ¨¡å¼ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   äº‹ä»¶é©±åŠ¨   â”‚    â”‚   æµå¼è®¡ç®—   â”‚    â”‚   ç»“æœè¾“å‡º   â”‚
â”‚             â”‚    â”‚             â”‚    â”‚             â”‚
â”‚ â€¢ æ•°æ®å˜æ›´   â”‚ â†’  â”‚ â€¢ å®æ—¶èšåˆ   â”‚ â†’  â”‚ â€¢ å®æ—¶ä»ªè¡¨ç›˜ â”‚
â”‚ â€¢ äº‹ä»¶æ—¶é—´   â”‚    â”‚ â€¢ çª—å£è®¡ç®—   â”‚    â”‚ â€¢ å‘Šè­¦ç³»ç»Ÿ   â”‚
â”‚ â€¢ æ°´ä½çº¿     â”‚    â”‚ â€¢ çŠ¶æ€ç®¡ç†   â”‚    â”‚ â€¢ ä¸‹æ¸¸ç³»ç»Ÿ   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ğŸ”§ Flinkæµå¤„ç†ä½œä¸š**
```java
public class BinlogStreamProcessor {
    
    public static void main(String[] args) throws Exception {
        StreamExecutionEnvironment env = 
            StreamExecutionEnvironment.getExecutionEnvironment();
        
        // å¯ç”¨Checkpoint
        env.enableCheckpointing(60000); // 1åˆ†é’Ÿ
        env.getCheckpointConfig().setCheckpointingMode(
            CheckpointingMode.EXACTLY_ONCE);
        
        // 1. è¯»å–BINLOGæµ
        DataStream<BinlogEvent> binlogStream = env
            .addSource(new BinlogKafkaSource())
            .assignTimestampsAndWatermarks(
                WatermarkStrategy.<BinlogEvent>forBoundedOutOfOrderness(
                    Duration.ofSeconds(10))
                .withTimestampAssigner((event, timestamp) -> 
                    event.getTimestamp()));
        
        // 2. è¿‡æ»¤è®¢å•äº‹ä»¶
        DataStream<OrderEvent> orderStream = binlogStream
            .filter(event -> "orders".equals(event.getTable()))
            .map(new OrderEventMapper());
        
        // 3. å®æ—¶èšåˆï¼šæ¯åˆ†é’Ÿè®¢å•ç»Ÿè®¡
        DataStream<OrderStats> minuteStats = orderStream
            .keyBy(OrderEvent::getShopId)
            .window(TumblingEventTimeWindows.of(Time.minutes(1)))
            .aggregate(new OrderStatsAggregator());
        
        // 4. å®æ—¶å‘Šè­¦ï¼šè®¢å•é‡å¼‚å¸¸æ£€æµ‹  
        DataStream<Alert> alerts = minuteStats
            .keyBy(OrderStats::getShopId)
            .process(new AnomalyDetector());
        
        // 5. è¾“å‡ºç»“æœ
        minuteStats.addSink(new ClickHouseSink());
        alerts.addSink(new AlertingSink());
        
        env.execute("BINLOG Stream Processing");
    }
}

// å¼‚å¸¸æ£€æµ‹å™¨
public class AnomalyDetector extends KeyedProcessFunction<String, OrderStats, Alert> {
    
    private ValueState<Double> avgOrderCount;
    
    @Override
    public void processElement(OrderStats stats, Context ctx, Collector<Alert> out) {
        double currentCount = stats.getOrderCount();
        double avgCount = avgOrderCount.value() != null ? avgOrderCount.value() : currentCount;
        
        // æ›´æ–°å¹³å‡å€¼
        double newAvg = avgCount * 0.9 + currentCount * 0.1;
        avgOrderCount.update(newAvg);
        
        // å¼‚å¸¸æ£€æµ‹ï¼šè¶…è¿‡å¹³å‡å€¼2å€
        if (currentCount > avgCount * 2) {
            Alert alert = new Alert(
                stats.getShopId(),
                "è®¢å•é‡å¼‚å¸¸å¢é•¿",
                currentCount,
                avgCount,
                ctx.timestamp()
            );
            out.collect(alert);
        }
    }
}
```

### 8.2 çŠ¶æ€ç®¡ç†ä¸å®¹é”™


**ğŸ’¾ çŠ¶æ€åç«¯é…ç½®**
```java
// RocksDBçŠ¶æ€åç«¯ï¼ˆæ¨èç”Ÿäº§ç¯å¢ƒï¼‰
env.setStateBackend(new RocksDBStateBackend(
    "hdfs://namenode:9000/flink/checkpoints", true));

// çŠ¶æ€TTLé…ç½®
StateTtlConfig ttlConfig = StateTtlConfig
    .newBuilder(Time.hours(24))  // 24å°æ—¶è¿‡æœŸ
    .setUpdateType(StateTtlConfig.UpdateType.OnCreateAndWrite)
    .setStateVisibility(StateTtlConfig.StateVisibility.NeverReturnExpired)
    .build();

ValueStateDescriptor<Double> descriptor = 
    new ValueStateDescriptor<>("avg-count", Double.class);
descriptor.enableTimeToLive(ttlConfig);
```

**ğŸ”„ å®¹é”™ä¸æ¢å¤**
```
æ•…éšœæ¢å¤æœºåˆ¶ï¼š
1. Checkpointæœºåˆ¶ï¼š
   â€¢ æ¯åˆ†é’Ÿä¿å­˜çŠ¶æ€å¿«ç…§
   â€¢ åˆ†å¸ƒå¼å¿«ç…§ç®—æ³•
   â€¢ ç²¾ç¡®ä¸€æ¬¡è¯­ä¹‰ä¿è¯

2. é‡å¯ç­–ç•¥ï¼š
   â€¢ å›ºå®šå»¶è¿Ÿé‡å¯
   â€¢ å¤±è´¥ç‡é‡å¯
   â€¢ æ— é‡å¯ï¼ˆç”Ÿäº§è°ƒè¯•ï¼‰

3. çŠ¶æ€æ¢å¤ï¼š
   â€¢ ä»æœ€è¿‘çš„Checkpointæ¢å¤
   â€¢ ä¿æŒå¤„ç†çš„ä¸€è‡´æ€§
   â€¢ è‡ªåŠ¨å¤„ç†é‡å¤æ•°æ®

é…ç½®ç¤ºä¾‹ï¼š
env.setRestartStrategy(RestartStrategies.failureRateRestart(
    3,                        // æ¯ä¸ªæ—¶é—´é—´éš”çš„æœ€å¤§å¤±è´¥æ¬¡æ•°
    Time.of(5, TimeUnit.MINUTES), // å¤±è´¥ç‡æµ‹é‡çš„æ—¶é—´é—´éš”
    Time.of(10, TimeUnit.SECONDS) // é‡å¯å»¶è¿Ÿ
));
```

---

## 9. ğŸ“Š CDCå˜æ›´æ•°æ®æ•è·


### 9.1 CDCæ ¸å¿ƒæ¦‚å¿µ


**ğŸ” ä»€ä¹ˆæ˜¯CDC**
```
CDCï¼ˆChange Data Captureï¼‰= å˜æ›´æ•°æ®æ•è·

ç®€å•ç†è§£ï¼š
æƒ³è±¡ä¸€ä¸ªå›¾ä¹¦é¦†ï¼š
â€¢ ä¼ ç»Ÿæ–¹å¼ï¼šæ¯å¤©æ™šä¸Šæ¸…ç‚¹æ‰€æœ‰ä¹¦ç±ï¼ˆå…¨é‡åŒæ­¥ï¼‰
â€¢ CDCæ–¹å¼ï¼šåªè®°å½•ä»Šå¤©å€Ÿå‡ºå’Œå½’è¿˜çš„ä¹¦ç±ï¼ˆå¢é‡åŒæ­¥ï¼‰

æŠ€æœ¯ä¼˜åŠ¿ï¼š
âœ… å®æ—¶æ€§ï¼šæ¯«ç§’çº§æ•°æ®åŒæ­¥
âœ… æ•ˆç‡é«˜ï¼šåªä¼ è¾“å˜æ›´æ•°æ®
âœ… å½±å“å°ï¼šå¯¹æºç³»ç»Ÿå½±å“æœ€å°
âœ… å‡†ç¡®æ€§ï¼šä¿è¯æ•°æ®ä¸€è‡´æ€§
```

**ğŸ’¡ CDCå·¥ä½œåŸç†**
```
æ•°æ®å˜æ›´æµç¨‹ï¼š
åº”ç”¨å†™å…¥ â†’ MySQL â†’ BINLOG â†’ CDCå·¥å…· â†’ ç›®æ ‡ç³»ç»Ÿ

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   æºæ•°æ®åº“   â”‚    â”‚  CDCå·¥å…·    â”‚    â”‚  ç›®æ ‡ç³»ç»Ÿ   â”‚
â”‚            â”‚    â”‚            â”‚    â”‚            â”‚
â”‚ INSERT      â”‚ â†’  â”‚ è§£æBINLOG  â”‚ â†’  â”‚ æ•°æ®ä»“åº“    â”‚
â”‚ UPDATE      â”‚    â”‚ æ ¼å¼è½¬æ¢    â”‚    â”‚ ç¼“å­˜ç³»ç»Ÿ    â”‚
â”‚ DELETE      â”‚    â”‚ è·¯ç”±åˆ†å‘    â”‚    â”‚ æœç´¢å¼•æ“    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 9.2 Debezium CDCå®ç°


**ğŸ› ï¸ Debeziumé…ç½®**
```json
{
  "name": "mysql-cdc-connector",
  "config": {
    "connector.class": "io.debezium.connector.mysql.MySqlConnector",
    "tasks.max": "1",
    "database.hostname": "mysql-master",
    "database.port": "3306",
    "database.user": "debezium_user",
    "database.password": "password",
    "database.server.id": "184054",
    "database.server.name": "ecommerce",
    
    "table.include.list": "ecommerce.users,ecommerce.orders,ecommerce.products",
    "database.include.list": "ecommerce",
    
    "database.history.kafka.bootstrap.servers": "kafka:9092",
    "database.history.kafka.topic": "schema-changes.ecommerce",
    
    "transforms": "route",
    "transforms.route.type": "org.apache.kafka.connect.transforms.RegexRouter",
    "transforms.route.regex": "([^.]+)\\.([^.]+)\\.([^.]+)",
    "transforms.route.replacement": "$3"
  }
}
```

**ğŸ“ CDCäº‹ä»¶æ ¼å¼**
```json
{
  "before": {
    "id": 1001,
    "name": "å¼ ä¸‰",
    "email": "zhangsan@example.com",
    "status": "ACTIVE",
    "create_time": 1640995200000
  },
  "after": {
    "id": 1001,
    "name": "å¼ ä¸‰",
    "email": "zhangsan@newmail.com",
    "status": "ACTIVE", 
    "update_time": 1640995800000
  },
  "source": {
    "version": "1.8.0.Final",
    "connector": "mysql",
    "name": "ecommerce",
    "ts_ms": 1640995800000,
    "snapshot": "false",
    "db": "ecommerce",
    "table": "users",
    "server_id": 184054,
    "gtid": null,
    "file": "mysql-bin.000003",
    "pos": 154,
    "row": 0
  },
  "op": "u",  // æ“ä½œç±»å‹ï¼šc=create, u=update, d=delete, r=read
  "ts_ms": 1640995800000,
  "transaction": null
}
```

### 9.3 CDCæ•°æ®å¤„ç†


**ğŸ”„ äº‹ä»¶å¤„ç†é€»è¾‘**
```python
class CDCEventProcessor:
    def __init__(self):
        self.handlers = {
            'c': self.handle_insert,
            'u': self.handle_update, 
            'd': self.handle_delete,
            'r': self.handle_read
        }
    
    def process_event(self, event):
        """å¤„ç†CDCäº‹ä»¶"""
        operation = event.get('op')
        table = event.get('source', {}).get('table')
        
        if operation in self.handlers:
            return self.handlers[operation](event, table)
        else:
            self.log_error(f"æœªçŸ¥æ“ä½œç±»å‹: {operation}")
    
    def handle_insert(self, event, table):
        """å¤„ç†æ’å…¥äº‹ä»¶"""
        after_data = event.get('after')
        
        if table == 'users':
            self.sync_user_to_cache(after_data)
            self.send_welcome_email(after_data['email'])
        elif table == 'orders':
            self.update_inventory(after_data)
            self.trigger_fulfillment(after_data)
    
    def handle_update(self, event, table):
        """å¤„ç†æ›´æ–°äº‹ä»¶"""
        before_data = event.get('before')
        after_data = event.get('after')
        
        # æ£€æŸ¥å“ªäº›å­—æ®µå‘ç”Ÿäº†å˜åŒ–
        changed_fields = self.find_changed_fields(before_data, after_data)
        
        if table == 'users' and 'email' in changed_fields:
            self.update_user_cache(after_data)
            self.send_email_change_notification(before_data['email'], after_data['email'])
    
    def handle_delete(self, event, table):
        """å¤„ç†åˆ é™¤äº‹ä»¶"""
        before_data = event.get('before')
        
        if table == 'users':
            self.remove_user_from_cache(before_data['id'])
            self.audit_user_deletion(before_data)
    
    def find_changed_fields(self, before, after):
        """æ‰¾å‡ºå˜æ›´çš„å­—æ®µ"""
        changed = []
        for key in after.keys():
            if before.get(key) != after.get(key):
                changed.append(key)
        return changed
```

---

## 10. ğŸš€ Kafkaé›†æˆåº”ç”¨


### 10.1 Kafkaé›†ç¾¤æ¶æ„


**ğŸ—ï¸ Kafkaé›†ç¾¤è®¾è®¡**
```
Kafkaé›†ç¾¤æ¶æ„ï¼š
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚              Kafka Cluster              â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚  â”‚ Broker1 â”‚ Broker2 â”‚   Broker3   â”‚   â”‚
    â”‚  â”‚         â”‚         â”‚             â”‚   â”‚
    â”‚  â”‚Topic: mysql-users   mysql-orders â”‚   â”‚
    â”‚  â”‚Part: 0,1,2         Part: 0,1,2  â”‚   â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†‘
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚     Canalç¾¤          â”‚
            â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
            â”‚ â”‚Canal-1  â”‚Canal-2  â”‚ â”‚
            â”‚ â”‚(users)  â”‚(orders) â”‚ â”‚
            â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â†‘
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚     MySQLç¾¤          â”‚
            â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
            â”‚ â”‚Master-1 â”‚Master-2 â”‚ â”‚
            â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 10.2 Topicè®¾è®¡ç­–ç•¥


**ğŸ“‹ Topicåˆ†åŒºç­–ç•¥**
```yaml
# Topicé…ç½®è§„åˆ’
topics:
  mysql-users:
    partitions: 12        # æŒ‰ç”¨æˆ·IDåˆ†åŒº
    replication_factor: 3
    key_strategy: user_id
    retention_hours: 168  # 7å¤©
    
  mysql-orders:
    partitions: 24        # æŒ‰è®¢å•IDåˆ†åŒº  
    replication_factor: 3
    key_strategy: order_id
    retention_hours: 720  # 30å¤©
    
  mysql-products:
    partitions: 6         # æŒ‰å•†å“åˆ†ç±»åˆ†åŒº
    replication_factor: 3
    key_strategy: category_id
    retention_hours: 8760 # 1å¹´

# åˆ†åŒºç­–ç•¥
partition_strategy:
  - table: users
    key: "user_id % 12"
    reason: ç”¨æˆ·æ“ä½œåˆ†å¸ƒå‡åŒ€
    
  - table: orders  
    key: "hash(order_id) % 24"
    reason: è®¢å•å†™å…¥é‡å¤§ï¼Œéœ€è¦æ›´å¤šåˆ†åŒº
    
  - table: products
    key: "category_id % 6" 
    reason: å•†å“æŒ‰ç±»åˆ«é›†ä¸­å¤„ç†
```

**âš™ï¸ Produceré…ç½®ä¼˜åŒ–**
```properties
# Canalåˆ°Kafkaçš„Produceré…ç½®
bootstrap.servers=kafka1:9092,kafka2:9092,kafka3:9092
acks=all                    # ç­‰å¾…æ‰€æœ‰å‰¯æœ¬ç¡®è®¤
retries=2147483647         # æœ€å¤§é‡è¯•æ¬¡æ•°
enable.idempotence=true    # å¯ç”¨å¹‚ç­‰æ€§
max.in.flight.requests.per.connection=5

# æ€§èƒ½è°ƒä¼˜
batch.size=65536           # 64KBæ‰¹æ¬¡å¤§å°
linger.ms=10              # 10msç­‰å¾…æ—¶é—´
compression.type=snappy   # å‹ç¼©ç®—æ³•
buffer.memory=134217728   # 128MBç¼“å†²åŒº

# åˆ†åŒºç­–ç•¥
partitioner.class=com.example.CustomPartitioner
```

### 10.3 Consumeræ¶ˆè´¹ä¼˜åŒ–


**ğŸ”§ Consumeré…ç½®**
```java
@Service
public class BinlogKafkaConsumer {
    
    @Value("${kafka.bootstrap-servers}")
    private String bootstrapServers;
    
    @PostConstruct
    public void startConsumers() {
        // ç”¨æˆ·æ•°æ®æ¶ˆè´¹è€…
        startUserConsumer();
        // è®¢å•æ•°æ®æ¶ˆè´¹è€…  
        startOrderConsumer();
    }
    
    private void startUserConsumer() {
        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        props.put(ConsumerConfig.GROUP_ID_CONFIG, "user-sync-service");
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, 
                 StringDeserializer.class);
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, 
                 JsonDeserializer.class);
        
        // æ¶ˆè´¹ä¼˜åŒ–é…ç½®
        props.put(ConsumerConfig.FETCH_MIN_BYTES_CONFIG, 50000);     // 50KB
        props.put(ConsumerConfig.FETCH_MAX_WAIT_MS_CONFIG, 500);     // 500ms
        props.put(ConsumerConfig.MAX_POLL_RECORDS_CONFIG, 500);      // 500æ¡/æ‰¹æ¬¡
        props.put(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG, false);  // æ‰‹åŠ¨æäº¤
        
        KafkaConsumer<String, UserEvent> consumer = new KafkaConsumer<>(props);
        consumer.subscribe(Arrays.asList("mysql-users"));
        
        // æ¶ˆè´¹å¤„ç†å¾ªç¯
        while (true) {
            ConsumerRecords<String, UserEvent> records = consumer.poll(Duration.ofMillis(100));
            
            if (!records.isEmpty()) {
                List<UserEvent> events = new ArrayList<>();
                for (ConsumerRecord<String, UserEvent> record : records) {
                    events.add(record.value());
                }
                
                // æ‰¹é‡å¤„ç†
                try {
                    userService.batchProcessUsers(events);
                    consumer.commitSync(); // åŒæ­¥æäº¤
                } catch (Exception e) {
                    log.error("å¤„ç†ç”¨æˆ·äº‹ä»¶å¤±è´¥", e);
                    // å¯ä»¥é€‰æ‹©é‡è¯•æˆ–è·³è¿‡
                }
            }
        }
    }
}
```

**ğŸ“Š æ¶ˆè´¹æ€§èƒ½ç›‘æ§**
```java
@Component
public class KafkaConsumerMetrics {
    
    private final MeterRegistry meterRegistry;
    
    @EventListener
    public void handleConsumerEvent(ConsumerEvent event) {
        // æ¶ˆè´¹å»¶è¿Ÿç›‘æ§
        Timer.Sample sample = Timer.start(meterRegistry);
        sample.stop(Timer.builder("kafka.consumer.latency")
                   .tag("topic", event.getTopic())
                   .register(meterRegistry));
        
        // æ¶ˆè´¹é‡ç›‘æ§
        Counter.builder("kafka.consumer.records")
               .tag("topic", event.getTopic())
               .register(meterRegistry)
               .increment(event.getRecordCount());
        
        // é”™è¯¯ç‡ç›‘æ§
        if (event.hasError()) {
            Counter.builder("kafka.consumer.errors")
                   .tag("topic", event.getTopic())
                   .tag("error", event.getErrorType())
                   .register(meterRegistry)
                   .increment();
        }
    }
}
```

---

## 11. ğŸ›ï¸ å¤§æ•°æ®æ¶æ„è®¾è®¡


### 11.1 Lambdaæ¶æ„è®¾è®¡


**ğŸ—ï¸ Lambdaæ¶æ„å›¾**
```
                        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                        â”‚   æ•°æ®æº (MySQL) â”‚
                        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â”‚ BINLOG
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â”‚   æ•°æ®æ‘„å–å±‚     â”‚
                         â”‚  (Canal/Kafka)  â”‚
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â”‚
                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                   â”‚               â”‚               â”‚
                   â–¼               â–¼               â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚   æ‰¹å¤„ç†å±‚   â”‚ â”‚  æµå¤„ç†å±‚    â”‚ â”‚   æœåŠ¡å±‚     â”‚
            â”‚ (Spark/Hive)â”‚ â”‚ (Flink/Storm)â”‚ â”‚(API/Query)  â”‚
            â”‚             â”‚ â”‚             â”‚ â”‚             â”‚
            â”‚â€¢ å†å²æ•°æ®åˆ†æâ”‚ â”‚â€¢ å®æ—¶è®¡ç®—    â”‚ â”‚â€¢ ç»Ÿä¸€æŸ¥è¯¢    â”‚
            â”‚â€¢ å¤æ‚ETL   â”‚ â”‚â€¢ æµå¼èšåˆ    â”‚ â”‚â€¢ ç»“æœåˆå¹¶    â”‚
            â”‚â€¢ æœºå™¨å­¦ä¹    â”‚ â”‚â€¢ å¼‚å¸¸æ£€æµ‹    â”‚ â”‚â€¢ ç”¨æˆ·æ¥å£    â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                   â”‚               â”‚               â”‚
                   â–¼               â–¼               â–¼
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚  æ‰¹è§†å›¾å­˜å‚¨  â”‚ â”‚ å®æ—¶è§†å›¾å­˜å‚¨ â”‚ â”‚  ç»Ÿä¸€æŸ¥è¯¢å±‚  â”‚
            â”‚(HDFS/Hive) â”‚ â”‚(Redis/ES)   â”‚ â”‚(GraphQL)    â”‚
            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ğŸ”§ æ¶æ„ç»„ä»¶é…ç½®**
```yaml
# Lambdaæ¶æ„é…ç½®
architecture:
  data_ingestion:
    tools: [Canal, Kafka, Flume]
    throughput: "1M events/sec"
    latency: "< 100ms"
    
  batch_layer:
    engine: Spark
    storage: HDFS
    schedule: "Every 1 hour"
    processing: 
      - ETL jobs
      - ML training
      - Complex analytics
      
  speed_layer:
    engine: Flink
    storage: Redis
    latency: "< 1 second"
    processing:
      - Real-time aggregation
      - Anomaly detection
      - Live dashboards
      
  serving_layer:
    query_engine: Presto
    cache: Redis
    apis: [REST, GraphQL]
    features:
      - Query federation
      - Result merging
      - Auto-refresh
```

### 11.2 Kappaæ¶æ„å®ç°


**âš¡ çº¯æµå¼æ¶æ„**
```
Kappaæ¶æ„ = åªæœ‰æµå¤„ç†ï¼Œæ²¡æœ‰æ‰¹å¤„ç†

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  å®æ—¶æ•°æ®æµ      â”‚
                    â”‚  (BINLOG)       â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   æµå¤„ç†å¼•æ“     â”‚
                    â”‚   (Flink)       â”‚
                    â”‚                 â”‚
                    â”‚ â€¢ å®æ—¶ETL       â”‚
                    â”‚ â€¢ çŠ¶æ€ç®¡ç†      â”‚
                    â”‚ â€¢ çª—å£è®¡ç®—      â”‚
                    â”‚ â€¢ å®¹é”™æ¢å¤      â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚             â”‚             â”‚
                â–¼             â–¼             â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   å®æ—¶å­˜å‚¨    â”‚ â”‚   å†å²å­˜å‚¨    â”‚ â”‚   æŸ¥è¯¢å¼•æ“   â”‚
        â”‚  (Kafka)     â”‚ â”‚ (ClickHouse) â”‚ â”‚  (Presto)   â”‚
        â”‚              â”‚ â”‚              â”‚ â”‚              â”‚
        â”‚â€¢ æ¯«ç§’çº§å“åº”   â”‚ â”‚â€¢ æµ·é‡å†å²æ•°æ® â”‚ â”‚â€¢ ç»Ÿä¸€SQLæŸ¥è¯¢ â”‚
        â”‚â€¢ æ»‘åŠ¨çª—å£    â”‚ â”‚â€¢ åˆ—å¼å­˜å‚¨     â”‚ â”‚â€¢ è”é‚¦æŸ¥è¯¢   â”‚
        â”‚â€¢ çŠ¶æ€æŸ¥è¯¢    â”‚ â”‚â€¢ å‹ç¼©ä¼˜åŒ–     â”‚ â”‚â€¢ ç¼“å­˜åŠ é€Ÿ   â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ä¼˜åŠ¿ï¼š
âœ… æ¶æ„ç®€å•ï¼šåªæœ‰ä¸€å¥—æµå¤„ç†ç³»ç»Ÿ
âœ… å»¶è¿Ÿæ›´ä½ï¼šç«¯åˆ°ç«¯æµå¼å¤„ç†
âœ… ç»´æŠ¤å®¹æ˜“ï¼šå‡å°‘ç³»ç»Ÿå¤æ‚åº¦
âœ… ä¸€è‡´æ€§å¥½ï¼šç»Ÿä¸€çš„å¤„ç†é€»è¾‘
```

### 11.3 æ··åˆæ¶æ„é€‰æ‹©


**ğŸ¯ æ¶æ„é€‰æ‹©æŒ‡å—**
```
Lambdaæ¶æ„é€‚ç”¨åœºæ™¯ï¼š
âœ… å†å²æ•°æ®åˆ†æéœ€æ±‚é‡
âœ… å¤æ‚çš„æ‰¹å¤„ç†ä½œä¸š
âœ… æœºå™¨å­¦ä¹ æ¨¡å‹è®­ç»ƒ
âœ… å¯¹ç²¾ç¡®æ€§è¦æ±‚æé«˜

ç¤ºä¾‹ï¼šé‡‘èé£æ§ç³»ç»Ÿ
- å®æ—¶åæ¬ºè¯ˆæ£€æµ‹ (æµå¤„ç†)
- å†å²äº¤æ˜“æ¨¡å¼åˆ†æ (æ‰¹å¤„ç†)  
- ä¿¡ç”¨è¯„åˆ†æ¨¡å‹è®­ç»ƒ (æ‰¹å¤„ç†)

Kappaæ¶æ„é€‚ç”¨åœºæ™¯ï¼š
âœ… å®æ—¶æ€§è¦æ±‚é«˜
âœ… æ•°æ®å¤„ç†é€»è¾‘ç›¸å¯¹ç®€å•
âœ… å¸Œæœ›é™ä½ç³»ç»Ÿå¤æ‚åº¦
âœ… æµå¼å¤„ç†èƒ½åŠ›è¶³å¤Ÿå¼º

ç¤ºä¾‹ï¼šå®æ—¶æ¨èç³»ç»Ÿ
- ç”¨æˆ·è¡Œä¸ºå®æ—¶åˆ†æ
- å•†å“çƒ­åº¦å®æ—¶è®¡ç®—
- ä¸ªæ€§åŒ–æ¨èå®æ—¶ç”Ÿæˆ

æ··åˆæ¶æ„é€‰æ‹©çŸ©é˜µï¼š
                    ç®€å•å¤„ç†    å¤æ‚å¤„ç†
å®æ—¶è¦æ±‚é«˜           Kappa      Lambda
å®æ—¶è¦æ±‚ä¸€èˆ¬         Kappa      Lambda
```

---

## 12. ğŸ“‹ æ ¸å¿ƒè¦ç‚¹æ€»ç»“


### 12.1 å¿…é¡»æŒæ¡çš„æ ¸å¿ƒæ¦‚å¿µ


```
ğŸ”¸ å¤§æ•°æ®BINLOGç‰¹å¾ï¼šTBçº§æ—¥å¿—ã€ä¸‡çº§å¹¶å‘ã€æ¯«ç§’å»¶è¿Ÿ
ğŸ”¸ ç®¡ç†ç­–ç•¥ï¼šæ–‡ä»¶åˆ†å‰²ã€è¿‡æœŸæ¸…ç†ã€åˆ†åŒºå­˜å‚¨
ğŸ”¸ æ€§èƒ½ä¼˜åŒ–ï¼šåŒæ­¥å‚æ•°ã€åˆ†åº“åˆ†è¡¨ã€æ‰¹é‡æ“ä½œ
ğŸ”¸ åˆ†å¸ƒå¼æ¶æ„ï¼šæ°´å¹³æ‰©å±•ã€é«˜å¯ç”¨ã€æ•…éšœè½¬ç§»
ğŸ”¸ å®æ—¶åŒæ­¥ï¼šå‡†å®æ—¶çº§ã€ä¸€è‡´æ€§ä¿éšœã€è´¨é‡ç›‘æ§
ğŸ”¸ ETLåº”ç”¨ï¼šæµå¼å¤„ç†ã€æ•°æ®è½¬æ¢ã€è´¨é‡æ§åˆ¶
ğŸ”¸ æ•°æ®æ¹–é›†æˆï¼šåˆ†å±‚å­˜å‚¨ã€ç‰ˆæœ¬ç®¡ç†ã€Pipeline
ğŸ”¸ æµå¤„ç†ï¼šäº‹ä»¶é©±åŠ¨ã€çŠ¶æ€ç®¡ç†ã€å®¹é”™æœºåˆ¶
ğŸ”¸ CDCæ•è·ï¼šå˜æ›´è¯†åˆ«ã€æ ¼å¼è½¬æ¢ã€äº‹ä»¶è·¯ç”±
ğŸ”¸ Kafkaé›†æˆï¼šTopicè®¾è®¡ã€æ€§èƒ½ä¼˜åŒ–ã€ç›‘æ§å‘Šè­¦
ğŸ”¸ æ¶æ„è®¾è®¡ï¼šLambda/Kappaã€æŠ€æœ¯é€‰å‹ã€æ‰©å±•æ€§
```

### 12.2 å…³é”®ç†è§£è¦ç‚¹


**ğŸ”¹ å¤§æ•°æ®ç¯å¢ƒçš„æ ¸å¿ƒæŒ‘æˆ˜**
```
å­˜å‚¨æŒ‘æˆ˜ï¼š
â€¢ TBçº§BINLOGå¿«é€Ÿå¢é•¿
â€¢ æˆæœ¬æ§åˆ¶ä¸æ€§èƒ½å¹³è¡¡
â€¢ åˆ†å±‚å­˜å‚¨ç­–ç•¥

æ€§èƒ½æŒ‘æˆ˜ï¼š
â€¢ é«˜å¹¶å‘å†™å…¥ä¼˜åŒ–
â€¢ ç½‘ç»œä¼ è¾“ç“¶é¢ˆ
â€¢ ä¸‹æ¸¸å¤„ç†èƒ½åŠ›

ä¸€è‡´æ€§æŒ‘æˆ˜ï¼š
â€¢ åˆ†å¸ƒå¼æ•°æ®ä¸€è‡´æ€§
â€¢ æ•…éšœæ¢å¤æœºåˆ¶
â€¢ æ•°æ®è´¨é‡ä¿éšœ
```

**ğŸ”¹ æŠ€æœ¯é€‰å‹åŸåˆ™**
```
å®æ—¶æ€§è¦æ±‚ï¼š
æ¯«ç§’çº§ â†’ Kafka + Flink
ç§’çº§ â†’ Kafka + Storm  
åˆ†é’Ÿçº§ â†’ Kafka + Spark Streaming

æ•°æ®é‡çº§ï¼š
GBçº§ â†’ å•æœºå¤„ç†
TBçº§ â†’ é›†ç¾¤å¤„ç†
PBçº§ â†’ åˆ†å¸ƒå¼å­˜å‚¨

ä¸šåŠ¡å¤æ‚åº¦ï¼š
ç®€å•ETL â†’ Canal + Kafka
å¤æ‚åˆ†æ â†’ Flink + ClickHouse
æœºå™¨å­¦ä¹  â†’ Spark + HDFS
```

**ğŸ”¹ æ¶æ„æ¼”è¿›è·¯å¾„**
```
åˆçº§é˜¶æ®µï¼š
MySQL â†’ Canal â†’ Kafka â†’ å•ä¸€æ¶ˆè´¹è€…
é€‚ç”¨ï¼šå°å›¢é˜Ÿã€ç®€å•éœ€æ±‚

ä¸­çº§é˜¶æ®µï¼š  
MySQL â†’ Canalé›†ç¾¤ â†’ Kafkaé›†ç¾¤ â†’ å¤šæ¶ˆè´¹è€…
é€‚ç”¨ï¼šä¸­ç­‰è§„æ¨¡ã€å¤šä¸šåŠ¡çº¿

é«˜çº§é˜¶æ®µï¼š
MySQL â†’ åˆ†å¸ƒå¼CDC â†’ æµå¤„ç†é›†ç¾¤ â†’ æ•°æ®æ¹–
é€‚ç”¨ï¼šå¤§è§„æ¨¡ã€å¤æ‚ä¸šåŠ¡
```

### 12.3 å®é™…åº”ç”¨ä»·å€¼


**ğŸ’¼ ä¸šåŠ¡åº”ç”¨åœºæ™¯**
- **ç”µå•†å¹³å°**ï¼šå®æ—¶ç”¨æˆ·ç”»åƒã€åº“å­˜åŒæ­¥ã€è®¢å•è·Ÿè¸ª
- **é‡‘èç³»ç»Ÿ**ï¼šå®æ—¶é£æ§ã€äº¤æ˜“ç›‘æ§ã€åˆè§„å®¡è®¡
- **ç¤¾äº¤åº”ç”¨**ï¼šåŠ¨æ€æ›´æ–°ã€æ¶ˆæ¯æ¨é€ã€ç”¨æˆ·åˆ†æ
- **IoTå¹³å°**ï¼šè®¾å¤‡çŠ¶æ€åŒæ­¥ã€å¼‚å¸¸æ£€æµ‹ã€æ•°æ®åˆ†æ

**ğŸ”§ æŠ€æœ¯å®è·µæŒ‡å¯¼**
- **å®¹é‡è§„åˆ’**ï¼šæŒ‰å³°å€¼çš„3å€è§„åˆ’èµ„æº
- **ç›‘æ§å‘Šè­¦**ï¼šå»¶è¿Ÿã€ååé‡ã€é”™è¯¯ç‡å…¨è¦†ç›–
- **æ•…éšœæ¼”ç»ƒ**ï¼šå®šæœŸè¿›è¡Œç¾éš¾æ¢å¤æµ‹è¯•
- **æ€§èƒ½è°ƒä¼˜**ï¼šåŸºäºå®é™…è´Ÿè½½æŒç»­ä¼˜åŒ–

**ğŸ“ˆ å‘å±•è¶‹åŠ¿**
- **äº‘åŸç”ŸåŒ–**ï¼šKubernetes + å®¹å™¨åŒ–éƒ¨ç½²
- **Serverless**ï¼šå‡½æ•°è®¡ç®—å¤„ç†BINLOGäº‹ä»¶
- **AIå¢å¼º**ï¼šæ™ºèƒ½å¼‚å¸¸æ£€æµ‹ã€è‡ªåŠ¨æ‰©ç¼©å®¹
- **è¾¹ç¼˜è®¡ç®—**ï¼šå°±è¿‘å¤„ç†ã€é™ä½å»¶è¿Ÿ

**æ ¸å¿ƒè®°å¿†**ï¼š
- å¤§æ•°æ®BINLOGå¤„ç†çš„æ ¸å¿ƒæ˜¯å¹³è¡¡æ€§èƒ½ã€ä¸€è‡´æ€§å’Œæˆæœ¬
- åˆ†å¸ƒå¼æ¶æ„æ˜¯åº”å¯¹å¤§æ•°æ®é‡çš„å¿…ç„¶é€‰æ‹©
- æµå¼å¤„ç†æ˜¯å®ç°å®æ—¶æ€§çš„å…³é”®æŠ€æœ¯
- ç›‘æ§å’Œè¿ç»´æ˜¯ä¿éšœç³»ç»Ÿç¨³å®šçš„é‡è¦ç¯èŠ‚