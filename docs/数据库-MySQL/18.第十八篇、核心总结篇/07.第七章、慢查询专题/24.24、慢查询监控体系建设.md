---
title: 24ã€æ…¢æŸ¥è¯¢ç›‘æ§ä½“ç³»å»ºè®¾
---
## ğŸ“š ç›®å½•

1. [æ…¢æŸ¥è¯¢ç›‘æ§ä½“ç³»æ¦‚è¿°](#1-æ…¢æŸ¥è¯¢ç›‘æ§ä½“ç³»æ¦‚è¿°)
2. [ç›‘æ§æŒ‡æ ‡å®šä¹‰ä¸æ ‡å‡†](#2-ç›‘æ§æŒ‡æ ‡å®šä¹‰ä¸æ ‡å‡†)
3. [å®æ—¶ç›‘æ§ç³»ç»Ÿæ­å»º](#3-å®æ—¶ç›‘æ§ç³»ç»Ÿæ­å»º)
4. [ç›‘æ§æ•°æ®é‡‡é›†æ–¹æ³•](#4-ç›‘æ§æ•°æ®é‡‡é›†æ–¹æ³•)
5. [å‘Šè­¦é˜ˆå€¼è®¾ç½®ç­–ç•¥](#5-å‘Šè­¦é˜ˆå€¼è®¾ç½®ç­–ç•¥)
6. [ç›‘æ§Dashboardè®¾è®¡](#6-ç›‘æ§dashboardè®¾è®¡)
7. [å†å²æ•°æ®è¶‹åŠ¿åˆ†æ](#7-å†å²æ•°æ®è¶‹åŠ¿åˆ†æ)
8. [ç›‘æ§æ•°æ®å­˜å‚¨æ–¹æ¡ˆ](#8-ç›‘æ§æ•°æ®å­˜å‚¨æ–¹æ¡ˆ)
9. [å¤šå®ä¾‹ç›‘æ§ç®¡ç†](#9-å¤šå®ä¾‹ç›‘æ§ç®¡ç†)
10. [ç›‘æ§ç³»ç»Ÿé«˜å¯ç”¨](#10-ç›‘æ§ç³»ç»Ÿé«˜å¯ç”¨)
11. [æ ¸å¿ƒè¦ç‚¹æ€»ç»“](#11-æ ¸å¿ƒè¦ç‚¹æ€»ç»“)

---

## 1. ğŸ“Š æ…¢æŸ¥è¯¢ç›‘æ§ä½“ç³»æ¦‚è¿°


### 1.1 ä»€ä¹ˆæ˜¯æ…¢æŸ¥è¯¢ç›‘æ§ä½“ç³»


> ğŸ’¡ **é€šä¿—ç†è§£**  
> æ…¢æŸ¥è¯¢ç›‘æ§ä½“ç³»å°±åƒæ˜¯ç»™æ•°æ®åº“å®‰è£…çš„"å¥åº·ç›‘æ§è®¾å¤‡"ï¼Œå®æ—¶ç›‘æµ‹æ•°æ®åº“çš„æŸ¥è¯¢æ€§èƒ½ï¼ŒåŠæ—¶å‘ç°æ€§èƒ½é—®é¢˜å¹¶é¢„è­¦

**ğŸ”¸ ç›‘æ§ä½“ç³»çš„æœ¬è´¨**
```
æ…¢æŸ¥è¯¢ç›‘æ§ = æ•°æ®æ”¶é›† + å®æ—¶åˆ†æ + æ™ºèƒ½å‘Šè­¦ + å¯è§†åŒ–å±•ç¤º

æ ¸å¿ƒä½œç”¨ï¼š
â€¢ å‘ç°é—®é¢˜ï¼šåŠæ—¶å‘ç°æ…¢æŸ¥è¯¢å’Œæ€§èƒ½ç“¶é¢ˆ
â€¢ é¢„é˜²æ•…éšœï¼šåœ¨é—®é¢˜æ¶åŒ–å‰æå‰é¢„è­¦
â€¢ åˆ†æè¶‹åŠ¿ï¼šé€šè¿‡å†å²æ•°æ®åˆ†ææ€§èƒ½å˜åŒ–è¶‹åŠ¿
â€¢ è¾…åŠ©ä¼˜åŒ–ï¼šä¸ºæ€§èƒ½ä¼˜åŒ–æä¾›æ•°æ®æ”¯æŒ
```

### 1.2 ä¸ºä»€ä¹ˆéœ€è¦ç›‘æ§ä½“ç³»


**ğŸ¯ ä¸šåŠ¡ä»·å€¼**
```
ç”¨æˆ·ä½“éªŒè§’åº¦ï¼š
â€¢ é¡µé¢å“åº”æ…¢ â†’ ç”¨æˆ·ä½“éªŒå·® â†’ ä¸šåŠ¡æµå¤±
â€¢ ç›‘æ§ä½“ç³»èƒ½æå‰å‘ç°å¹¶è§£å†³æ€§èƒ½é—®é¢˜

ä¸šåŠ¡ç¨³å®šæ€§è§’åº¦ï¼š
â€¢ æ…¢æŸ¥è¯¢ç§¯ç´¯ â†’ æ•°æ®åº“å‹åŠ› â†’ ç³»ç»Ÿå´©æºƒ
â€¢ åŠæ—¶ç›‘æ§èƒ½é¿å…ç³»ç»Ÿé›ªå´©

è¿ç»´æ•ˆç‡è§’åº¦ï¼š
â€¢ è¢«åŠ¨å¤„ç† â†’ ä¸»åŠ¨é¢„é˜²
â€¢ è‡ªåŠ¨åŒ–ç›‘æ§å‡å°‘äººå·¥æ’æŸ¥æ—¶é—´
```

### 1.3 ç›‘æ§ä½“ç³»æ¶æ„æ¦‚è§ˆ


```
æ•°æ®å±‚                ç›‘æ§å±‚              å±•ç¤ºå±‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MySQLå®ä¾‹  â”‚ â”€â”€â”€â–º â”‚  æ•°æ®é‡‡é›†   â”‚â”€â”€â”€â–ºâ”‚  Dashboard  â”‚
â”‚   æ…¢æŸ¥è¯¢    â”‚      â”‚  å®æ—¶åˆ†æ   â”‚    â”‚   å›¾è¡¨å±•ç¤º  â”‚
â”‚   æ€§èƒ½æŒ‡æ ‡  â”‚      â”‚  å‘Šè­¦åˆ¤æ–­   â”‚    â”‚   è¶‹åŠ¿åˆ†æ  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                     â”‚  å‘Šè­¦é€šçŸ¥   â”‚
                     â”‚  é‚®ä»¶/çŸ­ä¿¡  â”‚
                     â”‚  é’‰é’‰/å¾®ä¿¡  â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. ğŸ“ ç›‘æ§æŒ‡æ ‡å®šä¹‰ä¸æ ‡å‡†


### 2.1 æ ¸å¿ƒç›‘æ§æŒ‡æ ‡ä½“ç³»


**ğŸ”¸ åŸºç¡€æ€§èƒ½æŒ‡æ ‡**
```
æŸ¥è¯¢æ‰§è¡Œæ—¶é—´ï¼ˆQuery Timeï¼‰ï¼š
â€¢ å®šä¹‰ï¼šSQLè¯­å¥ä»å¼€å§‹æ‰§è¡Œåˆ°è¿”å›ç»“æœçš„æ€»æ—¶é—´
â€¢ å•ä½ï¼šç§’ï¼ˆsï¼‰æˆ–æ¯«ç§’ï¼ˆmsï¼‰
â€¢ é‡è¦æ€§ï¼šæœ€ç›´è§‚çš„æ€§èƒ½æŒ‡æ ‡

é”ç­‰å¾…æ—¶é—´ï¼ˆLock Timeï¼‰ï¼š
â€¢ å®šä¹‰ï¼šæŸ¥è¯¢ç­‰å¾…è·å–é”çš„æ—¶é—´
â€¢ æ„ä¹‰ï¼šåæ˜ å¹¶å‘å†²çªä¸¥é‡ç¨‹åº¦
â€¢ é«˜å€¼è­¦ç¤ºï¼šå¯èƒ½å­˜åœ¨é”ç«äº‰é—®é¢˜

æ‰«æè¡Œæ•°ï¼ˆRows Examinedï¼‰ï¼š
â€¢ å®šä¹‰ï¼šæ‰§è¡ŒæŸ¥è¯¢æ—¶æ£€æŸ¥çš„æ•°æ®è¡Œæ•°
â€¢ ä½œç”¨ï¼šè¯„ä¼°æŸ¥è¯¢æ•ˆç‡
â€¢ ä¼˜åŒ–ç›®æ ‡ï¼šå‡å°‘ä¸å¿…è¦çš„è¡Œæ‰«æ
```

**ğŸ“Š ç›‘æ§æŒ‡æ ‡å¯¹æ¯”è¡¨**

| æŒ‡æ ‡ç±»å‹ | **æŒ‡æ ‡åç§°** | **æ­£å¸¸èŒƒå›´** | **å‘Šè­¦é˜ˆå€¼** | **ä¸¥é‡é˜ˆå€¼** |
|---------|-------------|-------------|-------------|-------------|
| ğŸ• **æ‰§è¡Œæ—¶é—´** | `Query_time` | `< 1s` | `> 3s` | `> 10s` |
| ğŸ”’ **é”ç­‰å¾…** | `Lock_time` | `< 0.1s` | `> 1s` | `> 5s` |
| ğŸ“Š **æ‰«æè¡Œæ•°** | `Rows_examined` | `< 1000` | `> 10000` | `> 100000` |
| ğŸ“¤ **è¿”å›è¡Œæ•°** | `Rows_sent` | `< 100` | `> 1000` | `> 10000` |
| ğŸ“ˆ **QPS** | `Queries/sec` | `< 1000` | `> 2000` | `> 5000` |

### 2.2 è‡ªå®šä¹‰ç›‘æ§æ ‡å‡†


**ğŸ¯ ä¸šåŠ¡ç›¸å…³æŒ‡æ ‡**
```java
// è‡ªå®šä¹‰æ…¢æŸ¥è¯¢æ ‡å‡†
public class SlowQueryStandard {
    // åŸºç¡€æ ‡å‡†
    private static final long SLOW_QUERY_TIME = 2000; // 2ç§’
    
    // ä¸šåŠ¡çº§åˆ«æ ‡å‡†
    private static final Map<String, Long> BUSINESS_STANDARDS = Map.of(
        "order_query", 1000L,    // è®¢å•æŸ¥è¯¢ï¼š1ç§’
        "user_login", 500L,      // ç”¨æˆ·ç™»å½•ï¼š0.5ç§’
        "product_search", 2000L, // å•†å“æœç´¢ï¼š2ç§’
        "report_generate", 10000L // æŠ¥è¡¨ç”Ÿæˆï¼š10ç§’
    );
    
    // æ ¹æ®ä¸šåŠ¡ç±»å‹åˆ¤æ–­æ˜¯å¦ä¸ºæ…¢æŸ¥è¯¢
    public boolean isSlowQuery(String queryType, long executeTime) {
        long threshold = BUSINESS_STANDARDS.getOrDefault(
            queryType, SLOW_QUERY_TIME
        );
        return executeTime > threshold;
    }
}
```

### 2.3 æŒ‡æ ‡æƒé‡ä¸è¯„åˆ†


**ğŸ“Š æ€§èƒ½è¯„åˆ†ä½“ç³»**
```
ç»¼åˆæ€§èƒ½å¾—åˆ† = æ‰§è¡Œæ—¶é—´æƒé‡ Ã— æ—¶é—´å¾—åˆ† 
             + èµ„æºæ¶ˆè€—æƒé‡ Ã— èµ„æºå¾—åˆ†
             + å¹¶å‘å½±å“æƒé‡ Ã— å¹¶å‘å¾—åˆ†

æƒé‡åˆ†é…ï¼š
â€¢ æ‰§è¡Œæ—¶é—´ï¼š40%ï¼ˆæœ€ç›´è§‚çš„ç”¨æˆ·ä½“éªŒï¼‰
â€¢ èµ„æºæ¶ˆè€—ï¼š35%ï¼ˆCPUã€å†…å­˜ã€IOä½¿ç”¨ï¼‰
â€¢ å¹¶å‘å½±å“ï¼š25%ï¼ˆå¯¹å…¶ä»–æŸ¥è¯¢çš„å½±å“ï¼‰

è¯„åˆ†æ ‡å‡†ï¼š
â€¢ 90-100åˆ†ï¼šä¼˜ç§€ï¼Œæ— éœ€ä¼˜åŒ–
â€¢ 70-89åˆ†ï¼šè‰¯å¥½ï¼Œå»ºè®®è§‚å¯Ÿ
â€¢ 50-69åˆ†ï¼šä¸€èˆ¬ï¼Œéœ€è¦å…³æ³¨
â€¢ 30-49åˆ†ï¼šè¾ƒå·®ï¼Œå»ºè®®ä¼˜åŒ–
â€¢ < 30åˆ†ï¼šä¸¥é‡ï¼Œç«‹å³ä¼˜åŒ–
```

---

## 3. âš¡ å®æ—¶ç›‘æ§ç³»ç»Ÿæ­å»º


### 3.1 ç›‘æ§æ¶æ„é€‰å‹


**ğŸ—ï¸ å¸¸è§ç›‘æ§æ¶æ„**
```
Agenté‡‡é›†æ¨¡å¼ï¼š
MySQL â† Agentæ”¶é›† â†’ ç›‘æ§ä¸­å¿ƒ â†’ Dashboard
ä¼˜ç‚¹ï¼šéƒ¨ç½²ç®€å•ï¼Œä¾µå…¥æ€§å°
ç¼ºç‚¹ï¼šéœ€è¦é¢å¤–éƒ¨ç½²Agent

æ—¥å¿—åˆ†ææ¨¡å¼ï¼š
MySQL â†’ æ…¢æŸ¥è¯¢æ—¥å¿— â†’ æ—¥å¿—åˆ†æå™¨ â†’ ç›‘æ§ä¸­å¿ƒ
ä¼˜ç‚¹ï¼šåˆ©ç”¨ç°æœ‰æ—¥å¿—ï¼Œæ— éœ€é¢å¤–é…ç½®
ç¼ºç‚¹ï¼šä¸å¤Ÿå®æ—¶ï¼Œæ—¥å¿—è§£æå¤æ‚

Hookæ‹¦æˆªæ¨¡å¼ï¼š
åº”ç”¨ â†’ æ•°æ®åº“ä¸­é—´ä»¶ â†’ MySQL
ä¼˜ç‚¹ï¼šå¯ä»¥æ‹¦æˆªæ‰€æœ‰SQLï¼Œç›‘æ§å…¨é¢
ç¼ºç‚¹ï¼šéœ€è¦æ”¹é€ åº”ç”¨æ¶æ„
```

### 3.2 åŸºäºPrometheusçš„ç›‘æ§æ­å»º


**ğŸ”§ æ ¸å¿ƒç»„ä»¶é…ç½®**
```yaml
# prometheus.yml é…ç½®
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "mysql_slow_query_rules.yml"

scrape_configs:
  # MySQLæ…¢æŸ¥è¯¢ç›‘æ§
  - job_name: 'mysql-slow-query'
    static_configs:
      - targets: ['mysql-exporter:9104']
    scrape_interval: 10s
    metrics_path: /metrics
    
alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093
```

**ğŸš€ æ…¢æŸ¥è¯¢æŒ‡æ ‡é‡‡é›†å™¨**
```python
# ç®€åŒ–çš„æ…¢æŸ¥è¯¢é‡‡é›†å™¨ç¤ºä¾‹
import time
import mysql.connector
from prometheus_client import Counter, Histogram, Gauge

class SlowQueryCollector:
    def __init__(self, mysql_config):
        self.mysql_config = mysql_config
        
        # å®šä¹‰ç›‘æ§æŒ‡æ ‡
        self.slow_query_total = Counter(
            'mysql_slow_queries_total',
            'Total number of slow queries',
            ['database', 'table']
        )
        
        self.query_duration = Histogram(
            'mysql_query_duration_seconds',
            'Time spent executing queries',
            ['database', 'query_type']
        )
        
        self.active_connections = Gauge(
            'mysql_active_connections',
            'Number of active connections'
        )
    
    def collect_slow_queries(self):
        """é‡‡é›†æ…¢æŸ¥è¯¢æ•°æ®"""
        try:
            conn = mysql.connector.connect(**self.mysql_config)
            cursor = conn.cursor()
            
            # æŸ¥è¯¢å½“å‰è¿è¡Œçš„æ…¢æŸ¥è¯¢
            cursor.execute("""
                SELECT db, time, state, info 
                FROM information_schema.processlist 
                WHERE time > 2 AND state != ''
            """)
            
            for row in cursor.fetchall():
                db, exec_time, state, query = row
                if query and not query.startswith('SELECT * FROM'):
                    # è®°å½•æ…¢æŸ¥è¯¢
                    self.slow_query_total.labels(
                        database=db or 'unknown',
                        table=self._extract_table(query)
                    ).inc()
                    
        except Exception as e:
            print(f"é‡‡é›†é”™è¯¯: {e}")
        finally:
            if 'conn' in locals():
                conn.close()
    
    def _extract_table(self, query):
        """ä»SQLä¸­æå–è¡¨åï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        if 'FROM' in query.upper():
            parts = query.upper().split('FROM')[1].strip().split()
            return parts[0] if parts else 'unknown'
        return 'unknown'
```

### 3.3 å®æ—¶å‘Šè­¦é…ç½®


**âš ï¸ å‘Šè­¦è§„åˆ™å®šä¹‰**
```yaml
# mysql_slow_query_rules.yml
groups:
  - name: mysql_slow_query_alerts
    rules:
      # æ…¢æŸ¥è¯¢æ•°é‡å‘Šè­¦
      - alert: HighSlowQueryRate
        expr: rate(mysql_slow_queries_total[5m]) > 10
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "æ…¢æŸ¥è¯¢æ•°é‡è¿‡å¤š"
          description: "æ•°æ®åº“ {{ $labels.database }} åœ¨è¿‡å»5åˆ†é’Ÿå†…æ…¢æŸ¥è¯¢ç‡ä¸º {{ $value }} queries/sec"
      
      # é•¿æ—¶é—´è¿è¡ŒæŸ¥è¯¢å‘Šè­¦
      - alert: LongRunningQuery
        expr: mysql_query_duration_seconds > 30
        for: 0m
        labels:
          severity: critical
        annotations:
          summary: "å‘ç°é•¿æ—¶é—´è¿è¡Œçš„æŸ¥è¯¢"
          description: "æŸ¥è¯¢æ‰§è¡Œæ—¶é—´è¶…è¿‡30ç§’ï¼Œæ•°æ®åº“: {{ $labels.database }}"
      
      # è¿æ¥æ•°è¿‡å¤šå‘Šè­¦
      - alert: TooManyConnections
        expr: mysql_active_connections > 80
        for: 1m
        labels:
          severity: warning
        annotations:
          summary: "æ•°æ®åº“è¿æ¥æ•°è¿‡å¤š"
          description: "å½“å‰æ´»è·ƒè¿æ¥æ•°: {{ $value }}"
```

---

## 4. ğŸ“¡ ç›‘æ§æ•°æ®é‡‡é›†æ–¹æ³•


### 4.1 æ…¢æŸ¥è¯¢æ—¥å¿—é‡‡é›†


**ğŸ”¸ æ—¥å¿—é…ç½®ä¼˜åŒ–**
```sql
-- MySQLæ…¢æŸ¥è¯¢æ—¥å¿—é…ç½®
SET GLOBAL slow_query_log = 'ON';
SET GLOBAL slow_query_log_file = '/var/log/mysql/slow.log';
SET GLOBAL long_query_time = 2;
SET GLOBAL log_queries_not_using_indexes = 'ON';
SET GLOBAL log_slow_admin_statements = 'ON';

-- æŸ¥çœ‹å½“å‰é…ç½®
SHOW VARIABLES LIKE 'slow_query%';
SHOW VARIABLES LIKE 'long_query_time';
```

**ğŸ“Š æ—¥å¿—è§£æè„šæœ¬**
```python
import re
from datetime import datetime

class SlowQueryLogParser:
    def __init__(self, log_file):
        self.log_file = log_file
        self.query_pattern = re.compile(
            r'# Time: (\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}.\d+Z)\s+'
            r'# User@Host: (\w+)\[(\w+)\] @ \[(.*?)\]\s+'
            r'# Query_time: ([\d.]+)\s+Lock_time: ([\d.]+)\s+'
            r'Rows_sent: (\d+)\s+Rows_examined: (\d+)\s+'
            r'(.*?);',
            re.DOTALL
        )
    
    def parse_queries(self):
        """è§£ææ…¢æŸ¥è¯¢æ—¥å¿—"""
        with open(self.log_file, 'r') as f:
            content = f.read()
            
        queries = []
        for match in self.query_pattern.finditer(content):
            query_info = {
                'timestamp': match.group(1),
                'user': match.group(2),
                'host': match.group(4),
                'query_time': float(match.group(5)),
                'lock_time': float(match.group(6)),
                'rows_sent': int(match.group(7)),
                'rows_examined': int(match.group(8)),
                'sql': match.group(9).strip()
            }
            queries.append(query_info)
            
        return queries
    
    def get_top_slow_queries(self, limit=10):
        """è·å–æœ€æ…¢çš„æŸ¥è¯¢"""
        queries = self.parse_queries()
        return sorted(
            queries, 
            key=lambda x: x['query_time'], 
            reverse=True
        )[:limit]
```

### 4.2 Performance Schemaé‡‡é›†


**ğŸ¯ å®æ—¶æ€§èƒ½æ•°æ®é‡‡é›†**
```sql
-- å¯ç”¨Performance Schema
UPDATE performance_schema.setup_consumers 
SET ENABLED = 'YES' 
WHERE NAME LIKE 'events_statements%';

-- æŸ¥è¯¢æœ€æ…¢çš„SQLè¯­å¥
SELECT 
    ROUND(SUM_TIMER_WAIT/1000000000000, 2) AS total_time_sec,
    ROUND(AVG_TIMER_WAIT/1000000000000, 2) AS avg_time_sec,
    COUNT_STAR as exec_count,
    ROUND(SUM_ROWS_EXAMINED/COUNT_STAR) as avg_rows_examined,
    DIGEST_TEXT as sql_template
FROM performance_schema.events_statements_summary_by_digest 
WHERE DIGEST_TEXT IS NOT NULL 
ORDER BY SUM_TIMER_WAIT DESC 
LIMIT 10;
```

**ğŸ“ˆ è‡ªåŠ¨åŒ–é‡‡é›†è„šæœ¬**
```python
import mysql.connector
import time
import json

class PerformanceSchemaCollector:
    def __init__(self, mysql_config):
        self.mysql_config = mysql_config
        
    def collect_top_queries(self, limit=20):
        """é‡‡é›†æœ€æ…¢æŸ¥è¯¢ç»Ÿè®¡"""
        conn = mysql.connector.connect(**self.mysql_config)
        cursor = conn.cursor(dictionary=True)
        
        query = """
        SELECT 
            ROUND(SUM_TIMER_WAIT/1000000000000, 2) AS total_time,
            ROUND(AVG_TIMER_WAIT/1000000000000, 2) AS avg_time,
            ROUND(MAX_TIMER_WAIT/1000000000000, 2) AS max_time,
            COUNT_STAR as exec_count,
            SUM_ROWS_EXAMINED as total_rows_examined,
            ROUND(SUM_ROWS_EXAMINED/COUNT_STAR) as avg_rows_examined,
            DIGEST_TEXT as sql_template,
            FIRST_SEEN,
            LAST_SEEN
        FROM performance_schema.events_statements_summary_by_digest 
        WHERE DIGEST_TEXT IS NOT NULL 
        AND DIGEST_TEXT NOT LIKE 'SELECT * FROM `performance_schema%'
        ORDER BY SUM_TIMER_WAIT DESC 
        LIMIT %s
        """
        
        cursor.execute(query, (limit,))
        results = cursor.fetchall()
        
        cursor.close()
        conn.close()
        
        return results
    
    def reset_statistics(self):
        """é‡ç½®ç»Ÿè®¡æ•°æ®"""
        conn = mysql.connector.connect(**self.mysql_config)
        cursor = conn.cursor()
        
        # æ¸…ç©ºç»Ÿè®¡è¡¨
        cursor.execute(
            "TRUNCATE TABLE performance_schema.events_statements_summary_by_digest"
        )
        
        cursor.close()
        conn.close()
```

### 4.3 åº”ç”¨å±‚é¢é‡‡é›†


**ğŸ”§ ORMå±‚é¢ç›‘æ§**
```java
// Spring Boot + MyBatis æ…¢æŸ¥è¯¢ç›‘æ§
@Component
@Slf4j
public class SlowQueryInterceptor implements Interceptor {
    
    private static final long SLOW_QUERY_THRESHOLD = 2000; // 2ç§’
    
    @Override
    public Object intercept(Invocation invocation) throws Throwable {
        long startTime = System.currentTimeMillis();
        
        try {
            Object result = invocation.proceed();
            return result;
        } finally {
            long endTime = System.currentTimeMillis();
            long executeTime = endTime - startTime;
            
            if (executeTime > SLOW_QUERY_THRESHOLD) {
                // è®°å½•æ…¢æŸ¥è¯¢
                logSlowQuery(invocation, executeTime);
                
                // å‘é€ç›‘æ§æŒ‡æ ‡
                sendMetrics(executeTime, invocation);
            }
        }
    }
    
    private void logSlowQuery(Invocation invocation, long executeTime) {
        MappedStatement ms = (MappedStatement) invocation.getArgs()[0];
        String sqlId = ms.getId();
        
        log.warn("æ…¢æŸ¥è¯¢å‘Šè­¦: sqlId={}, æ‰§è¡Œæ—¶é—´={}ms", sqlId, executeTime);
    }
    
    private void sendMetrics(long executeTime, Invocation invocation) {
        // å‘é€åˆ°ç›‘æ§ç³»ç»Ÿ
        // å¯ä»¥é›†æˆMicrometerã€Prometheusç­‰
    }
}
```

---

## 5. ğŸš¨ å‘Šè­¦é˜ˆå€¼è®¾ç½®ç­–ç•¥


### 5.1 é˜ˆå€¼è®¾ç½®åŸåˆ™


**ğŸ“Š åˆ†çº§å‘Šè­¦ç­–ç•¥**
```
å‘Šè­¦çº§åˆ«åˆ†ç±»ï¼š

ğŸŸ¡ Infoï¼ˆä¿¡æ¯çº§ï¼‰ï¼š
â€¢ æŸ¥è¯¢æ—¶é—´ï¼š1-3ç§’
â€¢ ä½œç”¨ï¼šä¿¡æ¯æ”¶é›†ï¼Œæ— éœ€ç«‹å³å¤„ç†
â€¢ é€šçŸ¥æ–¹å¼ï¼šæ—¥å¿—è®°å½•

ğŸŸ  Warningï¼ˆè­¦å‘Šçº§ï¼‰ï¼š
â€¢ æŸ¥è¯¢æ—¶é—´ï¼š3-10ç§’  
â€¢ ä½œç”¨ï¼šéœ€è¦å…³æ³¨ï¼Œè€ƒè™‘ä¼˜åŒ–
â€¢ é€šçŸ¥æ–¹å¼ï¼šé‚®ä»¶/ä¼ä¸šå¾®ä¿¡

ğŸ”´ Criticalï¼ˆä¸¥é‡çº§ï¼‰ï¼š
â€¢ æŸ¥è¯¢æ—¶é—´ï¼š10-30ç§’
â€¢ ä½œç”¨ï¼šä¸¥é‡å½±å“æ€§èƒ½ï¼Œéœ€è¦ç«‹å³å¤„ç†
â€¢ é€šçŸ¥æ–¹å¼ï¼šçŸ­ä¿¡/ç”µè¯/é’‰é’‰

ğŸš¨ Emergencyï¼ˆç´§æ€¥çº§ï¼‰ï¼š
â€¢ æŸ¥è¯¢æ—¶é—´ï¼š>30ç§’
â€¢ ä½œç”¨ï¼šå¯èƒ½å¯¼è‡´ç³»ç»Ÿå´©æºƒ
â€¢ é€šçŸ¥æ–¹å¼ï¼šçŸ­ä¿¡+ç”µè¯+å€¼ç­ç¾¤
```

### 5.2 åŠ¨æ€é˜ˆå€¼è°ƒæ•´


**ğŸ¯ åŸºäºå†å²æ•°æ®çš„æ™ºèƒ½é˜ˆå€¼**
```python
import numpy as np
from datetime import datetime, timedelta

class DynamicThresholdCalculator:
    def __init__(self, historical_data):
        self.historical_data = historical_data
    
    def calculate_threshold(self, percentile=95):
        """åŸºäºå†å²æ•°æ®è®¡ç®—åŠ¨æ€é˜ˆå€¼"""
        # è·å–æœ€è¿‘7å¤©çš„æ•°æ®
        recent_data = self._get_recent_data(days=7)
        
        if len(recent_data) < 100:
            # æ•°æ®ä¸è¶³ï¼Œä½¿ç”¨é»˜è®¤é˜ˆå€¼
            return self._get_default_thresholds()
        
        # è®¡ç®—ç™¾åˆ†ä½æ•°é˜ˆå€¼
        query_times = [d['query_time'] for d in recent_data]
        
        thresholds = {
            'info': np.percentile(query_times, 75),      # 75%
            'warning': np.percentile(query_times, 90),    # 90%
            'critical': np.percentile(query_times, 95),   # 95%
            'emergency': np.percentile(query_times, 99)   # 99%
        }
        
        # åº”ç”¨æœ€å°é˜ˆå€¼é™åˆ¶
        return self._apply_minimum_limits(thresholds)
    
    def _get_recent_data(self, days):
        """è·å–æœ€è¿‘å‡ å¤©çš„æ•°æ®"""
        cutoff_date = datetime.now() - timedelta(days=days)
        return [
            d for d in self.historical_data 
            if d['timestamp'] > cutoff_date
        ]
    
    def _apply_minimum_limits(self, thresholds):
        """åº”ç”¨æœ€å°é˜ˆå€¼é™åˆ¶"""
        min_limits = {
            'info': 1.0,
            'warning': 3.0, 
            'critical': 10.0,
            'emergency': 30.0
        }
        
        for level, threshold in thresholds.items():
            thresholds[level] = max(threshold, min_limits[level])
            
        return thresholds
```

### 5.3 ä¸šåŠ¡æ„ŸçŸ¥å‘Šè­¦


**ğŸ¢ ä¸åŒä¸šåŠ¡åœºæ™¯çš„é˜ˆå€¼ç­–ç•¥**
```yaml
# ä¸šåŠ¡æ„ŸçŸ¥å‘Šè­¦é…ç½®
business_thresholds:
  # æ ¸å¿ƒäº¤æ˜“ç³»ç»Ÿ
  core_transaction:
    warning: 0.5    # 500ms
    critical: 1.0   # 1s
    emergency: 2.0  # 2s
    
  # ç”¨æˆ·æŸ¥è¯¢ç³»ç»Ÿ  
  user_query:
    warning: 2.0    # 2s
    critical: 5.0   # 5s
    emergency: 10.0 # 10s
    
  # æŠ¥è¡¨åˆ†æç³»ç»Ÿ
  report_analysis:
    warning: 10.0   # 10s
    critical: 30.0  # 30s
    emergency: 60.0 # 1åˆ†é’Ÿ
    
  # æ•°æ®åŒæ­¥ä»»åŠ¡
  data_sync:
    warning: 60.0   # 1åˆ†é’Ÿ
    critical: 300.0 # 5åˆ†é’Ÿ
    emergency: 600.0 # 10åˆ†é’Ÿ
```

---

## 6. ğŸ“Š ç›‘æ§Dashboardè®¾è®¡


### 6.1 Dashboardæ•´ä½“å¸ƒå±€


**ğŸ–¥ï¸ åˆ†å±‚å±•ç¤ºè®¾è®¡**
```
ç›‘æ§Dashboardä¸‰å±‚ç»“æ„ï¼š

ç¬¬ä¸€å±‚ï¼šå…¨å±€æ¦‚è§ˆ
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ¯ æ ¸å¿ƒKPIæŒ‡æ ‡ï¼ˆå¤§æ•°å­—æ˜¾ç¤ºï¼‰                      â”‚
â”‚  å¹³å‡å“åº”æ—¶é—´ | æ…¢æŸ¥è¯¢æ•°é‡ | QPS | é”™è¯¯ç‡         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“ˆ è¶‹åŠ¿å›¾è¡¨ï¼ˆæ—¶é—´åºåˆ—ï¼‰                          â”‚
â”‚  å“åº”æ—¶é—´è¶‹åŠ¿ | æ…¢æŸ¥è¯¢è¶‹åŠ¿ | è¿æ¥æ•°è¶‹åŠ¿           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ç¬¬äºŒå±‚ï¼šè¯¦ç»†åˆ†æ
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ“Š TOPæ…¢æŸ¥è¯¢åˆ—è¡¨ | ğŸ“‹ æ•°æ®åº“å®ä¾‹çŠ¶æ€            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ” æŸ¥è¯¢ç±»å‹åˆ†å¸ƒ | ğŸ“… æ—¶é—´åˆ†å¸ƒçƒ­åŠ›å›¾             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ç¬¬ä¸‰å±‚ï¼šæ·±å…¥è¯Šæ–­
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ğŸ”§ SQLæ‰§è¡Œè®¡åˆ’ | ğŸš¨ å‘Šè­¦å†å²è®°å½•               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ğŸ“ˆ èµ„æºä½¿ç”¨æƒ…å†µ | ğŸ”„ ä¼˜åŒ–å»ºè®®                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 å…³é”®å›¾è¡¨è®¾è®¡


**ğŸ“ˆ æ ¸å¿ƒç›‘æ§å›¾è¡¨**
```json
{
  "dashboard_config": {
    "panels": [
      {
        "title": "æ…¢æŸ¥è¯¢æ•°é‡è¶‹åŠ¿",
        "type": "graph",
        "metrics": [
          {
            "query": "rate(mysql_slow_queries_total[5m])",
            "legend": "æ…¢æŸ¥è¯¢/ç§’"
          }
        ],
        "alert_lines": [
          {"value": 10, "color": "yellow", "label": "è­¦å‘Šçº¿"},
          {"value": 50, "color": "red", "label": "ä¸¥é‡çº¿"}
        ]
      },
      {
        "title": "æŸ¥è¯¢å“åº”æ—¶é—´åˆ†å¸ƒ",
        "type": "heatmap", 
        "metrics": [
          {
            "query": "mysql_query_duration_seconds_bucket",
            "legend": "å“åº”æ—¶é—´åˆ†å¸ƒ"
          }
        ]
      },
      {
        "title": "TOP 10 æ…¢æŸ¥è¯¢",
        "type": "table",
        "columns": [
          "SQLæ‘˜è¦",
          "å¹³å‡æ‰§è¡Œæ—¶é—´", 
          "æ‰§è¡Œæ¬¡æ•°",
          "æœ€åæ‰§è¡Œæ—¶é—´"
        ]
      }
    ]
  }
}
```

### 6.3 äº¤äº’å¼åŠŸèƒ½è®¾è®¡


**ğŸ” é’»å–åˆ†æåŠŸèƒ½**
```javascript
// Dashboardäº¤äº’åŠŸèƒ½ç¤ºä¾‹
class SlowQueryDashboard {
    constructor() {
        this.timeRange = '1h'; // é»˜è®¤1å°æ—¶
        this.selectedDB = 'all'; // é»˜è®¤æ‰€æœ‰æ•°æ®åº“
    }
    
    // æ—¶é—´èŒƒå›´é€‰æ‹©
    onTimeRangeChange(newRange) {
        this.timeRange = newRange;
        this.refreshAllCharts();
    }
    
    // æ•°æ®åº“ç­›é€‰
    onDatabaseFilter(dbName) {
        this.selectedDB = dbName;
        this.refreshAllCharts();
    }
    
    // ç‚¹å‡»æ…¢æŸ¥è¯¢è¿›å…¥è¯¦æƒ…
    onSlowQueryClick(queryId) {
        // æ˜¾ç¤ºSQLè¯¦æƒ…é¢æ¿
        this.showQueryDetail(queryId);
        
        // åŠ è½½æ‰§è¡Œè®¡åˆ’
        this.loadExecutionPlan(queryId);
        
        // æ˜¾ç¤ºå†å²æ‰§è¡Œè¶‹åŠ¿
        this.showQueryTrend(queryId);
    }
    
    // è‡ªåŠ¨åˆ·æ–°
    startAutoRefresh(interval = 30000) {
        setInterval(() => {
            this.refreshAllCharts();
        }, interval);
    }
}
```

---

## 7. ğŸ“ˆ å†å²æ•°æ®è¶‹åŠ¿åˆ†æ


### 7.1 è¶‹åŠ¿åˆ†æç»´åº¦


**ğŸ“Š å¤šç»´åº¦è¶‹åŠ¿åˆ†æ**
```
æ—¶é—´ç»´åº¦åˆ†æï¼š
â€¢ å°æ—¶çº§ï¼šå‘ç°ä¸šåŠ¡é«˜å³°æœŸçš„æ€§èƒ½å˜åŒ–
â€¢ æ—¥çº§ï¼šè¯†åˆ«æ—¥å¸¸ä¸šåŠ¡å‘¨æœŸæ€§è§„å¾‹  
â€¢ å‘¨çº§ï¼šå‘ç°å‘¨æœ«ä¸å·¥ä½œæ—¥çš„å·®å¼‚
â€¢ æœˆçº§ï¼šè§‚å¯Ÿé•¿æœŸæ€§èƒ½è¶‹åŠ¿å˜åŒ–

ä¸šåŠ¡ç»´åº¦åˆ†æï¼š
â€¢ æŒ‰æ•°æ®åº“ï¼šä¸åŒä¸šåŠ¡æ¨¡å—çš„æ€§èƒ½å¯¹æ¯”
â€¢ æŒ‰è¡¨ï¼šè¯†åˆ«çƒ­ç‚¹è¡¨å’Œæ€§èƒ½ç“¶é¢ˆè¡¨
â€¢ æŒ‰æŸ¥è¯¢ç±»å‹ï¼šSELECT/INSERT/UPDATE/DELETEæ€§èƒ½åˆ†å¸ƒ
â€¢ æŒ‰ç”¨æˆ·ï¼šä¸åŒç”¨æˆ·çš„æŸ¥è¯¢è¡Œä¸ºåˆ†æ
```

### 7.2 è¶‹åŠ¿å¼‚å¸¸æ£€æµ‹


**ğŸš¨ è‡ªåŠ¨å¼‚å¸¸è¯†åˆ«ç®—æ³•**
```python
import pandas as pd
import numpy as np
from scipy import stats

class TrendAnomalyDetector:
    def __init__(self, historical_data):
        self.data = pd.DataFrame(historical_data)
        
    def detect_anomalies(self, metric='query_time', method='zscore'):
        """æ£€æµ‹è¶‹åŠ¿å¼‚å¸¸"""
        if method == 'zscore':
            return self._zscore_detection(metric)
        elif method == 'isolation_forest':
            return self._isolation_forest_detection(metric)
        elif method == 'statistical':
            return self._statistical_detection(metric)
    
    def _zscore_detection(self, metric, threshold=3):
        """Z-Scoreå¼‚å¸¸æ£€æµ‹"""
        z_scores = np.abs(stats.zscore(self.data[metric]))
        anomalies = self.data[z_scores > threshold]
        
        return {
            'anomaly_count': len(anomalies),
            'anomaly_timestamps': anomalies['timestamp'].tolist(),
            'anomaly_values': anomalies[metric].tolist()
        }
    
    def _statistical_detection(self, metric):
        """åŸºäºç»Ÿè®¡å­¦çš„å¼‚å¸¸æ£€æµ‹"""
        Q1 = self.data[metric].quantile(0.25)
        Q3 = self.data[metric].quantile(0.75)
        IQR = Q3 - Q1
        
        lower_bound = Q1 - 1.5 * IQR
        upper_bound = Q3 + 1.5 * IQR
        
        anomalies = self.data[
            (self.data[metric] < lower_bound) | 
            (self.data[metric] > upper_bound)
        ]
        
        return {
            'anomaly_count': len(anomalies),
            'lower_bound': lower_bound,
            'upper_bound': upper_bound,
            'anomalies': anomalies.to_dict('records')
        }
    
    def generate_trend_report(self):
        """ç”Ÿæˆè¶‹åŠ¿åˆ†ææŠ¥å‘Š"""
        report = {
            'summary': {
                'total_queries': len(self.data),
                'avg_query_time': self.data['query_time'].mean(),
                'max_query_time': self.data['query_time'].max(),
                'std_query_time': self.data['query_time'].std()
            },
            'trends': {
                'hourly_pattern': self._get_hourly_pattern(),
                'daily_pattern': self._get_daily_pattern(),
                'growth_rate': self._calculate_growth_rate()
            },
            'anomalies': self.detect_anomalies()
        }
        return report
    
    def _get_hourly_pattern(self):
        """è·å–å°æ—¶çº§åˆ«çš„æŸ¥è¯¢æ¨¡å¼"""
        self.data['hour'] = pd.to_datetime(self.data['timestamp']).dt.hour
        hourly_avg = self.data.groupby('hour')['query_time'].mean()
        return hourly_avg.to_dict()
    
    def _calculate_growth_rate(self):
        """è®¡ç®—æŸ¥è¯¢é‡å¢é•¿ç‡"""
        daily_counts = self.data.groupby(
            pd.to_datetime(self.data['timestamp']).dt.date
        ).size()
        
        if len(daily_counts) < 2:
            return 0
            
        recent_avg = daily_counts.tail(7).mean()
        previous_avg = daily_counts.head(7).mean()
        
        growth_rate = (recent_avg - previous_avg) / previous_avg * 100
        return round(growth_rate, 2)
```

### 7.3 æ€§èƒ½åŸºçº¿å»ºç«‹


**ğŸ“ å»ºç«‹æ€§èƒ½åŸºçº¿æ ‡å‡†**
```python
class PerformanceBaseline:
    def __init__(self, historical_data):
        self.data = historical_data
        
    def establish_baseline(self, stable_period_days=30):
        """å»ºç«‹æ€§èƒ½åŸºçº¿"""
        # è·å–ç¨³å®šæœŸæ•°æ®
        stable_data = self._get_stable_period_data(stable_period_days)
        
        baseline = {
            'query_time': {
                'p50': np.percentile(stable_data['query_time'], 50),
                'p90': np.percentile(stable_data['query_time'], 90),
                'p95': np.percentile(stable_data['query_time'], 95),
                'p99': np.percentile(stable_data['query_time'], 99)
            },
            'qps': {
                'avg': stable_data['qps'].mean(),
                'max': stable_data['qps'].max(),
                'min': stable_data['qps'].min()
            },
            'connection_count': {
                'avg': stable_data['connections'].mean(),
                'peak': stable_data['connections'].max()
            },
            'established_date': datetime.now().isoformat(),
            'data_period': f"{stable_period_days} days"
        }
        
        return baseline
    
    def compare_with_baseline(self, current_metrics, baseline):
        """ä¸åŸºçº¿å¯¹æ¯”"""
        comparison = {}
        
        for metric_type, baseline_values in baseline.items():
            if metric_type in current_metrics:
                current_value = current_metrics[metric_type]
                baseline_p95 = baseline_values.get('p95', baseline_values.get('avg'))
                
                deviation = (current_value - baseline_p95) / baseline_p95 * 100
                
                comparison[metric_type] = {
                    'current': current_value,
                    'baseline': baseline_p95,
                    'deviation_percent': round(deviation, 2),
                    'status': self._get_status(deviation)
                }
        
        return comparison
    
    def _get_status(self, deviation):
        """æ ¹æ®åå·®ç¨‹åº¦ç¡®å®šçŠ¶æ€"""
        if deviation <= 10:
            return 'normal'
        elif deviation <= 50:
            return 'warning'
        else:
            return 'critical'
```

---

## 8. ğŸ’¾ ç›‘æ§æ•°æ®å­˜å‚¨æ–¹æ¡ˆ


### 8.1 å­˜å‚¨æ¶æ„è®¾è®¡


**ğŸ—ï¸ åˆ†å±‚å­˜å‚¨ç­–ç•¥**
```
ç›‘æ§æ•°æ®å­˜å‚¨åˆ†å±‚ï¼š

å®æ—¶å±‚ï¼ˆHot Dataï¼‰ï¼š
â€¢ å­˜å‚¨æ—¶é—´ï¼šæœ€è¿‘24å°æ—¶
â€¢ å­˜å‚¨ä»‹è´¨ï¼šRedis/å†…å­˜æ•°æ®åº“
â€¢ æŸ¥è¯¢é¢‘ç‡ï¼šæé«˜
â€¢ æ•°æ®ç²¾åº¦ï¼šç§’çº§

è¿‘çº¿å±‚ï¼ˆWarm Dataï¼‰ï¼š
â€¢ å­˜å‚¨æ—¶é—´ï¼šæœ€è¿‘30å¤©  
â€¢ å­˜å‚¨ä»‹è´¨ï¼šæ—¶åºæ•°æ®åº“ï¼ˆInfluxDB/OpenTSDBï¼‰
â€¢ æŸ¥è¯¢é¢‘ç‡ï¼šé«˜
â€¢ æ•°æ®ç²¾åº¦ï¼šåˆ†é’Ÿçº§

å†·å­˜å‚¨å±‚ï¼ˆCold Dataï¼‰ï¼š
â€¢ å­˜å‚¨æ—¶é—´ï¼š1å¹´ä»¥ä¸Š
â€¢ å­˜å‚¨ä»‹è´¨ï¼šå¯¹è±¡å­˜å‚¨ï¼ˆS3/OSSï¼‰
â€¢ æŸ¥è¯¢é¢‘ç‡ï¼šä½
â€¢ æ•°æ®ç²¾åº¦ï¼šå°æ—¶çº§èšåˆ
```

### 8.2 æ—¶åºæ•°æ®åº“é€‰å‹


**ğŸ“Š InfluxDBå­˜å‚¨æ–¹æ¡ˆ**
```sql
-- InfluxDBæ•°æ®æ¨¡å‹è®¾è®¡
CREATE DATABASE mysql_monitoring

-- æ…¢æŸ¥è¯¢æŒ‡æ ‡è¡¨
-- measurement: slow_queries
-- tags: database, table, query_type, host  
-- fields: query_time, lock_time, rows_examined, rows_sent
-- time: timestamp

-- å†™å…¥æ•°æ®ç¤ºä¾‹
INSERT slow_queries,database=ecommerce,table=orders,query_type=SELECT,host=mysql01 
query_time=3.45,lock_time=0.1,rows_examined=15000,rows_sent=100 
1633024800000000000

-- æŸ¥è¯¢ç¤ºä¾‹ï¼šæœ€è¿‘1å°æ—¶çš„æ…¢æŸ¥è¯¢è¶‹åŠ¿
SELECT MEAN(query_time) as avg_time, MAX(query_time) as max_time
FROM slow_queries 
WHERE time >= now() - 1h 
GROUP BY time(5m), database
```

**ğŸ”§ æ•°æ®ä¿ç•™ç­–ç•¥é…ç½®**
```sql
-- InfluxDBæ•°æ®ä¿ç•™ç­–ç•¥
CREATE RETENTION POLICY "realtime" ON "mysql_monitoring" 
DURATION 24h REPLICATION 1 DEFAULT

CREATE RETENTION POLICY "short_term" ON "mysql_monitoring" 
DURATION 30d REPLICATION 1

CREATE RETENTION POLICY "long_term" ON "mysql_monitoring" 
DURATION 365d REPLICATION 1

-- è¿ç»­æŸ¥è¯¢å®ç°æ•°æ®é™é‡‡æ ·
CREATE CONTINUOUS QUERY "downsample_hourly" ON "mysql_monitoring"
BEGIN
  SELECT MEAN(query_time) as avg_query_time,
         MAX(query_time) as max_query_time,
         COUNT(query_time) as query_count
  INTO "long_term"."slow_queries_hourly"
  FROM "slow_queries"
  GROUP BY time(1h), database, host
END
```

### 8.3 æ•°æ®å‹ç¼©ä¸æ¸…ç†


**ğŸ—œï¸ è‡ªåŠ¨æ•°æ®ç®¡ç†**
```python
import schedule
import time
from datetime import datetime, timedelta

class MonitoringDataManager:
    def __init__(self, influx_client, s3_client):
        self.influx_client = influx_client
        self.s3_client = s3_client
        
    def setup_data_lifecycle(self):
        """è®¾ç½®æ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
        # æ¯å¤©å‡Œæ™¨2ç‚¹æ‰§è¡Œæ•°æ®æ¸…ç†
        schedule.every().day.at("02:00").do(self.daily_cleanup)
        
        # æ¯å‘¨æ—¥å‡Œæ™¨æ‰§è¡Œæ•°æ®å½’æ¡£
        schedule.every().sunday.at("01:00").do(self.weekly_archive)
        
        # æ¯æœˆ1å·æ‰§è¡Œæ•°æ®å‹ç¼©
        schedule.every().month.do(self.monthly_compression)
        
    def daily_cleanup(self):
        """æ—¥å¸¸æ•°æ®æ¸…ç†"""
        # åˆ é™¤7å¤©å‰çš„åŸå§‹è¯¦ç»†æ•°æ®
        cutoff_time = datetime.now() - timedelta(days=7)
        
        delete_query = f"""
        DELETE FROM slow_queries 
        WHERE time < '{cutoff_time.isoformat()}'
        """
        
        self.influx_client.query(delete_query)
        print(f"å·²æ¸…ç† {cutoff_time} ä¹‹å‰çš„è¯¦ç»†æ•°æ®")
        
    def weekly_archive(self):
        """å‘¨åº¦æ•°æ®å½’æ¡£"""
        # å°†30å¤©å‰çš„èšåˆæ•°æ®å½’æ¡£åˆ°å¯¹è±¡å­˜å‚¨
        start_time = datetime.now() - timedelta(days=37)
        end_time = datetime.now() - timedelta(days=30)
        
        # å¯¼å‡ºæ•°æ®
        export_query = f"""
        SELECT * FROM slow_queries_hourly
        WHERE time >= '{start_time.isoformat()}'
        AND time < '{end_time.isoformat()}'
        """
        
        result = self.influx_client.query(export_query)
        
        # ä¸Šä¼ åˆ°S3
        archive_key = f"mysql_monitoring/{start_time.strftime('%Y/%m')}/slow_queries.csv"
        self._upload_to_s3(result, archive_key)
        
        # åˆ é™¤å·²å½’æ¡£çš„æ•°æ®
        delete_query = f"""
        DELETE FROM slow_queries_hourly 
        WHERE time >= '{start_time.isoformat()}'
        AND time < '{end_time.isoformat()}'
        """
        
        self.influx_client.query(delete_query)
        
    def _upload_to_s3(self, data, key):
        """ä¸Šä¼ æ•°æ®åˆ°S3"""
        import csv
        import io
        
        # è½¬æ¢ä¸ºCSVæ ¼å¼
        csv_buffer = io.StringIO()
        writer = csv.writer(csv_buffer)
        
        # å†™å…¥æ•°æ®
        for record in data:
            writer.writerow(record)
        
        # ä¸Šä¼ 
        self.s3_client.put_object(
            Bucket='mysql-monitoring-archive',
            Key=key,
            Body=csv_buffer.getvalue()
        )
        
        print(f"æ•°æ®å·²å½’æ¡£åˆ° {key}")
```

---

## 9. ğŸ¢ å¤šå®ä¾‹ç›‘æ§ç®¡ç†


### 9.1 å¤šå®ä¾‹æ¶æ„è®¾è®¡


**ğŸ”— é›†ä¸­å¼ç›‘æ§æ¶æ„**
```
å¤šå®ä¾‹ç›‘æ§æ‹“æ‰‘ï¼š

                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚   ç›‘æ§ä¸­å¿ƒ      â”‚
                    â”‚ (Prometheus)    â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
            â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
            â”‚               â”‚               â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  MySQL-01   â”‚ â”‚  MySQL-02   â”‚ â”‚  MySQL-03   â”‚
     â”‚   ä¸»åº“      â”‚ â”‚   ä»åº“      â”‚ â”‚   åˆ†æåº“    â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚               â”‚               â”‚
     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
     â”‚  Exporter   â”‚ â”‚  Exporter   â”‚ â”‚  Exporter   â”‚
     â”‚   :9104     â”‚ â”‚   :9104     â”‚ â”‚   :9104     â”‚
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 9.2 å®ä¾‹åˆ†ç»„ç®¡ç†


**ğŸ·ï¸ å®ä¾‹åˆ†ç»„ç­–ç•¥**
```yaml
# ç›‘æ§å®ä¾‹é…ç½®
mysql_instances:
  production:
    - name: "mysql-prod-01"
      host: "10.0.1.10"
      port: 3306
      role: "master"
      business: "ecommerce"
      criticality: "high"
      slow_query_threshold: 1.0
      
    - name: "mysql-prod-02" 
      host: "10.0.1.11"
      port: 3306
      role: "slave"
      business: "ecommerce"
      criticality: "medium"
      slow_query_threshold: 2.0
      
  staging:
    - name: "mysql-staging-01"
      host: "10.0.2.10"
      port: 3306
      role: "master"
      business: "testing"
      criticality: "low"
      slow_query_threshold: 5.0
      
  analytics:
    - name: "mysql-analytics-01"
      host: "10.0.3.10"
      port: 3306
      role: "analytics"
      business: "reporting"
      criticality: "medium"
      slow_query_threshold: 10.0
```

### 9.3 å·®å¼‚åŒ–ç›‘æ§ç­–ç•¥


**âš™ï¸ ä¸ªæ€§åŒ–ç›‘æ§é…ç½®**
```python
class MultiInstanceMonitor:
    def __init__(self, config_file):
        self.instances = self._load_config(config_file)
        
    def configure_instance_monitoring(self, instance):
        """ä¸ºå®ä¾‹é…ç½®ä¸ªæ€§åŒ–ç›‘æ§"""
        config = {
            'scrape_interval': self._get_scrape_interval(instance),
            'slow_query_threshold': instance['slow_query_threshold'],
            'alert_rules': self._generate_alert_rules(instance),
            'dashboard_config': self._generate_dashboard_config(instance)
        }
        
        return config
    
    def _get_scrape_interval(self, instance):
        """æ ¹æ®å®ä¾‹é‡è¦æ€§ç¡®å®šé‡‡é›†é—´éš”"""
        criticality_intervals = {
            'high': '10s',      # é«˜é‡è¦æ€§ï¼š10ç§’é‡‡é›†
            'medium': '30s',    # ä¸­ç­‰é‡è¦æ€§ï¼š30ç§’é‡‡é›†
            'low': '60s'        # ä½é‡è¦æ€§ï¼š60ç§’é‡‡é›†
        }
        
        return criticality_intervals.get(
            instance['criticality'], '30s'
        )
    
    def _generate_alert_rules(self, instance):
        """ç”Ÿæˆå®ä¾‹ç‰¹å®šçš„å‘Šè­¦è§„åˆ™"""
        base_threshold = instance['slow_query_threshold']
        
        rules = {
            'slow_query_rate': {
                'warning': base_threshold * 2,
                'critical': base_threshold * 5
            },
            'connection_limit': {
                'warning': 80,  # è¿æ¥æ•°è¶…è¿‡80%
                'critical': 95  # è¿æ¥æ•°è¶…è¿‡95%
            }
        }
        
        # ç”Ÿäº§ç¯å¢ƒæ›´ä¸¥æ ¼çš„é˜ˆå€¼
        if instance.get('environment') == 'production':
            rules['slow_query_rate']['warning'] = base_threshold
            rules['slow_query_rate']['critical'] = base_threshold * 2
            
        return rules
    
    def generate_unified_dashboard(self):
        """ç”Ÿæˆç»Ÿä¸€çš„å¤šå®ä¾‹ç›‘æ§é¢æ¿"""
        dashboard = {
            'title': 'å¤šå®ä¾‹MySQLç›‘æ§æ€»è§ˆ',
            'panels': [
                {
                    'title': 'å®ä¾‹å¥åº·çŠ¶æ€',
                    'type': 'stat',
                    'query': 'up{job="mysql-exporter"}',
                    'group_by': ['instance', 'role']
                },
                {
                    'title': 'æ…¢æŸ¥è¯¢å¯¹æ¯”',
                    'type': 'graph',
                    'query': 'rate(mysql_slow_queries_total[5m])',
                    'group_by': ['instance']
                },
                {
                    'title': 'å®ä¾‹è´Ÿè½½åˆ†å¸ƒ',
                    'type': 'heatmap',
                    'query': 'mysql_global_status_threads_connected',
                    'group_by': ['instance']
                }
            ]
        }
        
        return dashboard
```

---

## 10. ğŸ›¡ï¸ ç›‘æ§ç³»ç»Ÿé«˜å¯ç”¨


### 10.1 é«˜å¯ç”¨æ¶æ„è®¾è®¡


**ğŸ—ï¸ ç›‘æ§ç³»ç»Ÿå®¹ç¾æ¶æ„**
```
é«˜å¯ç”¨ç›‘æ§æ¶æ„ï¼š

ä¸»ç›‘æ§ä¸­å¿ƒï¼ˆIDC-Aï¼‰:          å¤‡ç›‘æ§ä¸­å¿ƒï¼ˆIDC-Bï¼‰:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Prometheus-Master  â”‚â—„â”€â”€â”€â”€â–ºâ”‚  Prometheus-Backup â”‚
â”‚  AlertManager-01    â”‚      â”‚  AlertManager-02    â”‚  
â”‚  Grafana-Master     â”‚      â”‚  Grafana-Backup    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â”‚                             â”‚
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚   æ•°æ®åŒæ­¥æœåŠ¡       â”‚
              â”‚ (Federation/       â”‚
              â”‚  Remote Write)     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
       â”‚                 â”‚                 â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MySQLé›†ç¾¤-A â”‚  â”‚ MySQLé›†ç¾¤-B â”‚  â”‚ MySQLé›†ç¾¤-C â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 10.2 æ•°æ®åŒæ­¥ä¸å¤‡ä»½


**ğŸ”„ ç›‘æ§æ•°æ®é«˜å¯ç”¨æ–¹æ¡ˆ**
```yaml
# Prometheusé«˜å¯ç”¨é…ç½®
global:
  external_labels:
    region: 'idc-a'
    replica: 'prometheus-01'

# è¿œç¨‹å†™å…¥é…ç½®ï¼ˆæ•°æ®åŒæ­¥åˆ°å¤‡ä¸­å¿ƒï¼‰
remote_write:
  - url: "https://backup-prometheus.company.com/api/v1/write"
    basic_auth:
      username: "prometheus"
      password: "secure_password"
    queue_config:
      max_samples_per_send: 1000
      max_shards: 200
      capacity: 10000

# è”é‚¦é…ç½®ï¼ˆèšåˆå¤šä¸ªPrometheuså®ä¾‹ï¼‰
scrape_configs:
  - job_name: 'federate'
    scrape_interval: 15s
    honor_labels: true
    metrics_path: '/federate'
    params:
      'match[]':
        - '{job="mysql-exporter"}'
        - '{__name__=~"mysql_.*"}'
    static_configs:
      - targets:
        - 'prometheus-backup:9090'
```

### 10.3 æ•…éšœè‡ªåŠ¨åˆ‡æ¢


**ğŸ”„ ç›‘æ§ç³»ç»Ÿè‡ªåŠ¨æ•…éšœè½¬ç§»**
```python
import requests
import time
import logging
from datetime import datetime

class MonitoringFailover:
    def __init__(self, config):
        self.primary_endpoint = config['primary_endpoint']
        self.backup_endpoint = config['backup_endpoint'] 
        self.current_active = 'primary'
        self.check_interval = config.get('check_interval', 30)
        
    def start_health_check(self):
        """å¯åŠ¨å¥åº·æ£€æŸ¥å’Œè‡ªåŠ¨åˆ‡æ¢"""
        while True:
            try:
                if not self._check_primary_health():
                    self._failover_to_backup()
                elif self.current_active == 'backup' and self._check_primary_health():
                    self._failback_to_primary()
                    
                time.sleep(self.check_interval)
                
            except Exception as e:
                logging.error(f"å¥åº·æ£€æŸ¥å¼‚å¸¸: {e}")
                time.sleep(self.check_interval)
    
    def _check_primary_health(self):
        """æ£€æŸ¥ä¸»ç›‘æ§ç³»ç»Ÿå¥åº·çŠ¶æ€"""
        try:
            # æ£€æŸ¥Prometheus API
            prometheus_url = f"{self.primary_endpoint}/api/v1/query"
            params = {'query': 'up'}
            
            response = requests.get(
                prometheus_url, 
                params=params, 
                timeout=10
            )
            
            if response.status_code != 200:
                return False
                
            # æ£€æŸ¥æœ€è¿‘æ•°æ®æ›´æ–°æ—¶é—´
            data = response.json()
            if not data.get('data', {}).get('result'):
                return False
                
            # æ£€æŸ¥AlertManager
            alertmanager_url = f"{self.primary_endpoint.replace('9090', '9093')}/api/v1/status"
            am_response = requests.get(alertmanager_url, timeout=10)
            
            return am_response.status_code == 200
            
        except Exception as e:
            logging.warning(f"ä¸»ç³»ç»Ÿå¥åº·æ£€æŸ¥å¤±è´¥: {e}")
            return False
    
    def _failover_to_backup(self):
        """åˆ‡æ¢åˆ°å¤‡ç”¨ç›‘æ§ç³»ç»Ÿ"""
        if self.current_active == 'backup':
            return
            
        logging.warning("ä¸»ç›‘æ§ç³»ç»Ÿæ•…éšœï¼Œåˆ‡æ¢åˆ°å¤‡ç”¨ç³»ç»Ÿ")
        
        # æ›´æ–°DNSè®°å½•æˆ–è´Ÿè½½å‡è¡¡å™¨é…ç½®
        self._update_dns_record(self.backup_endpoint)
        
        # å‘é€åˆ‡æ¢é€šçŸ¥
        self._send_failover_notification('backup')
        
        self.current_active = 'backup'
        
    def _failback_to_primary(self):
        """åˆ‡æ¢å›ä¸»ç›‘æ§ç³»ç»Ÿ"""
        logging.info("ä¸»ç›‘æ§ç³»ç»Ÿæ¢å¤ï¼Œåˆ‡æ¢å›ä¸»ç³»ç»Ÿ")
        
        # ç¡®ä¿æ•°æ®åŒæ­¥å®Œæˆ
        if self._verify_data_sync():
            self._update_dns_record(self.primary_endpoint)
            self._send_failover_notification('primary')
            self.current_active = 'primary'
        else:
            logging.warning("æ•°æ®åŒæ­¥æœªå®Œæˆï¼Œå»¶è¿Ÿåˆ‡æ¢")
    
    def _verify_data_sync(self):
        """éªŒè¯æ•°æ®åŒæ­¥çŠ¶æ€"""
        try:
            # æ¯”è¾ƒä¸»å¤‡ç³»ç»Ÿçš„æœ€æ–°æ•°æ®æ—¶é—´æˆ³
            primary_latest = self._get_latest_timestamp(self.primary_endpoint)
            backup_latest = self._get_latest_timestamp(self.backup_endpoint)
            
            # æ—¶é—´å·®å°äº5åˆ†é’Ÿè®¤ä¸ºåŒæ­¥æ­£å¸¸
            time_diff = abs(primary_latest - backup_latest)
            return time_diff < 300  # 5åˆ†é’Ÿ
            
        except Exception as e:
            logging.error(f"æ•°æ®åŒæ­¥éªŒè¯å¤±è´¥: {e}")
            return False
    
    def _send_failover_notification(self, target_system):
        """å‘é€åˆ‡æ¢é€šçŸ¥"""
        message = f"""
        ç›‘æ§ç³»ç»Ÿæ•…éšœåˆ‡æ¢é€šçŸ¥
        
        åˆ‡æ¢æ—¶é—´: {datetime.now()}
        å½“å‰æ´»è·ƒç³»ç»Ÿ: {target_system}
        åˆ‡æ¢åŸå› : {'ä¸»ç³»ç»Ÿæ•…éšœ' if target_system == 'backup' else 'ä¸»ç³»ç»Ÿæ¢å¤'}
        
        è¯·åŠæ—¶æ£€æŸ¥ç›‘æ§ç³»ç»ŸçŠ¶æ€
        """
        
        # å‘é€åˆ°å¤šä¸ªé€šçŸ¥æ¸ é“
        self._send_to_slack(message)
        self._send_to_email(message)
        self._send_to_sms(message)
```

---

## 11. ğŸ“‹ æ ¸å¿ƒè¦ç‚¹æ€»ç»“


### 11.1 å¿…é¡»æŒæ¡çš„æ ¸å¿ƒæ¦‚å¿µ


```
ğŸ”¸ ç›‘æ§ä½“ç³»æœ¬è´¨ï¼šå®æ—¶å‘ç°é—®é¢˜ï¼Œé¢„é˜²ç³»ç»Ÿæ•…éšœçš„é¢„è­¦ç³»ç»Ÿ
ğŸ”¸ ç›‘æ§æŒ‡æ ‡æ ‡å‡†ï¼šæŸ¥è¯¢æ—¶é—´ã€é”ç­‰å¾…ã€æ‰«æè¡Œæ•°ç­‰æ ¸å¿ƒæ€§èƒ½æŒ‡æ ‡
ğŸ”¸ å®æ—¶ç›‘æ§ç³»ç»Ÿï¼šåŸºäºPrometheus+Grafanaçš„ç°ä»£ç›‘æ§æ¶æ„
ğŸ”¸ æ•°æ®é‡‡é›†æ–¹æ³•ï¼šæ…¢æŸ¥è¯¢æ—¥å¿—ã€Performance Schemaã€åº”ç”¨å±‚æ‹¦æˆª
ğŸ”¸ å‘Šè­¦é˜ˆå€¼ç­–ç•¥ï¼šåˆ†çº§å‘Šè­¦ã€åŠ¨æ€é˜ˆå€¼ã€ä¸šåŠ¡æ„ŸçŸ¥çš„æ™ºèƒ½å‘Šè­¦
ğŸ”¸ Dashboardè®¾è®¡ï¼šåˆ†å±‚å±•ç¤ºã€äº¤äº’åˆ†æã€è¶‹åŠ¿å¯è§†åŒ–
ğŸ”¸ å†å²è¶‹åŠ¿åˆ†æï¼šå¼‚å¸¸æ£€æµ‹ã€æ€§èƒ½åŸºçº¿ã€é•¿æœŸè¶‹åŠ¿é¢„æµ‹
ğŸ”¸ å­˜å‚¨æ–¹æ¡ˆï¼šåˆ†å±‚å­˜å‚¨ã€æ—¶åºæ•°æ®åº“ã€æ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†
ğŸ”¸ å¤šå®ä¾‹ç®¡ç†ï¼šé›†ä¸­ç›‘æ§ã€å·®å¼‚åŒ–ç­–ç•¥ã€ç»Ÿä¸€é¢æ¿
ğŸ”¸ é«˜å¯ç”¨ä¿éšœï¼šå®¹ç¾æ¶æ„ã€è‡ªåŠ¨åˆ‡æ¢ã€æ•°æ®åŒæ­¥
```

### 11.2 å…³é”®ç†è§£è¦ç‚¹


**ğŸ”¹ ä¸ºä»€ä¹ˆéœ€è¦å®Œæ•´çš„ç›‘æ§ä½“ç³»**
```
ä¸šåŠ¡è¿ç»­æ€§ä¿éšœï¼š
â€¢ åŠæ—¶å‘ç°æ€§èƒ½é—®é¢˜ï¼Œé¿å…ç”¨æˆ·ä½“éªŒå—æŸ
â€¢ é¢„é˜²æ€§èƒ½é—®é¢˜æ¼”å˜æˆç³»ç»Ÿæ•…éšœ
â€¢ ä¸ºä¸šåŠ¡å†³ç­–æä¾›æ•°æ®æ”¯æŒ

è¿ç»´æ•ˆç‡æå‡ï¼š
â€¢ ä»è¢«åŠ¨å“åº”å˜ä¸ºä¸»åŠ¨é¢„é˜²
â€¢ è‡ªåŠ¨åŒ–å‘Šè­¦å‡å°‘äººå·¥å·¡æ£€
â€¢ æ ‡å‡†åŒ–ç›‘æ§æµç¨‹ï¼Œé™ä½è¿ç»´å¤æ‚åº¦

æŠ€æœ¯å€ºåŠ¡ç®¡ç†ï¼š
â€¢ è¯†åˆ«æ€§èƒ½çƒ­ç‚¹ï¼ŒæŒ‡å¯¼ä¼˜åŒ–æ–¹å‘
â€¢ é‡åŒ–ä¼˜åŒ–æ•ˆæœï¼Œè¯æ˜æŠ€æœ¯ä»·å€¼
â€¢ å»ºç«‹æ€§èƒ½åŸºçº¿ï¼Œé˜²æ­¢æ€§èƒ½é€€åŒ–
```

**ğŸ”¹ ç›‘æ§ç³»ç»Ÿè®¾è®¡çš„å…³é”®åŸåˆ™**
```
å…¨é¢æ€§åŸåˆ™ï¼š
â€¢ è¦†ç›–æ‰€æœ‰å…³é”®æŒ‡æ ‡å’Œä¸šåŠ¡åœºæ™¯
â€¢ å¤šç»´åº¦ç›‘æ§ï¼šæ—¶é—´ã€ä¸šåŠ¡ã€æŠ€æœ¯ç»´åº¦

å®æ—¶æ€§åŸåˆ™ï¼š
â€¢ åŠæ—¶å‘ç°é—®é¢˜ï¼Œå¿«é€Ÿå“åº”
â€¢ åˆç†çš„é‡‡é›†é—´éš”å’Œå‘Šè­¦å»¶è¿Ÿ

å¯é æ€§åŸåˆ™ï¼š
â€¢ ç›‘æ§ç³»ç»Ÿæœ¬èº«è¦é«˜å¯ç”¨
â€¢ é¿å…ç›‘æ§é£æš´å’Œè¯¯æŠ¥

æ˜“ç”¨æ€§åŸåˆ™ï¼š
â€¢ ç›´è§‚çš„ç•Œé¢è®¾è®¡å’Œäº¤äº’ä½“éªŒ
â€¢ ç¬¦åˆè¿ç»´äººå‘˜çš„æ“ä½œä¹ æƒ¯
```

### 11.3 å®é™…åº”ç”¨æŒ‡å¯¼


**ğŸ¯ ç›‘æ§ä½“ç³»å»ºè®¾è·¯å¾„**
```
ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€ç›‘æ§æ­å»º
â€¢ éƒ¨ç½²Prometheus + Grafana
â€¢ é…ç½®åŸºç¡€çš„æ…¢æŸ¥è¯¢é‡‡é›†
â€¢ å»ºç«‹ç®€å•çš„å‘Šè­¦è§„åˆ™

ç¬¬äºŒé˜¶æ®µï¼šç›‘æ§å®Œå–„ä¼˜åŒ–  
â€¢ å¢åŠ å¤šç»´åº¦ç›‘æ§æŒ‡æ ‡
â€¢ å®Œå–„å‘Šè­¦ç­–ç•¥å’Œé€šçŸ¥æ¸ é“
â€¢ å»ºç«‹ç›‘æ§Dashboard

ç¬¬ä¸‰é˜¶æ®µï¼šæ™ºèƒ½åŒ–ç›‘æ§
â€¢ å¼•å…¥å¼‚å¸¸æ£€æµ‹ç®—æ³•
â€¢ å»ºç«‹åŠ¨æ€é˜ˆå€¼å’ŒåŸºçº¿
â€¢ å®ç°è‡ªåŠ¨åŒ–æ•…éšœå¤„ç†

ç¬¬å››é˜¶æ®µï¼šç›‘æ§ä½“ç³»æˆç†Ÿ
â€¢ å¤šå®ä¾‹ç»Ÿä¸€ç®¡ç†
â€¢ ç›‘æ§ç³»ç»Ÿé«˜å¯ç”¨
â€¢ ä¸DevOpså·¥å…·é“¾é›†æˆ
```

**ğŸ› ï¸ å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ**
```
ç›‘æ§æ•°æ®é‡è¿‡å¤§ï¼š
â†’ å®æ–½åˆ†å±‚å­˜å‚¨ç­–ç•¥
â†’ é…ç½®æ•°æ®ä¿ç•™ç­–ç•¥
â†’ ä½¿ç”¨æ•°æ®é™é‡‡æ ·æŠ€æœ¯

å‘Šè­¦é£æš´é—®é¢˜ï¼š
â†’ è®¾ç½®å‘Šè­¦æŠ‘åˆ¶è§„åˆ™
â†’ å®æ–½å‘Šè­¦åˆ†çº§ç­–ç•¥
â†’ é…ç½®å‘Šè­¦é™é»˜æ—¶é—´

ç›‘æ§ç³»ç»Ÿæ€§èƒ½ï¼š
â†’ ä¼˜åŒ–é‡‡é›†é¢‘ç‡
â†’ ä½¿ç”¨æ—¶åºæ•°æ®åº“
â†’ å®æ–½ç›‘æ§ç³»ç»Ÿé›†ç¾¤

è¯¯æŠ¥ç‡è¿‡é«˜ï¼š
â†’ è°ƒæ•´å‘Šè­¦é˜ˆå€¼
â†’ å¢åŠ ç¡®è®¤æœºåˆ¶
â†’ å»ºç«‹æ™ºèƒ½è¿‡æ»¤è§„åˆ™
```

### 11.4 æœ€ä½³å®è·µå»ºè®®


**ğŸ“Š ç›‘æ§æŒ‡æ ‡é€‰æ‹©**
- **é‡è¦æ€§æ’åº**ï¼šç”¨æˆ·ä½“éªŒ > ç³»ç»Ÿç¨³å®šæ€§ > èµ„æºä½¿ç”¨ç‡
- **æŒ‡æ ‡æ•°é‡æ§åˆ¶**ï¼šæ ¸å¿ƒæŒ‡æ ‡20-30ä¸ªï¼Œé¿å…ä¿¡æ¯è¿‡è½½
- **ä¸šåŠ¡ç›¸å…³æ€§**ï¼šç›‘æ§æŒ‡æ ‡è¦ä¸ä¸šåŠ¡ç›®æ ‡å¯¹é½

**ğŸš¨ å‘Šè­¦ç­–ç•¥è®¾è®¡**
- **åˆ†çº§åˆ†ç±»**ï¼šæ ¹æ®å½±å“ç¨‹åº¦å’Œç´§æ€¥ç¨‹åº¦åˆ†çº§
- **é¿å…æ‰“æ‰°**ï¼šéå·¥ä½œæ—¶é—´åªå‘é€ç´§æ€¥å‘Šè­¦
- **å¯æ“ä½œæ€§**ï¼šæ¯ä¸ªå‘Šè­¦éƒ½è¦æœ‰æ˜ç¡®çš„å¤„ç†æ­¥éª¤

**ğŸ“ˆ è¶‹åŠ¿åˆ†æåº”ç”¨**
- **å®šæœŸå›é¡¾**ï¼šæ¯å‘¨/æœˆåˆ†æç›‘æ§æ•°æ®è¶‹åŠ¿
- **é¢„æµ‹è§„åˆ’**ï¼šåŸºäºè¶‹åŠ¿æ•°æ®è¿›è¡Œå®¹é‡è§„åˆ’
- **æŒç»­æ”¹è¿›**ï¼šæ ¹æ®åˆ†æç»“æœä¼˜åŒ–ç³»ç»Ÿé…ç½®

**æ ¸å¿ƒè®°å¿†è¦ç‚¹**ï¼š
- ç›‘æ§ä½“ç³»æ˜¯æ•°æ®åº“å¥åº·çš„"ä½“æ£€ä¸­å¿ƒ"
- å®Œæ•´çš„ç›‘æ§ = é‡‡é›† + å­˜å‚¨ + åˆ†æ + å‘Šè­¦ + å±•ç¤º
- å¥½çš„ç›‘æ§ç³»ç»Ÿèƒ½æå‰å‘ç°é—®é¢˜ï¼Œè€Œä¸æ˜¯äº‹åé€šçŸ¥
- ç›‘æ§ç³»ç»Ÿæœ¬èº«ä¹Ÿéœ€è¦ç›‘æ§ï¼Œç¡®ä¿é«˜å¯ç”¨æ€§