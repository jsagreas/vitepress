---
title: 9、Otter Pipeline管道配置
---
## 📚 目录

1. [Pipeline管道概念](#1-Pipeline管道概念)
2. [源端数据库配置](#2-源端数据库配置)
3. [目标端数据库配置](#3-目标端数据库配置)
4. [数据表映射规则](#4-数据表映射规则)
5. [字段映射配置](#5-字段映射配置)
6. [数据类型转换](#6-数据类型转换)
7. [同步范围设定](#7-同步范围设定)
8. [过滤条件配置](#8-过滤条件配置)
9. [管道参数调优](#9-管道参数调优)
10. [管道状态管理](#10-管道状态管理)
11. [核心要点总结](#11-核心要点总结)

---

## 1. 🔗 Pipeline管道概念


### 1.1 什么是Pipeline管道


**通俗理解**：Pipeline管道就像一条"数据传输流水线"，负责把源数据库的数据搬运到目标数据库。

> 💡 **形象比喻**：想象成自来水管道系统
> - **源端** = 水厂（数据来源）
> - **管道** = 水管（传输通道）  
> - **目标端** = 用户家（数据目的地）
> - **过滤器** = 净水设备（数据处理）

```
数据同步流程图：
源数据库 ──┐
          │
          ▼
     ┌─────────┐    ┌──────────┐    ┌──────────┐
     │ Extract │───▶│Transform │───▶│   Load   │
     │ 提取数据 │    │ 转换处理 │    │ 加载入库 │
     └─────────┘    └──────────┘    └──────────┘
                                           │
                                           ▼
                                      目标数据库
```

### 1.2 Pipeline的核心作用


**🎯 主要功能**：
- **数据提取**：从源数据库读取变更数据
- **数据转换**：按照规则处理和转换数据格式
- **数据加载**：将处理后的数据写入目标数据库
- **监控管理**：实时监控同步状态和性能

**📊 Pipeline架构图**：
```
┌─────────────────────────────────────────────────────────┐
│                    Otter Pipeline                      │
├─────────────┬─────────────┬─────────────┬─────────────┤
│   Source    │  Extractor  │ Transformer │    Loader   │
│   源端配置   │   数据提取   │   数据转换   │   数据加载   │
├─────────────┼─────────────┼─────────────┼─────────────┤
│ - 连接信息   │ - binlog解析│ - 字段映射   │ - 批量写入   │
│ - 表过滤     │ - 增量识别  │ - 类型转换   │ - 事务控制   │
│ - 权限配置   │ - 位点管理  │ - 数据过滤   │ - 错误处理   │
└─────────────┴─────────────┴─────────────┴─────────────┘
```

### 1.3 Pipeline的工作方式


**🔄 同步模式**：

| 模式类型 | **工作原理** | **适用场景** | **特点** |
|---------|------------|-------------|---------|
| **实时同步** | `基于binlog实时解析` | `对延迟要求高` | `延迟秒级，资源消耗大` |
| **批量同步** | `定时批量传输数据` | `数据量大，延迟容忍` | `延迟分钟级，效率高` |
| **混合模式** | `实时+批量结合` | `平衡性能和延迟` | `灵活性强，配置复杂` |

---

## 2. 🔌 源端数据库配置


### 2.1 源端连接配置


**💻 基础连接参数**：
```yaml
# 源端MySQL配置示例
source:
  type: mysql
  host: 192.168.1.100
  port: 3306
  username: otter_user
  password: otter_pass
  database: source_db
  charset: utf8mb4
```

> ⚠️ **重要提醒**：源端数据库必须开启binlog，且格式为ROW模式

### 2.2 权限要求配置


**🔐 必需权限清单**：
```sql
-- 为Otter创建专用用户
CREATE USER 'otter_user'@'%' IDENTIFIED BY 'otter_pass';

-- 授予必要权限
GRANT SELECT ON source_db.* TO 'otter_user'@'%';
GRANT REPLICATION SLAVE ON *.* TO 'otter_user'@'%';
GRANT REPLICATION CLIENT ON *.* TO 'otter_user'@'%';

-- 刷新权限
FLUSH PRIVILEGES;
```

**📋 权限说明**：
- `SELECT`：读取表数据
- `REPLICATION SLAVE`：读取binlog日志
- `REPLICATION CLIENT`：获取binlog位置信息

### 2.3 binlog配置检查


**🔍 binlog状态检查**：
```sql
-- 检查binlog是否开启
SHOW VARIABLES LIKE 'log_bin';

-- 检查binlog格式
SHOW VARIABLES LIKE 'binlog_format';

-- 查看当前binlog文件
SHOW MASTER STATUS;
```

**⚙️ 推荐配置**：
```ini
# my.cnf配置
[mysqld]
log-bin = mysql-bin
binlog-format = ROW
expire_logs_days = 7
max_binlog_size = 100M
```

---

## 3. 🎯 目标端数据库配置


### 3.1 目标端连接配置


**💾 目标库连接参数**：
```yaml
# 目标端配置
target:
  type: mysql
  host: 192.168.1.200
  port: 3306
  username: otter_target
  password: target_pass
  database: target_db
  charset: utf8mb4
```

### 3.2 目标端权限配置


**🔧 写入权限设置**：
```sql
-- 创建目标端用户
CREATE USER 'otter_target'@'%' IDENTIFIED BY 'target_pass';

-- 授予写入权限
GRANT SELECT, INSERT, UPDATE, DELETE ON target_db.* TO 'otter_target'@'%';
GRANT CREATE, ALTER, DROP ON target_db.* TO 'otter_target'@'%';

FLUSH PRIVILEGES;
```

### 3.3 目标端优化配置


**⚡ 性能优化参数**：
```ini
# 目标库优化配置
[mysqld]
innodb_buffer_pool_size = 1G
innodb_log_file_size = 256M
innodb_flush_log_at_trx_commit = 2
sync_binlog = 0
# 同步期间可适当放宽一致性要求
```

> 🚀 **性能提示**：目标库可以适当放宽事务安全参数，提升写入性能

---

## 4. 📊 数据表映射规则


### 4.1 表映射概念


**🗂️ 什么是表映射**：
表映射就是告诉Otter"源库的哪张表对应目标库的哪张表"，可以是一对一，也可以改名。

**📋 映射类型**：

```
一对一映射（同名）：
源表: user_info  →  目标表: user_info

一对一映射（改名）：
源表: old_users  →  目标表: new_users

一对多映射（分表）：
源表: big_table  →  目标表: table_2023, table_2024
```

### 4.2 映射配置示例


**⚙️ 基础映射配置**：
```yaml
# 表映射配置
table_mappings:
  - source_table: "user_info"
    target_table: "user_info"
    sync_mode: "full"  # 全量+增量
    
  - source_table: "order_data"
    target_table: "orders"
    sync_mode: "increment"  # 仅增量
    
  - source_table: "product_*"  # 通配符支持
    target_table: "product_*"
    sync_mode: "full"
```

### 4.3 高级映射规则


**🔄 动态表名映射**：
```yaml
# 按时间分表映射
dynamic_mapping:
  source_pattern: "log_data"
  target_pattern: "log_${YYYY}_${MM}"
  split_rule: "monthly"  # 按月分表
```

**📊 映射规则图解**：
```
源库表结构：           目标库表结构：
┌──────────────┐      ┌──────────────┐
│  user_info   │ ────▶│  user_info   │
├──────────────┤      ├──────────────┤
│  old_orders  │ ────▶│  new_orders  │  
├──────────────┤      ├──────────────┤
│  log_data    │ ────▶│  log_2024_01 │
└──────────────┘      │  log_2024_02 │
                      └──────────────┘
```

---

## 5. 🔧 字段映射配置


### 5.1 字段映射基础


**📝 什么是字段映射**：
字段映射就是指定源表的字段对应目标表的哪个字段，可以改名、忽略、或者增加计算字段。

**🎯 映射类型说明**：

| 映射类型 | **说明** | **示例** |
|---------|---------|---------|
| **直接映射** | `字段名相同，直接对应` | `name → name` |
| **重命名映射** | `字段名不同，需要指定对应关系` | `user_name → name` |
| **忽略字段** | `源字段不同步到目标` | `password 字段忽略` |
| **计算字段** | `根据源字段计算生成新字段` | `full_name = first_name + last_name` |

### 5.2 字段映射配置


**⚙️ 详细映射配置**：
```yaml
# 字段映射示例
field_mappings:
  user_info:
    # 直接映射（字段名相同）
    - source: "id"
      target: "id"
      
    # 重命名映射
    - source: "user_name"
      target: "name"
      
    # 忽略敏感字段
    - source: "password"
      target: null  # 不同步
      
    # 计算字段
    - source: "first_name,last_name"
      target: "full_name"
      expression: "CONCAT(first_name, ' ', last_name)"
      
    # 默认值字段
    - source: null
      target: "sync_time"
      default_value: "NOW()"
```

### 5.3 字段映射规则图


**🗺️ 字段映射流程**：
```
源表字段                     目标表字段
┌─────────────┐             ┌─────────────┐
│     id      │ ─直接映射───▶│     id      │
├─────────────┤             ├─────────────┤
│ user_name   │ ─重命名───▶  │    name     │
├─────────────┤             ├─────────────┤
│  password   │ ─忽略─────┐  │             │
├─────────────┤          │  ├─────────────┤
│ first_name  │ ─┐       │  │ full_name   │◀─计算生成
│ last_name   │ ─┘计算───┘  ├─────────────┤
└─────────────┘             │ sync_time   │◀─默认值
                            └─────────────┘
```

---

## 6. 🔄 数据类型转换


### 6.1 类型转换概念


**🎨 为什么需要类型转换**：
不同数据库的数据类型可能不完全相同，或者业务需要改变字段类型，这时就需要类型转换。

**📊 常见转换场景**：
- MySQL的`DATETIME` → PostgreSQL的`TIMESTAMP`
- `INT` → `VARCHAR`（ID转字符串）
- `VARCHAR` → `TEXT`（扩大字段长度）
- JSON格式转换

### 6.2 自动类型转换


**⚡ Otter内置转换规则**：

| 源类型 | **目标类型** | **转换规则** | **注意事项** |
|-------|------------|------------|------------|
| `INT` | `BIGINT` | `自动扩展` | `无精度损失` |
| `VARCHAR(50)` | `VARCHAR(100)` | `长度扩展` | `自动适配` |
| `DATE` | `DATETIME` | `补充时间` | `默认00:00:00` |
| `DECIMAL(10,2)` | `DECIMAL(12,2)` | `精度扩展` | `保持小数位` |

### 6.3 自定义类型转换


**🔧 自定义转换配置**：
```yaml
# 类型转换规则
type_conversions:
  user_info:
    # 字符串转数字
    - field: "age"
      source_type: "VARCHAR"
      target_type: "INT"
      conversion: "CAST(age AS SIGNED)"
      
    # 时间戳转日期
    - field: "create_time"
      source_type: "TIMESTAMP"
      target_type: "DATE"
      conversion: "DATE(create_time)"
      
    # JSON字符串转JSON类型
    - field: "extra_info"
      source_type: "TEXT"
      target_type: "JSON"
      conversion: "JSON_VALID(extra_info)"
```

**⚠️ 转换注意事项**：
> - 类型转换可能导致数据精度损失
> - 转换失败会导致同步中断
> - 建议先在测试环境验证转换规则

---

## 7. 📏 同步范围设定


### 7.1 同步范围概念


**🎯 什么是同步范围**：
同步范围就是告诉Otter要同步哪些数据，可以按表、按字段、按条件来限定范围。

**📊 范围类型**：
```
┌─────────────────────────────────┐
│          数据库全部数据           │
├─────────────────────────────────┤
│ ┌─────────────────────────────┐ │
│ │        指定数据库数据         │ │  ← 数据库级别
│ │ ┌─────────────────────────┐ │ │
│ │ │      指定表数据         │ │ │  ← 表级别
│ │ │ ┌─────────────────────┐ │ │ │
│ │ │ │    指定字段数据     │ │ │ │  ← 字段级别
│ │ │ │ ┌─────────────────┐ │ │ │ │
│ │ │ │ │   过滤条件数据   │ │ │ │ │  ← 条件级别
│ │ │ │ └─────────────────┘ │ │ │ │
│ │ │ └─────────────────────┘ │ │ │
│ │ └─────────────────────────┘ │ │
│ └─────────────────────────────┘ │
└─────────────────────────────────┘
```

### 7.2 表级别范围设定


**📋 表范围配置**：
```yaml
# 同步范围配置
sync_scope:
  # 包含的表（白名单）
  include_tables:
    - "user_*"      # 所有user开头的表
    - "order_info"  # 指定表名
    - "product_*"   # 所有product开头的表
    
  # 排除的表（黑名单）
  exclude_tables:
    - "test_*"      # 排除测试表
    - "temp_*"      # 排除临时表
    - "log_debug"   # 排除调试日志表
```

### 7.3 字段级别范围设定


**📝 字段范围配置**：
```yaml
# 字段同步配置
field_scope:
  user_info:
    # 只同步指定字段
    include_fields:
      - "id"
      - "name"
      - "email"
      - "create_time"
    
    # 排除敏感字段
    exclude_fields:
      - "password"
      - "phone"
      - "id_card"
```

**🔒 安全考虑**：
> 敏感字段（密码、身份证号、手机号）建议设置为排除字段

---

## 8. 🔍 过滤条件配置


### 8.1 过滤条件概念


**🎪 什么是过滤条件**：
过滤条件就是设置一些规则，只有满足条件的数据才会被同步，不满足的数据会被忽略。

**🎯 过滤应用场景**：
- **时间过滤**：只同步最近30天的数据
- **状态过滤**：只同步状态为"有效"的记录
- **用户过滤**：只同步VIP用户的数据
- **地区过滤**：只同步特定地区的数据

### 8.2 基础过滤配置


**⚙️ 过滤条件示例**：
```yaml
# 数据过滤配置
data_filters:
  user_info:
    # 时间过滤
    - condition: "create_time >= DATE_SUB(NOW(), INTERVAL 30 DAY)"
      description: "只同步最近30天的用户"
    
    # 状态过滤
    - condition: "status = 'active'"
      description: "只同步激活状态的用户"
      
  order_data:
    # 金额过滤
    - condition: "amount > 0"
      description: "只同步有效订单"
    
    # 组合条件
    - condition: "status IN ('paid', 'shipped') AND amount >= 100"
      description: "只同步已支付且金额>=100的订单"
```

### 8.3 动态过滤条件


**🔄 动态过滤配置**：
```yaml
# 动态过滤（支持变量）
dynamic_filters:
  log_data:
    # 使用系统变量
    - condition: "log_date >= '${START_DATE}'"
      variables:
        START_DATE: "2024-01-01"
    
    # 使用函数
    - condition: "create_time >= CURRENT_DATE - INTERVAL ${DAYS} DAY"
      variables:
        DAYS: 7
```

**📊 过滤流程图**：
```
原始数据流 ──┐
           │
           ▼
     ┌──────────┐
     │ 过滤器1  │ ── 时间条件 ──┐
     └──────────┘              │
           ▼                   │
     ┌──────────┐              │
     │ 过滤器2  │ ── 状态条件 ──┤
     └──────────┘              │
           ▼                   │
     ┌──────────┐              │
     │ 过滤器3  │ ── 自定义条件 ─┘
     └──────────┘
           ▼
      符合条件的数据 ──▶ 目标库
```

---

## 9. ⚡ 管道参数调优


### 9.1 性能参数概述


**🚀 为什么要调优**：
默认参数可能不适合所有场景，通过调优可以：
- 提升同步速度
- 降低资源消耗  
- 减少同步延迟
- 提高稳定性

### 9.2 核心性能参数


**📊 关键参数配置**：
```yaml
# 性能调优参数
performance_config:
  # 批量处理参数
  batch_size: 1000          # 每批处理记录数
  batch_timeout: 30         # 批量超时时间(秒)
  
  # 并发控制参数
  parallel_count: 4         # 并发处理线程数
  max_memory: "512M"        # 最大内存使用
  
  # 重试机制参数
  retry_times: 3            # 失败重试次数
  retry_interval: 5         # 重试间隔(秒)
  
  # 缓冲区参数
  buffer_size: 16384        # 数据缓冲区大小
  queue_size: 10000         # 队列最大长度
```

### 9.3 参数调优策略


**🎯 调优建议表**：

| 场景类型 | **建议配置** | **说明** |
|---------|------------|---------|
| **高吞吐量** | `batch_size: 5000, parallel_count: 8` | `适合大数据量同步` |
| **低延迟** | `batch_size: 100, batch_timeout: 5` | `适合实时性要求高` |
| **资源受限** | `parallel_count: 2, max_memory: 256M` | `适合配置较低环境` |
| **稳定优先** | `retry_times: 5, buffer_size: 8192` | `适合网络不稳定环境` |

**📈 性能监控指标**：
```
关键指标：
┌─────────────────┬──────────────┬─────────────┐
│      指标       │    正常范围   │   调优方向   │
├─────────────────┼──────────────┼─────────────┤
│   同步延迟      │    < 10秒    │  减少批次   │
│   CPU使用率     │    < 80%     │  降低并发   │
│   内存使用率    │    < 90%     │  减少缓存   │
│   错误率        │    < 1%      │  增加重试   │
└─────────────────┴──────────────┴─────────────┘
```

---

## 10. 📊 管道状态管理


### 10.1 管道状态概念


**🎭 什么是管道状态**：
管道状态就是Pipeline当前的工作状态，通过状态可以了解同步是否正常、有没有错误等。

**📋 状态类型说明**：

| 状态 | **含义** | **说明** | **操作** |
|-----|---------|---------|---------|
| `INIT` | `初始化` | `管道刚创建，未启动` | `可以启动` |
| `START` | `启动中` | `正在启动，准备工作` | `等待完成` |
| `RUNNING` | `运行中` | `正常同步数据` | `可以暂停/停止` |
| `PAUSE` | `暂停中` | `暂时停止同步` | `可以恢复/停止` |
| `STOP` | `已停止` | `完全停止同步` | `可以重新启动` |
| `ERROR` | `错误状态` | `出现异常，需要处理` | `查看日志，修复后重启` |

### 10.2 状态管理操作


**🎮 管道控制命令**：
```bash
# 启动管道
curl -X POST "http://otter-admin:8080/api/pipeline/start" \
  -d "pipelineId=1"

# 暂停管道  
curl -X POST "http://otter-admin:8080/api/pipeline/pause" \
  -d "pipelineId=1"

# 恢复管道
curl -X POST "http://otter-admin:8080/api/pipeline/resume" \
  -d "pipelineId=1"

# 停止管道
curl -X POST "http://otter-admin:8080/api/pipeline/stop" \
  -d "pipelineId=1"
```

### 10.3 状态监控


**📊 监控信息查看**：
```yaml
# 管道状态信息
pipeline_status:
  id: 1
  name: "user_sync_pipeline"
  status: "RUNNING"
  
  # 运行统计
  statistics:
    start_time: "2024-01-15 10:00:00"
    total_processed: 150000      # 已处理记录数
    success_count: 149850        # 成功记录数
    error_count: 150             # 错误记录数
    current_delay: "5s"          # 当前延迟
    
  # 最后位点信息
  position:
    binlog_file: "mysql-bin.000023"
    binlog_position: 1024567
    
  # 错误信息
  last_error:
    time: "2024-01-15 12:30:15"
    message: "Connection timeout"
    table: "user_info"
```

**🔍 状态转换图**：
```
         启动
INIT ─────────▶ START ─────────▶ RUNNING
                  │                │
                  │                │ 暂停
                  ▼                ▼
                ERROR ◀─────── PAUSE
                  │                │
                  │ 修复           │ 恢复
                  ▼                ▼
                STOP ◀──────── RUNNING
                  ▲                │
                  │                │ 停止
                  └────────────────┘
```

---

## 11. 📋 核心要点总结


### 11.1 必须掌握的核心概念


```
🔸 Pipeline本质：数据同步的流水线，负责ETL全过程
🔸 源端配置：连接信息、权限设置、binlog配置
🔸 目标端配置：连接信息、写入权限、性能优化
🔸 映射规则：表映射、字段映射、类型转换
🔸 同步控制：范围设定、过滤条件、参数调优
🔸 状态管理：运行状态、监控指标、错误处理
```

### 11.2 关键理解要点


**🔹 Pipeline配置的层次性**
```
数据库级别 → 表级别 → 字段级别 → 记录级别
每一层都可以设置不同的规则和过滤条件
```

**🔹 性能与稳定性的平衡**
```
高性能配置：
- 大批量、高并发、大缓冲
- 适合离线同步场景

高稳定配置：
- 小批量、低并发、多重试
- 适合在线业务场景
```

**🔹 配置的可维护性**
```
配置原则：
- 使用有意义的命名
- 添加详细的注释说明
- 合理分组相关配置
- 定期备份配置文件
```

### 11.3 最佳实践建议


**🎯 配置建议**
- **测试优先**：新配置先在测试环境验证
- **渐进配置**：从简单配置开始，逐步完善
- **监控为王**：重点关注同步延迟和错误率
- **备份重要**：定期备份Pipeline配置

**🔧 故障处理**
- **日志分析**：出现问题先查看详细日志
- **状态检查**：确认源端和目标端连接状态
- **权限验证**：检查数据库用户权限是否充足
- **资源监控**：关注CPU、内存、网络使用情况

**核心记忆口诀**：
- Pipeline配置有层次，源端目标要配齐
- 表映射字段要对应，类型转换要小心
- 过滤范围设定好，性能调优别忘记
- 状态监控很重要，出错及时来处理