---
title: 8、Gravity支持的数据源配置
---
## 📚 目录

1. [Gravity数据源概述](#1-Gravity数据源概述)
2. [MySQL数据源配置](#2-MySQL数据源配置)
3. [MongoDB数据源配置](#3-MongoDB数据源配置)
4. [PostgreSQL数据源配置](#4-PostgreSQL数据源配置)
5. [输出端配置详解](#5-输出端配置详解)
6. [连接管理与优化](#6-连接管理与优化)
7. [数据类型映射规则](#7-数据类型映射规则)
8. [核心要点总结](#8-核心要点总结)

---

## 1. 🌐 Gravity数据源概述


### 1.1 什么是Gravity数据源


**🔸 核心定义**
```
Gravity数据源：摩拜单车开源的MySQL同步工具中的数据连接配置
本质：定义数据从哪里来、到哪里去的桥梁配置
作用：让Gravity知道如何连接各种不同类型的数据库和服务
```

**💡 工作原理**
```
数据流转过程：
输入数据源 → Gravity处理引擎 → 输出数据源

支持的连接类型：
🔸 关系型数据库：MySQL、PostgreSQL
🔸 NoSQL数据库：MongoDB
🔸 消息队列：Kafka
🔸 搜索引擎：Elasticsearch
🔸 数据仓库：ClickHouse
🔸 HTTP接口：RESTful API
```

### 1.2 数据源分类


**📋 输入数据源（Source）**
```
主要作用：提供原始数据
常见类型：
• MySQL：主要的关系型数据库
• MongoDB：文档型NoSQL数据库
• PostgreSQL：开源关系型数据库

特点：通常是业务系统的主数据库
```

**📋 输出数据源（Target）**
```
主要作用：接收同步后的数据
常见类型：
• Kafka：消息队列，用于数据流转
• Elasticsearch：搜索引擎，用于全文检索
• ClickHouse：列式数据库，用于数据分析
• HTTP：接口调用，用于业务通知

特点：通常是数据消费系统或中间件
```

---

## 2. 🗄️ MySQL数据源配置


### 2.1 基础连接配置


**🔧 核心配置参数**
```yaml
# MySQL输入源配置示例
mysql-source:
  type: "mysql"
  host: "localhost"
  port: 3306
  username: "gravity_user"
  password: "your_password"
  schema: "your_database"
  
  # 核心参数说明
  server-id: 12345          # MySQL复制服务器ID，必须唯一
  flavor: "mysql"           # 数据库类型标识
  charset: "utf8mb4"        # 字符集，推荐utf8mb4
```

> 💡 **重要提醒**：server-id必须在整个MySQL集群中唯一，建议使用IP最后一段+端口号组合

### 2.2 复制相关配置


**📊 Binlog配置**
```yaml
mysql-source:
  # Binlog相关设置
  binlog-file: "mysql-bin.000001"  # 起始binlog文件
  binlog-pos: 4                    # 起始binlog位置
  gtid-enabled: true               # 是否启用GTID模式
  
  # 表过滤配置
  include-table-regex: 
    - "user_db\\.user_.*"          # 只同步user_开头的表
  exclude-table-regex:
    - ".*\\.tmp_.*"                # 排除临时表
```

**🔸 配置说明**
- **binlog-file/pos**：指定从哪个binlog位置开始同步
- **gtid-enabled**：全局事务ID，更可靠的复制方式
- **表过滤**：只同步需要的表，提高效率

### 2.3 连接池优化


**⚡ 性能配置**
```yaml
mysql-source:
  # 连接池设置
  max-open-conns: 20        # 最大连接数
  max-idle-conns: 5         # 最大空闲连接数
  conn-max-lifetime: "1h"   # 连接最大生存时间
  
  # 超时设置  
  connect-timeout: "10s"    # 连接超时时间
  read-timeout: "30s"       # 读取超时时间
  write-timeout: "30s"      # 写入超时时间
```

---

## 3. 🍃 MongoDB数据源配置


### 3.1 基本连接设置


**🔧 MongoDB配置示例**
```yaml
mongodb-source:
  type: "mongodb"
  host: "mongodb://localhost:27017"
  database: "your_database"
  
  # 认证配置
  username: "mongo_user"
  password: "your_password"
  auth-database: "admin"     # 认证数据库
```

**💡 连接字符串格式**
```
标准格式：mongodb://username:password@host:port/database
集群格式：mongodb://user:pass@host1:27017,host2:27017/db?replicaSet=rs0

参数说明：
• replicaSet：副本集名称
• authSource：认证数据库
• ssl：是否使用SSL连接
```

### 3.2 Change Stream配置


**📊 实时同步设置**
```yaml
mongodb-source:
  # Change Stream配置
  change-stream-enabled: true
  resume-token: null          # 断点续传令牌
  
  # 集合过滤
  watch-collections:
    - "users"
    - "orders"
  
  # 操作类型过滤
  watch-operations:
    - "insert"
    - "update"
    - "delete"
```

> ⚠️ **注意**：Change Stream需要MongoDB 3.6+版本，且数据库必须是副本集模式

---

## 4. 🐘 PostgreSQL数据源配置


### 4.1 基础连接配置


**🔧 PostgreSQL配置**
```yaml
postgresql-source:
  type: "postgresql"
  host: "localhost"
  port: 5432
  database: "your_database"
  username: "postgres"
  password: "your_password"
  
  # PostgreSQL特有配置
  sslmode: "disable"         # SSL模式：disable/require/verify-full
  search-path: "public"      # 默认模式搜索路径
```

### 4.2 逻辑复制配置


**📊 WAL配置**
```yaml
postgresql-source:
  # 逻辑复制设置
  replication-slot: "gravity_slot"    # 复制槽名称
  publication: "gravity_pub"          # 发布名称
  
  # 表选择
  include-tables:
    - "public.users"
    - "public.orders"
```

**🔸 PostgreSQL复制前置条件**
```sql
-- 1. 创建发布（在源数据库执行）
CREATE PUBLICATION gravity_pub FOR ALL TABLES;

-- 2. 创建复制槽
SELECT pg_create_logical_replication_slot('gravity_slot', 'pgoutput');

-- 3. 授予权限
GRANT SELECT ON ALL TABLES IN SCHEMA public TO gravity_user;
```

---

## 5. 📤 输出端配置详解


### 5.1 Kafka输出配置


**🔧 Kafka基础配置**
```yaml
kafka-output:
  type: "kafka"
  brokers:
    - "localhost:9092"
    - "kafka2:9092"
  
  # 主题配置
  topic: "gravity-sync"
  partition-key: "table_name"  # 分区键
  
  # 消息格式
  message-format: "json"       # json/avro/protobuf
```

**📊 高级配置**
```yaml
kafka-output:
  # 生产者配置
  producer-config:
    acks: "all"                # 确认级别
    retries: 3                 # 重试次数
    batch-size: 16384          # 批次大小
    linger-ms: 10              # 延迟发送时间
    
  # 安全配置
  security-protocol: "SASL_SSL"
  sasl-mechanism: "PLAIN"
  sasl-username: "kafka_user"
  sasl-password: "kafka_pass"
```

### 5.2 Elasticsearch输出配置


**🔍 ES基础配置**
```yaml
elasticsearch-output:
  type: "elasticsearch"
  endpoints:
    - "http://localhost:9200"
  
  # 索引配置
  index-prefix: "gravity"      # 索引前缀
  index-type: "_doc"           # 文档类型
  
  # 认证配置
  username: "elastic"
  password: "elastic_pass"
```

**📊 批量写入优化**
```yaml
elasticsearch-output:
  # 批量配置
  bulk-size: 1000              # 批量大小
  bulk-timeout: "5s"           # 批量超时
  flush-interval: "10s"        # 刷新间隔
  
  # 重试配置
  max-retries: 3
  retry-backoff: "1s"
```

### 5.3 ClickHouse配置


**📊 ClickHouse配置**
```yaml
clickhouse-output:
  type: "clickhouse"
  dsn: "tcp://localhost:9000/default"
  
  # 连接配置
  username: "default"
  password: ""
  database: "gravity_sync"
  
  # 批量插入配置
  batch-size: 10000
  flush-interval: "30s"
```

### 5.4 HTTP接口输出


**🌐 HTTP配置**
```yaml
http-output:
  type: "http"
  url: "http://api.example.com/webhook"
  method: "POST"
  
  # 请求配置
  headers:
    Content-Type: "application/json"
    Authorization: "Bearer your-token"
  
  # 超时和重试
  timeout: "10s"
  max-retries: 3
  retry-interval: "5s"
```

---

## 6. 🔗 连接管理与优化


### 6.1 连接池参数设置


**⚡ 通用连接池配置**
```yaml
connection-pool:
  # 核心参数
  max-open-conns: 100          # 最大连接数
  max-idle-conns: 10           # 最大空闲连接数
  conn-max-lifetime: "1h"      # 连接最大生存时间
  conn-max-idle-time: "30m"    # 连接最大空闲时间
  
  # 健康检查
  health-check-interval: "30s" # 健康检查间隔
  ping-timeout: "5s"           # ping超时时间
```

**📊 参数调优建议**
```
根据数据量调整：
• 小数据量：max-open-conns = 5-10
• 中等数据量：max-open-conns = 20-50  
• 大数据量：max-open-conns = 50-100

根据网络环境调整：
• 内网环境：timeout可以设置较短（5-10s）
• 跨网络：timeout需要设置较长（30-60s）
```

### 6.2 数据源健康检查


**🏥 健康检查配置**
```yaml
health-check:
  enabled: true
  interval: "30s"              # 检查间隔
  timeout: "5s"                # 检查超时
  failure-threshold: 3         # 失败阈值
  
  # 检查策略
  check-on-startup: true       # 启动时检查
  check-on-idle: true          # 空闲时检查
```

### 6.3 连接失败重试机制


**🔄 重试策略配置**
```yaml
retry-policy:
  # 重试配置
  max-retries: 5               # 最大重试次数
  initial-delay: "1s"          # 初始延迟
  max-delay: "30s"             # 最大延迟
  backoff-multiplier: 2.0      # 退避倍数
  
  # 重试条件
  retry-on-connection-error: true
  retry-on-timeout: true
  retry-on-auth-failure: false
```

**📈 重试时间计算**
```
指数退避算法：
第1次重试：1s
第2次重试：2s  
第3次重试：4s
第4次重试：8s
第5次重试：16s (如果超过max-delay则使用max-delay)
```

---

## 7. 🔄 数据类型映射规则


### 7.1 MySQL到其他数据源映射


**📋 MySQL类型映射表**
| MySQL类型 | Kafka/JSON | Elasticsearch | ClickHouse | 说明 |
|-----------|------------|---------------|------------|------|
| `INT` | `number` | `integer` | `Int32` | 32位整数 |
| `BIGINT` | `number` | `long` | `Int64` | 64位整数 |
| `VARCHAR` | `string` | `text/keyword` | `String` | 可变字符串 |
| `TEXT` | `string` | `text` | `String` | 长文本 |
| `DECIMAL` | `string` | `scaled_float` | `Decimal` | 精确小数 |
| `DATETIME` | `string` | `date` | `DateTime` | 日期时间 |
| `JSON` | `object` | `object` | `String` | JSON对象 |

### 7.2 字符集编码转换


**🔤 编码配置**
```yaml
charset-mapping:
  # 源数据库编码
  source-charset: "utf8mb4"
  
  # 目标编码
  target-charset: "utf8"
  
  # 编码转换规则
  encoding-rules:
    - from: "latin1"
      to: "utf8"
      tables: ["legacy_table"]
```

### 7.3 自定义映射规则


**🛠️ 自定义类型映射**
```yaml
custom-type-mapping:
  # 字段级别映射
  field-mappings:
    - source-table: "users"
      source-field: "status"
      target-type: "keyword"
      
    - source-table: "orders"
      source-field: "amount"
      target-type: "scaled_float"
      scaling-factor: 100
```

---

## 8. 🔧 多数据源路由策略


### 8.1 基于表的路由


**📊 表级路由配置**
```yaml
routing-rules:
  # 基于表名路由
  table-routing:
    - pattern: "user_.*"
      target: "user-kafka-topic"
      
    - pattern: "order_.*"  
      target: "order-elasticsearch-index"
      
    - pattern: "log_.*"
      target: "clickhouse-log-table"
```

### 8.2 基于数据内容路由


**🎯 内容路由配置**
```yaml
routing-rules:
  # 基于字段值路由
  content-routing:
    - condition: "user_type = 'vip'"
      target: "vip-user-pipeline"
      
    - condition: "amount > 1000"
      target: "high-value-orders"
```

### 8.3 输入输出插件配置


**🔌 插件配置示例**
```yaml
plugins:
  # 数据转换插件
  transformers:
    - name: "field-mapper"
      config:
        mappings:
          old_field: "new_field"
    
    - name: "date-formatter"
      config:
        format: "yyyy-MM-dd HH:mm:ss"
  
  # 过滤插件
  filters:
    - name: "field-filter"
      config:
        include-fields: ["id", "name", "email"]
```

---

## 9. 📋 核心要点总结


### 9.1 必须掌握的基本概念


```
🔸 数据源类型：输入源(Source)和输出源(Target)的区别
🔸 连接配置：主机、端口、认证信息等基础参数
🔸 特殊配置：binlog、Change Stream、逻辑复制等同步机制
🔸 性能优化：连接池、批量处理、超时重试等配置
🔸 数据映射：不同数据库间的类型转换规则
```

### 9.2 关键配置要点


**🔹 MySQL配置核心**
```
必须配置：
- server-id：必须全局唯一
- binlog位置：确定同步起点
- 字符集：推荐utf8mb4
- 连接池：根据负载调整
```

**🔹 输出端选择原则**
```
Kafka：适合高吞吐量的消息流转
Elasticsearch：适合全文搜索和日志分析
ClickHouse：适合大数据分析和报表
HTTP：适合业务系统集成和通知
```

**🔹 性能优化关键点**
```
连接管理：
- 合理设置连接池大小
- 配置健康检查和重试
- 监控连接使用情况

批量处理：
- 设置合适的批量大小
- 控制刷新间隔
- 平衡延迟和吞吐量
```

### 9.3 实际应用价值


**🎯 业务场景应用**
- **数据同步**：MySQL到Elasticsearch的实时搜索
- **数据分析**：MySQL到ClickHouse的数据仓库
- **消息通知**：数据变更到Kafka的事件驱动
- **系统集成**：通过HTTP接口与第三方系统对接

**🔧 运维实践**
- **配置管理**：使用配置文件模板化部署
- **监控告警**：监控连接状态和同步延迟
- **故障处理**：基于重试机制和健康检查自愈
- **扩容规划**：根据数据量增长调整连接参数

**核心记忆**：
- 数据源配置是Gravity的基础，连接参数要精确
- 不同数据库有各自的特殊配置要求
- 性能优化主要在连接池和批量处理
- 类型映射和字符集转换关系数据正确性