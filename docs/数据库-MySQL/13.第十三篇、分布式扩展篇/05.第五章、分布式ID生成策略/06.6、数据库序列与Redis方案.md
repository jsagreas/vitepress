---
title: 6ã€æ•°æ®åº“åºåˆ—ä¸Redisæ–¹æ¡ˆ
---
## ğŸ“š ç›®å½•

1. [IDç”Ÿæˆæ–¹æ¡ˆæ¦‚è¿°](#1-IDç”Ÿæˆæ–¹æ¡ˆæ¦‚è¿°)
2. [æ•°æ®åº“åºåˆ—æ–¹æ¡ˆè¯¦è§£](#2-æ•°æ®åº“åºåˆ—æ–¹æ¡ˆè¯¦è§£)
3. [Redisè®¡æ•°å™¨æ–¹æ¡ˆè¯¦è§£](#3-Redisè®¡æ•°å™¨æ–¹æ¡ˆè¯¦è§£)
4. [æ‰¹é‡IDè·å–ç­–ç•¥](#4-æ‰¹é‡IDè·å–ç­–ç•¥)
5. [æŒä¹…åŒ–ä¸å®¹ç¾è®¾è®¡](#5-æŒä¹…åŒ–ä¸å®¹ç¾è®¾è®¡)
6. [é›†ç¾¤ä¸åˆ†å¸ƒå¼æ–¹æ¡ˆ](#6-é›†ç¾¤ä¸åˆ†å¸ƒå¼æ–¹æ¡ˆ)
7. [æ€§èƒ½ä¼˜åŒ–ä¸ç“¶é¢ˆåˆ†æ](#7-æ€§èƒ½ä¼˜åŒ–ä¸ç“¶é¢ˆåˆ†æ)
8. [æ··åˆæ–¹æ¡ˆè®¾è®¡](#8-æ··åˆæ–¹æ¡ˆè®¾è®¡)
9. [æ ¸å¿ƒè¦ç‚¹æ€»ç»“](#9-æ ¸å¿ƒè¦ç‚¹æ€»ç»“)

---

## 1. ğŸ¯ IDç”Ÿæˆæ–¹æ¡ˆæ¦‚è¿°


### 1.1 ä»€ä¹ˆæ˜¯IDç”Ÿæˆé—®é¢˜


**ç°å®åœºæ™¯ç†è§£**ï¼š
```
ç”µå•†ç³»ç»Ÿä¸­çš„éœ€æ±‚ï¼š
â€¢ è®¢å•IDï¼šæ¯ä¸ªè®¢å•éƒ½éœ€è¦å”¯ä¸€æ ‡è¯†
â€¢ ç”¨æˆ·IDï¼šæ¯ä¸ªæ–°ç”¨æˆ·æ³¨å†Œéƒ½éœ€è¦å”¯ä¸€ç¼–å·
â€¢ å•†å“IDï¼šæ¯ä¸ªå•†å“éƒ½éœ€è¦å”¯ä¸€è¯†åˆ«ç 

é—®é¢˜çš„æœ¬è´¨ï¼šå¦‚ä½•ä¿è¯åœ¨é«˜å¹¶å‘æƒ…å†µä¸‹ï¼Œæ¯æ¬¡ç”Ÿæˆçš„IDéƒ½æ˜¯å”¯ä¸€çš„ï¼Ÿ
```

**ğŸ’¡ æ ¸å¿ƒæŒ‘æˆ˜**ï¼š
- **å”¯ä¸€æ€§**ï¼šç»å¯¹ä¸èƒ½é‡å¤
- **é«˜æ€§èƒ½**ï¼šæ”¯æŒé«˜å¹¶å‘è¯·æ±‚
- **é«˜å¯ç”¨**ï¼šæœåŠ¡ä¸èƒ½åœæ­¢
- **æœ‰åºæ€§**ï¼šæœ€å¥½èƒ½ä¿æŒé€’å¢ï¼ˆä¾¿äºæ’åºå’Œåˆ†é¡µï¼‰

### 1.2 å¸¸è§æ–¹æ¡ˆå¯¹æ¯”


```
æ–¹æ¡ˆé€‰æ‹©å‚è€ƒï¼š

æ•°æ®åº“è‡ªå¢åºåˆ—ï¼š
ä¼˜ç‚¹ï¼šç®€å•å¯é ï¼Œå¤©ç„¶æœ‰åº
ç¼ºç‚¹ï¼šå•ç‚¹ç“¶é¢ˆï¼Œæ‰©å±•å›°éš¾

Redisè®¡æ•°å™¨ï¼š
ä¼˜ç‚¹ï¼šæ€§èƒ½é«˜ï¼Œæ”¯æŒæ‰¹é‡è·å–
ç¼ºç‚¹ï¼šéœ€è¦è€ƒè™‘æŒä¹…åŒ–é—®é¢˜

UUIDæ–¹æ¡ˆï¼š
ä¼˜ç‚¹ï¼šå®Œå…¨åˆ†å¸ƒå¼ï¼Œæ— ä¾èµ–
ç¼ºç‚¹ï¼šæ— åºï¼Œå ç”¨ç©ºé—´å¤§

é›ªèŠ±ç®—æ³•ï¼š
ä¼˜ç‚¹ï¼šåˆ†å¸ƒå¼å‹å¥½ï¼Œæœ‰åº
ç¼ºç‚¹ï¼šä¾èµ–æ—¶é’Ÿï¼Œå®ç°å¤æ‚
```

---

## 2. ğŸ—ƒï¸ æ•°æ®åº“åºåˆ—æ–¹æ¡ˆè¯¦è§£


### 2.1 åŸºæœ¬æ¦‚å¿µä¸åŸç†


**ä»€ä¹ˆæ˜¯æ•°æ®åº“åºåˆ—**ï¼š
æ•°æ®åº“åºåˆ—å°±æ˜¯æ•°æ®åº“æä¾›çš„ä¸€ä¸ªç‰¹æ®Šå¯¹è±¡ï¼Œä¸“é—¨ç”¨æ¥ç”Ÿæˆè¿ç»­çš„æ•°å­—ã€‚æ¯æ¬¡è°ƒç”¨éƒ½ä¼šè¿”å›ä¸‹ä¸€ä¸ªæ•°å­—ï¼Œæ•°æ®åº“è‡ªåŠ¨ä¿è¯å”¯ä¸€æ€§ã€‚

**å·¥ä½œåŸç†**ï¼š
```
æ•°æ®åº“å†…éƒ¨æœºåˆ¶ï¼š
1. åºåˆ—å¯¹è±¡ç»´æŠ¤ä¸€ä¸ªè®¡æ•°å™¨
2. æ¯æ¬¡è¯·æ±‚æ—¶ï¼Œè®¡æ•°å™¨è‡ªåŠ¨+1
3. è¿”å›æ–°çš„æ•°å€¼ç»™åº”ç”¨ç¨‹åº
4. æ•°æ®åº“ä¿è¯æ“ä½œçš„åŸå­æ€§

å°±åƒé“¶è¡Œçš„æ’å·æœºï¼š
æ¯æŒ‰ä¸€æ¬¡æŒ‰é’® â†’ è‡ªåŠ¨ç”Ÿæˆä¸‹ä¸€ä¸ªå·ç  â†’ ç»å¯¹ä¸ä¼šé‡å¤
```

### 2.2 åŸºæœ¬ä½¿ç”¨æ–¹æ³•


#### ğŸ“ MySQLè‡ªå¢å­—æ®µæ–¹å¼


```sql
-- åˆ›å»ºå¸¦è‡ªå¢IDçš„è¡¨
CREATE TABLE user_info (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    username VARCHAR(50) NOT NULL,
    email VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- æ’å…¥æ•°æ®ï¼ŒIDè‡ªåŠ¨ç”Ÿæˆ
INSERT INTO user_info (username, email) 
VALUES ('å¼ ä¸‰', 'zhangsan@example.com');

-- è·å–åˆšæ’å…¥çš„ID
SELECT LAST_INSERT_ID();
```

**ğŸ”¸ MySQLè‡ªå¢ç‰¹ç‚¹**ï¼š
- **è‡ªåŠ¨é€’å¢**ï¼šæ¯æ¬¡æ’å…¥æ•°æ®IDè‡ªåŠ¨+1
- **çº¿ç¨‹å®‰å…¨**ï¼šå¤šä¸ªè¿æ¥åŒæ—¶æ’å…¥ä¸ä¼šå†²çª
- **ç®€å•æ˜“ç”¨**ï¼šä¸éœ€è¦é¢å¤–ä»£ç é€»è¾‘

#### ğŸ“ Oracleåºåˆ—æ–¹å¼


```sql
-- åˆ›å»ºåºåˆ—å¯¹è±¡
CREATE SEQUENCE user_id_seq 
START WITH 1000          -- ä»1000å¼€å§‹
INCREMENT BY 1           -- æ¯æ¬¡é€’å¢1
MAXVALUE 999999999      -- æœ€å¤§å€¼
CACHE 100;              -- ç¼“å­˜100ä¸ªå€¼æé«˜æ€§èƒ½

-- ä½¿ç”¨åºåˆ—ç”ŸæˆID
INSERT INTO user_info (id, username, email)
VALUES (user_id_seq.NEXTVAL, 'æå››', 'lisi@example.com');

-- æŸ¥çœ‹åºåˆ—å½“å‰å€¼
SELECT user_id_seq.CURRVAL FROM dual;
```

**ğŸ”¸ Oracleåºåˆ—ä¼˜åŠ¿**ï¼š
- **çµæ´»é…ç½®**ï¼šå¯ä»¥è®¾ç½®èµ·å§‹å€¼ã€æ­¥é•¿ç­‰
- **ç¼“å­˜æœºåˆ¶**ï¼šé¢„åˆ†é…å¤šä¸ªå€¼å‡å°‘æ•°æ®åº“è®¿é—®
- **ç‹¬ç«‹å¯¹è±¡**ï¼šå¯ä»¥è¢«å¤šä¸ªè¡¨å…±äº«ä½¿ç”¨

### 2.3 åºåˆ—è¡¨è®¾è®¡ä¼˜åŒ–


#### ğŸ¯ ä¸“é—¨çš„åºåˆ—è¡¨è®¾è®¡


```sql
-- åˆ›å»ºé€šç”¨åºåˆ—è¡¨
CREATE TABLE id_generator (
    sequence_name VARCHAR(50) PRIMARY KEY,    -- åºåˆ—åç§°
    current_value BIGINT NOT NULL,           -- å½“å‰å€¼
    step_size INT DEFAULT 1,                 -- æ­¥é•¿
    max_value BIGINT,                        -- æœ€å¤§å€¼
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- åˆå§‹åŒ–åºåˆ—
INSERT INTO id_generator (sequence_name, current_value, step_size) 
VALUES 
('user_id', 10000, 1),
('order_id', 100000, 1),
('product_id', 1000, 1);
```

#### ğŸ“Š è·å–IDçš„ä¼˜åŒ–æ–¹æ³•


```sql
-- å®‰å…¨çš„IDè·å–æ–¹æ³•ï¼ˆä½¿ç”¨äº‹åŠ¡ï¼‰
DELIMITER //
CREATE FUNCTION get_next_id(seq_name VARCHAR(50))
RETURNS BIGINT
READS SQL DATA
MODIFIES SQL DATA
BEGIN
    DECLARE next_id BIGINT;
    
    -- å¼€å§‹äº‹åŠ¡
    START TRANSACTION;
    
    -- æ›´æ–°å¹¶è·å–æ–°IDï¼ˆåŸå­æ“ä½œï¼‰
    UPDATE id_generator 
    SET current_value = current_value + step_size,
        updated_at = CURRENT_TIMESTAMP
    WHERE sequence_name = seq_name;
    
    -- è·å–æ›´æ–°åçš„å€¼
    SELECT current_value INTO next_id 
    FROM id_generator 
    WHERE sequence_name = seq_name;
    
    COMMIT;
    RETURN next_id;
END//
DELIMITER ;

-- ä½¿ç”¨ç¤ºä¾‹
SELECT get_next_id('user_id') as new_user_id;
```

### 2.4 åˆ†ç‰‡ä¼˜åŒ–ç­–ç•¥


**é—®é¢˜èƒŒæ™¯**ï¼šå•å¼ åºåˆ—è¡¨åœ¨é«˜å¹¶å‘ä¸‹å¯èƒ½æˆä¸ºç“¶é¢ˆ

**åˆ†ç‰‡è§£å†³æ–¹æ¡ˆ**ï¼š
```sql
-- åˆ›å»ºå¤šä¸ªåºåˆ—è¡¨åˆ†ç‰‡
CREATE TABLE id_generator_shard_0 (
    sequence_name VARCHAR(50) PRIMARY KEY,
    current_value BIGINT NOT NULL,
    step_size INT DEFAULT 1
);

CREATE TABLE id_generator_shard_1 (
    sequence_name VARCHAR(50) PRIMARY KEY,
    current_value BIGINT NOT NULL,
    step_size INT DEFAULT 1
);

-- ä¸åŒåºåˆ—ä½¿ç”¨ä¸åŒåˆ†ç‰‡
-- user_id ä½¿ç”¨ shard_0
-- order_id ä½¿ç”¨ shard_1
```

**è·¯ç”±ç­–ç•¥**ï¼š
```python
def get_shard_table(sequence_name):
    """æ ¹æ®åºåˆ—åç§°å†³å®šä½¿ç”¨å“ªä¸ªåˆ†ç‰‡"""
    if sequence_name.startswith('user'):
        return 'id_generator_shard_0'
    elif sequence_name.startswith('order'):
        return 'id_generator_shard_1'
    else:
        return 'id_generator_shard_0'  # é»˜è®¤åˆ†ç‰‡
```

---

## 3. ğŸ”´ Redisè®¡æ•°å™¨æ–¹æ¡ˆè¯¦è§£


### 3.1 Redis INCRå‘½ä»¤åŸºç¡€


**ä»€ä¹ˆæ˜¯Redis INCR**ï¼š
INCRæ˜¯Redisæä¾›çš„åŸå­æ€§é€’å¢å‘½ä»¤ï¼Œæ¯æ¬¡è°ƒç”¨éƒ½ä¼šè®©æŒ‡å®šçš„keyçš„å€¼+1ï¼Œå¦‚æœkeyä¸å­˜åœ¨ä¼šè‡ªåŠ¨åˆ›å»ºå¹¶è®¾ä¸º1ã€‚

**åŸºæœ¬ä½¿ç”¨**ï¼š
```bash
# ç¬¬ä¸€æ¬¡è°ƒç”¨ï¼Œkeyä¸å­˜åœ¨ï¼Œè‡ªåŠ¨åˆ›å»ºå¹¶è®¾ä¸º1
redis> INCR user_id_counter
(integer) 1

# ç¬¬äºŒæ¬¡è°ƒç”¨ï¼Œå€¼å˜æˆ2
redis> INCR user_id_counter
(integer) 2

# ç¬¬ä¸‰æ¬¡è°ƒç”¨ï¼Œå€¼å˜æˆ3
redis> INCR user_id_counter
(integer) 3

# æŸ¥çœ‹å½“å‰å€¼
redis> GET user_id_counter
"3"
```

**ğŸ”¸ INCRå‘½ä»¤ç‰¹ç‚¹**ï¼š
- **åŸå­æ€§**ï¼šå³ä½¿é«˜å¹¶å‘ä¹Ÿä¸ä¼šå‡ºç°é‡å¤
- **è‡ªåŠ¨åˆ›å»º**ï¼škeyä¸å­˜åœ¨æ—¶è‡ªåŠ¨åˆ›å»º
- **è¿”å›æ–°å€¼**ï¼šç›´æ¥è¿”å›é€’å¢åçš„ç»“æœ
- **é«˜æ€§èƒ½**ï¼šå†…å­˜æ“ä½œï¼Œé€Ÿåº¦æå¿«

### 3.2 Redis IDç”ŸæˆåŸºç¡€åº”ç”¨


#### ğŸ“ ç®€å•çš„IDç”Ÿæˆå™¨


```python
import redis

class RedisIdGenerator:
    def __init__(self, redis_client):
        self.redis = redis_client
    
    def generate_id(self, sequence_name):
        """ç”Ÿæˆå•ä¸ªID"""
        key = f"id:sequence:{sequence_name}"
        return self.redis.incr(key)
    
    def generate_user_id(self):
        """ç”Ÿæˆç”¨æˆ·ID"""
        return self.generate_id("user")
    
    def generate_order_id(self):
        """ç”Ÿæˆè®¢å•ID"""
        return self.generate_id("order")

# ä½¿ç”¨ç¤ºä¾‹
r = redis.Redis(host='localhost', port=6379, db=0)
id_gen = RedisIdGenerator(r)

# ç”Ÿæˆæ–°çš„ç”¨æˆ·ID
new_user_id = id_gen.generate_user_id()
print(f"æ–°ç”¨æˆ·ID: {new_user_id}")

# ç”Ÿæˆæ–°çš„è®¢å•ID
new_order_id = id_gen.generate_order_id()
print(f"æ–°è®¢å•ID: {new_order_id}")
```

#### ğŸ¯ å¸¦å‰ç¼€çš„IDç”Ÿæˆ


```python
class PrefixedIdGenerator:
    def __init__(self, redis_client):
        self.redis = redis_client
    
    def generate_prefixed_id(self, prefix, sequence_name):
        """ç”Ÿæˆå¸¦å‰ç¼€çš„ID"""
        # è·å–é€’å¢æ•°å­—
        key = f"id:sequence:{sequence_name}"
        number = self.redis.incr(key)
        
        # ç»„åˆå‰ç¼€å’Œæ•°å­—
        return f"{prefix}{number:08d}"  # 8ä½æ•°å­—ï¼Œä¸è¶³è¡¥0
    
    def generate_order_id(self):
        """ç”Ÿæˆè®¢å•ID: ORD00000001"""
        return self.generate_prefixed_id("ORD", "order")
    
    def generate_product_id(self):
        """ç”Ÿæˆå•†å“ID: PRD00000001"""
        return self.generate_prefixed_id("PRD", "product")

# ä½¿ç”¨ç¤ºä¾‹
id_gen = PrefixedIdGenerator(r)
print(id_gen.generate_order_id())   # ORD00000001
print(id_gen.generate_order_id())   # ORD00000002
print(id_gen.generate_product_id()) # PRD00000001
```

### 3.3 Redisè®¡æ•°å™¨é«˜çº§ç‰¹æ€§


#### ğŸ“Š è®¾ç½®è®¡æ•°å™¨è¿‡æœŸæ—¶é—´


```python
def generate_daily_id(self, sequence_name):
    """ç”Ÿæˆæ¯æ—¥é‡ç½®çš„ID"""
    import datetime
    
    # ä½¿ç”¨æ—¥æœŸä½œä¸ºkeyçš„ä¸€éƒ¨åˆ†
    today = datetime.date.today().strftime("%Y%m%d")
    key = f"id:daily:{sequence_name}:{today}"
    
    # é€’å¢è®¡æ•°å™¨
    current_id = self.redis.incr(key)
    
    # è®¾ç½®ç¬¬äºŒå¤©å‡Œæ™¨è¿‡æœŸï¼ˆè‡ªåŠ¨é‡ç½®ï¼‰
    if current_id == 1:  # ç¬¬ä¸€æ¬¡åˆ›å»ºæ—¶è®¾ç½®è¿‡æœŸæ—¶é—´
        tomorrow = datetime.datetime.now().replace(hour=0, minute=0, second=0, microsecond=0) + datetime.timedelta(days=1)
        self.redis.expireat(key, int(tomorrow.timestamp()))
    
    return f"{today}{current_id:06d}"  # 20250909000001

# ä½¿ç”¨ç¤ºä¾‹
daily_id = id_gen.generate_daily_id("order")
print(f"ä»Šæ—¥è®¢å•ID: {daily_id}")  # 20250909000001
```

#### ğŸ”§ æ‰¹é‡æ“ä½œä¼˜åŒ–


```python
def generate_batch_ids(self, sequence_name, count):
    """æ‰¹é‡ç”Ÿæˆå¤šä¸ªID"""
    key = f"id:sequence:{sequence_name}"
    
    # ä½¿ç”¨pipelineæé«˜æ€§èƒ½
    pipe = self.redis.pipeline()
    for _ in range(count):
        pipe.incr(key)
    
    # æ‰¹é‡æ‰§è¡Œ
    results = pipe.execute()
    return results

# ä½¿ç”¨ç¤ºä¾‹
batch_ids = id_gen.generate_batch_ids("user", 5)
print(f"æ‰¹é‡ç”Ÿæˆçš„ç”¨æˆ·ID: {batch_ids}")  # [1, 2, 3, 4, 5]
```

---

## 4. ğŸ“¦ æ‰¹é‡IDè·å–ç­–ç•¥


### 4.1 ä¸ºä»€ä¹ˆéœ€è¦æ‰¹é‡è·å–


**æ€§èƒ½é—®é¢˜åˆ†æ**ï¼š
```
å•æ¬¡è·å–IDçš„é—®é¢˜ï¼š
â€¢ é«˜å¹¶å‘æ—¶é¢‘ç¹ç½‘ç»œè¯·æ±‚
â€¢ æ•°æ®åº“/Redisè¿æ¥æ•°æ¶ˆè€—å¤§
â€¢ æ•´ä½“å“åº”æ—¶é—´æ…¢

æ‰¹é‡è·å–çš„ä¼˜åŠ¿ï¼š
â€¢ å‡å°‘ç½‘ç»œå¾€è¿”æ¬¡æ•°
â€¢ é™ä½æ•°æ®åº“å‹åŠ›
â€¢ æé«˜æ•´ä½“ååé‡
```

**å®é™…åœºæ™¯ä¸¾ä¾‹**ï¼š
```
ç”µå•†ç§’æ€åœºæ™¯ï¼š
â€¢ 1ç§’å†…éœ€è¦ç”Ÿæˆ10000ä¸ªè®¢å•ID
â€¢ å•æ¬¡è·å–ï¼š10000æ¬¡ç½‘ç»œè¯·æ±‚
â€¢ æ‰¹é‡è·å–ï¼š100æ¬¡è¯·æ±‚ï¼Œæ¯æ¬¡è·å–100ä¸ªID
```

### 4.2 æ•°æ®åº“æ‰¹é‡è·å–å®ç°


#### ğŸ“ æ­¥é•¿æ–¹å¼æ‰¹é‡è·å–


```sql
-- ä¿®æ”¹åºåˆ—è¡¨ï¼Œæ”¯æŒæ­¥é•¿è·å–
ALTER TABLE id_generator ADD COLUMN batch_size INT DEFAULT 1;

-- æ‰¹é‡è·å–IDçš„å­˜å‚¨è¿‡ç¨‹
DELIMITER //
CREATE FUNCTION get_batch_ids(seq_name VARCHAR(50), batch_count INT)
RETURNS JSON
READS SQL DATA
MODIFIES SQL DATA
BEGIN
    DECLARE start_id BIGINT;
    DECLARE end_id BIGINT;
    DECLARE step_value INT;
    
    -- è·å–å½“å‰æ­¥é•¿
    SELECT step_size INTO step_value 
    FROM id_generator 
    WHERE sequence_name = seq_name;
    
    -- æ‰¹é‡å¢é•¿
    UPDATE id_generator 
    SET current_value = current_value + (step_value * batch_count)
    WHERE sequence_name = seq_name;
    
    -- è·å–IDèŒƒå›´
    SELECT current_value - (step_value * batch_count) + step_value INTO start_id
    FROM id_generator 
    WHERE sequence_name = seq_name;
    
    SELECT current_value INTO end_id
    FROM id_generator 
    WHERE sequence_name = seq_name;
    
    -- è¿”å›JSONæ ¼å¼çš„èŒƒå›´
    RETURN JSON_OBJECT('start_id', start_id, 'end_id', end_id, 'count', batch_count);
END//
DELIMITER ;

-- ä½¿ç”¨ç¤ºä¾‹
SELECT get_batch_ids('user_id', 100);
-- è¿”å›: {"start_id": 10001, "end_id": 10100, "count": 100}
```

#### ğŸ¯ åº”ç”¨å±‚æ‰¹é‡ç®¡ç†


```python
class BatchIdManager:
    def __init__(self, db_connection):
        self.db = db_connection
        self.id_cache = {}  # ç¼“å­˜é¢„åˆ†é…çš„ID
        self.batch_size = 100
    
    def get_id(self, sequence_name):
        """è·å–å•ä¸ªIDï¼ˆä»ç¼“å­˜ä¸­å–ï¼‰"""
        cache_key = sequence_name
        
        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦ä¸ºç©º
        if cache_key not in self.id_cache or not self.id_cache[cache_key]:
            self._refill_cache(sequence_name)
        
        # ä»ç¼“å­˜ä¸­å–å‡ºä¸€ä¸ªID
        return self.id_cache[cache_key].pop(0)
    
    def _refill_cache(self, sequence_name):
        """é‡æ–°å¡«å……IDç¼“å­˜"""
        # ä»æ•°æ®åº“æ‰¹é‡è·å–ID
        result = self._execute_sql(
            "SELECT get_batch_ids(%s, %s)",
            (sequence_name, self.batch_size)
        )
        
        batch_info = json.loads(result[0])
        start_id = batch_info['start_id']
        end_id = batch_info['end_id']
        
        # ç”ŸæˆIDåˆ—è¡¨å¹¶å­˜å…¥ç¼“å­˜
        id_list = list(range(start_id, end_id + 1))
        self.id_cache[sequence_name] = id_list
        
        print(f"å·²é¢„åˆ†é… {sequence_name} ID: {start_id} åˆ° {end_id}")

# ä½¿ç”¨ç¤ºä¾‹
batch_manager = BatchIdManager(db_connection)

# è¿ç»­è·å–å¤šä¸ªIDï¼Œå†…éƒ¨è‡ªåŠ¨ç®¡ç†æ‰¹é‡è·å–
for i in range(150):  # è¶…è¿‡batch_sizeï¼Œä¼šè‡ªåŠ¨é‡æ–°æ‰¹é‡è·å–
    user_id = batch_manager.get_id('user_id')
    print(f"ç”¨æˆ·ID: {user_id}")
```

### 4.3 Redisæ‰¹é‡è·å–å®ç°


#### ğŸ“Š RedisåŸç”Ÿæ‰¹é‡å‘½ä»¤


```python
class RedisBatchIdGenerator:
    def __init__(self, redis_client, batch_size=100):
        self.redis = redis_client
        self.batch_size = batch_size
        self.id_cache = {}
    
    def get_batch_ids_direct(self, sequence_name, count):
        """ç›´æ¥æ‰¹é‡è·å–ID"""
        key = f"id:sequence:{sequence_name}"
        
        # ä½¿ç”¨Luaè„šæœ¬ä¿è¯åŸå­æ€§
        lua_script = """
        local key = KEYS[1]
        local count = tonumber(ARGV[1])
        local start_id = redis.call('INCRBY', key, count) - count + 1
        local end_id = redis.call('GET', key)
        return {start_id, end_id}
        """
        
        result = self.redis.eval(lua_script, 1, key, count)
        start_id, end_id = result
        
        return list(range(int(start_id), int(end_id) + 1))
    
    def get_id_with_cache(self, sequence_name):
        """å¸¦ç¼“å­˜çš„IDè·å–"""
        cache_key = sequence_name
        
        # æ£€æŸ¥ç¼“å­˜
        if cache_key not in self.id_cache or not self.id_cache[cache_key]:
            # æ‰¹é‡è·å–å¹¶ç¼“å­˜
            id_list = self.get_batch_ids_direct(sequence_name, self.batch_size)
            self.id_cache[cache_key] = id_list
            print(f"æ‰¹é‡è·å–äº† {len(id_list)} ä¸ª {sequence_name} ID")
        
        # ä»ç¼“å­˜è¿”å›ä¸€ä¸ªID
        return self.id_cache[cache_key].pop(0)

# ä½¿ç”¨ç¤ºä¾‹
batch_gen = RedisBatchIdGenerator(redis_client, batch_size=50)

# æ¨¡æ‹Ÿé«˜å¹¶å‘è·å–ID
import threading
import time

def worker(worker_id):
    for i in range(20):
        user_id = batch_gen.get_id_with_cache('user')
        print(f"Worker {worker_id} è·å–ID: {user_id}")
        time.sleep(0.01)

# å¯åŠ¨å¤šä¸ªçº¿ç¨‹æ¨¡æ‹Ÿå¹¶å‘
threads = []
for i in range(5):
    t = threading.Thread(target=worker, args=(i,))
    threads.append(t)
    t.start()

for t in threads:
    t.join()
```

---

## 5. ğŸ’¾ æŒä¹…åŒ–ä¸å®¹ç¾è®¾è®¡


### 5.1 RedisæŒä¹…åŒ–ç­–ç•¥


**ä¸ºä»€ä¹ˆéœ€è¦æŒä¹…åŒ–**ï¼š
Redisæ˜¯å†…å­˜æ•°æ®åº“ï¼Œå¦‚æœæœåŠ¡å™¨é‡å¯æˆ–æ–­ç”µï¼Œå†…å­˜ä¸­çš„è®¡æ•°å™¨å°±ä¼šä¸¢å¤±ï¼Œå¯èƒ½å¯¼è‡´IDé‡å¤ã€‚

#### ğŸ“ RDBæŒä¹…åŒ–é…ç½®


```bash
# redis.conf é…ç½®
# æ¯900ç§’å†…è‡³å°‘1ä¸ªkeyå‘ç”Ÿå˜åŒ–å°±ä¿å­˜
save 900 1
# æ¯300ç§’å†…è‡³å°‘10ä¸ªkeyå‘ç”Ÿå˜åŒ–å°±ä¿å­˜  
save 300 10
# æ¯60ç§’å†…è‡³å°‘10000ä¸ªkeyå‘ç”Ÿå˜åŒ–å°±ä¿å­˜
save 60 10000

# å¯ç”¨å‹ç¼©
rdbcompression yes
# RDBæ–‡ä»¶å
dbfilename dump.rdb
```

**RDBæŒä¹…åŒ–ç‰¹ç‚¹**ï¼š
- **å¿«ç…§æ–¹å¼**ï¼šä¿å­˜æŸä¸ªæ—¶é—´ç‚¹çš„å®Œæ•´æ•°æ®
- **æ¢å¤å¿«é€Ÿ**ï¼šé‡å¯æ—¶ç›´æ¥åŠ è½½RDBæ–‡ä»¶
- **æ•°æ®å¯èƒ½ä¸¢å¤±**ï¼šæœ€åä¸€æ¬¡å¿«ç…§åçš„æ•°æ®ä¼šä¸¢å¤±

#### ğŸ“ AOFæŒä¹…åŒ–é…ç½®


```bash
# å¯ç”¨AOF
appendonly yes
# AOFæ–‡ä»¶å
appendfilename "appendonly.aof"

# åŒæ­¥ç­–ç•¥
# always: æ¯ä¸ªå†™å‘½ä»¤éƒ½åŒæ­¥ï¼ˆæœ€å®‰å…¨ä½†æ€§èƒ½å·®ï¼‰
# everysec: æ¯ç§’åŒæ­¥ä¸€æ¬¡ï¼ˆæ¨èï¼‰
# no: è®©æ“ä½œç³»ç»Ÿå†³å®šä½•æ—¶åŒæ­¥
appendfsync everysec

# AOFé‡å†™ä¼˜åŒ–
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb
```

**AOFæŒä¹…åŒ–ç‰¹ç‚¹**ï¼š
- **è®°å½•æ“ä½œ**ï¼šè®°å½•æ¯ä¸ªå†™æ“ä½œå‘½ä»¤
- **æ•°æ®å®‰å…¨**ï¼šå¯ä»¥åšåˆ°å‡ ä¹ä¸ä¸¢å¤±æ•°æ®
- **æ¢å¤è¾ƒæ…¢**ï¼šéœ€è¦é‡æ”¾æ‰€æœ‰æ“ä½œå‘½ä»¤

#### ğŸ¯ æ··åˆæŒä¹…åŒ–ï¼ˆæ¨èï¼‰


```bash
# å¯ç”¨æ··åˆæŒä¹…åŒ–ï¼ˆRedis 4.0+ï¼‰
aof-use-rdb-preamble yes
```

**æ··åˆæŒä¹…åŒ–ä¼˜åŠ¿**ï¼š
```
æ¢å¤è¿‡ç¨‹ï¼š
1. å…ˆåŠ è½½RDBå¿«ç…§ï¼ˆå¿«é€Ÿæ¢å¤å¤§éƒ¨åˆ†æ•°æ®ï¼‰
2. å†é‡æ”¾AOFä¸­å¿«ç…§åçš„å¢é‡æ“ä½œ
3. å…¼é¡¾äº†é€Ÿåº¦å’Œæ•°æ®å®‰å…¨æ€§
```

### 5.2 IDç”Ÿæˆå™¨å®¹ç¾è®¾è®¡


#### ğŸ“Š å¯åŠ¨æ—¶IDæ¢å¤ç­–ç•¥


```python
class SafeRedisIdGenerator:
    def __init__(self, redis_client, backup_db_connection=None):
        self.redis = redis_client
        self.backup_db = backup_db_connection
        self.safety_gap = 1000  # å®‰å…¨é—´éš”
    
    def initialize_sequence(self, sequence_name):
        """åˆå§‹åŒ–åºåˆ—ï¼Œç¡®ä¿ä¸ä¼šäº§ç”Ÿé‡å¤ID"""
        redis_key = f"id:sequence:{sequence_name}"
        
        # 1. å°è¯•ä»Redisè·å–å½“å‰å€¼
        redis_current = self.redis.get(redis_key)
        
        if redis_current is None:
            # Redisä¸­æ²¡æœ‰å€¼ï¼Œå¯èƒ½æ˜¯é¦–æ¬¡å¯åŠ¨æˆ–æ•°æ®ä¸¢å¤±
            max_id_from_business = self._get_max_id_from_business_table(sequence_name)
            
            if max_id_from_business > 0:
                # ä»ä¸šåŠ¡è¡¨ä¸­æ‰¾åˆ°äº†æœ€å¤§IDï¼Œè®¾ç½®å®‰å…¨èµ·å§‹å€¼
                safe_start = max_id_from_business + self.safety_gap
                self.redis.set(redis_key, safe_start)
                print(f"åºåˆ— {sequence_name} ä» {safe_start} å¼€å§‹ï¼ˆåŸºäºä¸šåŠ¡è¡¨æœ€å¤§å€¼ {max_id_from_business}ï¼‰")
            else:
                # å…¨æ–°åºåˆ—ï¼Œä»1å¼€å§‹
                self.redis.set(redis_key, 1)
                print(f"åºåˆ— {sequence_name} ä» 1 å¼€å§‹ï¼ˆå…¨æ–°åºåˆ—ï¼‰")
        else:
            print(f"åºåˆ— {sequence_name} ç»§ç»­ä» {redis_current} å¼€å§‹")
    
    def _get_max_id_from_business_table(self, sequence_name):
        """ä»ä¸šåŠ¡è¡¨ä¸­è·å–æœ€å¤§ID"""
        if not self.backup_db:
            return 0
        
        # æ ¹æ®åºåˆ—åç§°æŸ¥è¯¢å¯¹åº”çš„ä¸šåŠ¡è¡¨
        table_mapping = {
            'user': 'user_info',
            'order': 'order_info',
            'product': 'product_info'
        }
        
        table_name = table_mapping.get(sequence_name)
        if not table_name:
            return 0
        
        try:
            cursor = self.backup_db.cursor()
            cursor.execute(f"SELECT MAX(id) FROM {table_name}")
            result = cursor.fetchone()
            return result[0] if result[0] else 0
        except Exception as e:
            print(f"æŸ¥è¯¢ä¸šåŠ¡è¡¨å¤±è´¥: {e}")
            return 0

# ä½¿ç”¨ç¤ºä¾‹
safe_gen = SafeRedisIdGenerator(redis_client, db_connection)

# ç³»ç»Ÿå¯åŠ¨æ—¶åˆå§‹åŒ–æ‰€æœ‰åºåˆ—
safe_gen.initialize_sequence('user')
safe_gen.initialize_sequence('order')
safe_gen.initialize_sequence('product')
```

#### ğŸ”„ åŒå†™å¤‡ä»½ç­–ç•¥


```python
class DualWriteIdGenerator:
    def __init__(self, redis_client, backup_db):
        self.redis = redis_client
        self.backup_db = backup_db
    
    def generate_id_with_backup(self, sequence_name):
        """ç”ŸæˆIDå¹¶åŒæ—¶å¤‡ä»½åˆ°æ•°æ®åº“"""
        redis_key = f"id:sequence:{sequence_name}"
        
        try:
            # 1. ä»Redisè·å–æ–°ID
            new_id = self.redis.incr(redis_key)
            
            # 2. å¼‚æ­¥å¤‡ä»½åˆ°æ•°æ®åº“ï¼ˆé¿å…å½±å“æ€§èƒ½ï¼‰
            self._backup_to_db_async(sequence_name, new_id)
            
            return new_id
            
        except redis.RedisError as e:
            # Rediså¤±è´¥ï¼Œå°è¯•ä»æ•°æ®åº“è·å–
            print(f"Rediså¤±è´¥ï¼Œåˆ‡æ¢åˆ°æ•°æ®åº“: {e}")
            return self._generate_id_from_db(sequence_name)
    
    def _backup_to_db_async(self, sequence_name, current_value):
        """å¼‚æ­¥å¤‡ä»½å½“å‰å€¼åˆ°æ•°æ®åº“"""
        import threading
        
        def backup_worker():
            try:
                cursor = self.backup_db.cursor()
                sql = """
                INSERT INTO id_backup (sequence_name, current_value, updated_at)
                VALUES (%s, %s, NOW())
                ON DUPLICATE KEY UPDATE 
                current_value = VALUES(current_value),
                updated_at = NOW()
                """
                cursor.execute(sql, (sequence_name, current_value))
                self.backup_db.commit()
            except Exception as e:
                print(f"å¤‡ä»½å¤±è´¥: {e}")
        
        # å¯åŠ¨åå°çº¿ç¨‹è¿›è¡Œå¤‡ä»½
        threading.Thread(target=backup_worker, daemon=True).start()
    
    def _generate_id_from_db(self, sequence_name):
        """ä»æ•°æ®åº“ç”ŸæˆIDï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        try:
            cursor = self.backup_db.cursor()
            
            # ä½¿ç”¨æ•°æ®åº“çš„åŸå­æ“ä½œè·å–ID
            cursor.execute("""
                UPDATE id_backup 
                SET current_value = current_value + 1 
                WHERE sequence_name = %s
            """, (sequence_name,))
            
            cursor.execute("""
                SELECT current_value FROM id_backup 
                WHERE sequence_name = %s
            """, (sequence_name,))
            
            result = cursor.fetchone()
            self.backup_db.commit()
            
            return result[0] if result else None
            
        except Exception as e:
            print(f"æ•°æ®åº“ä¹Ÿå¤±è´¥äº†: {e}")
            raise Exception("IDç”ŸæˆæœåŠ¡å®Œå…¨ä¸å¯ç”¨")

# ä½¿ç”¨ç¤ºä¾‹
dual_gen = DualWriteIdGenerator(redis_client, db_connection)

# æ­£å¸¸æƒ…å†µä¸‹ä½¿ç”¨Redisï¼Œå¼‚å¸¸æ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°æ•°æ®åº“
try:
    for i in range(10):
        user_id = dual_gen.generate_id_with_backup('user')
        print(f"ç”Ÿæˆç”¨æˆ·ID: {user_id}")
except Exception as e:
    print(f"IDç”Ÿæˆå¤±è´¥: {e}")
```

---

## 6. ğŸŒ é›†ç¾¤ä¸åˆ†å¸ƒå¼æ–¹æ¡ˆ


### 6.1 Redis Cluster IDç”Ÿæˆæ–¹æ¡ˆ


**Redisé›†ç¾¤çš„æŒ‘æˆ˜**ï¼š
åœ¨Redisé›†ç¾¤ä¸­ï¼Œä¸åŒçš„keyå¯èƒ½åˆ†å¸ƒåœ¨ä¸åŒçš„èŠ‚ç‚¹ä¸Šï¼Œéœ€è¦ä¿è¯IDç”Ÿæˆçš„ä¸€è‡´æ€§ã€‚

#### ğŸ“ ä¸€è‡´æ€§å“ˆå¸Œåˆ†é…


```python
import hashlib

class RedisClusterIdGenerator:
    def __init__(self, redis_cluster):
        self.cluster = redis_cluster
        self.node_count = len(redis_cluster.get_nodes())
    
    def generate_id(self, sequence_name):
        """åœ¨é›†ç¾¤ä¸­ç”ŸæˆID"""
        # ä¸ºæ¯ä¸ªåºåˆ—åˆ†é…å›ºå®šçš„èŠ‚ç‚¹
        node_index = self._get_node_for_sequence(sequence_name)
        key = f"id:sequence:{sequence_name}"
        
        # ç¡®ä¿keyæ€»æ˜¯è·¯ç”±åˆ°åŒä¸€ä¸ªèŠ‚ç‚¹
        key_with_tag = f"id:sequence:{{node{node_index}}}:{sequence_name}"
        
        return self.cluster.incr(key_with_tag)
    
    def _get_node_for_sequence(self, sequence_name):
        """ä¸ºåºåˆ—åˆ†é…å›ºå®šçš„èŠ‚ç‚¹"""
        # ä½¿ç”¨å“ˆå¸Œç¡®ä¿åŒä¸€åºåˆ—æ€»æ˜¯åˆ†é…åˆ°åŒä¸€èŠ‚ç‚¹
        hash_value = hashlib.md5(sequence_name.encode()).hexdigest()
        return int(hash_value, 16) % self.node_count

# ä½¿ç”¨ç¤ºä¾‹
from rediscluster import RedisCluster

startup_nodes = [
    {"host": "127.0.0.1", "port": "7000"},
    {"host": "127.0.0.1", "port": "7001"},
    {"host": "127.0.0.1", "port": "7002"}
]

cluster = RedisCluster(startup_nodes=startup_nodes, decode_responses=True)
cluster_gen = RedisClusterIdGenerator(cluster)

# ä¸åŒåºåˆ—ä¼šåˆ†é…åˆ°ä¸åŒèŠ‚ç‚¹ï¼Œä½†åŒä¸€åºåˆ—æ€»æ˜¯åœ¨åŒä¸€èŠ‚ç‚¹
user_id = cluster_gen.generate_id('user')    # å¯èƒ½åœ¨èŠ‚ç‚¹1
order_id = cluster_gen.generate_id('order')  # å¯èƒ½åœ¨èŠ‚ç‚¹2
```

#### ğŸ¯ åˆ†æ®µIDåˆ†é…ç­–ç•¥


```python
class SegmentedIdGenerator:
    def __init__(self, redis_cluster, node_id, total_nodes):
        self.cluster = redis_cluster
        self.node_id = node_id      # å½“å‰èŠ‚ç‚¹ID (0, 1, 2...)
        self.total_nodes = total_nodes  # æ€»èŠ‚ç‚¹æ•°
        self.segment_size = 1000000  # æ¯ä¸ªèŠ‚ç‚¹çš„IDæ®µå¤§å°
    
    def generate_id(self, sequence_name):
        """ç”Ÿæˆåˆ†æ®µID"""
        key = f"id:segment:{sequence_name}:node{self.node_id}"
        
        # æ¯ä¸ªèŠ‚ç‚¹åœ¨è‡ªå·±çš„æ®µå†…é€’å¢
        local_counter = self.cluster.incr(key)
        
        # è®¡ç®—å…¨å±€å”¯ä¸€ID
        # èŠ‚ç‚¹0: 1, 1000001, 2000001...
        # èŠ‚ç‚¹1: 2, 1000002, 2000002...
        # èŠ‚ç‚¹2: 3, 1000003, 2000003...
        segment_number = (local_counter - 1) // self.segment_size
        offset_in_segment = (local_counter - 1) % self.segment_size
        
        global_id = (segment_number * self.segment_size * self.total_nodes + 
                    offset_in_segment * self.total_nodes + 
                    self.node_id + 1)
        
        return global_id

# å„èŠ‚ç‚¹ä½¿ç”¨ç¤ºä¾‹
# èŠ‚ç‚¹0
gen_node0 = SegmentedIdGenerator(cluster, node_id=0, total_nodes=3)
id1 = gen_node0.generate_id('user')  # è¿”å› 1

# èŠ‚ç‚¹1  
gen_node1 = SegmentedIdGenerator(cluster, node_id=1, total_nodes=3)
id2 = gen_node1.generate_id('user')  # è¿”å› 2

# èŠ‚ç‚¹2
gen_node2 = SegmentedIdGenerator(cluster, node_id=2, total_nodes=3)
id3 = gen_node2.generate_id('user')  # è¿”å› 3
```

### 6.2 å¤šæ•°æ®ä¸­å¿ƒIDåŒæ­¥æ–¹æ¡ˆ


**è·¨æ•°æ®ä¸­å¿ƒçš„æŒ‘æˆ˜**ï¼š
```
é—®é¢˜åœºæ™¯ï¼š
â€¢ åŒ—äº¬æ•°æ®ä¸­å¿ƒï¼šç”Ÿæˆç”¨æˆ·ID 1-1000
â€¢ ä¸Šæµ·æ•°æ®ä¸­å¿ƒï¼šä¹Ÿç”Ÿæˆç”¨æˆ·ID 1-1000
â€¢ ç»“æœï¼šIDé‡å¤å†²çª

è§£å†³æ€è·¯ï¼š
â€¢ æ¯ä¸ªæ•°æ®ä¸­å¿ƒåˆ†é…ä¸åŒçš„IDæ®µ
â€¢ æˆ–è€…åœ¨IDä¸­åŒ…å«æ•°æ®ä¸­å¿ƒæ ‡è¯†
```

#### ğŸ“Š æ•°æ®ä¸­å¿ƒåˆ†æ®µæ–¹æ¡ˆ


```python
class MultiDCIdGenerator:
    def __init__(self, redis_client, datacenter_id, total_datacenters):
        self.redis = redis_client
        self.dc_id = datacenter_id      # æ•°æ®ä¸­å¿ƒID (1, 2, 3...)
        self.total_dc = total_datacenters
        self.dc_range = 1000000         # æ¯ä¸ªæ•°æ®ä¸­å¿ƒçš„IDèŒƒå›´
    
    def generate_id(self, sequence_name):
        """ç”ŸæˆåŒ…å«æ•°æ®ä¸­å¿ƒä¿¡æ¯çš„ID"""
        key = f"id:dc{self.dc_id}:{sequence_name}"
        
        # åœ¨æ•°æ®ä¸­å¿ƒå†…é€’å¢
        local_id = self.redis.incr(key)
        
        # è®¡ç®—å…¨å±€IDï¼šæ•°æ®ä¸­å¿ƒID + æœ¬åœ°ID
        # DC1: 1000001, 1000002, 1000003...
        # DC2: 2000001, 2000002, 2000003...
        # DC3: 3000001, 3000002, 3000003...
        global_id = self.dc_id * self.dc_range + local_id
        
        return global_id
    
    def parse_id(self, global_id):
        """è§£æIDï¼Œè·å–æ•°æ®ä¸­å¿ƒå’Œæœ¬åœ°IDä¿¡æ¯"""
        dc_id = global_id // self.dc_range
        local_id = global_id % self.dc_range
        return {
            'datacenter_id': dc_id,
            'local_id': local_id,
            'global_id': global_id
        }

# ä½¿ç”¨ç¤ºä¾‹
# åŒ—äº¬æ•°æ®ä¸­å¿ƒï¼ˆDC1ï¼‰
beijing_gen = MultiDCIdGenerator(redis_beijing, datacenter_id=1, total_datacenters=3)
beijing_user_id = beijing_gen.generate_id('user')  # 1000001

# ä¸Šæµ·æ•°æ®ä¸­å¿ƒï¼ˆDC2ï¼‰  
shanghai_gen = MultiDCIdGenerator(redis_shanghai, datacenter_id=2, total_datacenters=3)
shanghai_user_id = shanghai_gen.generate_id('user')  # 2000001

# è§£æIDä¿¡æ¯
id_info = beijing_gen.parse_id(beijing_user_id)
print(f"IDä¿¡æ¯: {id_info}")
# è¾“å‡º: {'datacenter_id': 1, 'local_id': 1, 'global_id': 1000001}
```

#### ğŸ”„ ä¸»ä»åŒæ­¥ç­–ç•¥


```python
class MasterSlaveIdGenerator:
    def __init__(self, master_redis, slave_redis_list, is_master=False):
        self.master = master_redis
        self.slaves = slave_redis_list
        self.is_master = is_master
    
    def generate_id(self, sequence_name):
        """ä¸»èŠ‚ç‚¹ç”ŸæˆIDï¼Œä»èŠ‚ç‚¹åŒæ­¥"""
        if self.is_master:
            return self._generate_as_master(sequence_name)
        else:
            return self._generate_as_slave(sequence_name)
    
    def _generate_as_master(self, sequence_name):
        """ä¸»èŠ‚ç‚¹ç”ŸæˆID"""
        key = f"id:master:{sequence_name}"
        new_id = self.master.incr(key)
        
        # å¼‚æ­¥åŒæ­¥åˆ°æ‰€æœ‰ä»èŠ‚ç‚¹
        self._sync_to_slaves(key, new_id)
        
        return new_id
    
    def _sync_to_slaves(self, key, value):
        """åŒæ­¥IDåˆ°ä»èŠ‚ç‚¹"""
        import threading
        
        def sync_worker():
            for slave in self.slaves:
                try:
                    slave.set(key, value)
                except Exception as e:
                    print(f"åŒæ­¥åˆ°ä»èŠ‚ç‚¹å¤±è´¥: {e}")
        
        threading.Thread(target=sync_worker, daemon=True).start()
    
    def _generate_as_slave(self, sequence_name):
        """ä»èŠ‚ç‚¹åœ¨masterä¸å¯ç”¨æ—¶ç”ŸæˆID"""
        try:
            # é¦–å…ˆå°è¯•ä»ä¸»èŠ‚ç‚¹è·å–
            key = f"id:master:{sequence_name}"
            return self.master.incr(key)
        except:
            # ä¸»èŠ‚ç‚¹ä¸å¯ç”¨ï¼Œä½¿ç”¨æœ¬åœ°ç”Ÿæˆï¼ˆå¸¦ç‰¹æ®Šæ ‡è¯†ï¼‰
            local_key = f"id:slave:{sequence_name}"
            local_id = self.slaves[0].incr(local_key)
            
            # ä»èŠ‚ç‚¹ç”Ÿæˆçš„IDä½¿ç”¨è´Ÿæ•°é¿å…å†²çª
            return -local_id

# ä½¿ç”¨ç¤ºä¾‹
master_redis = redis.Redis(host='master-host', port=6379)
slave1_redis = redis.Redis(host='slave1-host', port=6379)
slave2_redis = redis.Redis(host='slave2-host', port=6379)

# ä¸»èŠ‚ç‚¹
master_gen = MasterSlaveIdGenerator(
    master_redis, [slave1_redis, slave2_redis], is_master=True
)

# ä»èŠ‚ç‚¹
slave_gen = MasterSlaveIdGenerator(
    master_redis, [slave1_redis], is_master=False
)
```

---

## 7. âš¡ æ€§èƒ½ä¼˜åŒ–ä¸ç“¶é¢ˆåˆ†æ


### 7.1 æ€§èƒ½ç“¶é¢ˆè¯†åˆ«


**å¸¸è§æ€§èƒ½ç“¶é¢ˆ**ï¼š
```
æ•°æ®åº“æ–¹æ¡ˆç“¶é¢ˆï¼š
â€¢ å•è¡¨é”ç«äº‰ï¼šå¤§é‡å¹¶å‘æ›´æ–°åŒä¸€è¡Œ
â€¢ ç½‘ç»œå»¶è¿Ÿï¼šæ¯æ¬¡éƒ½è¦è®¿é—®æ•°æ®åº“
â€¢ è¿æ¥æ± è€—å°½ï¼šé«˜å¹¶å‘æ—¶è¿æ¥ä¸å¤Ÿç”¨

Redisæ–¹æ¡ˆç“¶é¢ˆï¼š
â€¢ å•å®ä¾‹é™åˆ¶ï¼šå•Rediså®ä¾‹QPSä¸Šé™çº¦10ä¸‡
â€¢ ç½‘ç»œå¸¦å®½ï¼šå¤§é‡å°è¯·æ±‚å ç”¨å¸¦å®½
â€¢ å†…å­˜é™åˆ¶ï¼šæ•°æ®é‡å¤§æ—¶å†…å­˜ä¸è¶³
```

#### ğŸ“Š æ€§èƒ½æµ‹è¯•å¯¹æ¯”


```python
import time
import threading
from concurrent.futures import ThreadPoolExecutor

class PerformanceTest:
    def __init__(self):
        self.db_gen = DatabaseIdGenerator(db_connection)
        self.redis_gen = RedisIdGenerator(redis_client)
        self.batch_gen = RedisBatchIdGenerator(redis_client)
    
    def test_single_request_performance(self, method, requests=1000):
        """æµ‹è¯•å•æ¬¡è¯·æ±‚æ€§èƒ½"""
        start_time = time.time()
        
        for i in range(requests):
            if method == 'database':
                self.db_gen.generate_id('user')
            elif method == 'redis':
                self.redis_gen.generate_id('user')
            elif method == 'batch':
                self.batch_gen.get_id_with_cache('user')
        
        end_time = time.time()
        duration = end_time - start_time
        qps = requests / duration
        
        return {
            'method': method,
            'requests': requests,
            'duration': duration,
            'qps': qps
        }
    
    def test_concurrent_performance(self, method, threads=10, requests_per_thread=100):
        """æµ‹è¯•å¹¶å‘æ€§èƒ½"""
        start_time = time.time()
        
        def worker():
            for _ in range(requests_per_thread):
                if method == 'database':
                    self.db_gen.generate_id('user')
                elif method == 'redis':
                    self.redis_gen.generate_id('user')
                elif method == 'batch':
                    self.batch_gen.get_id_with_cache('user')
        
        # å¯åŠ¨å¤šçº¿ç¨‹å¹¶å‘æµ‹è¯•
        with ThreadPoolExecutor(max_workers=threads) as executor:
            futures = [executor.submit(worker) for _ in range(threads)]
            for future in futures:
                future.result()
        
        end_time = time.time()
        duration = end_time - start_time
        total_requests = threads * requests_per_thread
        qps = total_requests / duration
        
        return {
            'method': method,
            'threads': threads,
            'total_requests': total_requests,
            'duration': duration,
            'qps': qps
        }

# æ‰§è¡Œæ€§èƒ½æµ‹è¯•
tester = PerformanceTest()

print("=== å•çº¿ç¨‹æ€§èƒ½æµ‹è¯• ===")
for method in ['database', 'redis', 'batch']:
    result = tester.test_single_request_performance(method, 1000)
    print(f"{result['method']}: {result['qps']:.0f} QPS")

print("\n=== å¹¶å‘æ€§èƒ½æµ‹è¯• ===")
for method in ['database', 'redis', 'batch']:
    result = tester.test_concurrent_performance(method, threads=20, requests_per_thread=50)
    print(f"{result['method']}: {result['qps']:.0f} QPS")

# å…¸å‹è¾“å‡ºç»“æœï¼š
# database: 500 QPS
# redis: 5000 QPS  
# batch: 15000 QPS
```

### 7.2 æ•°æ®åº“åºåˆ—é«˜æ€§èƒ½ä¼˜åŒ–


#### ğŸ“ è¿æ¥æ± ä¼˜åŒ–


```python
import mysql.connector.pooling

class OptimizedDatabaseIdGenerator:
    def __init__(self, pool_config):
        # åˆ›å»ºè¿æ¥æ± 
        self.pool = mysql.connector.pooling.MySQLConnectionPool(
            pool_name="id_gen_pool",
            pool_size=20,          # è¿æ¥æ± å¤§å°
            pool_reset_session=True,
            **pool_config
        )
    
    def generate_id(self, sequence_name):
        """ä¼˜åŒ–çš„IDç”Ÿæˆï¼ˆä½¿ç”¨è¿æ¥æ± ï¼‰"""
        connection = None
        try:
            # ä»æ± ä¸­è·å–è¿æ¥
            connection = self.pool.get_connection()
            cursor = connection.cursor()
            
            # ä½¿ç”¨é¢„ç¼–è¯‘è¯­å¥æé«˜æ€§èƒ½
            if not hasattr(self, '_prepared_stmt'):
                cursor.execute("""
                    PREPARE get_next_id FROM 
                    'UPDATE id_generator SET current_value = current_value + 1 
                     WHERE sequence_name = ?; 
                     SELECT current_value FROM id_generator WHERE sequence_name = ?'
                """)
                self._prepared_stmt = True
            
            # æ‰§è¡Œé¢„ç¼–è¯‘è¯­å¥
            cursor.execute("EXECUTE get_next_id USING %s, %s", (sequence_name, sequence_name))
            result = cursor.fetchone()
            connection.commit()
            
            return result[0]
            
        finally:
            if connection:
                connection.close()  # å½’è¿˜åˆ°è¿æ¥æ± 

# ä½¿ç”¨ç¤ºä¾‹
pool_config = {
    'host': 'localhost',
    'user': 'username',
    'password': 'password',
    'database': 'id_service'
}

optimized_db_gen = OptimizedDatabaseIdGenerator(pool_config)
```

#### ğŸ¯ è¯»å†™åˆ†ç¦»ä¼˜åŒ–


```python
class ReadWriteSplitIdGenerator:
    def __init__(self, master_pool, slave_pools):
        self.master_pool = master_pool  # å†™æ“ä½œç”¨ä¸»åº“
        self.slave_pools = slave_pools  # è¯»æ“ä½œç”¨ä»åº“
        self.current_slave = 0
    
    def generate_id(self, sequence_name):
        """IDç”Ÿæˆåªåœ¨ä¸»åº“è¿›è¡Œ"""
        connection = self.master_pool.get_connection()
        try:
            cursor = connection.cursor()
            cursor.execute("""
                UPDATE id_generator 
                SET current_value = current_value + 1 
                WHERE sequence_name = %s
            """, (sequence_name,))
            
            cursor.execute("""
                SELECT current_value 
                FROM id_generator 
                WHERE sequence_name = %s
            """, (sequence_name,))
            
            result = cursor.fetchone()
            connection.commit()
            return result[0]
        finally:
            connection.close()
    
    def get_current_value(self, sequence_name):
        """æŸ¥è¯¢å½“å‰å€¼ä½¿ç”¨ä»åº“ï¼ˆè´Ÿè½½å‡è¡¡ï¼‰"""
        slave_pool = self._get_next_slave()
        connection = slave_pool.get_connection()
        try:
            cursor = connection.cursor()
            cursor.execute("""
                SELECT current_value 
                FROM id_generator 
                WHERE sequence_name = %s
            """, (sequence_name,))
            
            result = cursor.fetchone()
            return result[0] if result else 0
        finally:
            connection.close()
    
    def _get_next_slave(self):
        """è½®è¯¢é€‰æ‹©ä»åº“"""
        pool = self.slave_pools[self.current_slave % len(self.slave_pools)]
        self.current_slave += 1
        return pool
```

### 7.3 Redisé«˜æ€§èƒ½ä¼˜åŒ–ç­–ç•¥


#### ğŸ“Š Pipelineæ‰¹é‡æ“ä½œ


```python
class PipelineOptimizedGenerator:
    def __init__(self, redis_client):
        self.redis = redis_client
    
    def generate_multiple_ids(self, sequence_names, counts):
        """ä½¿ç”¨pipelineæ‰¹é‡ç”Ÿæˆå¤šç§ID"""
        pipe = self.redis.pipeline()
        
        # æ‰¹é‡æ·»åŠ å‘½ä»¤åˆ°pipeline
        for seq_name, count in zip(sequence_names, counts):
            key = f"id:sequence:{seq_name}"
            for _ in range(count):
                pipe.incr(key)
        
        # ä¸€æ¬¡æ€§æ‰§è¡Œæ‰€æœ‰å‘½ä»¤
        results = pipe.execute()
        
        # æ•´ç†ç»“æœ
        result_dict = {}
        result_index = 0
        
        for seq_name, count in zip(sequence_names, counts):
            result_dict[seq_name] = results[result_index:result_index + count]
            result_index += count
        
        return result_dict

# ä½¿ç”¨ç¤ºä¾‹
pipeline_gen = PipelineOptimizedGenerator(redis_client)

# ä¸€æ¬¡æ€§ç”Ÿæˆå¤šç§ç±»å‹çš„ID
results = pipeline_gen.generate_multiple_ids(
    ['user', 'order', 'product'], 
    [5, 3, 2]
)

print("æ‰¹é‡ç”Ÿæˆç»“æœ:")
for seq_name, ids in results.items():
    print(f"{seq_name}: {ids}")
# è¾“å‡ºï¼š
# user: [1, 2, 3, 4, 5]
# order: [1, 2, 3]  
# product: [1, 2]
```

#### ğŸ”§ Luaè„šæœ¬åŸå­æ“ä½œ


```python
class LuaScriptOptimizedGenerator:
    def __init__(self, redis_client):
        self.redis = redis_client
        self._register_scripts()
    
    def _register_scripts(self):
        """æ³¨å†ŒLuaè„šæœ¬"""
        # æ‰¹é‡è·å–IDçš„Luaè„šæœ¬
        self.batch_script = self.redis.register_script("""
            local key = KEYS[1]
            local count = tonumber(ARGV[1])
            local start_val = redis.call('INCRBY', key, count) - count + 1
            local result = {}
            for i = 0, count - 1 do
                result[i + 1] = start_val + i
            end
            return result
        """)
        
        # å¤šåºåˆ—æ‰¹é‡è·å–çš„Luaè„šæœ¬
        self.multi_batch_script = self.redis.register_script("""
            local result = {}
            for i = 1, #KEYS do
                local key = KEYS[i]
                local count = tonumber(ARGV[i])
                local start_val = redis.call('INCRBY', key, count) - count + 1
                local seq_result = {}
                for j = 0, count - 1 do
                    seq_result[j + 1] = start_val + j
                end
                result[i] = seq_result
            end
            return result
        """)
    
    def get_batch_ids(self, sequence_name, count):
        """åŸå­æ€§æ‰¹é‡è·å–ID"""
        key = f"id:sequence:{sequence_name}"
        return self.batch_script(keys=[key], args=[count])
    
    def get_multi_batch_ids(self, sequences):
        """åŸå­æ€§è·å–å¤šç§åºåˆ—çš„æ‰¹é‡ID"""
        keys = [f"id:sequence:{seq}" for seq, _ in sequences]
        args = [str(count) for _, count in sequences]
        
        results = self.multi_batch_script(keys=keys, args=args)
        
        # æ•´ç†è¿”å›ç»“æœ
        result_dict = {}
        for i, (seq_name, _) in enumerate(sequences):
            result_dict[seq_name] = results[i]
        
        return result_dict

# ä½¿ç”¨ç¤ºä¾‹
lua_gen = LuaScriptOptimizedGenerator(redis_client)

# æ‰¹é‡è·å–å•ç§ID
user_ids = lua_gen.get_batch_ids('user', 10)
print(f"ç”¨æˆ·IDæ‰¹é‡: {user_ids}")

# æ‰¹é‡è·å–å¤šç§ID
multi_results = lua_gen.get_multi_batch_ids([
    ('user', 5),
    ('order', 3),
    ('product', 2)
])
print(f"å¤šç§IDæ‰¹é‡: {multi_results}")
```

---

## 8. ğŸ”„ æ··åˆæ–¹æ¡ˆè®¾è®¡


### 8.1 æ•°æ®åº“ä¸Redisæ··åˆæ¶æ„


**æ··åˆæ–¹æ¡ˆçš„ä¼˜åŠ¿**ï¼š
```
ç»“åˆä¸¤è€…ä¼˜ç‚¹ï¼š
â€¢ Redisï¼šé«˜æ€§èƒ½ï¼Œä½å»¶è¿Ÿ
â€¢ æ•°æ®åº“ï¼šæŒä¹…åŒ–ï¼Œæ•°æ®å®‰å…¨

å…¸å‹æ¶æ„ï¼š
â€¢ Redisä½œä¸ºä¸»è¦IDç”Ÿæˆå™¨ï¼ˆæ€§èƒ½ï¼‰
â€¢ æ•°æ®åº“ä½œä¸ºå¤‡ä»½å’Œæ¢å¤æœºåˆ¶ï¼ˆå¯é æ€§ï¼‰
â€¢ å®šæœŸåŒæ­¥ç¡®ä¿æ•°æ®ä¸€è‡´æ€§
```

#### ğŸ“Š æ™ºèƒ½åˆ‡æ¢ç­–ç•¥


```python
class HybridIdGenerator:
    def __init__(self, redis_client, db_connection):
        self.redis = redis_client
        self.db = db_connection
        self.redis_available = True
        self.health_check_interval = 10  # å¥åº·æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
        self.last_health_check = 0
    
    def generate_id(self, sequence_name):
        """æ™ºèƒ½é€‰æ‹©IDç”Ÿæˆæ–¹å¼"""
        current_time = time.time()
        
        # å®šæœŸæ£€æŸ¥Rediså¥åº·çŠ¶æ€
        if current_time - self.last_health_check > self.health_check_interval:
            self._check_redis_health()
            self.last_health_check = current_time
        
        if self.redis_available:
            try:
                return self._generate_from_redis(sequence_name)
            except Exception as e:
                print(f"Rediså¤±è´¥ï¼Œåˆ‡æ¢åˆ°æ•°æ®åº“: {e}")
                self.redis_available = False
                return self._generate_from_database(sequence_name)
        else:
            return self._generate_from_database(sequence_name)
    
    def _generate_from_redis(self, sequence_name):
        """ä»Redisç”ŸæˆID"""
        key = f"id:sequence:{sequence_name}"
        new_id = self.redis.incr(key)
        
        # å¼‚æ­¥å¤‡ä»½åˆ°æ•°æ®åº“
        self._backup_to_database_async(sequence_name, new_id)
        
        return new_id
    
    def _generate_from_database(self, sequence_name):
        """ä»æ•°æ®åº“ç”ŸæˆIDï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        cursor = self.db.cursor()
        try:
            cursor.execute("""
                UPDATE id_generator 
                SET current_value = current_value + 1 
                WHERE sequence_name = %s
            """, (sequence_name,))
            
            cursor.execute("""
                SELECT current_value 
                FROM id_generator 
                WHERE sequence_name = %s
            """, (sequence_name,))
            
            result = cursor.fetchone()
            self.db.commit()
            
            # å¦‚æœRedisæ¢å¤äº†ï¼Œå°è¯•åŒæ­¥æ•°æ®
            if self.redis_available:
                self._sync_db_to_redis(sequence_name, result[0])
            
            return result[0]
        finally:
            cursor.close()
    
    def _check_redis_health(self):
        """æ£€æŸ¥Rediså¥åº·çŠ¶æ€"""
        try:
            self.redis.ping()
            if not self.redis_available:
                print("Rediså·²æ¢å¤ï¼Œé‡æ–°å¯ç”¨")
                self.redis_available = True
        except:
            if self.redis_available:
                print("Redisä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°æ•°æ®åº“æ¨¡å¼")
                self.redis_available = False
    
    def _backup_to_database_async(self, sequence_name, current_value):
        """å¼‚æ­¥å¤‡ä»½Redisæ•°æ®åˆ°æ•°æ®åº“"""
        import threading
        
        def backup():
            try:
                cursor = self.db.cursor()
                cursor.execute("""
                    UPDATE id_generator 
                    SET current_value = %s 
                    WHERE sequence_name = %s AND current_value < %s
                """, (current_value, sequence_name, current_value))
                self.db.commit()
                cursor.close()
            except Exception as e:
                print(f"å¤‡ä»½å¤±è´¥: {e}")
        
        threading.Thread(target=backup, daemon=True).start()
    
    def _sync_db_to_redis(self, sequence_name, db_value):
        """å°†æ•°æ®åº“å€¼åŒæ­¥åˆ°Redis"""
        try:
            redis_key = f"id:sequence:{sequence_name}"
            redis_value = self.redis.get(redis_key)
            
            if redis_value is None or int(redis_value) < db_value:
                self.redis.set(redis_key, db_value)
                print(f"å·²åŒæ­¥ {sequence_name} åˆ°Redis: {db_value}")
        except Exception as e:
            print(f"åŒæ­¥å¤±è´¥: {e}")

# ä½¿ç”¨ç¤ºä¾‹
hybrid_gen = HybridIdGenerator(redis_client, db_connection)

# æ­£å¸¸ä½¿ç”¨ï¼Œå†…éƒ¨è‡ªåŠ¨å¤„ç†åˆ‡æ¢
for i in range(20):
    try:
        user_id = hybrid_gen.generate_id('user')
        print(f"ç”Ÿæˆç”¨æˆ·ID: {user_id}")
        time.sleep(0.1)
    except Exception as e:
        print(f"IDç”Ÿæˆå¤±è´¥: {e}")
```

### 8.2 åˆ†å±‚ç¼“å­˜ç­–ç•¥


#### ğŸ¯ å¤šçº§ç¼“å­˜æ¶æ„


```python
class TieredCacheIdGenerator:
    def __init__(self, redis_client, db_connection, local_cache_size=1000):
        self.redis = redis_client
        self.db = db_connection
        self.local_cache = {}      # æœ¬åœ°ç¼“å­˜
        self.cache_size = local_cache_size
        self.redis_batch_size = 100
    
    def generate_id(self, sequence_name):
        """åˆ†å±‚è·å–IDï¼šæœ¬åœ°ç¼“å­˜ -> Redisç¼“å­˜ -> æ•°æ®åº“"""
        # ç¬¬ä¸€å±‚ï¼šæœ¬åœ°ç¼“å­˜
        if self._has_local_cache(sequence_name):
            return self._get_from_local_cache(sequence_name)
        
        # ç¬¬äºŒå±‚ï¼šRedisç¼“å­˜
        try:
            self._refill_from_redis(sequence_name)
            return self._get_from_local_cache(sequence_name)
        except Exception as e:
            print(f"Redisè·å–å¤±è´¥: {e}")
        
        # ç¬¬ä¸‰å±‚ï¼šæ•°æ®åº“ï¼ˆæœ€åæ‰‹æ®µï¼‰
        return self._get_from_database(sequence_name)
    
    def _has_local_cache(self, sequence_name):
        """æ£€æŸ¥æœ¬åœ°ç¼“å­˜æ˜¯å¦è¿˜æœ‰ID"""
        return (sequence_name in self.local_cache and 
                len(self.local_cache[sequence_name]) > 0)
    
    def _get_from_local_cache(self, sequence_name):
        """ä»æœ¬åœ°ç¼“å­˜è·å–ID"""
        if sequence_name in self.local_cache:
            return self.local_cache[sequence_name].pop(0)
        raise Exception("æœ¬åœ°ç¼“å­˜ä¸ºç©º")
    
    def _refill_from_redis(self, sequence_name):
        """ä»Redisæ‰¹é‡è·å–IDå¡«å……æœ¬åœ°ç¼“å­˜"""
        # ä½¿ç”¨Luaè„šæœ¬æ‰¹é‡è·å–
        lua_script = """
            local key = KEYS[1]
            local count = tonumber(ARGV[1])
            local start_id = redis.call('INCRBY', key, count) - count + 1
            local result = {}
            for i = 0, count - 1 do
                result[i + 1] = start_id + i
            end
            return result
        """
        
        key = f"id:sequence:{sequence_name}"
        ids = self.redis.eval(lua_script, 1, key, self.redis_batch_size)
        
        # å­˜å…¥æœ¬åœ°ç¼“å­˜
        self.local_cache[sequence_name] = [int(id_val) for id_val in ids]
        print(f"ä»Redisæ‰¹é‡è·å–äº† {len(ids)} ä¸ª {sequence_name} ID")
    
    def _get_from_database(self, sequence_name):
        """ä»æ•°æ®åº“è·å–IDï¼ˆé™çº§æ–¹æ¡ˆï¼‰"""
        cursor = self.db.cursor()
        try:
            # æ•°æ®åº“ä¹Ÿä½¿ç”¨æ‰¹é‡è·å–æé«˜æ•ˆç‡
            cursor.execute("""
                UPDATE id_generator 
                SET current_value = current_value + %s 
                WHERE sequence_name = %s
            """, (self.redis_batch_size, sequence_name))
            
            cursor.execute("""
                SELECT current_value 
                FROM id_generator 
                WHERE sequence_name = %s
            """, (sequence_name,))
            
            end_id = cursor.fetchone()[0]
            start_id = end_id - self.redis_batch_size + 1
            
            # å¡«å……æœ¬åœ°ç¼“å­˜
            ids = list(range(start_id, end_id + 1))
            self.local_cache[sequence_name] = ids
            
            self.db.commit()
            print(f"ä»æ•°æ®åº“æ‰¹é‡è·å–äº† {len(ids)} ä¸ª {sequence_name} ID")
            
            return self._get_from_local_cache(sequence_name)
        finally:
            cursor.close()

# ä½¿ç”¨ç¤ºä¾‹
tiered_gen = TieredCacheIdGenerator(redis_client, db_connection)

# é«˜é¢‘è°ƒç”¨ï¼Œå†…éƒ¨è‡ªåŠ¨ç®¡ç†å¤šçº§ç¼“å­˜
for i in range(250):  # è¶…è¿‡æ‰¹é‡å¤§å°ï¼Œä¼šè‡ªåŠ¨é‡æ–°è·å–
    user_id = tiered_gen.generate_id('user')
    if i % 50 == 0:
        print(f"ç¬¬{i}æ¬¡è°ƒç”¨ï¼Œç”¨æˆ·ID: {user_id}")
```

### 8.3 å®¹ç¾æ¢å¤ç­–ç•¥


#### ğŸ”„ æ•°æ®ä¸€è‡´æ€§æ¢å¤


```python
class DisasterRecoveryManager:
    def __init__(self, redis_client, db_connection):
        self.redis = redis_client
        self.db = db_connection
    
    def full_recovery_check(self):
        """å…¨é¢æ¢å¤æ£€æŸ¥å’Œä¿®å¤"""
        print("å¼€å§‹å®¹ç¾æ¢å¤æ£€æŸ¥...")
        
        # 1. æ£€æŸ¥Rediså’Œæ•°æ®åº“çš„æ•°æ®ä¸€è‡´æ€§
        inconsistencies = self._check_data_consistency()
        
        # 2. ä¿®å¤å‘ç°çš„ä¸ä¸€è‡´
        if inconsistencies:
            self._repair_inconsistencies(inconsistencies)
        
        # 3. éªŒè¯ä¿®å¤ç»“æœ
        self._verify_repair_results()
        
        print("å®¹ç¾æ¢å¤æ£€æŸ¥å®Œæˆ")
    
    def _check_data_consistency(self):
        """æ£€æŸ¥æ•°æ®ä¸€è‡´æ€§"""
        inconsistencies = []
        
        # è·å–æ‰€æœ‰åºåˆ—åç§°
        cursor = self.db.cursor()
        cursor.execute("SELECT sequence_name, current_value FROM id_generator")
        db_sequences = dict(cursor.fetchall())
        cursor.close()
        
        # æ£€æŸ¥æ¯ä¸ªåºåˆ—çš„ä¸€è‡´æ€§
        for seq_name, db_value in db_sequences.items():
            redis_key = f"id:sequence:{seq_name}"
            try:
                redis_value = self.redis.get(redis_key)
                redis_value = int(redis_value) if redis_value else 0
                
                # æ£€æŸ¥æ•°å€¼å·®å¼‚
                if abs(redis_value - db_value) > 1000:  # å…è®¸å°èŒƒå›´å·®å¼‚
                    inconsistencies.append({
                        'sequence': seq_name,
                        'redis_value': redis_value,
                        'db_value': db_value,
                        'difference': redis_value - db_value
                    })
                    
            except Exception as e:
                print(f"æ£€æŸ¥åºåˆ— {seq_name} æ—¶å‡ºé”™: {e}")
        
        return inconsistencies
    
    def _repair_inconsistencies(self, inconsistencies):
        """ä¿®å¤æ•°æ®ä¸ä¸€è‡´"""
        for item in inconsistencies:
            seq_name = item['sequence']
            redis_value = item['redis_value']
            db_value = item['db_value']
            
            # ä½¿ç”¨è¾ƒå¤§çš„å€¼ä½œä¸ºæ ‡å‡†ï¼ˆä¿è¯IDä¸å›é€€ï¼‰
            max_value = max(redis_value, db_value)
            
            print(f"ä¿®å¤åºåˆ— {seq_name}: Redis={redis_value}, DB={db_value}, ç»Ÿä¸€ä¸º={max_value}")
            
            # æ›´æ–°Redis
            redis_key = f"id:sequence:{seq_name}"
            self.redis.set(redis_key, max_value)
            
            # æ›´æ–°æ•°æ®åº“
            cursor = self.db.cursor()
            cursor.execute("""
                UPDATE id_generator 
                SET current_value = %s 
                WHERE sequence_name = %s
            """, (max_value, seq_name))
            self.db.commit()
            cursor.close()
    
    def _verify_repair_results(self):
        """éªŒè¯ä¿®å¤ç»“æœ"""
        remaining_issues = self._check_data_consistency()
        if remaining_issues:
            print(f"è­¦å‘Šï¼šä»æœ‰ {len(remaining_issues)} ä¸ªåºåˆ—å­˜åœ¨ä¸ä¸€è‡´")
            for item in remaining_issues:
                print(f"  {item['sequence']}: Redis={item['redis_value']}, DB={item['db_value']}")
        else:
            print("æ‰€æœ‰åºåˆ—æ•°æ®ä¸€è‡´æ€§éªŒè¯é€šè¿‡")

# ä½¿ç”¨ç¤ºä¾‹
recovery_manager = DisasterRecoveryManager(redis_client, db_connection)

# å®šæœŸæ‰§è¡Œæ¢å¤æ£€æŸ¥ï¼ˆå¯ä»¥é…ç½®ä¸ºå®šæ—¶ä»»åŠ¡ï¼‰
recovery_manager.full_recovery_check()
```

---

## 9. ğŸ“‹ æ ¸å¿ƒè¦ç‚¹æ€»ç»“


### 9.1 å¿…é¡»æŒæ¡çš„åŸºæœ¬æ¦‚å¿µ


```
ğŸ”¸ IDç”Ÿæˆæœ¬è´¨ï¼šåœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­ä¿è¯å”¯ä¸€æ€§çš„æŠ€æœ¯æŒ‘æˆ˜
ğŸ”¸ æ•°æ®åº“åºåˆ—ï¼šåˆ©ç”¨æ•°æ®åº“çš„è‡ªå¢å­—æ®µæˆ–åºåˆ—å¯¹è±¡ç”Ÿæˆè¿ç»­ID
ğŸ”¸ Redisè®¡æ•°å™¨ï¼šåˆ©ç”¨Redisçš„INCRå‘½ä»¤åŸå­æ€§ç”Ÿæˆé«˜æ€§èƒ½ID
ğŸ”¸ æ‰¹é‡è·å–ï¼šé€šè¿‡ä¸€æ¬¡è·å–å¤šä¸ªIDå‡å°‘ç½‘ç»œå¼€é”€æå‡æ€§èƒ½
ğŸ”¸ æŒä¹…åŒ–å®¹ç¾ï¼šç¡®ä¿IDç”ŸæˆæœåŠ¡çš„é«˜å¯ç”¨å’Œæ•°æ®å®‰å…¨
ğŸ”¸ é›†ç¾¤åŒæ­¥ï¼šåœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­ä¿è¯IDå”¯ä¸€æ€§çš„ç­–ç•¥
ğŸ”¸ æ··åˆæ–¹æ¡ˆï¼šç»“åˆä¸åŒæŠ€æœ¯ä¼˜åŠ¿çš„ç»¼åˆè§£å†³æ–¹æ¡ˆ
```

### 9.2 æ–¹æ¡ˆé€‰æ‹©æŒ‡å¯¼åŸåˆ™


**ğŸ”¹ æ€§èƒ½è¦æ±‚å¯¼å‘**
```
ä½å¹¶å‘åœºæ™¯ï¼ˆ< 1000 QPSï¼‰ï¼š
â†’ æ•°æ®åº“è‡ªå¢å­—æ®µ
â†’ ç®€å•å¯é ï¼Œæ— éœ€é¢å¤–ç»„ä»¶

ä¸­ç­‰å¹¶å‘ï¼ˆ1000-10000 QPSï¼‰ï¼š
â†’ Rediså•å®ä¾‹ + æ‰¹é‡è·å–
â†’ æ€§èƒ½å¥½ï¼Œå®ç°ç›¸å¯¹ç®€å•

é«˜å¹¶å‘åœºæ™¯ï¼ˆ> 10000 QPSï¼‰ï¼š
â†’ Redisé›†ç¾¤ + åˆ†æ®µç­–ç•¥
â†’ æˆ–æ··åˆæ–¹æ¡ˆ + å¤šçº§ç¼“å­˜
```

**ğŸ”¹ å¯é æ€§è¦æ±‚å¯¼å‘**
```
æ•°æ®ä¸¢å¤±å®¹å¿åº¦ä½ï¼š
â†’ æ•°æ®åº“æ–¹æ¡ˆ + äº‹åŠ¡ä¿è¯
â†’ æˆ–æ··åˆæ–¹æ¡ˆ + å®æ—¶å¤‡ä»½

æ•°æ®ä¸¢å¤±å®¹å¿åº¦ä¸­ç­‰ï¼š
â†’ Redis + AOFæŒä¹…åŒ–
â†’ å®šæœŸå¤‡ä»½æ¢å¤ç­–ç•¥

æ•°æ®ä¸¢å¤±å®¹å¿åº¦é«˜ï¼š
â†’ çº¯å†…å­˜æ–¹æ¡ˆ
â†’ é‡å¯åé‡æ–°åˆå§‹åŒ–
```

### 9.3 å…³é”®æŠ€æœ¯è¦ç‚¹


**ğŸ”¹ åŸå­æ€§ä¿è¯**
```
æ•°æ®åº“ï¼šUPDATE + SELECT åœ¨äº‹åŠ¡ä¸­æ‰§è¡Œ
Redisï¼šINCRå‘½ä»¤å¤©ç„¶åŸå­æ€§
æ‰¹é‡æ“ä½œï¼šä½¿ç”¨Luaè„šæœ¬ä¿è¯åŸå­æ€§
åˆ†å¸ƒå¼ï¼šé€šè¿‡åˆ†æ®µé¿å…å†²çª
```

**ğŸ”¹ æ€§èƒ½ä¼˜åŒ–è¦ç‚¹**
```
å‡å°‘ç½‘ç»œå¾€è¿”ï¼šæ‰¹é‡è·å–ã€Pipelineã€è¿æ¥æ± 
å‡å°‘é”ç«äº‰ï¼šåˆ†ç‰‡ã€åˆ†æ®µã€è¯»å†™åˆ†ç¦»
ç¼“å­˜ç­–ç•¥ï¼šå¤šçº§ç¼“å­˜ã€é¢„åŠ è½½ã€å¼‚æ­¥æ›´æ–°
```

**ğŸ”¹ å®¹ç¾è®¾è®¡è¦ç‚¹**
```
æ•°æ®å¤‡ä»½ï¼šRedis + æ•°æ®åº“åŒå†™
å¥åº·æ£€æŸ¥ï¼šå®šæœŸæ£€æµ‹æœåŠ¡å¯ç”¨æ€§
è‡ªåŠ¨åˆ‡æ¢ï¼šæ•…éšœæ—¶è‡ªåŠ¨é™çº§åˆ°å¤‡ç”¨æ–¹æ¡ˆ
ä¸€è‡´æ€§æ¢å¤ï¼šå®šæœŸæ£€æŸ¥å¹¶ä¿®å¤æ•°æ®ä¸ä¸€è‡´
```

### 9.4 å®é™…åº”ç”¨å»ºè®®


**ğŸ“Š å°å‹é¡¹ç›®æ¨èæ–¹æ¡ˆ**
```sql
-- ç®€å•æœ‰æ•ˆçš„æ•°æ®åº“è‡ªå¢æ–¹æ¡ˆ
CREATE TABLE user_info (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    username VARCHAR(50),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- è·å–æ–°ID
INSERT INTO user_info (username) VALUES ('æ–°ç”¨æˆ·');
SELECT LAST_INSERT_ID();
```

**ğŸš€ ä¸­å¤§å‹é¡¹ç›®æ¨èæ–¹æ¡ˆ**
```python
# Redis + æ‰¹é‡ç¼“å­˜ + æ•°æ®åº“å¤‡ä»½
class ProductionIdGenerator:
    def __init__(self, redis_client, db_connection):
        self.redis = redis_client
        self.db = db_connection
        self.local_cache = {}
        self.batch_size = 100
    
    def generate_id(self, sequence_name):
        # ä¼˜å…ˆä»æœ¬åœ°ç¼“å­˜è·å–
        if self._has_cache(sequence_name):
            return self._get_from_cache(sequence_name)
        
        # ä»Redisæ‰¹é‡è·å–å¡«å……ç¼“å­˜
        self._refill_cache_from_redis(sequence_name)
        return self._get_from_cache(sequence_name)
```

**ğŸ’¡ æ ¸å¿ƒè®°å¿†å£è¯€**
```
é€‰æ–¹æ¡ˆçœ‹åœºæ™¯ï¼Œæ€§èƒ½å¯é è¦å¹³è¡¡
æ•°æ®åº“ç®€å•ç¨³ï¼ŒRediså¿«é€Ÿèƒ½æ‰›é‡
æ‰¹é‡è·å–å‡ç½‘ç»œï¼Œç¼“å­˜åˆ†å±‚ææ€§èƒ½
æŒä¹…å¤‡ä»½ä¿å®‰å…¨ï¼Œç›‘æ§æ¢å¤è¦è·Ÿä¸Š
æ··åˆæ–¹æ¡ˆå–é•¿è¡¥ï¼Œå®é™…åº”ç”¨æœ€é‡è¦
```
