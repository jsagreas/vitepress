---
title: 2ã€å®¡è®¡æ—¥å¿—åˆ†æå¤„ç†
---
## ğŸ“š ç›®å½•

1. [å®¡è®¡æ—¥å¿—åŸºç¡€æ¦‚å¿µ](#1-å®¡è®¡æ—¥å¿—åŸºç¡€æ¦‚å¿µ)
2. [å®¡è®¡æ—¥å¿—æ ¼å¼ä¸è§£æ](#2-å®¡è®¡æ—¥å¿—æ ¼å¼ä¸è§£æ)
3. [æ—¥å¿—èšåˆåˆ†ææŠ€æœ¯](#3-æ—¥å¿—èšåˆåˆ†ææŠ€æœ¯)
4. [å®æ—¶ç›‘æ§ä¸å‘Šè­¦ç³»ç»Ÿ](#4-å®æ—¶ç›‘æ§ä¸å‘Šè­¦ç³»ç»Ÿ)
5. [æ—¥å¿—å­˜å‚¨ä¸å½’æ¡£ç®¡ç†](#5-æ—¥å¿—å­˜å‚¨ä¸å½’æ¡£ç®¡ç†)
6. [å®‰å…¨ä¿æŠ¤ä¸åˆè§„è¦æ±‚](#6-å®‰å…¨ä¿æŠ¤ä¸åˆè§„è¦æ±‚)
7. [æ ¸å¿ƒè¦ç‚¹æ€»ç»“](#7-æ ¸å¿ƒè¦ç‚¹æ€»ç»“)

---

## 1. ğŸ“Š å®¡è®¡æ—¥å¿—åŸºç¡€æ¦‚å¿µ


### 1.1 ä»€ä¹ˆæ˜¯MySQLå®¡è®¡æ—¥å¿—


å®¡è®¡æ—¥å¿—å°±æ˜¯è®°å½•æ•°æ®åº“æ‰€æœ‰æ“ä½œæ´»åŠ¨çš„"ç›‘æ§å½•åƒ"ï¼ŒåŒ…æ‹¬è°åœ¨ä»€ä¹ˆæ—¶å€™åšäº†ä»€ä¹ˆæ“ä½œã€‚

**ğŸ”¸ æ ¸å¿ƒä½œç”¨**
```
å®‰å…¨ç›‘æ§ï¼šå‘ç°æœªæˆæƒè®¿é—®å’Œå¯ç–‘æ“ä½œ
åˆè§„è¦æ±‚ï¼šæ»¡è¶³æ³•è§„å¯¹æ•°æ®æ“ä½œçš„è¿½è¸ªè¦æ±‚
æ•…éšœæ’æŸ¥ï¼šåˆ†æé—®é¢˜å‘ç”Ÿçš„æ“ä½œåºåˆ—
æ€§èƒ½åˆ†æï¼šè¯†åˆ«å½±å“æ€§èƒ½çš„SQLè¯­å¥
```

**ğŸ”¸ MySQLå®¡è®¡æ—¥å¿—ç±»å‹**
```
General Logï¼šè®°å½•æ‰€æœ‰è¿æ¥å’ŒSQLè¯­å¥
Binary Logï¼šè®°å½•æ•°æ®å˜æ›´æ“ä½œï¼ˆä¸»è¦ç”¨äºå¤åˆ¶ï¼‰
Error Logï¼šè®°å½•é”™è¯¯å’Œè­¦å‘Šä¿¡æ¯
Slow Query Logï¼šè®°å½•æ‰§è¡Œæ—¶é—´é•¿çš„æŸ¥è¯¢
Audit Logï¼šä¼ä¸šç‰ˆä¸“ç”¨çš„è¯¦ç»†å®¡è®¡åŠŸèƒ½
```

### 1.2 å®¡è®¡æ—¥å¿—ä¸æ™®é€šæ—¥å¿—çš„åŒºåˆ«


**ğŸ“‹ åŠŸèƒ½å¯¹æ¯”è¡¨**

| ç‰¹æ€§ | **æ™®é€šæ—¥å¿—** | **å®¡è®¡æ—¥å¿—** |
|------|------------|------------|
| **è®°å½•èŒƒå›´** | `åŸºæœ¬æ“ä½œä¿¡æ¯` | `è¯¦ç»†çš„æ“ä½œä¸Šä¸‹æ–‡` |
| **å®‰å…¨ç­‰çº§** | `ä¸€èˆ¬` | `é«˜å®‰å…¨ç­‰çº§` |
| **åˆè§„æ€§** | `ä¸æ»¡è¶³` | `æ»¡è¶³æ³•è§„è¦æ±‚` |
| **å­˜å‚¨è¦æ±‚** | `å¯é€‰` | `å¼ºåˆ¶ä¸”ä¸å¯ç¯¡æ”¹` |
| **åˆ†æèƒ½åŠ›** | `ç®€å•` | `æ·±åº¦åˆ†ææ”¯æŒ` |

### 1.3 å®¡è®¡æ—¥å¿—çš„ä¸šåŠ¡ä»·å€¼


**ğŸ¯ å®é™…åº”ç”¨åœºæ™¯**
- **æ•°æ®æ³„éœ²è°ƒæŸ¥**ï¼šè¿½è¸ªæ•æ„Ÿæ•°æ®çš„è®¿é—®è®°å½•
- **å†…éƒ¨å®¡è®¡**ï¼šå‘˜å·¥æ“ä½œè¡Œä¸ºåˆè§„æ€§æ£€æŸ¥
- **æ³•è§„éµä»**ï¼šæ»¡è¶³GDPRã€SOXç­‰æ³•è§„è¦æ±‚
- **å®‰å…¨äº‹ä»¶å“åº”**ï¼šå¿«é€Ÿå®šä½å®‰å…¨é—®é¢˜æ ¹æº

---

## 2. ğŸ“ å®¡è®¡æ—¥å¿—æ ¼å¼ä¸è§£æ


### 2.1 MySQLå®¡è®¡æ—¥å¿—æ ¼å¼æ ‡å‡†


MySQLçš„å®¡è®¡æ—¥å¿—ä¸»è¦é‡‡ç”¨ä¸¤ç§æ ¼å¼ï¼šæ–‡æœ¬æ ¼å¼å’ŒJSONæ ¼å¼ã€‚

**ğŸ”¸ é€šç”¨æ—¥å¿—æ ¼å¼ç¤ºä¾‹**
```
æ—¶é—´æˆ³ è¿æ¥ID å‘½ä»¤ç±»å‹ SQLè¯­å¥
2025-09-07T10:30:15.123456Z 42 Query SELECT * FROM users WHERE id=1
2025-09-07T10:30:16.234567Z 42 Query UPDATE users SET name='John' WHERE id=1
```

**ğŸ”¸ JSONæ ¼å¼å®¡è®¡æ—¥å¿—**
```json
{
  "timestamp": "2025-09-07T10:30:15.123456Z",
  "connection_id": 42,
  "account": {
    "user": "app_user",
    "host": "192.168.1.100"
  },
  "command_class": "select",
  "sql_command": "SELECT * FROM users WHERE id=1",
  "table_list": ["db1.users"],
  "rows_affected": 1
}
```

### 2.2 æ—¥å¿—è§£æå·¥å…·ä¸æŠ€æœ¯


**ğŸ› ï¸ ä¸»è¦è§£æå·¥å…·**

**å†…ç½®è§£æå™¨ï¼š**
```bash
# MySQLè‡ªå¸¦çš„æ—¥å¿—è§£æ
mysqlbinlog --start-datetime="2025-09-07 10:00:00" binary-log-file

# é€šç”¨æ—¥å¿—åˆ†æ
grep "SELECT.*users" general.log | head -20
```

**ç¬¬ä¸‰æ–¹å·¥å…·ï¼š**
```
Percona Toolkitï¼špt-query-digest åˆ†ææ…¢æŸ¥è¯¢
ELK Stackï¼šElasticsearch + Logstash + Kibana
Fluentdï¼šæ—¥å¿—æ”¶é›†å’Œè½¬å‘
Grafanaï¼šå¯è§†åŒ–å±•ç¤º
```

### 2.3 è‡ªå®šä¹‰æ—¥å¿—è§£æè„šæœ¬


**ğŸ“Š Pythonè§£æç¤ºä¾‹**
```python
import re
import json
from datetime import datetime

class MySQLAuditParser:
    def __init__(self):
        # é€šç”¨æ—¥å¿—æ­£åˆ™è¡¨è¾¾å¼
        self.general_pattern = r'(\d{4}-\d{2}-\d{2}T\d{2}:\d{2}:\d{2}\.\d+Z)\s+(\d+)\s+(\w+)\s+(.*)'
    
    def parse_general_log(self, log_line):
        """è§£æé€šç”¨æ—¥å¿—æ ¼å¼"""
        match = re.match(self.general_pattern, log_line)
        if match:
            return {
                'timestamp': match.group(1),
                'connection_id': int(match.group(2)),
                'command': match.group(3),
                'query': match.group(4)
            }
        return None
    
    def identify_sensitive_operations(self, parsed_log):
        """è¯†åˆ«æ•æ„Ÿæ“ä½œ"""
        sensitive_keywords = ['DROP', 'DELETE', 'UPDATE', 'INSERT']
        query = parsed_log.get('query', '').upper()
        
        for keyword in sensitive_keywords:
            if keyword in query:
                return keyword
        return None

# ä½¿ç”¨ç¤ºä¾‹
parser = MySQLAuditParser()
log_entry = parser.parse_general_log(log_line)
if log_entry:
    sensitive_op = parser.identify_sensitive_operations(log_entry)
    if sensitive_op:
        print(f"å‘ç°æ•æ„Ÿæ“ä½œ: {sensitive_op}")
```

---

## 3. ğŸ“ˆ æ—¥å¿—èšåˆåˆ†ææŠ€æœ¯


### 3.1 æ—¥å¿—èšåˆåˆ†æçš„åŸºæœ¬æ¦‚å¿µ


æ—¥å¿—èšåˆåˆ†æå°±æ˜¯æŠŠå¤§é‡é›¶æ•£çš„å®¡è®¡æ—¥å¿—æ•´åˆèµ·æ¥ï¼Œä»ä¸­æ‰¾å‡ºæœ‰æ„ä¹‰çš„æ¨¡å¼å’Œè¶‹åŠ¿ã€‚

**ğŸ”¸ èšåˆåˆ†æçš„æ ¸å¿ƒç›®æ ‡**
```
æ¨¡å¼è¯†åˆ«ï¼šå‘ç°å¼‚å¸¸çš„æ“ä½œæ¨¡å¼
è¶‹åŠ¿åˆ†æï¼šåˆ†ææ“ä½œé¢‘ç‡å’Œæ—¶é—´è§„å¾‹
å…³è”åˆ†æï¼šæ‰¾å‡ºç›¸å…³æ“ä½œä¹‹é—´çš„è”ç³»
ç»Ÿè®¡æ±‡æ€»ï¼šç”Ÿæˆå„ç±»ç»Ÿè®¡æŠ¥è¡¨
```

### 3.2 å¼‚å¸¸è¡Œä¸ºè¯†åˆ«æŠ€æœ¯


**ğŸš¨ å¼‚å¸¸è¡Œä¸ºæ£€æµ‹æ–¹æ³•**

**åŸºäºè§„åˆ™çš„æ£€æµ‹ï¼š**
```sql
-- æ£€æµ‹å¼‚å¸¸ç™»å½•æ—¶é—´ï¼ˆéå·¥ä½œæ—¶é—´çš„ç™»å½•ï¼‰
SELECT 
    account_user,
    account_host,
    COUNT(*) as login_count
FROM audit_log 
WHERE HOUR(timestamp) NOT BETWEEN 8 AND 18
    AND command_class = 'connect'
    AND DATE(timestamp) = CURDATE()
GROUP BY account_user, account_host
HAVING login_count > 5;

-- æ£€æµ‹å¤§é‡æ•°æ®å¯¼å‡ºæ“ä½œ
SELECT 
    connection_id,
    account_user,
    COUNT(*) as select_count,
    SUM(rows_examined) as total_rows
FROM audit_log 
WHERE command_class = 'select'
    AND timestamp >= DATE_SUB(NOW(), INTERVAL 1 HOUR)
GROUP BY connection_id, account_user
HAVING total_rows > 100000;
```

**åŸºäºæœºå™¨å­¦ä¹ çš„æ£€æµ‹ï¼š**
```python
from sklearn.ensemble import IsolationForest
import pandas as pd

class AnomalyDetector:
    def __init__(self):
        self.model = IsolationForest(contamination=0.1)
    
    def prepare_features(self, audit_logs):
        """æå–ç‰¹å¾å‘é‡"""
        features = []
        for log in audit_logs:
            features.append([
                log['hour_of_day'],           # æ“ä½œæ—¶é—´
                log['query_length'],          # SQLé•¿åº¦
                log['affected_rows'],         # å½±å“è¡Œæ•°
                log['execution_time'],        # æ‰§è¡Œæ—¶é—´
                log['connection_frequency']   # è¿æ¥é¢‘ç‡
            ])
        return pd.DataFrame(features)
    
    def detect_anomalies(self, audit_logs):
        """æ£€æµ‹å¼‚å¸¸æ“ä½œ"""
        features = self.prepare_features(audit_logs)
        anomaly_scores = self.model.fit_predict(features)
        
        anomalies = []
        for i, score in enumerate(anomaly_scores):
            if score == -1:  # å¼‚å¸¸å€¼
                anomalies.append(audit_logs[i])
        
        return anomalies
```

### 3.3 å¤§æ•°æ®æ—¥å¿—åˆ†æå¹³å°


**ğŸ—ï¸ ç°ä»£æ—¥å¿—åˆ†ææ¶æ„**
```
æ•°æ®æºå±‚ï¼šMySQLå®¡è®¡æ—¥å¿—ã€åº”ç”¨æ—¥å¿—ã€ç³»ç»Ÿæ—¥å¿—
    â†“
é‡‡é›†å±‚ï¼šFluentd/Filebeatæ”¶é›†æ—¥å¿—
    â†“
æ¶ˆæ¯é˜Ÿåˆ—ï¼šKafkaç¼“å†²å’Œåˆ†å‘
    â†“
å¤„ç†å±‚ï¼šSpark/Flinkå®æ—¶å¤„ç†
    â†“
å­˜å‚¨å±‚ï¼šElasticsearch/HDFSå­˜å‚¨
    â†“
åˆ†æå±‚ï¼šKibana/Grafanaå¯è§†åŒ–
```

**ğŸ“Š å®æ—¶æµå¤„ç†ç¤ºä¾‹**
```python
from pyspark.streaming import StreamingContext
from pyspark.sql import SparkSession

class RealTimeAuditAnalyzer:
    def __init__(self):
        self.spark = SparkSession.builder.appName("AuditAnalysis").getOrCreate()
        self.ssc = StreamingContext(self.spark.sparkContext, 10)  # 10ç§’æ‰¹æ¬¡
    
    def process_audit_stream(self, kafka_stream):
        """å¤„ç†å®¡è®¡æ—¥å¿—æµ"""
        def analyze_batch(rdd):
            if not rdd.isEmpty():
                df = self.spark.read.json(rdd)
                
                # æ£€æµ‹é«˜é¢‘æ“ä½œ
                high_frequency = df.groupBy("account_user") \
                                  .count() \
                                  .filter("count > 100")
                
                # å‘é€å‘Šè­¦
                if high_frequency.count() > 0:
                    self.send_alert(high_frequency.collect())
        
        kafka_stream.foreachRDD(analyze_batch)
    
    def send_alert(self, suspicious_users):
        """å‘é€å‘Šè­¦æ¶ˆæ¯"""
        for user in suspicious_users:
            print(f"é«˜é¢‘æ“ä½œå‘Šè­¦: ç”¨æˆ· {user['account_user']} åœ¨10ç§’å†…æ‰§è¡Œäº† {user['count']} æ¬¡æ“ä½œ")
```

---

## 4. âš¡ å®æ—¶ç›‘æ§ä¸å‘Šè­¦ç³»ç»Ÿ


### 4.1 å®æ—¶ç›‘æ§ç³»ç»Ÿæ¶æ„


å®æ—¶ç›‘æ§å°±æ˜¯å¯¹å®¡è®¡æ—¥å¿—è¿›è¡Œ"å®æ—¶ä½“æ£€"ï¼Œä¸€æ—¦å‘ç°é—®é¢˜ç«‹å³æŠ¥è­¦ã€‚

**ğŸ”¸ ç›‘æ§ç³»ç»Ÿæ ¸å¿ƒç»„ä»¶**
```
æ—¥å¿—é‡‡é›†å™¨ï¼šå®æ—¶æ”¶é›†å®¡è®¡æ—¥å¿—
è§„åˆ™å¼•æ“ï¼šåº”ç”¨å‘Šè­¦è§„åˆ™è¿›è¡Œå®æ—¶åˆ†æ
å‘Šè­¦ç®¡ç†å™¨ï¼šç®¡ç†å‘Šè­¦çš„ç”Ÿæˆå’Œå‘é€
å¯è§†åŒ–é¢æ¿ï¼šå®æ—¶å±•ç¤ºç›‘æ§çŠ¶æ€
```

**ğŸ¯ å…³é”®ç›‘æ§æŒ‡æ ‡**
```
è¿æ¥ç›‘æ§ï¼šå¼‚å¸¸IPã€é¢‘ç¹è¿æ¥ã€å¤±è´¥ç™»å½•
æ“ä½œç›‘æ§ï¼šæ•æ„Ÿè¡¨è®¿é—®ã€å¤§æ‰¹é‡æ“ä½œã€æƒé™æå‡
æ€§èƒ½ç›‘æ§ï¼šæ…¢æŸ¥è¯¢ã€é”ç­‰å¾…ã€èµ„æºæ¶ˆè€—
å®‰å…¨ç›‘æ§ï¼šSQLæ³¨å…¥ã€ææƒæ“ä½œã€æ•°æ®å¯¼å‡º
```

### 4.2 æ™ºèƒ½å‘Šè­¦è§„åˆ™é…ç½®


**ğŸ“‹ å‘Šè­¦è§„åˆ™åˆ†çº§**

| å‘Šè­¦çº§åˆ« | **è§¦å‘æ¡ä»¶** | **å“åº”æ—¶é—´** | **å¤„ç†æ–¹å¼** |
|---------|------------|------------|------------|
| ğŸ”´ **ä¸¥é‡** | `æ•°æ®æ³„éœ²ã€æƒé™æå‡` | `ç«‹å³` | `çŸ­ä¿¡+ç”µè¯+é‚®ä»¶` |
| ğŸŸ¡ **è­¦å‘Š** | `å¼‚å¸¸è®¿é—®æ¨¡å¼` | `5åˆ†é’Ÿå†…` | `é‚®ä»¶+é’‰é’‰` |
| ğŸŸ¢ **ä¿¡æ¯** | `æ€§èƒ½é—®é¢˜` | `30åˆ†é’Ÿå†…` | `é‚®ä»¶è®°å½•` |

**âš™ï¸ è§„åˆ™é…ç½®ç¤ºä¾‹**
```yaml
# å‘Šè­¦è§„åˆ™é…ç½®æ–‡ä»¶
alert_rules:
  - name: "é¢‘ç¹å¤±è´¥ç™»å½•"
    condition: "failed_login_count > 5 in 10m"
    level: "warning"
    action: ["email", "slack"]
    
  - name: "æ•æ„Ÿè¡¨å¤§é‡è®¿é—®"
    condition: "table_name in ['users', 'orders'] and select_count > 1000 in 1h"
    level: "critical"
    action: ["email", "sms", "phone"]
    
  - name: "éå·¥ä½œæ—¶é—´å¤§é‡æ“ä½œ"
    condition: "hour not between 8 and 18 and operation_count > 100 in 1h"
    level: "warning"
    action: ["email"]
```

### 4.3 å‘Šè­¦æ¶ˆæ¯æ¨é€æœºåˆ¶


**ğŸ“² å¤šæ¸ é“å‘Šè­¦å®ç°**
```python
import requests
import smtplib
from email.mime.text import MIMEText

class AlertManager:
    def __init__(self, config):
        self.config = config
    
    def send_email_alert(self, alert_data):
        """é‚®ä»¶å‘Šè­¦"""
        msg = MIMEText(f"""
        å‘Šè­¦ç±»å‹: {alert_data['type']}
        è§¦å‘æ—¶é—´: {alert_data['timestamp']}
        è¯¦ç»†ä¿¡æ¯: {alert_data['details']}
        """)
        
        msg['Subject'] = f"[{alert_data['level']}] MySQLå®¡è®¡å‘Šè­¦"
        msg['From'] = self.config['email']['from']
        msg['To'] = ','.join(alert_data['recipients'])
        
        smtp = smtplib.SMTP(self.config['email']['smtp_server'])
        smtp.send_message(msg)
        smtp.quit()
    
    def send_webhook_alert(self, alert_data):
        """Webhookå‘Šè­¦ï¼ˆé’‰é’‰ã€ä¼å¾®ç­‰ï¼‰"""
        webhook_data = {
            "msgtype": "text",
            "text": {
                "content": f"ğŸš¨ MySQLå®¡è®¡å‘Šè­¦\n"
                          f"ç±»å‹: {alert_data['type']}\n"
                          f"æ—¶é—´: {alert_data['timestamp']}\n"
                          f"è¯¦æƒ…: {alert_data['details']}"
            }
        }
        
        response = requests.post(
            self.config['webhook']['url'],
            json=webhook_data,
            headers={'Content-Type': 'application/json'}
        )
        
        return response.status_code == 200
    
    def process_alert(self, alert_data):
        """å¤„ç†å‘Šè­¦"""
        alert_level = alert_data['level']
        
        if alert_level == 'critical':
            self.send_email_alert(alert_data)
            self.send_webhook_alert(alert_data)
            # å¯ä»¥æ·»åŠ çŸ­ä¿¡ã€ç”µè¯å‘Šè­¦
        elif alert_level == 'warning':
            self.send_email_alert(alert_data)
        else:
            # è®°å½•åˆ°æ—¥å¿—å³å¯
            print(f"ä¿¡æ¯çº§å‘Šè­¦: {alert_data}")
```

---

## 5. ğŸ’¾ æ—¥å¿—å­˜å‚¨ä¸å½’æ¡£ç®¡ç†


### 5.1 å®¡è®¡æ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†


å®¡è®¡æ—¥å¿—çš„ç”Ÿå‘½å‘¨æœŸç®¡ç†å°±åƒç®¡ç†æ–‡ä»¶æ¡£æ¡ˆï¼Œéœ€è¦åˆ†é˜¶æ®µå¤„ç†ä¸åŒæ—¶æœŸçš„æ•°æ®ã€‚

**ğŸ”¸ æ•°æ®ç”Ÿå‘½å‘¨æœŸé˜¶æ®µ**
```
çƒ­æ•°æ®ï¼ˆ0-30å¤©ï¼‰ï¼šé¢‘ç¹æŸ¥è¯¢ï¼Œé«˜æ€§èƒ½å­˜å‚¨
æ¸©æ•°æ®ï¼ˆ30å¤©-1å¹´ï¼‰ï¼šå¶å°”æŸ¥è¯¢ï¼Œå¹³è¡¡å­˜å‚¨
å†·æ•°æ®ï¼ˆ1å¹´ä»¥ä¸Šï¼‰ï¼šå¾ˆå°‘æŸ¥è¯¢ï¼Œä½æˆæœ¬å­˜å‚¨
```

**ğŸ“Š å­˜å‚¨ç­–ç•¥è®¾è®¡**
```
å®æ—¶å­˜å‚¨ï¼šRedis/å†…å­˜æ•°æ®åº“ï¼ˆæœ€è¿‘1å°æ—¶ï¼‰
    â†“
åœ¨çº¿å­˜å‚¨ï¼šMySQL/PostgreSQLï¼ˆæœ€è¿‘30å¤©ï¼‰
    â†“
è¿‘çº¿å­˜å‚¨ï¼šå¯¹è±¡å­˜å‚¨/HDFSï¼ˆ30å¤©-1å¹´ï¼‰
    â†“
å½’æ¡£å­˜å‚¨ï¼šç£å¸¦/å†·å­˜å‚¨ï¼ˆ1å¹´ä»¥ä¸Šï¼‰
```

### 5.2 æ—¥å¿—å½’æ¡£ç®¡ç†æœºåˆ¶


**ğŸ—‚ï¸ è‡ªåŠ¨å½’æ¡£è„šæœ¬**
```python
import os
import gzip
import shutil
from datetime import datetime, timedelta

class AuditLogArchiver:
    def __init__(self, config):
        self.source_dir = config['source_dir']
        self.archive_dir = config['archive_dir']
        self.retention_days = config['retention_days']
    
    def compress_old_logs(self):
        """å‹ç¼©æ—§æ—¥å¿—æ–‡ä»¶"""
        cutoff_date = datetime.now() - timedelta(days=30)
        
        for filename in os.listdir(self.source_dir):
            file_path = os.path.join(self.source_dir, filename)
            file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
            
            if file_mtime < cutoff_date and not filename.endswith('.gz'):
                # å‹ç¼©æ–‡ä»¶
                compressed_path = f"{file_path}.gz"
                with open(file_path, 'rb') as f_in:
                    with gzip.open(compressed_path, 'wb') as f_out:
                        shutil.copyfileobj(f_in, f_out)
                
                # åˆ é™¤åŸæ–‡ä»¶
                os.remove(file_path)
                print(f"å·²å‹ç¼©: {filename}")
    
    def archive_old_logs(self):
        """å½’æ¡£æ—§æ—¥å¿—åˆ°å†·å­˜å‚¨"""
        archive_date = datetime.now() - timedelta(days=self.retention_days)
        
        for filename in os.listdir(self.source_dir):
            file_path = os.path.join(self.source_dir, filename)
            file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
            
            if file_mtime < archive_date:
                # ç§»åŠ¨åˆ°å½’æ¡£ç›®å½•
                archive_path = os.path.join(self.archive_dir, filename)
                shutil.move(file_path, archive_path)
                print(f"å·²å½’æ¡£: {filename}")
    
    def cleanup_expired_archives(self):
        """æ¸…ç†è¿‡æœŸçš„å½’æ¡£æ–‡ä»¶"""
        cleanup_date = datetime.now() - timedelta(days=self.retention_days * 2)
        
        for filename in os.listdir(self.archive_dir):
            file_path = os.path.join(self.archive_dir, filename)
            file_mtime = datetime.fromtimestamp(os.path.getmtime(file_path))
            
            if file_mtime < cleanup_date:
                os.remove(file_path)
                print(f"å·²æ¸…ç†: {filename}")
```

### 5.3 é•¿æœŸå­˜å‚¨æŠ€æœ¯é€‰å‹


**ğŸ—ï¸ å­˜å‚¨æŠ€æœ¯å¯¹æ¯”**

| å­˜å‚¨ç±»å‹ | **æˆæœ¬** | **æŸ¥è¯¢æ€§èƒ½** | **é€‚ç”¨åœºæ™¯** | **ä¿ç•™æ—¶é—´** |
|---------|---------|------------|------------|------------|
| **SSDå­˜å‚¨** | `é«˜` | `æå¿«` | `å®æ—¶æŸ¥è¯¢` | `30å¤©` |
| **æœºæ¢°ç¡¬ç›˜** | `ä¸­` | `è¾ƒå¿«` | `å®šæœŸæŸ¥è¯¢` | `1å¹´` |
| **å¯¹è±¡å­˜å‚¨** | `ä½` | `ä¸€èˆ¬` | `åˆè§„å½’æ¡£` | `3-7å¹´` |
| **ç£å¸¦åº“** | `æä½` | `æ…¢` | `é•¿æœŸä¿å­˜` | `10å¹´+` |

**ğŸ” å½’æ¡£æ•°æ®æŸ¥è¯¢æ£€ç´¢**
```python
class ArchiveQueryEngine:
    def __init__(self, storage_config):
        self.storage_config = storage_config
    
    def query_archived_logs(self, start_date, end_date, filters=None):
        """æŸ¥è¯¢å½’æ¡£æ—¥å¿—"""
        results = []
        
        # æ ¹æ®æ—¶é—´èŒƒå›´ç¡®å®šå­˜å‚¨ä½ç½®
        if self.is_recent_data(start_date):
            results.extend(self.query_online_storage(start_date, end_date, filters))
        
        if self.needs_archive_query(start_date, end_date):
            results.extend(self.query_archive_storage(start_date, end_date, filters))
        
        return results
    
    def query_archive_storage(self, start_date, end_date, filters):
        """æŸ¥è¯¢å½’æ¡£å­˜å‚¨"""
        # è¿™é‡Œå¯ä»¥é›†æˆHadoopã€ElasticSearchç­‰å¤§æ•°æ®æŸ¥è¯¢å¼•æ“
        query = self.build_archive_query(start_date, end_date, filters)
        
        # ç¤ºä¾‹ï¼šä½¿ç”¨ElasticsearchæŸ¥è¯¢å½’æ¡£æ•°æ®
        from elasticsearch import Elasticsearch
        es = Elasticsearch(self.storage_config['elasticsearch']['hosts'])
        
        response = es.search(
            index="audit-archive-*",
            body=query,
            size=10000
        )
        
        return [hit['_source'] for hit in response['hits']['hits']]
    
    def build_archive_query(self, start_date, end_date, filters):
        """æ„å»ºå½’æ¡£æŸ¥è¯¢è¯­å¥"""
        query = {
            "query": {
                "bool": {
                    "must": [
                        {
                            "range": {
                                "timestamp": {
                                    "gte": start_date.isoformat(),
                                    "lte": end_date.isoformat()
                                }
                            }
                        }
                    ]
                }
            }
        }
        
        # æ·»åŠ è¿‡æ»¤æ¡ä»¶
        if filters:
            for key, value in filters.items():
                query["query"]["bool"]["must"].append({
                    "term": {key: value}
                })
        
        return query
```

---

## 6. ğŸ”’ å®‰å…¨ä¿æŠ¤ä¸åˆè§„è¦æ±‚


### 6.1 å®¡è®¡æ—¥å¿—å®‰å…¨ç®¡ç†


å®¡è®¡æ—¥å¿—æœ¬èº«ä¹Ÿéœ€è¦ä¿æŠ¤ï¼Œé˜²æ­¢è¢«æ¶æ„ç¯¡æ”¹æˆ–åˆ é™¤ã€‚

**ğŸ”¸ æ—¥å¿—å®‰å…¨ä¿æŠ¤åŸåˆ™**
```
å®Œæ•´æ€§ä¿æŠ¤ï¼šé˜²æ­¢æ—¥å¿—è¢«ç¯¡æ”¹
å¯ç”¨æ€§ä¿éšœï¼šç¡®ä¿æ—¥å¿—å¯ä»¥æ­£å¸¸è®¿é—®
æœºå¯†æ€§ä¿æŠ¤ï¼šæ•æ„Ÿä¿¡æ¯è„±æ•å¤„ç†
ä¸å¯å¦è®¤æ€§ï¼šæ“ä½œè®°å½•ä¸å¯æŠµèµ–
```

**ğŸ›¡ï¸ å®‰å…¨ä¿æŠ¤æªæ–½**
```python
import hashlib
import hmac
from cryptography.fernet import Fernet

class AuditLogSecurity:
    def __init__(self, encryption_key):
        self.cipher = Fernet(encryption_key)
        self.integrity_key = b"audit_log_integrity_key"
    
    def encrypt_sensitive_data(self, log_entry):
        """åŠ å¯†æ•æ„Ÿæ•°æ®"""
        sensitive_fields = ['sql_text', 'user_info', 'client_ip']
        
        for field in sensitive_fields:
            if field in log_entry:
                encrypted_data = self.cipher.encrypt(
                    log_entry[field].encode()
                )
                log_entry[f"{field}_encrypted"] = encrypted_data.decode()
                # åˆ é™¤æ˜æ–‡æ•°æ®
                del log_entry[field]
        
        return log_entry
    
    def calculate_integrity_hash(self, log_entry):
        """è®¡ç®—å®Œæ•´æ€§å“ˆå¸Œ"""
        log_string = json.dumps(log_entry, sort_keys=True)
        integrity_hash = hmac.new(
            self.integrity_key,
            log_string.encode(),
            hashlib.sha256
        ).hexdigest()
        
        log_entry['integrity_hash'] = integrity_hash
        return log_entry
    
    def verify_log_integrity(self, log_entry):
        """éªŒè¯æ—¥å¿—å®Œæ•´æ€§"""
        stored_hash = log_entry.pop('integrity_hash', None)
        if not stored_hash:
            return False
        
        calculated_hash = hmac.new(
            self.integrity_key,
            json.dumps(log_entry, sort_keys=True).encode(),
            hashlib.sha256
        ).hexdigest()
        
        return hmac.compare_digest(stored_hash, calculated_hash)
```

### 6.2 æ—¥å¿—åˆ†æå®‰å…¨ç­–ç•¥


**ğŸ” è®¿é—®æ§åˆ¶ç­–ç•¥**
```sql
-- åˆ›å»ºå®¡è®¡æ—¥å¿—è®¿é—®è§’è‰²
CREATE ROLE audit_readonly;
CREATE ROLE audit_analyzer;
CREATE ROLE audit_admin;

-- åˆ†é…ä¸åŒæƒé™
GRANT SELECT ON audit_log_table TO audit_readonly;
GRANT SELECT, INSERT ON audit_log_table TO audit_analyzer;
GRANT ALL PRIVILEGES ON audit_log_table TO audit_admin;

-- åˆ›å»ºç”¨æˆ·å¹¶åˆ†é…è§’è‰²
CREATE USER 'security_analyst'@'%' IDENTIFIED BY 'strong_password';
GRANT audit_analyzer TO 'security_analyst'@'%';
```

**ğŸ“ æ“ä½œå®¡è®¡è®°å½•**
```python
class AuditAccessLogger:
    def __init__(self, db_connection):
        self.db = db_connection
    
    def log_audit_access(self, user, action, query_details):
        """è®°å½•å®¡è®¡æ—¥å¿—çš„è®¿é—®æ“ä½œ"""
        access_log = {
            'timestamp': datetime.now().isoformat(),
            'user': user,
            'action': action,
            'query_details': query_details,
            'source_ip': self.get_client_ip()
        }
        
        # æ’å…¥è®¿é—®æ—¥å¿—è¡¨
        insert_sql = """
        INSERT INTO audit_access_log 
        (timestamp, user, action, query_details, source_ip)
        VALUES (%(timestamp)s, %(user)s, %(action)s, %(query_details)s, %(source_ip)s)
        """
        
        self.db.execute(insert_sql, access_log)
        self.db.commit()
    
    def get_client_ip(self):
        """è·å–å®¢æˆ·ç«¯IP"""
        # å®é™…å®ç°ä¸­éœ€è¦æ ¹æ®ç¯å¢ƒè·å–çœŸå®IP
        return "192.168.1.100"
```

### 6.3 åˆè§„è¦æ±‚ä¸æœ€ä½³å®è·µ


**ğŸ“‹ ä¸»è¦åˆè§„æ ‡å‡†**

| åˆè§„æ ‡å‡† | **æ ¸å¿ƒè¦æ±‚** | **å®¡è®¡é‡ç‚¹** |
|---------|------------|------------|
| **GDPR** | `ä¸ªäººæ•°æ®ä¿æŠ¤` | `æ•°æ®è®¿é—®è®°å½•ã€åˆ é™¤æ“ä½œ` |
| **SOX** | `è´¢åŠ¡æ•°æ®å®Œæ•´æ€§` | `è´¢åŠ¡ç›¸å…³è¡¨çš„æ‰€æœ‰æ“ä½œ` |
| **PCI DSS** | `æ”¯ä»˜å¡æ•°æ®å®‰å…¨` | `æ”¯ä»˜ä¿¡æ¯è®¿é—®å’Œå¤„ç†` |
| **HIPAA** | `åŒ»ç–—æ•°æ®éšç§` | `æ‚£è€…ä¿¡æ¯è®¿é—®æ§åˆ¶` |

**âœ… åˆè§„æœ€ä½³å®è·µæ¸…å•**
```
æ—¥å¿—ä¿ç•™è¦æ±‚ï¼š
â–¡ è‡³å°‘ä¿ç•™7å¹´çš„å®¡è®¡æ—¥å¿—
â–¡ å»ºç«‹å®Œæ•´çš„æ•°æ®ç”Ÿå‘½å‘¨æœŸç®¡ç†
â–¡ ç¡®ä¿å½’æ¡£æ•°æ®çš„å¯æ£€ç´¢æ€§

è®¿é—®æ§åˆ¶è¦æ±‚ï¼š
â–¡ å®æ–½æœ€å°æƒé™åŸåˆ™
â–¡ å®šæœŸå®¡æŸ¥ç”¨æˆ·æƒé™
â–¡ è®°å½•æ‰€æœ‰ç‰¹æƒæ“ä½œ

æ•°æ®ä¿æŠ¤è¦æ±‚ï¼š
â–¡ æ•æ„Ÿæ•°æ®åŠ å¯†å­˜å‚¨
â–¡ ä¼ è¾“è¿‡ç¨‹åŠ å¯†ä¿æŠ¤
â–¡ è®¿é—®æ—¥å¿—å®Œæ•´æ€§ä¿æŠ¤

ç›‘æ§å‘Šè­¦è¦æ±‚ï¼š
â–¡ å®æ—¶ç›‘æ§å¼‚å¸¸è¡Œä¸º
â–¡ åŠæ—¶å“åº”å®‰å…¨äº‹ä»¶
â–¡ å»ºç«‹äº‹ä»¶å“åº”æµç¨‹
```

---

## 7. ğŸ“‹ æ ¸å¿ƒè¦ç‚¹æ€»ç»“


### 7.1 å¿…é¡»æŒæ¡çš„æ ¸å¿ƒæ¦‚å¿µ


```
ğŸ”¸ å®¡è®¡æ—¥å¿—æœ¬è´¨ï¼šè®°å½•æ•°æ®åº“æ‰€æœ‰æ“ä½œçš„"é»‘åŒ£å­"
ğŸ”¸ æ—¥å¿—æ ¼å¼è§£æï¼šæ–‡æœ¬æ ¼å¼å’ŒJSONæ ¼å¼çš„è§£ææ–¹æ³•
ğŸ”¸ å¼‚å¸¸è¡Œä¸ºè¯†åˆ«ï¼šåŸºäºè§„åˆ™å’Œæœºå™¨å­¦ä¹ çš„æ£€æµ‹æŠ€æœ¯
ğŸ”¸ å®æ—¶ç›‘æ§å‘Šè­¦ï¼šæ™ºèƒ½å‘Šè­¦è§„åˆ™å’Œå¤šæ¸ é“æ¨é€
ğŸ”¸ å­˜å‚¨ç”Ÿå‘½å‘¨æœŸï¼šçƒ­ã€æ¸©ã€å†·æ•°æ®çš„åˆ†å±‚å­˜å‚¨ç­–ç•¥
ğŸ”¸ å®‰å…¨ä¿æŠ¤æªæ–½ï¼šåŠ å¯†ã€å®Œæ•´æ€§ä¿æŠ¤å’Œè®¿é—®æ§åˆ¶
```

### 7.2 å…³é”®ç†è§£è¦ç‚¹


**ğŸ”¹ ä¸ºä»€ä¹ˆéœ€è¦å®¡è®¡æ—¥å¿—åˆ†æ**
```
å®‰å…¨éœ€æ±‚ï¼šåŠæ—¶å‘ç°å®‰å…¨å¨èƒå’Œå¼‚å¸¸è¡Œä¸º
åˆè§„è¦æ±‚ï¼šæ»¡è¶³æ³•è§„å¯¹æ•°æ®æ“ä½œçš„ç›‘ç®¡è¦æ±‚
è¿ç»´éœ€æ±‚ï¼šæ•…éšœæ’æŸ¥å’Œæ€§èƒ½åˆ†æçš„é‡è¦ä¾æ®
ç®¡ç†éœ€æ±‚ï¼šäº†è§£æ•°æ®åº“ä½¿ç”¨æƒ…å†µå’Œç”¨æˆ·è¡Œä¸º
```

**ğŸ”¹ å®¡è®¡æ—¥å¿—åˆ†æçš„æŠ€æœ¯æŒ‘æˆ˜**
```
æ•°æ®é‡å¤§ï¼šTBçº§åˆ«çš„æ—¥å¿—æ•°æ®å¤„ç†
å®æ—¶æ€§è¦æ±‚ï¼šç§’çº§çš„å¼‚å¸¸æ£€æµ‹å’Œå‘Šè­¦
å­˜å‚¨æˆæœ¬ï¼šé•¿æœŸä¿å­˜çš„æˆæœ¬æ§åˆ¶
æŸ¥è¯¢æ€§èƒ½ï¼šå†å²æ•°æ®çš„å¿«é€Ÿæ£€ç´¢
å®‰å…¨è¦æ±‚ï¼šæ—¥å¿—æœ¬èº«çš„å®‰å…¨ä¿æŠ¤
```

**ğŸ”¹ åˆ†æç³»ç»Ÿçš„æ ¸å¿ƒèƒ½åŠ›**
```
å®æ—¶å¤„ç†ï¼šæµå¼è®¡ç®—å¤„ç†å®æ—¶æ—¥å¿—
æ‰¹é‡åˆ†æï¼šç¦»çº¿åˆ†æå†å²æ•°æ®
æ™ºèƒ½è¯†åˆ«ï¼šæœºå™¨å­¦ä¹ è¾…åŠ©å¼‚å¸¸æ£€æµ‹
å¯è§†åŒ–å±•ç¤ºï¼šç›´è§‚çš„ç›‘æ§é¢æ¿
è‡ªåŠ¨åŒ–å“åº”ï¼šæ™ºèƒ½å‘Šè­¦å’Œå¤„ç½®å»ºè®®
```

### 7.3 å®é™…åº”ç”¨ä»·å€¼


**ğŸ¯ ä¸šåŠ¡åœºæ™¯åº”ç”¨**
- **é‡‘èæœºæ„**ï¼šæ»¡è¶³ç›‘ç®¡è¦æ±‚ï¼Œé˜²èŒƒé‡‘èé£é™©
- **ç”µå•†å¹³å°**ï¼šä¿æŠ¤ç”¨æˆ·æ•°æ®ï¼Œç›‘æ§å¼‚å¸¸äº¤æ˜“
- **åŒ»ç–—æœºæ„**ï¼šä¿æŠ¤æ‚£è€…éšç§ï¼Œç¡®ä¿æ•°æ®åˆè§„
- **æ”¿åºœéƒ¨é—¨**ï¼šç¡®ä¿æ•°æ®å®‰å…¨ï¼Œæ»¡è¶³å®¡è®¡è¦æ±‚

**ğŸ”§ æŠ€æœ¯å®æ–½è¦ç‚¹**
- **æ¶æ„è®¾è®¡**ï¼šåˆ†å±‚å­˜å‚¨ã€å®æ—¶æµå¤„ç†
- **æ€§èƒ½ä¼˜åŒ–**ï¼šç´¢å¼•è®¾è®¡ã€æŸ¥è¯¢ä¼˜åŒ–
- **å®‰å…¨åŠ å›º**ï¼šåŠ å¯†ä¼ è¾“ã€è®¿é—®æ§åˆ¶
- **è¿ç»´ç®¡ç†**ï¼šè‡ªåŠ¨åŒ–å½’æ¡£ã€æ™ºèƒ½å‘Šè­¦

**ğŸ’¡ å‘å±•è¶‹åŠ¿**
- **AIå¢å¼º**ï¼šæœºå™¨å­¦ä¹ æå‡å¼‚å¸¸æ£€æµ‹å‡†ç¡®æ€§
- **äº‘åŸç”Ÿ**ï¼šåŸºäºå®¹å™¨å’Œå¾®æœåŠ¡çš„éƒ¨ç½²æ¶æ„
- **å®æ—¶åŒ–**ï¼šæ›´å¿«çš„å“åº”æ—¶é—´å’Œå¤„ç†èƒ½åŠ›
- **æ™ºèƒ½åŒ–**ï¼šè‡ªåŠ¨åŒ–çš„åˆ†æå’Œå“åº”æœºåˆ¶

**æ ¸å¿ƒè®°å¿†**ï¼š
- å®¡è®¡æ—¥å¿—æ˜¯æ•°æ®å®‰å…¨çš„"ç›‘æ§çœ¼"ï¼Œè®°å½•ä¸€åˆ‡æ“ä½œè¡Œä¸º
- å®æ—¶åˆ†æå‘ç°å¼‚å¸¸ï¼Œå†å²åˆ†ææ”¯æ’‘åˆè§„å®¡è®¡
- åˆ†å±‚å­˜å‚¨æ§åˆ¶æˆæœ¬ï¼Œå®‰å…¨ä¿æŠ¤ç¡®ä¿å¯ä¿¡
- æ™ºèƒ½å‘Šè­¦åŠæ—¶å“åº”ï¼Œå¯è§†åŒ–å±•ç¤ºä¾¿äºç®¡ç†