---
title: 23、数据导入导出工具对比
---
## 📚 目录

1. [数据导入导出基础概念](#1-数据导入导出基础概念)
2. [MySQL内置导入导出工具](#2-MySQL内置导入导出工具)
3. [第三方专业工具对比](#3-第三方专业工具对比)
4. [物理备份与逻辑备份](#4-物理备份与逻辑备份)
5. [安全控制与权限管理](#5-安全控制与权限管理)
6. [大数据量处理策略](#6-大数据量处理策略)
7. [字符集与编码处理](#7-字符集与编码处理)
8. [核心要点总结](#8-核心要点总结)

---

## 1. 🗂️ 数据导入导出基础概念


### 1.1 什么是数据导入导出


**🔸 基本概念理解**
数据导入导出就像**搬家**一样：
- **导出**：把数据从数据库"搬"到文件里保存
- **导入**：把文件里的数据"搬"到数据库中使用
- **目的**：备份恢复、数据迁移、数据分析

```
数据流向示意图：

数据库 ←→ 文件
  |        |
  |←导入←←|
  |→导出→→|

常见场景：
生产库 → 导出 → 文件 → 导入 → 测试库
旧系统 → 导出 → 文件 → 导入 → 新系统
```

### 1.2 逻辑备份vs物理备份


**🔸 逻辑备份（文本格式）**
```
特点：将数据转换为SQL语句
优点：可读性强，跨平台兼容
缺点：文件大，恢复慢
格式示例：
INSERT INTO users VALUES (1, 'Zhang', 'zhang@email.com');
INSERT INTO users VALUES (2, 'Li', 'li@email.com');
```

**🔸 物理备份（二进制格式）**
```
特点：直接复制数据文件
优点：速度快，文件小
缺点：平台相关，不可读
格式：二进制文件，无法直接查看内容
```

### 1.3 工具分类概览


```
MySQL数据导入导出工具全景图：

内置工具
├── mysqldump ────────── 单线程逻辑导出（官方标配）
├── mysqlpump ───────── 多线程逻辑导出（MySQL 5.7+）
├── LOAD DATA INFILE ── 高速文本导入
└── SELECT INTO OUTFILE ── 文本格式导出

第三方工具  
├── mydumper/myloader ── 多线程逻辑备份（社区推荐）
├── Percona XtraBackup ── 物理备份（企业首选）
└── pt-archiver ────── 数据归档工具（运维利器）
```

---

## 2. 🛠️ MySQL内置导入导出工具


### 2.1 mysqldump - 经典逻辑导出工具


**🔸 工具定位**
mysqldump是MySQL的"老牌元老"，就像**万能的搬家工具**，虽然速度不是最快，但是最稳定可靠。

**🔸 基本使用语法**
```bash
# 导出单个数据库
mysqldump -u用户名 -p密码 数据库名 > 备份文件.sql

# 导出单个表
mysqldump -u用户名 -p密码 数据库名 表名 > 表备份.sql

# 导出所有数据库
mysqldump -u用户名 -p密码 --all-databases > 全库备份.sql
```

**🔸 实际使用示例**
```bash
# 导出用户数据库（包含结构和数据）
mysqldump -uroot -p123456 userdb > userdb_backup.sql

# 只导出表结构，不要数据
mysqldump -uroot -p123456 --no-data userdb > userdb_structure.sql

# 只导出数据，不要表结构  
mysqldump -uroot -p123456 --no-create-info userdb > userdb_data.sql
```

**🔸 重要参数解释**
```bash
# 性能相关参数
--single-transaction    # 保证数据一致性（InnoDB表必备）
--lock-all-tables      # 锁定所有表（MyISAM表需要）
--quick               # 逐行检索，节省内存

# 数据控制参数
--where="condition"    # 按条件导出数据
--ignore-table=db.table # 忽略指定表
--complete-insert     # 生成完整INSERT语句

# 示例：导出最近一周的订单数据
mysqldump -uroot -p --where="create_time >= '2024-01-01'" \
  shop_db orders > recent_orders.sql
```

**🔸 性能特点分析**
```
mysqldump性能表现：
数据量     导出时间    文件大小    内存占用
1万条      5秒        2MB        50MB
10万条     30秒       20MB       100MB  
100万条    8分钟      200MB      200MB
1000万条   90分钟     2GB        500MB

优点：✅ 稳定可靠  ✅ 功能完整  ✅ 跨版本兼容
缺点：❌ 单线程慢  ❌ 大表耗时  ❌ 恢复速度慢
```

### 2.2 mysqlpump - 并行导出升级版


**🔸 工具定位**
mysqlpump是mysqldump的"加强版"，支持**多线程并行导出**，就像从单车道变成了多车道高速公路。

**🔸 并行导出原理**
```
mysqldump工作模式（单线程）：
表1 → 表2 → 表3 → 表4    总耗时：40分钟
 10分  10分  10分  10分

mysqlpump工作模式（4线程）：
表1 ↘
表2 ↗ 并行处理 ↘ 完成    总耗时：10分钟
表3 ↗         ↙
表4 ↙
```

**🔸 基本使用方法**
```bash
# 基本并行导出
mysqlpump -uroot -p --default-parallelism=4 userdb > userdb_fast.sql

# 高级配置：不同数据库使用不同线程数
mysqlpump -uroot -p \
  --parallel-schemas=4:userdb \
  --parallel-schemas=2:logdb \
  --all-databases > all_db_parallel.sql
```

**🔸 性能对比测试**
```
相同环境下性能对比（100万条数据）：
┌─────────────┬──────────┬──────────┬──────────┐
│    工具     │  导出时间  │  文件大小  │  CPU使用  │
├─────────────┼──────────┼──────────┼──────────┤
│ mysqldump   │   8分钟   │   200MB  │   25%    │
│ mysqlpump   │   3分钟   │   200MB  │   80%    │ 
│ 性能提升    │   62%    │    相同   │   高     │
└─────────────┴──────────┴──────────┴──────────┘

注意：mysqlpump会占用更多CPU资源
```

### 2.3 LOAD DATA INFILE - 高速数据导入


**🔸 工具定位**
LOAD DATA INFILE是MySQL的**高速导入专家**，专门用来快速导入大量文本数据，速度比INSERT语句快10-20倍。

**🔸 工作原理**
```
传统INSERT方式：
解析SQL → 执行插入 → 解析SQL → 执行插入 → ...
每条数据都要经历完整的SQL解析过程

LOAD DATA INFILE方式：
读取文件 → 批量解析 → 批量插入
一次性处理大量数据，跳过重复的SQL解析
```

**🔸 基本语法格式**
```sql
-- 从服务器本地文件导入
LOAD DATA INFILE '/path/to/data.txt'
INTO TABLE target_table
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
IGNORE 1 ROWS;

-- 从客户端文件导入
LOAD DATA LOCAL INFILE 'C:/data.csv'
INTO TABLE target_table  
FIELDS TERMINATED BY ','
OPTIONALLY ENCLOSED BY '"'
LINES TERMINATED BY '\r\n'
IGNORE 1 ROWS;
```

**🔸 实际使用示例**
```sql
-- 导入CSV格式的用户数据
LOAD DATA LOCAL INFILE 'users.csv'
INTO TABLE users
FIELDS TERMINATED BY ','
OPTIONALLY ENCLOSED BY '"'
LINES TERMINATED BY '\n'
IGNORE 1 ROWS
(name, email, phone, address);

-- CSV文件内容示例：
-- name,email,phone,address  
-- "张三","zhang@email.com","13800138000","北京市"
-- "李四","li@email.com","13900139000","上海市"
```

**🔸 性能表现对比**
```
相同数据量导入速度对比（100万条记录）：
方式                    导入时间     相对速度
INSERT逐条插入          45分钟       1x（基准）
INSERT批量插入          8分钟        5.6x
LOAD DATA INFILE       2分钟        22.5x

结论：LOAD DATA INFILE是批量导入的首选方案
```

### 2.4 SELECT INTO OUTFILE - 快速数据导出


**🔸 导出语法格式**
```sql
-- 导出到服务器本地文件
SELECT * FROM users
INTO OUTFILE '/tmp/users_export.csv'
FIELDS TERMINATED BY ','
OPTIONALLY ENCLOSED BY '"'
LINES TERMINATED BY '\n';

-- 按条件导出部分数据
SELECT name, email, phone FROM users
WHERE register_date >= '2024-01-01'
INTO OUTFILE '/tmp/new_users.csv'
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n';
```

**🔸 格式控制参数**
```sql
-- 详细格式控制示例
SELECT id, name, email, balance 
FROM users
INTO OUTFILE '/tmp/users_detailed.txt'
FIELDS 
    TERMINATED BY '\t'        -- 字段分隔符：制表符
    OPTIONALLY ENCLOSED BY '"'  -- 字符串包围符
    ESCAPED BY '\\'           -- 转义字符
LINES 
    STARTING BY '>'           -- 行开始符
    TERMINATED BY '\n';       -- 行结束符
```

---

## 3. 🔧 第三方专业工具对比


### 3.1 mydumper/myloader - 社区推荐方案


**🔸 工具优势解释**
mydumper就像**专业的搬家公司**，不仅速度快，而且服务周到：
- **多线程并发**：同时处理多个表
- **断点续传**：导出中断可以继续
- **压缩存储**：自动压缩节省空间
- **灵活恢复**：可以选择性恢复表

**🔸 安装和基本使用**
```bash
# Ubuntu/Debian安装
apt-get install mydumper

# CentOS/RHEL安装  
yum install mydumper

# 基本导出命令
mydumper -u root -p password -B userdb -c -o /backup/userdb/

# 基本导入命令
myloader -u root -p password -B userdb -d /backup/userdb/
```

**🔸 高级参数配置**
```bash
# 完整导出配置示例
mydumper \
  -u root -p123456 \           # 连接信息
  -B userdb \                  # 指定数据库
  -t 8 \                       # 8个线程并发
  -c \                         # 启用压缩
  -e \                         # 导出存储过程和函数
  --trx-consistency-only \     # 事务一致性
  -o /backup/userdb_$(date +%Y%m%d)/  # 输出目录
```

**🔸 性能对比测试**
```
大数据量导出性能对比（1000万条记录，8核服务器）：
┌─────────────┬──────────┬──────────┬──────────┬──────────┐
│    工具     │  导出时间  │  文件大小  │  线程数   │ CPU使用  │
├─────────────┼──────────┼──────────┼──────────┼──────────┤
│ mysqldump   │   90分钟  │   2.0GB  │     1    │   25%    │
│ mysqlpump   │   35分钟  │   2.0GB  │     4    │   70%    │
│ mydumper    │   15分钟  │   1.2GB  │     8    │   85%    │
└─────────────┴──────────┴──────────┴──────────┴──────────┘

mydumper优势：✅ 速度最快  ✅ 压缩率高  ✅ 功能丰富
```

### 3.2 Percona XtraBackup - 物理备份专家


**🔸 物理备份的优势**
XtraBackup就像**专业的数据搬运机器人**，直接复制数据文件，不需要转换格式：

```
逻辑备份过程：
数据 → 转换为SQL → 写入文件 → 读取文件 → 执行SQL → 恢复数据
      (耗时)     (体积大)    (耗时)   (解析慢)

物理备份过程：  
数据文件 → 直接复制 → 直接替换 → 恢复完成
         (快速)     (快速)
```

**🔸 基本使用方法**
```bash
# 完整备份
xtrabackup --backup --target-dir=/backup/full_backup/

# 增量备份（基于上次备份）
xtrabackup --backup --target-dir=/backup/inc_backup/ \
  --incremental-basedir=/backup/full_backup/

# 准备恢复（必须步骤）
xtrabackup --prepare --target-dir=/backup/full_backup/

# 恢复数据
xtrabackup --copy-back --target-dir=/backup/full_backup/
```

**🔸 热备份原理**
```
XtraBackup热备份工作原理：

步骤1：开始备份
├── 复制非InnoDB表文件（需要锁定）
├── 复制InnoDB数据文件（无需锁定）
└── 持续记录redo log变化

步骤2：完成备份  
├── 短暂锁定所有表
├── 记录最终位置信息
└── 释放锁定

总锁定时间：通常小于1秒
```

**🔸 性能表现**
```
XtraBackup vs mysqldump（100GB数据库）：
┌─────────────┬──────────┬──────────┬──────────┐
│    指标     │ XtraBackup │ mysqldump │   对比   │
├─────────────┼──────────┼──────────┼──────────┤
│  备份时间    │   30分钟  │   8小时   │   16x    │
│  恢复时间    │   10分钟  │   10小时  │   60x    │
│  服务停机    │    无     │   长时间  │   完胜   │
│  文件大小    │   80GB   │   120GB  │  33%更小  │
└─────────────┴──────────┴──────────┴──────────┘
```

### 3.3 pt-archiver - 数据归档利器


**🔸 工具作用说明**
pt-archiver是**数据清理专家**，用来处理历史数据：
- **批量删除**：安全删除大量历史数据
- **数据归档**：删除前先备份到其他表
- **性能优化**：分批处理，不影响业务

**🔸 典型使用场景**
```sql
-- 场景：日志表数据太多，需要删除3个月前的数据
-- 传统方法（危险）：
DELETE FROM access_log WHERE create_time < '2024-01-01';
-- 问题：可能锁表几小时，影响业务

-- pt-archiver方法（安全）：
pt-archiver \
  --source h=localhost,D=logdb,t=access_log \
  --where "create_time < '2024-01-01'" \
  --limit 1000 \                 # 每次处理1000条
  --commit-each \                # 每批提交事务
  --sleep 1 \                   # 每批间隔1秒
  --purge                       # 删除数据
```

**🔸 安全归档示例**
```bash
# 归档订单数据：删除前先备份
pt-archiver \
  --source h=localhost,D=shop,t=orders \
  --dest h=archive_server,D=archive,t=old_orders \
  --where "create_time < '2023-01-01'" \
  --limit 500 \
  --commit-each \
  --sleep 2

# 执行过程：
# 1. 从orders表读取500条旧数据
# 2. 插入到archive服务器的old_orders表
# 3. 从orders表删除这500条数据  
# 4. 提交事务，休息2秒
# 5. 重复直到处理完所有数据
```

---

## 4. 💾 物理备份与逻辑备份


### 4.1 备份方式详细对比


**🔸 适用场景分析**
```
┌─────────────────┬─────────────────┬─────────────────┐
│   使用场景      │    逻辑备份      │    物理备份      │
├─────────────────┼─────────────────┼─────────────────┤
│ 完整数据库备份   │       ✅        │       ✅        │
│ 部分表备份      │       ✅        │       ❌        │
│ 跨版本迁移      │       ✅        │       ⚠️        │
│ 跨平台迁移      │       ✅        │       ❌        │
│ 大数据库备份    │       ❌        │       ✅        │
│ 快速恢复       │       ❌        │       ✅        │
│ 在线热备份     │       ⚠️        │       ✅        │
│ 增量备份       │       ❌        │       ✅        │
└─────────────────┴─────────────────┴─────────────────┘
```

### 4.2 备份策略制定


**🔸 企业级备份策略**
```
备份策略决策树：

数据库大小？
    |
   <50GB ─────── 逻辑备份 ─────── mysqldump/mydumper
    |
   >50GB 
    |
需要热备份？
    |
   是 ──────── 物理备份 ─────── XtraBackup
    |  
   否 ──────── 可接受停机 ────── 物理备份（停机模式）
```

**🔸 混合备份方案**
```bash
# 企业推荐：物理备份 + 逻辑备份组合
# 每日物理备份（快速恢复）
0 2 * * * /usr/bin/xtrabackup --backup --target-dir=/backup/$(date +\%Y\%m\%d)/

# 每周逻辑备份（灵活恢复）  
0 1 * * 0 /usr/bin/mydumper -u backup_user -p password -B production_db -c -o /backup/logic/$(date +\%Y\%m\%d)/

# 备份保留策略
物理备份：保留7天（快速恢复需求）
逻辑备份：保留30天（历史数据恢复）
```

---

## 5. 🔐 安全控制与权限管理


### 5.1 secure_file_priv安全限制


**🔸 安全机制解释**
`secure_file_priv`是MySQL的**文件安全护卫**，控制数据导入导出的文件位置，防止数据泄露。

**🔸 配置选项说明**
```sql
-- 查看当前设置
SHOW VARIABLES LIKE 'secure_file_priv';

-- 可能的返回值：
┌─────────────────────┬─────────────────────────────────┐
│       值           │              含义               │
├─────────────────────┼─────────────────────────────────┤
│ NULL               │ 禁止所有文件导入导出操作         │
│ 空字符串 ""         │ 允许任意目录的文件操作          │
│ 具体路径 "/tmp/"    │ 只允许指定目录下的文件操作       │
└─────────────────────┴─────────────────────────────────┘
```

**🔸 安全配置建议**
```ini
# my.cnf配置文件中设置
[mysqld]
secure_file_priv = /var/mysql/secure/

# 目录权限设置
mkdir -p /var/mysql/secure/
chmod 750 /var/mysql/secure/
chown mysql:mysql /var/mysql/secure/
```

**🔸 实际使用影响**
```sql
-- 安全目录内的操作（成功）
SELECT * FROM users 
INTO OUTFILE '/var/mysql/secure/users.csv'
FIELDS TERMINATED BY ',';

-- 安全目录外的操作（失败）
SELECT * FROM users 
INTO OUTFILE '/tmp/users.csv'
FIELDS TERMINATED BY ',';
-- Error: secure_file_priv 限制
```

### 5.2 local_infile安全配置


**🔸 安全风险说明**
`local_infile`功能允许客户端读取本地文件，可能被恶意利用读取服务器敏感文件。

**🔸 安全配置方案**
```sql
-- 服务器端配置（my.cnf）
[mysqld]
local_infile = OFF

-- 客户端连接时控制
mysql -u user -p --local-infile=0

-- 动态修改（需要SUPER权限）
SET GLOBAL local_infile = OFF;
```

**🔸 安全使用建议**
```bash
# 安全的导入方式：使用服务器端文件
# 1. 先将文件上传到secure目录
scp data.csv mysql_server:/var/mysql/secure/

# 2. 使用服务器端LOAD DATA
mysql -e "
LOAD DATA INFILE '/var/mysql/secure/data.csv'
INTO TABLE target_table
FIELDS TERMINATED BY ',';"

# 避免使用LOCAL关键字
```

### 5.3 用户权限控制


**🔸 导入导出权限管理**
```sql
-- 创建专用备份用户
CREATE USER 'backup_user'@'localhost' IDENTIFIED BY 'strong_password';

-- 授予必要权限
GRANT SELECT, LOCK TABLES, SHOW DATABASES ON *.* TO 'backup_user'@'localhost';
GRANT RELOAD, PROCESS ON *.* TO 'backup_user'@'localhost';

-- 文件操作权限（谨慎授予）
GRANT FILE ON *.* TO 'backup_user'@'localhost';

-- 创建只读用户（数据导出）
CREATE USER 'export_user'@'%' IDENTIFIED BY 'export_password';
GRANT SELECT ON production_db.* TO 'export_user'@'%';
```

---

## 6. 📊 大数据量处理策略


### 6.1 分批处理策略


**🔸 大表导出分批方案**
```sql
-- 方案1：按主键范围分批
-- 假设users表有1000万条记录，主键id连续

-- 第1批：导出id 1-100万
SELECT * FROM users WHERE id BETWEEN 1 AND 1000000
INTO OUTFILE '/backup/users_part1.csv';

-- 第2批：导出id 100万-200万  
SELECT * FROM users WHERE id BETWEEN 1000001 AND 2000000
INTO OUTFILE '/backup/users_part2.csv';

-- 自动化脚本示例
for i in {1..10}; do
    start=$((($i-1)*1000000+1))
    end=$(($i*1000000))
    echo "导出第${i}批：${start}-${end}"
    
    mysql -e "SELECT * FROM users WHERE id BETWEEN ${start} AND ${end}
              INTO OUTFILE '/backup/users_part${i}.csv'
              FIELDS TERMINATED BY ',';"
done
```

**🔸 按时间分片导出**
```bash
# 按月份导出历史数据
for month in {01..12}; do
    echo "导出2023年${month}月数据"
    
    mysqldump -uroot -p \
      --where="DATE_FORMAT(create_time,'%Y-%m') = '2023-${month}'" \
      logdb access_log > /backup/access_log_2023${month}.sql
      
    echo "2023年${month}月导出完成"
done
```

### 6.2 性能优化策略


**🔸 导出性能优化**
```sql
-- 优化1：调整MySQL参数
SET SESSION net_buffer_length = 1024*1024*16;  -- 16MB网络缓冲
SET SESSION max_allowed_packet = 1024*1024*64; -- 64MB数据包大小

-- 优化2：使用合适的SELECT语句
-- 避免SELECT *，明确指定需要的字段
SELECT id, name, email FROM users
INTO OUTFILE '/backup/users_minimal.csv';

-- 优化3：添加合适的索引
-- 确保WHERE条件有索引支持
CREATE INDEX idx_create_time ON orders(create_time);
```

**🔸 导入性能优化**
```sql
-- 导入前的优化设置
SET autocommit = 0;                    -- 关闭自动提交
SET unique_checks = 0;                 -- 关闭唯一性检查
SET foreign_key_checks = 0;           -- 关闭外键检查
SET sql_log_bin = 0;                  -- 关闭二进制日志

-- 执行导入
LOAD DATA INFILE '/backup/large_data.csv'
INTO TABLE target_table
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n';

-- 导入后恢复设置
COMMIT;
SET autocommit = 1;
SET unique_checks = 1;
SET foreign_key_checks = 1;
SET sql_log_bin = 1;
```

### 6.3 并发处理方案


**🔸 mydumper并发配置**
```bash
# 针对不同表设置不同并发度
mydumper \
  -u root -p password \
  -t 8 \                          # 总线程数
  --rows 100000 \                 # 每个文件最多10万行
  --compress \                    # 启用压缩
  --build-empty-files \           # 为空表创建文件
  -B production_db \
  -o /backup/production_$(date +%Y%m%d)/

# 大表单独处理
mydumper \
  -u root -p password \
  -t 16 \                         # 大表使用更多线程
  -T big_table \                  # 只处理大表
  --rows 50000 \                  # 减小每个文件行数
  -B production_db \
  -o /backup/big_table_$(date +%Y%m%d)/
```

---

## 7. 🌐 字符集与编码处理


### 7.1 字符集问题诊断


**🔸 常见字符集问题**
```sql
-- 检查当前字符集设置
SHOW VARIABLES LIKE 'character_set_%';
SHOW VARIABLES LIKE 'collation_%';

-- 典型输出和问题：
┌─────────────────────┬─────────────┬─────────────────┐
│       变量          │    当前值    │     推荐值      │
├─────────────────────┼─────────────┼─────────────────┤
│character_set_server │   latin1    │     utf8mb4    │
│character_set_database│   latin1    │     utf8mb4    │ 
│character_set_client │   utf8      │     utf8mb4    │
│character_set_results│   utf8      │     utf8mb4    │
└─────────────────────┴─────────────┴─────────────────┘

问题：字符集不一致导致乱码
```

**🔸 乱码问题示例**
```sql
-- 数据库中的数据
SELECT name FROM users WHERE id = 1;
-- 显示：ä¸­æ–‡  （应该是"中文"）

-- 问题分析：
-- 数据以utf8编码存储，但以latin1编码读取
-- 解决：统一字符集设置
```

### 7.2 导入导出字符集处理


**🔸 导出时字符集控制**
```bash
# 方法1：mysqldump指定字符集
mysqldump -uroot -p \
  --default-character-set=utf8mb4 \  # 指定字符集
  --single-transaction \
  userdb > userdb_utf8.sql

# 方法2：连接时指定字符集
mysql -uroot -p --default-character-set=utf8mb4 \
  -e "SELECT * FROM users INTO OUTFILE '/tmp/users_utf8.csv';"
```

**🔸 导入时字符集处理**
```sql
-- 导入前设置字符集
SET NAMES utf8mb4;
SET CHARACTER SET utf8mb4;

-- 导入CSV文件
LOAD DATA INFILE '/tmp/data_utf8.csv'
INTO TABLE users
CHARACTER SET utf8mb4          -- 明确指定字符集
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n';
```

### 7.3 字符集转换方案


**🔸 数据库字符集升级**
```sql
-- 步骤1：备份数据（重要！）
mysqldump -uroot -p --default-character-set=utf8mb4 \
  old_db > old_db_backup.sql

-- 步骤2：修改数据库字符集
ALTER DATABASE old_db CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;

-- 步骤3：修改表字符集
ALTER TABLE users CONVERT TO CHARACTER SET utf8mb4 COLLATE utf8mb4_unicode_ci;

-- 步骤4：验证数据
SELECT name, HEX(name) FROM users WHERE id = 1;
```

**🔸 文件编码转换**
```bash
# Linux命令行转换文件编码
# 从GBK转换到UTF-8
iconv -f GBK -t UTF-8 data_gbk.csv > data_utf8.csv

# 从UTF-8转换到GBK  
iconv -f UTF-8 -t GBK data_utf8.csv > data_gbk.csv

# 检查文件编码
file -i data.csv
# 输出：data.csv: text/plain; charset=utf-8
```

---

## 8. 📋 核心要点总结


### 8.1 工具选择决策表


```
┌─────────────────┬─────────────────┬─────────────────┬─────────────────┐
│   使用场景      │    首选工具      │    备选方案      │    不推荐       │
├─────────────────┼─────────────────┼─────────────────┼─────────────────┤
│ 小数据库备份     │   mysqldump     │   mysqlpump    │       -        │
│ 大数据库备份     │   XtraBackup    │   mydumper     │  mysqldump     │
│ 部分数据导出     │ SELECT OUTFILE  │   mysqldump    │       -        │
│ 批量数据导入     │ LOAD DATA INFILE│   myloader     │  INSERT逐条     │
│ 历史数据清理     │   pt-archiver   │   分批DELETE   │  直接DELETE     │
│ 跨版本迁移      │   mydumper      │   mysqldump    │  XtraBackup    │
│ 在线热备份      │   XtraBackup    │       -        │  mysqldump     │
└─────────────────┴─────────────────┴─────────────────┴─────────────────┘
```

### 8.2 性能优化要点


**🔹 导出性能优化**
```
关键要素：
- 工具选择：大数据首选mydumper/XtraBackup
- 并行处理：合理设置线程数（通常=CPU核心数）
- 分批处理：避免单次处理过大数据量
- 网络优化：调整缓冲区大小
- 索引利用：WHERE条件确保有索引
```

**🔹 导入性能优化**
```
优化策略：
- 批量导入：使用LOAD DATA INFILE替代INSERT
- 事务控制：适当调整autocommit和批量提交
- 约束处理：导入时临时关闭检查，完成后重新开启
- 表结构：先导入数据再创建索引
- 硬件优化：使用SSD存储，增大内存
```

### 8.3 安全控制要点


**🔹 文件安全**
```
- secure_file_priv：限制文件操作目录
- local_infile：控制客户端文件读取
- 权限最小化：只授予必要权限
- 敏感数据：加密存储和传输
```

**🔹 操作安全**
```
- 备份验证：定期验证备份可用性
- 权限隔离：导入导出使用专门账号
- 操作记录：记录所有导入导出操作
- 数据审计：监控敏感数据访问
```

### 8.4 实际应用建议


**🎯 企业级最佳实践**
- **物理备份为主**：XtraBackup做全量备份和增量备份
- **逻辑备份为辅**：mydumper做灵活性备份
- **自动化运维**：编写脚本实现定时备份
- **监控告警**：监控备份任务执行状态
- **定期演练**：定期测试备份恢复流程

**🔧 故障处理预案**
- **备份失败**：检查磁盘空间、权限、网络连接
- **恢复失败**：检查字符集、版本兼容性、数据完整性
- **性能问题**：调整并发数、批量大小、硬件资源
- **安全问题**：检查文件权限、用户权限、网络安全

**核心记忆要点**：
- 小数据用mysqldump，大数据用XtraBackup
- 批量导入首选LOAD DATA INFILE，速度快20倍
- 安全第一：配置secure_file_priv和用户权限
- 字符集统一：导入导出过程保持字符集一致
- 性能优化：合理并发+分批处理+参数调优