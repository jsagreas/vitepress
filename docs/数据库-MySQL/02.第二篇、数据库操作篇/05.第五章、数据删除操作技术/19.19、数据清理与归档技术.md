---
title: 19、数据清理与归档技术
---
## 📚 目录

1. [数据清理与归档概述](#1-数据清理与归档概述)
2. [历史数据清理策略](#2-历史数据清理策略)
3. [数据归档技术实现](#3-数据归档技术实现)
4. [分区表数据清理](#4-分区表数据清理)
5. [时间窗口数据归档](#5-时间窗口数据归档)
6. [pt-archiver工具详解](#6-pt-archiver工具详解)
7. [归档数据压缩存储](#7-归档数据压缩存储)
8. [清理任务调度EVENT](#8-清理任务调度event)
9. [存储空间回收机制](#9-存储空间回收机制)
10. [监控告警体系](#10-监控告警体系)
11. [核心要点总结](#11-核心要点总结)

---

## 1. 🗂️ 数据清理与归档概述


### 1.1 为什么需要数据清理和归档


**🌰 生活类比**：
```
数据清理就像整理房间：
• 旧衣服（历史数据）占用衣柜空间
• 经常穿的（热点数据）放在方便的地方
• 不穿但有纪念价值的（归档数据）打包存储
• 完全没用的（垃圾数据）直接丢掉

数据库也是一样的道理！
```

**💡 核心问题**：
```
数据库随时间增长会遇到的问题：

🔴 存储空间：
• 磁盘空间不断增大，成本上升
• 备份文件越来越大，备份时间延长

🔴 查询性能：
• 表数据量过大，查询变慢
• 索引文件增大，内存占用增加

🔴 维护困难：
• DDL操作（加字段、建索引）耗时过长
• 数据备份和恢复时间过长

解决方案：定期清理无用数据，归档历史数据
```

### 1.2 数据清理与归档的区别


**📊 概念对比**：

| 操作类型 | **数据清理** | **数据归档** |
|---------|-------------|-------------|
| 🎯 **目的** | `彻底删除无用数据` | `转移保存历史数据` |
| 📁 **数据去向** | `直接删除` | `移动到归档存储` |
| ⏰ **可恢复性** | `不可恢复` | `可以恢复查询` |
| 💰 **存储成本** | `零成本` | `低成本存储` |
| 📋 **典型场景** | `日志、临时数据` | `订单、用户行为记录` |

### 1.3 数据生命周期管理


**📈 数据价值变化曲线**：
```
数据价值
    ↑
    │ ████ 热数据期(0-3个月)：高频访问，在线存储
    │ ███░ 温数据期(3-12个月)：偶尔访问，近线存储  
    │ ██░░ 冷数据期(1-7年)：很少访问，归档存储
    │ █░░░ 历史数据期(7年+)：合规保存，冷存储
    │ ░░░░ 过期数据：可以安全删除
    └────────────────────────────────────────→ 时间
     0    3月   1年      7年              
```

**🔄 生命周期策略**：
```
第1阶段：在线热存储（0-90天）
• 存储位置：主数据库SSD存储
• 访问特点：频繁读写，实时查询
• 优化重点：查询性能，事务一致性

第2阶段：近线温存储（3-12个月）
• 存储位置：历史库或分区表
• 访问特点：定期查询，主要读操作
• 优化重点：存储成本，查询效率

第3阶段：离线冷存储（1-7年）
• 存储位置：归档系统或对象存储
• 访问特点：偶尔查询，主要用于审计
• 优化重点：存储成本，数据完整性

第4阶段：合规或删除（7年+）
• 处理方式：根据法规要求决定保留或删除
• 考虑因素：法律合规、业务价值、存储成本
```

---

## 2. 🧹 历史数据清理策略


### 2.1 清理策略制定原则


**🎯 清理策略设计**：
```
业务维度分析：
• 哪些数据必须永久保留？（核心业务数据）
• 哪些数据有保留期限？（日志、临时数据）
• 哪些数据可以立即删除？（垃圾数据、错误数据）

技术维度分析：
• 数据量增长速度如何？
• 存储空间限制是什么？
• 查询性能要求如何？
• 备份恢复时间是否可接受？
```

**📋 清理策略示例**：

| 数据类型 | **保留期限** | **清理方式** | **业务影响** |
|---------|-------------|-------------|-------------|
| 🔍 **用户访问日志** | `30天` | `直接删除` | `无影响` |
| 📊 **系统监控数据** | `90天` | `聚合后删除明细` | `历史分析受限` |
| 🛒 **订单数据** | `7年` | `归档到冷存储` | `查询变慢但可用` |
| 👤 **用户行为数据** | `2年` | `压缩归档` | `个性化推荐受影响` |
| 💬 **聊天记录** | `1年` | `分级归档` | `历史记录查询受限` |

### 2.2 清理时机选择


**⏰ 清理时机策略**：
```
🌙 业务低峰期执行：
• 凌晨1-5点：用户访问量最少
• 周末或节假日：业务活动较少
• 避开月初月末：财务结算高峰期

📅 清理频率设计：
• 日常清理：每天清理临时数据、日志
• 周度清理：每周清理用户行为数据
• 月度清理：每月清理历史订单、归档数据
• 季度清理：每季度深度清理，空间回收
```

**🎪 实际时间安排示例**：
```
每日清理计划：
02:00 - 清理当天访问日志（保留30天）
02:30 - 清理临时文件和缓存数据
03:00 - 清理错误日志和异常记录

每周清理计划：
周日02:00 - 清理用户行为数据（保留60天）
周日03:00 - 归档订单数据到历史表
周日04:00 - 数据库空间碎片整理

每月清理计划：
月初03:00 - 深度归档历史数据
月初04:00 - 归档文件压缩和迁移
月初05:00 - 存储空间统计和告警
```

### 2.3 清理策略制定流程


**📋 策略制定步骤**：
```
步骤1：数据调研分析
├── 统计各表数据量增长趋势
├── 分析查询访问模式
├── 评估业务数据价值
└── 计算存储成本压力

步骤2：制定清理规则
├── 确定各类数据保留期限
├── 选择清理或归档方式
├── 设计清理时间窗口
└── 制定异常处理流程

步骤3：测试验证
├── 小规模数据测试
├── 性能影响评估
├── 数据完整性验证
└── 回滚方案准备

步骤4：上线执行
├── 逐步推广到生产环境
├── 监控清理效果
├── 收集性能数据
└── 持续优化调整
```

---

## 3. 📦 数据归档技术实现


### 3.1 归档表设计原则


**🏗️ 归档表结构设计**：
```
🌰 主表与归档表的关系：
想象成图书馆的管理：
• 主表 = 开架阅览区（经常用的书）
• 归档表 = 密集书库（很少用但要保存的书）
• 结构相同，存储位置不同

设计原则：
✅ 保持表结构一致性
✅ 添加归档时间字段
✅ 考虑压缩存储方式
✅ 设计合理的索引策略
```

**📋 归档表设计示例**：
```sql
-- 原始用户订单表
CREATE TABLE user_orders (
    order_id BIGINT PRIMARY KEY,
    user_id INT NOT NULL,
    order_time DATETIME NOT NULL,
    amount DECIMAL(10,2),
    status TINYINT,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_user_time (user_id, order_time),
    INDEX idx_order_time (order_time)
);

-- 对应的归档表
CREATE TABLE user_orders_archive (
    order_id BIGINT PRIMARY KEY,
    user_id INT NOT NULL,
    order_time DATETIME NOT NULL,
    amount DECIMAL(10,2),
    status TINYINT,
    created_at DATETIME,
    archived_at DATETIME DEFAULT CURRENT_TIMESTAMP,  -- 归档时间
    archive_batch VARCHAR(50),                       -- 归档批次
    INDEX idx_archive_time (archived_at),
    INDEX idx_order_time (order_time)
) ENGINE=InnoDB ROW_FORMAT=COMPRESSED;               -- 压缩存储
```

### 3.2 归档数据迁移方法


**🔄 数据迁移流程**：
```
迁移步骤详解：

步骤1：数据选择 📊
├── 根据时间条件筛选要归档的数据
├── 验证数据完整性和一致性
├── 确认没有业务依赖关系
└── 计算预计迁移数据量

步骤2：数据复制 📁
├── 将数据插入到归档表
├── 批量处理，避免长事务
├── 记录迁移日志和进度
└── 验证复制结果正确性

步骤3：数据验证 ✅
├── 比对原表和归档表数据
├── 验证记录数量一致性
├── 检查关键字段完整性
└── 确认业务逻辑正确性

步骤4：原表清理 🗑️
├── 删除已归档的数据
├── 释放存储空间
├── 更新统计信息
└── 记录清理日志
```

**💻 基础归档脚本**：
```sql
-- 归档3个月前的订单数据
DELIMITER $$

CREATE PROCEDURE archive_old_orders()
BEGIN
    DECLARE done INT DEFAULT FALSE;
    DECLARE batch_size INT DEFAULT 1000;
    DECLARE archive_date DATETIME;
    
    -- 设置归档截止时间（3个月前）
    SET archive_date = DATE_SUB(NOW(), INTERVAL 3 MONTH);
    
    -- 开始事务
    START TRANSACTION;
    
    -- 第一步：复制数据到归档表
    INSERT INTO user_orders_archive 
    (order_id, user_id, order_time, amount, status, created_at)
    SELECT order_id, user_id, order_time, amount, status, created_at
    FROM user_orders 
    WHERE order_time < archive_date
    LIMIT batch_size;
    
    -- 检查是否有数据被插入
    IF ROW_COUNT() > 0 THEN
        -- 第二步：删除原表中的数据
        DELETE FROM user_orders 
        WHERE order_time < archive_date
        LIMIT batch_size;
        
        COMMIT;
        
        -- 记录归档日志
        INSERT INTO archive_log (table_name, archive_count, archive_time)
        VALUES ('user_orders', ROW_COUNT(), NOW());
    ELSE
        ROLLBACK;
    END IF;
    
END$$

DELIMITER ;
```

### 3.3 归档策略选择


**🎯 归档策略对比**：

| 策略类型 | **实现方式** | **优点** | **缺点** | **适用场景** |
|---------|-------------|---------|---------|-------------|
| 🏃 **在线归档** | `实时或准实时迁移` | `数据新鲜，空间及时释放` | `影响在线性能` | `数据量小，性能要求高` |
| 🌙 **离线归档** | `定期批量迁移` | `对在线业务影响小` | `数据延迟，占用空间时间长` | `数据量大，性能敏感` |
| 🔄 **混合归档** | `热数据在线，冷数据离线` | `兼顾性能和效率` | `逻辑复杂，维护成本高` | `数据访问模式复杂` |

**🎪 策略选择示例**：
```
电商订单系统归档策略：

近期订单（0-30天）：
• 保留在主表，满足用户查询
• 高频访问，需要最佳性能

历史订单（30天-2年）：
• 迁移到历史表，支持查询但稍慢
• 中频访问，平衡性能和成本

古老订单（2年以上）：
• 归档到文件系统或对象存储
• 低频访问，主要用于审计合规
```

---

## 4. 🗂️ 分区表数据清理


### 4.1 什么是分区表


**🌰 分区表类比**：
```
分区表就像图书馆的分类存放：

普通表：
所有书混在一起 📚📚📚📚📚 → 找书困难

分区表：
科学类 | 文学类 | 历史类 | 计算机类
📊📊📊 | 📖📖📖 | 📜📜📜 | 💻💻💻
→ 找书快速，管理方便
```

**💡 分区表的核心优势**：
```
查询性能：
• 分区裁剪：只扫描相关分区
• 并行处理：多分区同时查询
• 索引优化：每个分区独立索引

管理效率：
• 分区删除：直接DROP分区，秒级完成
• 分区备份：独立备份恢复
• 分区维护：分别进行索引重建等操作

存储优化：
• 分区压缩：不同分区可用不同压缩策略
• 分区迁移：老分区迁移到慢速存储
```

### 4.2 MySQL分区表实现


**🔧 时间分区表创建**：
```sql
-- 按月分区的订单表
CREATE TABLE user_orders_partitioned (
    order_id BIGINT NOT NULL,
    user_id INT NOT NULL,
    order_time DATETIME NOT NULL,
    amount DECIMAL(10,2),
    status TINYINT,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    
    PRIMARY KEY (order_id, order_time),  -- 分区键必须在主键中
    INDEX idx_user_id (user_id),
    INDEX idx_status (status)
)
PARTITION BY RANGE (YEAR(order_time)*100 + MONTH(order_time)) (
    PARTITION p202401 VALUES LESS THAN (202402),
    PARTITION p202402 VALUES LESS THAN (202403),
    PARTITION p202403 VALUES LESS THAN (202404),
    PARTITION p202404 VALUES LESS THAN (202405),
    PARTITION p202405 VALUES LESS THAN (202406),
    PARTITION pmax VALUES LESS THAN MAXVALUE
);
```

**⚡ 分区自动管理**：
```sql
-- 自动创建新分区的存储过程
DELIMITER $$

CREATE PROCEDURE create_monthly_partition(
    IN table_name VARCHAR(64),
    IN months_ahead INT DEFAULT 3
)
BEGIN
    DECLARE partition_name VARCHAR(64);
    DECLARE partition_value INT;
    DECLARE target_date DATE;
    DECLARE i INT DEFAULT 1;
    
    WHILE i <= months_ahead DO
        SET target_date = DATE_ADD(CURDATE(), INTERVAL i MONTH);
        SET partition_value = YEAR(target_date) * 100 + MONTH(target_date);
        SET partition_name = CONCAT('p', partition_value);
        
        -- 动态创建分区
        SET @sql = CONCAT('ALTER TABLE ', table_name, 
                         ' ADD PARTITION (PARTITION ', partition_name,
                         ' VALUES LESS THAN (', partition_value + 1, '))');
        
        PREPARE stmt FROM @sql;
        EXECUTE stmt;
        DEALLOCATE PREPARE stmt;
        
        SET i = i + 1;
    END WHILE;
END$$

DELIMITER ;

-- 定期调用创建新分区
CALL create_monthly_partition('user_orders_partitioned', 6);
```

### 4.3 分区数据清理实现


**🗑️ 分区删除操作**：
```sql
-- 删除3个月前的分区（速度极快）
ALTER TABLE user_orders_partitioned DROP PARTITION p202401;

-- 批量删除多个历史分区
ALTER TABLE user_orders_partitioned 
DROP PARTITION p202401, p202402, p202403;
```

**📊 分区清理效率对比**：
```
传统DELETE删除 vs 分区DROP删除：

传统方式：DELETE FROM table WHERE time < '2024-01-01'
• 执行时间：几小时到几天
• 系统影响：长时间锁表，影响在线业务
• 空间回收：需要手动OPTIMIZE TABLE
• 日志开销：产生大量binlog日志

分区方式：ALTER TABLE DROP PARTITION p202401
• 执行时间：几秒钟
• 系统影响：瞬间完成，几乎无影响
• 空间回收：立即释放空间
• 日志开销：只记录DDL操作日志

性能对比：分区删除比传统删除快100-1000倍！
```

**🔄 分区清理自动化**：
```sql
-- 自动清理历史分区的EVENT
DELIMITER $$

CREATE EVENT auto_drop_old_partitions
ON SCHEDULE EVERY 1 DAY
STARTS '2025-01-01 02:00:00'
DO
BEGIN
    DECLARE partition_to_drop VARCHAR(64);
    DECLARE cutoff_value INT;
    
    -- 计算3个月前的分区值
    SET cutoff_value = (YEAR(DATE_SUB(NOW(), INTERVAL 3 MONTH)) * 100 + 
                       MONTH(DATE_SUB(NOW(), INTERVAL 3 MONTH)));
    SET partition_to_drop = CONCAT('p', cutoff_value);
    
    -- 检查分区是否存在
    SET @check_sql = CONCAT(
        'SELECT COUNT(*) INTO @partition_exists FROM information_schema.PARTITIONS ',
        'WHERE TABLE_NAME = "user_orders_partitioned" AND PARTITION_NAME = "', 
        partition_to_drop, '"'
    );
    
    PREPARE stmt FROM @check_sql;
    EXECUTE stmt;
    DEALLOCATE PREPARE stmt;
    
    -- 如果分区存在则删除
    IF @partition_exists > 0 THEN
        SET @drop_sql = CONCAT('ALTER TABLE user_orders_partitioned DROP PARTITION ', partition_to_drop);
        PREPARE stmt FROM @drop_sql;
        EXECUTE stmt;
        DEALLOCATE PREPARE stmt;
        
        -- 记录清理日志
        INSERT INTO cleanup_log (operation, table_name, partition_name, cleanup_time)
        VALUES ('DROP_PARTITION', 'user_orders_partitioned', partition_to_drop, NOW());
    END IF;
    
END$$

DELIMITER ;

-- 启用EVENT调度器
SET GLOBAL event_scheduler = ON;
```

---

## 5. ⏰ 时间窗口数据归档


### 5.1 时间窗口概念


**🕐 时间窗口归档原理**：
```
🌰 时间窗口就像流水线生产：

传送带模型：
[新数据] → [热数据区] → [温数据区] → [冷数据区] → [归档删除]
    ↓         ↓          ↓          ↓          ↓
   实时      0-30天     30-90天    90天-2年    2年+

每个时间窗口有不同的：
• 存储位置：SSD → SATA → 归档存储
• 访问频率：高频 → 中频 → 低频
• 查询性能：极快 → 较快 → 慢但可用
```

### 5.2 滑动窗口实现


**📱 滑动窗口机制**：
```
滑动窗口示例（保留90天数据）：

第1天：[1天数据]
第30天：[30天数据]
第90天：[90天数据] ← 窗口填满
第91天：[2-91天数据] ← 删除第1天，添加第91天
第92天：[3-92天数据] ← 删除第2天，添加第92天

实现机制：
• 每天删除90天前的数据
• 每天添加新的当天数据
• 保持窗口大小恒定
```

**⚙️ 滑动窗口实现代码**：
```sql
-- 滑动窗口数据清理存储过程
DELIMITER $$

CREATE PROCEDURE sliding_window_cleanup(
    IN table_name VARCHAR(64),
    IN time_column VARCHAR(64), 
    IN keep_days INT DEFAULT 90
)
BEGIN
    DECLARE cleanup_date DATETIME;
    DECLARE affected_rows INT DEFAULT 0;
    
    -- 计算清理截止日期
    SET cleanup_date = DATE_SUB(NOW(), INTERVAL keep_days DAY);
    
    -- 构建删除语句
    SET @sql = CONCAT(
        'DELETE FROM ', table_name, 
        ' WHERE ', time_column, ' < "', cleanup_date, '"',
        ' LIMIT 10000'  -- 分批删除，避免长事务
    );
    
    -- 执行清理
    cleanup_loop: LOOP
        PREPARE stmt FROM @sql;
        EXECUTE stmt;
        DEALLOCATE PREPARE stmt;
        
        SET affected_rows = ROW_COUNT();
        
        -- 如果没有更多数据需要清理，退出循环
        IF affected_rows = 0 THEN
            LEAVE cleanup_loop;
        END IF;
        
        -- 记录清理进度
        INSERT INTO cleanup_progress (table_name, deleted_rows, cleanup_time)
        VALUES (table_name, affected_rows, NOW());
        
        -- 短暂休息，避免占用过多资源
        SELECT SLEEP(0.1);
        
    END LOOP cleanup_loop;
    
END$$

DELIMITER ;

-- 定期执行滑动窗口清理
CALL sliding_window_cleanup('user_access_log', 'access_time', 30);
CALL sliding_window_cleanup('system_monitor_log', 'monitor_time', 90);
```

### 5.3 多时间窗口策略


**📅 多层次时间窗口**：
```
分层归档策略示例：

实时层（0-7天）：
• 存储：主库SSD，最高性能
• 查询：毫秒级响应
• 用途：实时业务查询

近期层（7-30天）：
• 存储：主库SATA或历史库
• 查询：秒级响应
• 用途：用户历史查询、客服支持

历史层（30天-1年）：
• 存储：归档库，压缩存储
• 查询：分钟级响应
• 用途：数据分析、业务审计

冷存储层（1年以上）：
• 存储：对象存储或磁带
• 查询：小时级响应（需要恢复）
• 用途：合规保存、特殊调查
```

**🔄 多窗口自动迁移**：
```sql
-- 多层次数据迁移EVENT
DELIMITER $$

CREATE EVENT multi_tier_archive
ON SCHEDULE EVERY 1 DAY
STARTS '2025-01-01 03:00:00'
DO
BEGIN
    -- 第1层：7天前数据迁移到近期层
    INSERT INTO user_orders_recent 
    SELECT * FROM user_orders 
    WHERE order_time BETWEEN DATE_SUB(NOW(), INTERVAL 30 DAY) 
                         AND DATE_SUB(NOW(), INTERVAL 7 DAY);
    
    -- 第2层：30天前数据迁移到历史层  
    INSERT INTO user_orders_archive
    SELECT *, NOW() as archived_at FROM user_orders_recent
    WHERE order_time < DATE_SUB(NOW(), INTERVAL 30 DAY);
    
    -- 第3层：1年前数据导出到文件系统
    SELECT * INTO OUTFILE '/archive/orders_old.csv'
    FIELDS TERMINATED BY ',' ENCLOSED BY '"'
    FROM user_orders_archive 
    WHERE order_time < DATE_SUB(NOW(), INTERVAL 1 YEAR);
    
    -- 清理已迁移的数据
    DELETE FROM user_orders WHERE order_time < DATE_SUB(NOW(), INTERVAL 7 DAY);
    DELETE FROM user_orders_recent WHERE order_time < DATE_SUB(NOW(), INTERVAL 30 DAY);
    DELETE FROM user_orders_archive WHERE order_time < DATE_SUB(NOW(), INTERVAL 1 YEAR);
    
END$$

DELIMITER ;
```

---

## 6. 🛠️ pt-archiver工具详解


### 6.1 pt-archiver工具介绍


**🔧 什么是pt-archiver**：
```
🌰 pt-archiver就像专业的搬家公司：

普通搬家（手工DELETE）：
• 一件件搬，速度慢
• 容易出错，可能损坏
• 影响邻居（阻塞其他操作）

专业搬家（pt-archiver）：
• 有计划有步骤，效率高
• 专业工具，安全可靠  
• 不影响邻居（最小锁定）

pt-archiver特点：
✅ 增量归档，支持断点续传
✅ 主从安全，不影响复制
✅ 限流控制，不影响在线业务
✅ 数据校验，确保完整性
```

**📦 安装和基本使用**：
```bash
# 安装percona-toolkit
yum install percona-toolkit

# 或者
apt-get install percona-toolkit

# 基本语法
pt-archiver [OPTIONS] --source DSN --dest DSN --where WHERE_CLAUSE
```

### 6.2 pt-archiver核心参数


**⚙️ 重要参数详解**：
```bash
# 核心参数说明

--source：源数据库连接
格式：h=host,D=database,t=table,u=user,p=password

--dest：目标归档位置  
文件：--file /archive/data.txt
表：h=archive_host,D=archive_db,t=archive_table

--where：归档条件
示例：--where "created_at < DATE_SUB(NOW(), INTERVAL 90 DAY)"

--limit：每次处理行数（控制性能影响）
推荐：--limit 1000

--sleep：每批次间休息时间（秒）
推荐：--sleep 0.1

--txn-size：事务大小
推荐：--txn-size 1000

--progress：显示进度
格式：--progress 10000（每处理1万行显示进度）
```

### 6.3 pt-archiver实战示例


**💻 归档到文件示例**：
```bash
# 将90天前的日志归档到文件
pt-archiver \
  --source h=localhost,D=myapp,t=access_log,u=root,p=password \
  --file '/archive/access_log_%Y%m%d.txt' \
  --where "log_time < DATE_SUB(NOW(), INTERVAL 90 DAY)" \
  --limit 5000 \
  --sleep 0.1 \
  --progress 50000 \
  --statistics \
  --dry-run  # 先测试，确认无误后去掉这个参数

# 输出示例：
# Started at 2025-09-02T03:00:00
# Archiving D=myapp,t=access_log to file /archive/access_log_20250902.txt
# SELECT /*!40001 SQL_NO_CACHE */ `id`,`user_id`,`url`,`log_time` FROM `myapp`.`access_log` WHERE (log_time < DATE_SUB(NOW(), INTERVAL 90 DAY)) ORDER BY `id` LIMIT 5000
# Archived 1500000 rows in 1832.5 seconds (818 rows/sec)
```

**📁 归档到另一张表**：
```bash
# 将订单数据归档到历史表
pt-archiver \
  --source h=localhost,D=ecommerce,t=orders,u=archive_user,p=password \
  --dest h=archive-server,D=archive_db,t=orders_history \
  --where "order_time < DATE_SUB(NOW(), INTERVAL 180 DAY)" \
  --limit 1000 \
  --sleep 0.2 \
  --progress 10000 \
  --check-charset \
  --check-columns \
  --commit-each \
  --statistics

# 关键参数说明：
# --check-charset：检查字符集兼容性
# --check-columns：验证表结构一致性  
# --commit-each：每批次提交事务
# --statistics：显示详细统计信息
```

### 6.4 pt-archiver高级特性


**🎯 安全控制特性**：
```bash
# 主从复制安全归档
pt-archiver \
  --source h=master,D=app,t=logs \
  --dest h=archive,D=archive,t=logs_archive \
  --where "created_at < NOW() - INTERVAL 30 DAY" \
  --check-slave-lag h=slave1,h=slave2 \  # 检查从库延迟
  --max-lag 5 \                          # 从库延迟超过5秒时暂停
  --check-interval 10 \                  # 每10秒检查一次
  --limit 1000 \
  --sleep 0.1

# 业务安全控制
pt-archiver \
  --source h=localhost,D=app,t=user_actions \
  --dest h=archive,D=archive,t=user_actions_archive \
  --where "action_time < NOW() - INTERVAL 60 DAY" \
  --ask-pass \          # 交互式输入密码
  --dry-run \           # 测试模式，不实际执行
  --print \             # 打印将要执行的SQL
  --statistics          # 显示处理统计
```

**📊 归档监控和验证**：
```bash
# 带完整性验证的归档
pt-archiver \
  --source h=localhost,D=app,t=transactions \
  --dest h=archive,D=archive,t=transactions_archive \
  --where "trans_time < DATE_SUB(NOW(), INTERVAL 365 DAY)" \
  --limit 2000 \
  --sleep 0.05 \
  --progress 20000 \
  --check-columns \     # 验证列结构
  --optimize s \        # 归档后优化源表
  --analyze s \         # 归档后分析源表
  --statistics \        # 显示详细统计
  --plugin dsn=h=monitor,D=pt_tools,t=archive_log  # 记录到监控表

# 生成的监控信息包括：
# - 归档开始和结束时间
# - 处理的行数和速度
# - 源表和目标表的状态
# - 可能的错误和警告
```

---

## 7. 🗜️ 归档数据压缩存储


### 7.1 压缩存储的必要性


**💰 存储成本对比**：
```
🌰 存储成本就像房租：

未压缩存储：
1TB历史数据 × $0.1/GB/月 × 12月 = $1,200/年

压缩存储（压缩比5:1）：
200GB压缩数据 × $0.1/GB/月 × 12月 = $240/年

节省成本：$960/年 (80%成本节省)

额外收益：
• 备份时间缩短80%
• 网络传输加速5倍
• 磁盘IO减少，性能提升
```

### 7.2 MySQL表级压缩


**🔧 InnoDB表压缩配置**：
```sql
-- 创建压缩归档表
CREATE TABLE user_orders_archive (
    order_id BIGINT PRIMARY KEY,
    user_id INT NOT NULL,
    order_time DATETIME NOT NULL,
    amount DECIMAL(10,2),
    status TINYINT,
    order_detail JSON,          -- JSON数据特别适合压缩
    archived_at DATETIME DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_user_time (user_id, order_time),
    INDEX idx_archive_time (archived_at)
) 
ENGINE=InnoDB 
ROW_FORMAT=COMPRESSED         -- 启用行压缩
KEY_BLOCK_SIZE=8;            -- 压缩页大小8KB（可选1,2,4,8,16）

-- 压缩效果验证
SELECT 
    table_name,
    ROUND(data_length/1024/1024,2) AS 'Data Size (MB)',
    ROUND(index_length/1024/1024,2) AS 'Index Size (MB)',
    ROUND((data_length+index_length)/1024/1024,2) AS 'Total Size (MB)',
    table_rows AS 'Row Count'
FROM information_schema.tables 
WHERE table_name IN ('user_orders', 'user_orders_archive');
```

**📊 压缩效果监控**：
```sql
-- 查看压缩统计信息
SELECT 
    table_schema,
    table_name,
    engine,
    row_format,
    ROUND(data_length/1024/1024,2) AS data_mb,
    ROUND(data_free/1024/1024,2) AS data_free_mb,
    ROUND((data_length-data_free)/data_length*100,2) AS compression_ratio
FROM information_schema.tables
WHERE row_format = 'Compressed'
ORDER BY data_length DESC;
```

### 7.3 文件级压缩存储


**📁 归档文件压缩策略**：
```bash
# 数据导出并压缩
mysql -h localhost -u user -p database << EOF | gzip > /archive/orders_$(date +%Y%m%d).csv.gz
SELECT order_id, user_id, order_time, amount, status, order_detail
FROM user_orders_archive 
WHERE archived_at < DATE_SUB(NOW(), INTERVAL 1 YEAR)
ORDER BY order_id;
EOF

# 多种压缩格式对比
# gzip压缩（通用性好）
gzip /archive/data.csv                    # 压缩比约3-5:1

# bzip2压缩（压缩率更高）
bzip2 /archive/data.csv                   # 压缩比约5-8:1

# xz压缩（压缩率最高）
xz /archive/data.csv                      # 压缩比约8-12:1

# 压缩效果对比示例：
原始文件：1GB CSV文件
gzip压缩：200MB (压缩比5:1)
bzip2压缩：150MB (压缩比6.7:1)  
xz压缩：100MB (压缩比10:1)
```

**📦 分段压缩归档**：
```bash
#!/bin/bash
# 按月分段归档压缩脚本

ARCHIVE_DIR="/archive/orders"
DB_HOST="localhost"
DB_USER="archive_user"
DB_PASS="password"
DB_NAME="ecommerce"

# 创建归档目录
mkdir -p $ARCHIVE_DIR

# 按月归档最近12-24个月的数据
for i in {12..24}; do
    MONTH_START=$(date -d "$i months ago" +%Y-%m-01)
    MONTH_END=$(date -d "$((i-1)) months ago" +%Y-%m-01)
    MONTH_LABEL=$(date -d "$i months ago" +%Y%m)
    
    echo "正在归档 $MONTH_LABEL 月数据..."
    
    # 导出并压缩
    mysql -h$DB_HOST -u$DB_USER -p$DB_PASS $DB_NAME << EOF | xz > $ARCHIVE_DIR/orders_$MONTH_LABEL.csv.xz
SELECT order_id, user_id, order_time, amount, status, 
       COMPRESS(order_detail) as compressed_detail
FROM user_orders 
WHERE order_time >= '$MONTH_START' 
  AND order_time < '$MONTH_END'
ORDER BY order_id;
EOF
    
    # 验证压缩文件
    if [ -f "$ARCHIVE_DIR/orders_$MONTH_LABEL.csv.xz" ]; then
        FILE_SIZE=$(du -h "$ARCHIVE_DIR/orders_$MONTH_LABEL.csv.xz" | cut -f1)
        echo "归档完成：$MONTH_LABEL 月，文件大小：$FILE_SIZE"
        
        # 删除原始数据
        mysql -h$DB_HOST -u$DB_USER -p$DB_PASS $DB_NAME << EOF
DELETE FROM user_orders 
WHERE order_time >= '$MONTH_START' 
  AND order_time < '$MONTH_END'
LIMIT 50000;
EOF
    else
        echo "归档失败：$MONTH_LABEL 月"
    fi
    
    sleep 60  # 休息1分钟
done
```

### 7.4 云存储归档


**☁️ 对象存储归档**：
```python
import boto3
import gzip
import pymysql
from datetime import datetime, timedelta

class CloudArchiver:
    def __init__(self, db_config, s3_config):
        self.db = pymysql.connect(**db_config)
        self.s3 = boto3.client('s3', **s3_config)
        
    def archive_to_s3(self, table_name, days_old=90):
        """将历史数据归档到S3"""
        
        # 查询需要归档的数据
        cutoff_date = datetime.now() - timedelta(days=days_old)
        
        with self.db.cursor() as cursor:
            sql = f"""
            SELECT * FROM {table_name} 
            WHERE created_at < %s 
            ORDER BY id 
            LIMIT 100000
            """
            cursor.execute(sql, (cutoff_date,))
            
            # 生成压缩文件
            archive_file = f"/tmp/{table_name}_{cutoff_date.strftime('%Y%m%d')}.csv.gz"
            
            with gzip.open(archive_file, 'wt') as f:
                # 写入CSV头
                f.write('id,user_id,created_at,data\n')
                
                # 写入数据行
                while True:
                    rows = cursor.fetchmany(1000)
                    if not rows:
                        break
                        
                    for row in rows:
                        f.write(f"{row[0]},{row[1]},{row[2]},{row[3]}\n")
            
            # 上传到S3
            s3_key = f"archive/{table_name}/{cutoff_date.year}/{cutoff_date.month}/{table_name}_{cutoff_date.strftime('%Y%m%d')}.csv.gz"
            
            self.s3.upload_file(
                archive_file, 
                'my-data-archive-bucket', 
                s3_key,
                ExtraArgs={
                    'StorageClass': 'GLACIER',  # 使用Glacier冷存储
                    'Metadata': {
                        'source_table': table_name,
                        'archive_date': cutoff_date.isoformat(),
                        'record_count': str(cursor.rowcount)
                    }
                }
            )
            
            print(f"归档完成：{cursor.rowcount} 行数据已上传到 {s3_key}")
            
            return cursor.rowcount

# 使用示例
archiver = CloudArchiver(
    db_config={'host': 'localhost', 'user': 'root', 'password': 'pwd', 'db': 'app'},
    s3_config={'region_name': 'us-west-2'}
)

# 归档90天前的用户行为数据
archiver.archive_to_s3('user_behavior_log', days_old=90)
```

---

## 8. ⏱️ 清理任务调度EVENT


### 8.1 MySQL EVENT调度器


**🔧 EVENT调度器概念**：
```
🌰 EVENT就像定时闹钟：

普通闹钟：只能响一次
EVENT调度器：可以设置复杂的重复规律
• 每天早上8点执行
• 每周一凌晨2点执行  
• 每月第一天凌晨3点执行
• 条件触发执行

EVENT的核心作用：
✅ 自动化数据清理，无需人工干预
✅ 精确时间控制，避开业务高峰
✅ 可重复执行，持续维护数据库健康
✅ 异常处理，确保清理任务稳定运行
```

### 8.2 EVENT基础语法


**📝 EVENT创建语法**：
```sql
-- 基本EVENT语法结构
CREATE EVENT event_name
ON SCHEDULE schedule_definition
[ON COMPLETION [NOT] PRESERVE]
[ENABLE | DISABLE | DISABLE ON SLAVE]
[COMMENT 'comment']
DO
    event_body;

-- 调度定义的几种方式：
AT timestamp                    -- 一次性执行
EVERY interval                  -- 重复执行
EVERY interval STARTS timestamp -- 从指定时间开始重复
EVERY interval ENDS timestamp   -- 重复到指定时间结束
```

**⏰ 时间间隔表达式**：
```sql
-- 时间间隔的各种写法
EVERY 1 DAY                    -- 每天
EVERY 2 HOUR                   -- 每2小时
EVERY 1 WEEK                   -- 每周
EVERY 1 MONTH                  -- 每月
EVERY '1 2:30:00' DAY_SECOND   -- 每1天2小时30分
EVERY '1-5' DAY_HOUR           -- 每1天5小时

-- 复杂时间控制
EVERY 1 DAY STARTS '2025-01-01 02:00:00'  -- 从2025年1月1日凌晨2点开始，每天执行
EVERY 1 WEEK STARTS '2025-01-06 03:00:00' -- 从2025年1月6日开始，每周执行
```

### 8.3 数据清理EVENT实现


**🗑️ 日志清理EVENT**：
```sql
-- 每日清理访问日志EVENT
DELIMITER $$

CREATE EVENT daily_access_log_cleanup
ON SCHEDULE EVERY 1 DAY 
STARTS '2025-01-01 02:00:00'
ON COMPLETION PRESERVE
ENABLE
COMMENT '每日清理30天前的访问日志'
DO
BEGIN
    DECLARE cleaned_rows INT DEFAULT 0;
    DECLARE cleanup_date DATETIME;
    DECLARE exit_loop BOOLEAN DEFAULT FALSE;
    
    -- 设置清理日期（30天前）
    SET cleanup_date = DATE_SUB(NOW(), INTERVAL 30 DAY);
    
    -- 记录清理开始
    INSERT INTO cleanup_log (event_name, start_time, status)
    VALUES ('daily_access_log_cleanup', NOW(), 'STARTED');
    
    -- 分批删除，避免长事务
    cleanup_loop: LOOP
        DELETE FROM access_log 
        WHERE access_time < cleanup_date 
        LIMIT 10000;
        
        SET cleaned_rows = cleaned_rows + ROW_COUNT();
        
        -- 如果没有更多数据需要清理，退出循环
        IF ROW_COUNT() = 0 THEN
            SET exit_loop = TRUE;
        END IF;
        
        -- 检查是否超过最大清理时间（30分钟）
        IF TIMESTAMPDIFF(MINUTE, (SELECT start_time FROM cleanup_log 
                                 WHERE event_name = 'daily_access_log_cleanup' 
                                 ORDER BY start_time DESC LIMIT 1), NOW()) > 30 THEN
            SET exit_loop = TRUE;
        END IF;
        
        -- 退出条件检查
        IF exit_loop THEN
            LEAVE cleanup_loop;
        END IF;
        
        -- 短暂休息，避免影响在线业务
        SELECT SLEEP(0.5);
        
    END LOOP cleanup_loop;
    
    -- 记录清理结果
    UPDATE cleanup_log 
    SET end_time = NOW(), 
        cleaned_rows = cleaned_rows, 
        status = 'COMPLETED'
    WHERE event_name = 'daily_access_log_cleanup' 
      AND end_time IS NULL;
    
END$$

DELIMITER ;
```

**📊 多表协调清理EVENT**：
```sql
-- 周度综合清理EVENT
DELIMITER $$

CREATE EVENT weekly_comprehensive_cleanup
ON SCHEDULE EVERY 1 WEEK 
STARTS '2025-01-05 03:00:00'  -- 每周日凌晨3点
ON COMPLETION PRESERVE
ENABLE
COMMENT '每周综合数据清理和归档'
DO
BEGIN
    DECLARE cleanup_batch VARCHAR(50);
    
    -- 生成清理批次号
    SET cleanup_batch = CONCAT('WEEKLY_', DATE_FORMAT(NOW(), '%Y%m%d_%H%i%s'));
    
    -- 记录批次开始
    INSERT INTO cleanup_batch_log (batch_id, start_time, status)
    VALUES (cleanup_batch, NOW(), 'STARTED');
    
    -- 任务1：清理用户行为数据（保留60天）
    CALL cleanup_user_behavior(60, cleanup_batch);
    
    -- 任务2：归档订单数据（归档180天前）
    CALL archive_order_data(180, cleanup_batch);
    
    -- 任务3：清理系统日志（保留90天）
    CALL cleanup_system_logs(90, cleanup_batch);
    
    -- 任务4：归档用户反馈数据（归档365天前）
    CALL archive_user_feedback(365, cleanup_batch);
    
    -- 任务5：优化清理后的表
    CALL optimize_cleaned_tables();
    
    -- 记录批次完成
    UPDATE cleanup_batch_log 
    SET end_time = NOW(), status = 'COMPLETED'
    WHERE batch_id = cleanup_batch;
    
    -- 发送清理报告
    CALL send_cleanup_report(cleanup_batch);
    
END$$

DELIMITER ;
```

### 8.4 EVENT监控和管理


**👀 EVENT状态监控**：
```sql
-- 查看EVENT运行状态
SELECT 
    event_schema,
    event_name,
    status,
    event_type,
    execute_at,
    interval_value,
    interval_field,
    starts,
    ends,
    last_executed,
    event_comment
FROM information_schema.events
WHERE event_schema = 'your_database'
ORDER BY last_executed DESC;

-- 查看EVENT执行历史
SELECT 
    event_name,
    start_time,
    end_time,
    TIMESTAMPDIFF(MINUTE, start_time, end_time) as duration_minutes,
    cleaned_rows,
    status
FROM cleanup_log 
WHERE start_time >= DATE_SUB(NOW(), INTERVAL 7 DAY)
ORDER BY start_time DESC;
```

**🔧 EVENT管理操作**：
```sql
-- EVENT管理操作

-- 启用EVENT调度器
SET GLOBAL event_scheduler = ON;

-- 禁用特定EVENT
ALTER EVENT daily_access_log_cleanup DISABLE;

-- 启用特定EVENT
ALTER EVENT daily_access_log_cleanup ENABLE;

-- 修改EVENT执行时间
ALTER EVENT daily_access_log_cleanup
ON SCHEDULE EVERY 1 DAY STARTS '2025-01-01 01:30:00';

-- 临时禁用EVENT（主从复制时很有用）
ALTER EVENT daily_access_log_cleanup DISABLE ON SLAVE;

-- 删除EVENT
DROP EVENT daily_access_log_cleanup;

-- 查看EVENT调度器状态
SHOW VARIABLES LIKE 'event_scheduler';
```

---

## 9. 💾 存储空间回收机制


### 9.1 MySQL存储空间回收原理


**🏠 存储空间回收类比**：
```
🌰 存储空间就像房子的使用：

DELETE删除数据：
• 就像搬走家具，但房间还在
• 空间标记为"可用"，但文件大小不变
• 新数据可以使用这些空间

空间回收：
• 就像重新装修，压缩房间大小
• 实际释放磁盘空间
• 提高空间利用率
```

**💡 空间回收的必要性**：
```
问题现象：
• 删除大量数据后，数据文件大小不变
• 磁盘空间没有释放
• 查询性能可能反而下降（碎片增加）

解决方案：
• 表重组：OPTIMIZE TABLE
• 重建表：ALTER TABLE ... ENGINE=InnoDB
• 分区重组：ALTER TABLE ... REBUILD PARTITION
```

### 9.2 InnoDB空间回收方法


**🔧 表优化方法对比**：

| 方法 | **命令** | **原理** | **优点** | **缺点** |
|------|---------|---------|---------|---------|
| 🔄 **OPTIMIZE** | `OPTIMIZE TABLE table_name` | `重建表和索引` | `简单易用，一条命令完成` | `需要双倍空间，锁表时间长` |
| 🏗️ **ALTER重建** | `ALTER TABLE table_name ENGINE=InnoDB` | `重建表结构` | `效果好，完全重组` | `需要双倍空间，停机时间长` |
| 📁 **导出重建** | `导出→删表→建表→导入` | `完全重建` | `空间利用最优` | `操作复杂，风险较大` |

**💻 实际操作示例**：
```sql
-- 方法1：直接优化表（适合小表）
OPTIMIZE TABLE user_orders_archive;

-- 方法2：重建表引擎（适合中等表）
ALTER TABLE user_orders_archive ENGINE=InnoDB;

-- 方法3：分批重建（适合大表）
-- 创建新表结构
CREATE TABLE user_orders_archive_new LIKE user_orders_archive;

-- 分批迁移数据
INSERT INTO user_orders_archive_new 
SELECT * FROM user_orders_archive 
WHERE id BETWEEN 1 AND 100000;

-- 继续分批...
INSERT INTO user_orders_archive_new 
SELECT * FROM user_orders_archive 
WHERE id BETWEEN 100001 AND 200000;

-- 原子切换表名
RENAME TABLE 
    user_orders_archive TO user_orders_archive_old,
    user_orders_archive_new TO user_orders_archive;

-- 删除旧表
DROP TABLE user_orders_archive_old;
```

### 9.3 分区表空间回收


**⚡ 分区级别空间回收**：
```sql
-- 重建特定分区（速度快，影响小）
ALTER TABLE user_orders_partitioned REBUILD PARTITION p202401;

-- 重建多个分区
ALTER TABLE user_orders_partitioned 
REBUILD PARTITION p202401, p202402, p202403;

-- 压缩分区（减少存储空间）
ALTER TABLE user_orders_partitioned 
COALESCE PARTITION 2;  -- 合并相邻的2个分区

-- 分区空间使用情况查询
SELECT 
    partition_name,
    table_rows,
    ROUND(data_length/1024/1024,2) AS data_mb,
    ROUND(index_length/1024/1024,2) AS index_mb,
    ROUND((data_length+index_length)/1024/1024,2) AS total_mb,
    ROUND(data_free/1024/1024,2) AS free_mb
FROM information_schema.partitions 
WHERE table_schema = 'your_database' 
  AND table_name = 'user_orders_partitioned'
  AND partition_name IS NOT NULL
ORDER BY partition_ordinal_position;
```

### 9.4 自动空间回收脚本


**🤖 智能空间回收EVENT**：
```sql
DELIMITER $$

CREATE EVENT intelligent_space_reclaim
ON SCHEDULE EVERY 1 WEEK
STARTS '2025-01-05 04:00:00'  -- 每周日凌晨4点
ON COMPLETION PRESERVE
ENABLE
COMMENT '智能空间回收，基于碎片率自动优化表'
DO
BEGIN
    DECLARE done INT DEFAULT FALSE;
    DECLARE table_name VARCHAR(64);
    DECLARE fragmentation_ratio DECIMAL(5,2);
    DECLARE table_size_mb DECIMAL(10,2);
    
    -- 游标：查找碎片率高的表
    DECLARE table_cursor CURSOR FOR
        SELECT 
            t.table_name,
            ROUND(t.data_free / (t.data_length + t.index_length + t.data_free) * 100, 2) as frag_ratio,
            ROUND((t.data_length + t.index_length + t.data_free)/1024/1024, 2) as size_mb
        FROM information_schema.tables t
        WHERE t.table_schema = DATABASE()
          AND t.engine = 'InnoDB'
          AND t.data_free > 0
          AND (t.data_length + t.index_length + t.data_free) > 100*1024*1024  -- 大于100MB
          AND t.data_free / (t.data_length + t.index_length + t.data_free) > 0.2  -- 碎片率>20%
        ORDER BY frag_ratio DESC;
    
    DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE;
    
    -- 记录回收批次开始
    INSERT INTO space_reclaim_log (batch_id, start_time, status)
    VALUES (CONCAT('RECLAIM_', DATE_FORMAT(NOW(), '%Y%m%d_%H%i%s')), NOW(), 'STARTED');
    
    OPEN table_cursor;
    
    reclaim_loop: LOOP
        FETCH table_cursor INTO table_name, fragmentation_ratio, table_size_mb;
        
        IF done THEN
            LEAVE reclaim_loop;
        END IF;
        
        -- 记录单表回收开始
        INSERT INTO space_reclaim_detail (table_name, fragmentation_before, size_mb, start_time)
        VALUES (table_name, fragmentation_ratio, table_size_mb, NOW());
        
        -- 根据表大小选择回收方法
        IF table_size_mb < 1000 THEN  -- 小于1GB，直接优化
            SET @sql = CONCAT('OPTIMIZE TABLE ', table_name);
        ELSE  -- 大于1GB，重建引擎
            SET @sql = CONCAT('ALTER TABLE ', table_name, ' ENGINE=InnoDB');
        END IF;
        
        -- 执行空间回收
        PREPARE stmt FROM @sql;
        EXECUTE stmt;
        DEALLOCATE PREPARE stmt;
        
        -- 记录回收结果
        UPDATE space_reclaim_detail 
        SET end_time = NOW(),
            status = 'COMPLETED'
        WHERE table_name = table_name 
          AND end_time IS NULL;
        
        -- 避免连续大量操作影响性能
        SELECT SLEEP(30);
        
    END LOOP reclaim_loop;
    
    CLOSE table_cursor;
    
    -- 记录批次完成
    UPDATE space_reclaim_log 
    SET end_time = NOW(), status = 'COMPLETED'
    WHERE end_time IS NULL;
    
END$$

DELIMITER ;
```

---

## 10. 📈 监控告警体系


### 10.1 清理监控指标设计


**📊 核心监控指标**：
```
存储空间指标：
• 数据库总大小趋势
• 各表空间增长速度  
• 可用磁盘空间百分比
• 碎片空间占比

清理效果指标：
• 每日清理数据量
• 清理任务执行时间
• 清理任务成功率
• 空间回收效果

性能影响指标：
• 清理期间的查询响应时间
• 清理对并发业务的影响
• 主从复制延迟变化
• 系统资源使用情况

业务健康指标：
• 数据完整性检查结果
• 归档数据可访问性
• 清理任务异常次数
• 用户业务影响投诉
```

### 10.2 监控数据收集实现


**📈 监控数据表设计**：
```sql
-- 清理监控主表
CREATE TABLE cleanup_monitor (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    monitor_time DATETIME DEFAULT CURRENT_TIMESTAMP,
    
    -- 存储空间指标
    total_db_size_mb DECIMAL(12,2),
    available_disk_space_mb DECIMAL(12,2),
    disk_usage_percent DECIMAL(5,2),
    
    -- 清理效果指标
    daily_cleaned_rows BIGINT DEFAULT 0,
    daily_archived_rows BIGINT DEFAULT 0,
    space_reclaimed_mb DECIMAL(10,2) DEFAULT 0,
    
    -- 性能指标
    avg_query_time_ms DECIMAL(8,2),
    cleanup_duration_minutes INT,
    
    INDEX idx_monitor_time (monitor_time)
);

-- 详细清理日志表
CREATE TABLE cleanup_detail_log (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    event_name VARCHAR(100),
    table_name VARCHAR(64),
    operation_type ENUM('DELETE', 'ARCHIVE', 'OPTIMIZE'),
    start_time DATETIME,
    end_time DATETIME,
    affected_rows BIGINT,
    error_message TEXT,
    status ENUM('STARTED', 'COMPLETED', 'FAILED', 'TIMEOUT'),
    
    INDEX idx_event_time (event_name, start_time),
    INDEX idx_table_time (table_name, start_time)
);
```

**📊 数据收集存储过程**：
```sql
DELIMITER $

CREATE PROCEDURE collect_cleanup_metrics()
BEGIN
    DECLARE total_size DECIMAL(12,2) DEFAULT 0;
    DECLARE free_space DECIMAL(12,2) DEFAULT 0;
    DECLARE avg_query DECIMAL(8,2) DEFAULT 0;
    
    -- 计算数据库总大小
    SELECT ROUND(SUM(data_length + index_length)/1024/1024, 2) INTO total_size
    FROM information_schema.tables 
    WHERE table_schema = DATABASE();
    
    -- 获取磁盘可用空间（需要配置相关权限）
    SELECT ROUND($$datadir_disk_usage_pct, 2) INTO free_space;
    
    -- 计算平均查询时间（从慢查询日志）
    SELECT AVG(query_time) INTO avg_query
    FROM mysql.slow_log 
    WHERE start_time >= DATE_SUB(NOW(), INTERVAL 1 DAY);
    
    -- 插入监控数据
    INSERT INTO cleanup_monitor (
        total_db_size_mb,
        available_disk_space_mb, 
        avg_query_time_ms
    ) VALUES (
        total_size,
        free_space,
        avg_query * 1000
    );
    
    -- 清理30天前的监控数据
    DELETE FROM cleanup_monitor 
    WHERE monitor_time < DATE_SUB(NOW(), INTERVAL 30 DAY);
    
END$

DELIMITER ;

-- 每小时收集一次监控数据
CREATE EVENT collect_metrics_hourly
ON SCHEDULE EVERY 1 HOUR
DO CALL collect_cleanup_metrics();
```

### 10.3 告警规则设置


**🚨 告警触发条件**：
```sql
-- 告警规则配置表
CREATE TABLE alert_rules (
    rule_id INT AUTO_INCREMENT PRIMARY KEY,
    rule_name VARCHAR(100),
    metric_name VARCHAR(50),
    threshold_value DECIMAL(10,2),
    comparison_operator ENUM('>', '<', '>=', '<=', '='),
    alert_level ENUM('INFO', 'WARNING', 'CRITICAL'),
    enabled BOOLEAN DEFAULT TRUE,
    
    INDEX idx_metric (metric_name, enabled)
);

-- 插入告警规则
INSERT INTO alert_rules (rule_name, metric_name, threshold_value, comparison_operator, alert_level) VALUES
('磁盘空间不足', 'disk_usage_percent', 85.0, '>', 'WARNING'),
('磁盘空间严重不足', 'disk_usage_percent', 95.0, '>', 'CRITICAL'),
('数据库过大', 'total_db_size_mb', 100000.0, '>', 'WARNING'),  -- 100GB
('清理任务失败', 'cleanup_failure_count', 3.0, '>', 'CRITICAL'),
('查询性能下降', 'avg_query_time_ms', 1000.0, '>', 'WARNING');  -- 1秒
```

**📧 告警通知实现**：
```sql
DELIMITER $

CREATE PROCEDURE check_and_alert()
BEGIN
    DECLARE done INT DEFAULT FALSE;
    DECLARE rule_name VARCHAR(100);
    DECLARE metric VARCHAR(50);
    DECLARE threshold_val DECIMAL(10,2);
    DECLARE operator_val VARCHAR(5);
    DECLARE alert_level VARCHAR(10);
    DECLARE current_value DECIMAL(10,2);
    DECLARE should_alert BOOLEAN DEFAULT FALSE;
    
    -- 游标：遍历所有启用的告警规则
    DECLARE alert_cursor CURSOR FOR
        SELECT rule_name, metric_name, threshold_value, comparison_operator, alert_level
        FROM alert_rules 
        WHERE enabled = TRUE;
    
    DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE;
    
    OPEN alert_cursor;
    
    check_loop: LOOP
        FETCH alert_cursor INTO rule_name, metric, threshold_val, operator_val, alert_level;
        
        IF done THEN
            LEAVE check_loop;
        END IF;
        
        -- 获取当前指标值
        CASE metric
            WHEN 'disk_usage_percent' THEN
                SELECT disk_usage_percent INTO current_value 
                FROM cleanup_monitor 
                ORDER BY monitor_time DESC LIMIT 1;
                
            WHEN 'total_db_size_mb' THEN
                SELECT total_db_size_mb INTO current_value 
                FROM cleanup_monitor 
                ORDER BY monitor_time DESC LIMIT 1;
                
            WHEN 'avg_query_time_ms' THEN
                SELECT avg_query_time_ms INTO current_value 
                FROM cleanup_monitor 
                ORDER BY monitor_time DESC LIMIT 1;
                
            WHEN 'cleanup_failure_count' THEN
                SELECT COUNT(*) INTO current_value 
                FROM cleanup_detail_log 
                WHERE status = 'FAILED' 
                  AND start_time >= DATE_SUB(NOW(), INTERVAL 1 DAY);
        END CASE;
        
        -- 判断是否触发告警
        CASE operator_val
            WHEN '>' THEN SET should_alert = (current_value > threshold_val);
            WHEN '<' THEN SET should_alert = (current_value < threshold_val);
            WHEN '>=' THEN SET should_alert = (current_value >= threshold_val);
            WHEN '<=' THEN SET should_alert = (current_value <= threshold_val);
            WHEN '=' THEN SET should_alert = (current_value = threshold_val);
        END CASE;
        
        -- 如果触发告警，记录并发送通知
        IF should_alert THEN
            INSERT INTO alert_history (
                rule_name, metric_name, current_value, threshold_value, 
                alert_level, alert_time, status
            ) VALUES (
                rule_name, metric, current_value, threshold_val,
                alert_level, NOW(), 'TRIGGERED'
            );
            
            -- 调用通知程序（可以是邮件、短信、钉钉等）
            CALL send_alert_notification(rule_name, metric, current_value, alert_level);
        END IF;
        
    END LOOP check_loop;
    
    CLOSE alert_cursor;
    
END$

DELIMITER ;

-- 每15分钟检查一次告警
CREATE EVENT check_alerts_regularly
ON SCHEDULE EVERY 15 MINUTE
DO CALL check_and_alert();
```

### 10.4 监控报表生成


**📋 清理效果报表**：
```sql
-- 生成每日清理报表
CREATE VIEW daily_cleanup_report AS
SELECT 
    DATE(start_time) as cleanup_date,
    COUNT(*) as total_tasks,
    SUM(affected_rows) as total_cleaned_rows,
    SUM(CASE WHEN status = 'COMPLETED' THEN 1 ELSE 0 END) as successful_tasks,
    SUM(CASE WHEN status = 'FAILED' THEN 1 ELSE 0 END) as failed_tasks,
    ROUND(AVG(TIMESTAMPDIFF(MINUTE, start_time, end_time)), 2) as avg_duration_minutes,
    GROUP_CONCAT(CASE WHEN status = 'FAILED' THEN table_name END) as failed_tables
FROM cleanup_detail_log
WHERE start_time >= DATE_SUB(NOW(), INTERVAL 30 DAY)
GROUP BY DATE(start_time)
ORDER BY cleanup_date DESC;

-- 生成空间趋势报表
CREATE VIEW space_usage_trend AS
SELECT 
    DATE(monitor_time) as monitor_date,
    MAX(total_db_size_mb) as max_db_size_mb,
    MIN(available_disk_space_mb) as min_available_space_mb,
    MAX(disk_usage_percent) as max_disk_usage_pct,
    AVG(avg_query_time_ms) as avg_query_performance_ms
FROM cleanup_monitor
WHERE monitor_time >= DATE_SUB(NOW(), INTERVAL 30 DAY)
GROUP BY DATE(monitor_time)
ORDER BY monitor_date DESC;

-- 查看最近清理情况
SELECT * FROM daily_cleanup_report LIMIT 7;

-- 查看空间使用趋势  
SELECT * FROM space_usage_trend LIMIT 7;
```

**📊 可视化监控脚本**：
```python
import matplotlib.pyplot as plt
import pymysql
from datetime import datetime, timedelta

class CleanupMonitor:
    def __init__(self, db_config):
        self.db = pymysql.connect(**db_config)
        
    def generate_space_trend_chart(self, days=30):
        """生成存储空间趋势图"""
        
        with self.db.cursor() as cursor:
            sql = """
            SELECT monitor_date, max_db_size_mb, max_disk_usage_pct
            FROM space_usage_trend
            WHERE monitor_date >= DATE_SUB(NOW(), INTERVAL %s DAY)
            ORDER BY monitor_date
            """
            cursor.execute(sql, (days,))
            data = cursor.fetchall()
        
        if not data:
            return
            
        dates = [row[0] for row in data]
        db_sizes = [row[1] for row in data]
        disk_usage = [row[2] for row in data]
        
        # 创建双Y轴图表
        fig, ax1 = plt.subplots(figsize=(12, 6))
        
        # 数据库大小趋势
        color = 'tab:blue'
        ax1.set_xlabel('日期')
        ax1.set_ylabel('数据库大小 (MB)', color=color)
        ax1.plot(dates, db_sizes, color=color, marker='o', label='数据库大小')
        ax1.tick_params(axis='y', labelcolor=color)
        
        # 磁盘使用率趋势
        ax2 = ax1.twinx()
        color = 'tab:red'
        ax2.set_ylabel('磁盘使用率 (%)', color=color)
        ax2.plot(dates, disk_usage, color=color, marker='s', label='磁盘使用率')
        ax2.tick_params(axis='y', labelcolor=color)
        
        # 添加告警线
        ax2.axhline(y=85, color='orange', linestyle='--', alpha=0.7, label='告警线85%')
        ax2.axhline(y=95, color='red', linestyle='--', alpha=0.7, label='危险线95%')
        
        plt.title('数据库存储空间趋势监控')
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(f'/tmp/space_trend_{datetime.now().strftime("%Y%m%d")}.png')
        plt.close()
        
    def generate_cleanup_report(self):
        """生成清理效果报告"""
        
        with self.db.cursor() as cursor:
            # 获取最近7天的清理统计
            cursor.execute("""
                SELECT 
                    cleanup_date,
                    total_cleaned_rows,
                    successful_tasks,
                    failed_tasks,
                    avg_duration_minutes
                FROM daily_cleanup_report 
                WHERE cleanup_date >= DATE_SUB(NOW(), INTERVAL 7 DAY)
                ORDER BY cleanup_date DESC
            """)
            
            recent_cleanups = cursor.fetchall()
            
            # 生成文本报告
            report = []
            report.append("=" * 60)
            report.append("📊 数据清理周报")
            report.append("=" * 60)
            report.append(f"📅 报告时间：{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
            report.append("")
            
            total_cleaned = sum(row[1] or 0 for row in recent_cleanups)
            total_tasks = sum(row[2] or 0 for row in recent_cleanups)
            total_failed = sum(row[3] or 0 for row in recent_cleanups)
            
            report.append(f"📈 本周清理汇总：")
            report.append(f"   • 总清理数据：{total_cleaned:,} 行")
            report.append(f"   • 执行任务数：{total_tasks} 个")
            report.append(f"   • 失败任务数：{total_failed} 个")
            report.append(f"   • 成功率：{((total_tasks-total_failed)/total_tasks*100):.1f}%")
            report.append("")
            
            report.append("📊 每日明细：")
            for row in recent_cleanups:
                date, cleaned, success, failed, duration = row
                report.append(f"   {date}: 清理 {cleaned:,} 行，"
                            f"成功 {success} 个任务，耗时 {duration:.1f} 分钟")
            
            return "\n".join(report)

# 使用示例
monitor = CleanupMonitor({
    'host': 'localhost',
    'user': 'monitor_user', 
    'password': 'password',
    'database': 'your_app'
})

# 生成趋势图
monitor.generate_space_trend_chart(30)

# 生成清理报告
report = monitor.generate_cleanup_report()
print(report)
```

### 10.5 实时告警通知


**📱 多渠道告警通知**：
```python
import smtplib
import requests
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart

class AlertNotifier:
    def __init__(self, config):
        self.email_config = config.get('email', {})
        self.webhook_config = config.get('webhook', {})
        
    def send_email_alert(self, rule_name, metric, current_value, level):
        """发送邮件告警"""
        
        if not self.email_config:
            return
            
        subject = f"🚨 数据库告警：{rule_name}"
        
        # 根据告警级别设置不同的模板
        if level == 'CRITICAL':
            emoji = "🔴"
            priority = "高危"
        elif level == 'WARNING':
            emoji = "🟡"  
            priority = "警告"
        else:
            emoji = "🔵"
            priority = "信息"
            
        body = f"""
        {emoji} 数据库告警通知
        
        📋 告警详情：
        • 规则名称：{rule_name}
        • 告警级别：{priority}
        • 监控指标：{metric}
        • 当前数值：{current_value}
        • 告警时间：{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
        
        🔧 建议处理：
        """
        
        if metric == 'disk_usage_percent':
            body += """
        1. 立即检查可清理的历史数据
        2. 执行紧急数据归档操作
        3. 考虑扩容存储空间
        """
        elif metric == 'cleanup_failure_count':
            body += """
        1. 检查清理任务日志，定位失败原因
        2. 验证数据库连接和权限
        3. 确认磁盘空间是否充足
        """
        
        # 发送邮件
        msg = MIMEMultipart()
        msg['From'] = self.email_config['sender']
        msg['To'] = self.email_config['recipient']
        msg['Subject'] = subject
        msg.attach(MIMEText(body, 'plain', 'utf-8'))
        
        with smtplib.SMTP(self.email_config['smtp_server'], 587) as server:
            server.starttls()
            server.login(self.email_config['username'], self.email_config['password'])
            server.send_message(msg)
            
    def send_webhook_alert(self, rule_name, metric, current_value, level):
        """发送Webhook告警（钉钉、企微等）"""
        
        if not self.webhook_config:
            return
            
        # 钉钉机器人消息格式
        message = {
            "msgtype": "markdown",
            "markdown": {
                "title": f"数据库告警：{rule_name}",
                "text": f"""
## 🚨 数据库告警通知


**告警规则：** {rule_name}  
**告警级别：** {level}  
**监控指标：** {metric}  
**当前数值：** {current_value}  
**告警时间：** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

> 请相关人员及时处理！
                """
            }
        }
        
        # 发送webhook
        response = requests.post(
            self.webhook_config['url'],
            json=message,
            headers={'Content-Type': 'application/json'}
        )
        
        return response.status_code == 200

# 集成到MySQL存储过程中
DELIMITER $

CREATE PROCEDURE send_alert_notification(
    IN rule_name VARCHAR(100),
    IN metric_name VARCHAR(50), 
    IN current_value DECIMAL(10,2),
    IN alert_level VARCHAR(10)
)
BEGIN
    -- 插入外部程序调用队列
    INSERT INTO alert_queue (
        rule_name, metric_name, current_value, alert_level, 
        created_at, status
    ) VALUES (
        rule_name, metric_name, current_value, alert_level,
        NOW(), 'PENDING'
    );
    
    -- 记录告警历史
    INSERT INTO alert_history (
        rule_name, metric_name, current_value, threshold_value,
        alert_level, alert_time, status
    ) VALUES (
        rule_name, metric_name, current_value, 
        (SELECT threshold_value FROM alert_rules WHERE rule_name = rule_name),
        alert_level, NOW(), 'TRIGGERED'
    );
    
END$

DELIMITER ;
```

---

## 11. 📋 核心要点总结


### 11.1 必须掌握的核心概念


```
🔸 数据清理：彻底删除无用数据，释放存储空间
🔸 数据归档：迁移历史数据到低成本存储，保持可访问性
🔸 分区清理：利用分区表特性，实现秒级数据删除
🔸 时间窗口：基于时间的数据生命周期管理策略
🔸 pt-archiver：专业的MySQL数据归档工具
🔸 压缩存储：减少归档数据的存储成本
🔸 EVENT调度：自动化的清理任务调度机制
🔸 监控告警：确保清理过程稳定可控的监控体系
```

### 11.2 关键理解要点


**🔹 清理vs归档的选择**：
```
直接删除场景：
• 日志数据：访问日志、错误日志、调试信息
• 临时数据：缓存数据、会话信息、中间结果
• 无价值数据：测试数据、重复数据、错误记录

归档保存场景：
• 业务数据：订单记录、用户行为、交易流水
• 合规数据：审计日志、操作记录、财务数据
• 分析数据：统计数据、报表数据、历史趋势
```

**🔹 分区表的威力**：
```
为什么分区表清理这么快？

传统表删除：
DELETE操作 → 逐行标记删除 → 空间标记可用但不释放
时间：线性增长，数据越多越慢

分区表删除：
DROP PARTITION → 直接删除整个分区文件 → 立即释放空间
时间：常数时间，无论数据多少都很快

就像：
传统方式 = 一本书一页页撕掉
分区方式 = 直接扔掉整本书
```

**🔹 pt-archiver的优势**：
```
为什么要用pt-archiver而不是普通SQL？

普通SQL的问题：
• 长事务：大批量操作可能锁表几小时
• 主从影响：大量binlog可能导致从库延迟
• 性能冲击：CPU和IO占用高，影响在线业务
• 无法断点续传：中途失败需要重新开始

pt-archiver的解决：
• 小批量：分批处理，避免长事务
• 流控制：监控主从延迟，自动限速
• 安全机制：多种检查，确保数据完整性
• 断点续传：支持中断恢复，提高可靠性
```

### 11.3 实际应用策略


**🎯 清理策略制定指南**：
```
第1步：数据价值评估
├── 核心业务数据 → 长期保留，归档存储
├── 辅助业务数据 → 中期保留，定期清理
├── 日志监控数据 → 短期保留，滚动删除
└── 临时测试数据 → 立即清理，不保留

第2步：技术方案选择
├── 小数据量(<10GB) → 直接DELETE + OPTIMIZE
├── 中数据量(10GB-100GB) → pt-archiver归档
├── 大数据量(>100GB) → 分区表管理
└── 超大数据量(>1TB) → 分库分表 + 多技术组合

第3步：清理时间规划
├── 日常清理：每天凌晨，清理日志类数据
├── 周度清理：每周末，归档用户行为数据
├── 月度清理：每月初，深度清理和空间回收
└── 季度清理：每季度，全面数据治理

第4步：监控保障
├── 实时监控：磁盘空间、清理任务状态
├── 告警通知：多渠道告警，及时响应
├── 定期报告：清理效果分析，持续优化
└── 应急预案：清理失败的处理流程
```

**📊 成本效益分析**：
```
投入成本：
• 开发成本：编写清理脚本和监控程序（一次性）
• 运维成本：日常监控和维护（持续性）
• 风险成本：数据误删的潜在损失（可控）

收益回报：
• 存储成本：节省60-80%的存储空间
• 性能提升：查询速度提升2-5倍
• 运维效率：自动化减少人工干预
• 系统稳定：避免磁盘满导致的故障

ROI计算示例：
投入：开发20人天 + 运维2人天/月 = 约10万元/年
节省：存储成本50万元/年 + 性能提升价值30万元/年
投资回报率：(80-10)/10 = 700%
```

**🎪 最佳实践总结**：

**⭐ 核心要点记忆**：
```
数据清理三原则：
1. 安全第一：先备份，后清理，验证完整性
2. 渐进处理：分批执行，避免影响在线业务  
3. 监控保障：全程监控，异常及时告警

技术选择策略：
• 数据量小 → 直接SQL清理
• 数据量大 → pt-archiver工具
• 超大数据 → 分区表管理
• 成本敏感 → 压缩存储

时间窗口管理：
• 热数据：0-30天，高性能存储
• 温数据：30天-1年，标准存储
• 冷数据：1年以上，归档存储
• 过期数据：按合规要求处理

自动化程度：
• EVENT调度：核心清理任务自动执行
• 监控告警：异常情况自动通知
• 空间回收：定期自动优化
• 报告生成：定期生成运营报告
```

**💡 实施建议**：
- **从小做起**：先从日志类非关键数据开始实践
- **逐步完善**：在小规模验证后再推广到核心业务数据
- **充分测试**：在测试环境验证所有脚本和流程
- **建立规范**：制定数据清理的标准操作程序
- **持续优化**：根据监控数据不断调整和改进策略

这套数据清理与归档体系能够帮您：
- **控制成本**：有效管理存储开销
- **提升性能**：保持数据库查询效率
- **降低风险**：自动化减少人为错误
- **规范管理**：建立标准的数据生命周期管理流程