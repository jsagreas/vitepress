---
title: 2、LOAD DATA INFILE导入
---
## 📚 目录

1. [LOAD DATA基础概念](#1-LOAD-DATA基础概念)
2. [完整语法结构详解](#2-完整语法结构详解)
3. [文件格式处理参数](#3-文件格式处理参数)
4. [数据导入模式控制](#4-数据导入模式控制)
5. [字段转换与处理](#5-字段转换与处理)
6. [安全性与权限控制](#6-安全性与权限控制)
7. [实际应用案例](#7-实际应用案例)
8. [核心要点总结](#8-核心要点总结)

---

## 1. 📂 LOAD DATA基础概念


### 1.1 什么是LOAD DATA INFILE


**🔸 核心定义**
```
LOAD DATA INFILE：MySQL提供的批量数据导入命令
作用：从文本文件快速导入大量数据到数据库表
优势：比INSERT语句快10-20倍，专为大数据量设计
原理：绕过SQL解析，直接操作存储引擎
```

**💡 使用场景对比**
```
传统INSERT方式：
INSERT INTO users VALUES (1,'张三','20'), (2,'李四','25'), ...
• 每条记录都要SQL解析
• 网络传输开销大  
• 适合小批量数据

LOAD DATA方式：
LOAD DATA INFILE '/data/users.txt' INTO TABLE users;
• 文件级别的批量操作
• 最小化网络开销
• 适合大批量数据导入
```

### 1.2 LOAD DATA的工作原理


**🔧 内部执行流程**
```
文件读取阶段：
1. MySQL读取指定的文本文件
2. 按照指定的分隔符解析数据
3. 进行字符集转换（如果需要）

数据处理阶段：
4. 验证数据格式和约束
5. 执行字段转换和计算
6. 处理重复数据（IGNORE/REPLACE）

存储写入阶段：
7. 批量写入存储引擎
8. 更新索引和统计信息
9. 记录导入结果统计
```

**⚡ 性能优势分析**
```
普通INSERT性能：
• 每条SQL都要语法分析
• 逐条网络传输和执行
• 频繁的事务提交

LOAD DATA性能：
• 批量文件解析
• 最小化网络传输
• 批量事务提交
• 存储引擎优化路径

性能对比示例：
100万条记录导入耗时：
- INSERT方式：约10-15分钟
- LOAD DATA方式：约1-2分钟
```

### 1.3 适用场景分析


**✅ 最适合的使用场景**
```
数据迁移：
• 从其他数据库系统迁移数据
• 历史数据的批量导入
• 系统升级时的数据转移

日志数据导入：
• Web服务器访问日志
• 应用程序运行日志
• 监控系统采集数据

数据分析：
• 外部数据源导入
• Excel/CSV文件导入
• 定期的数据更新任务
```

**❌ 不适合的场景**
```
实时数据插入：
• 高频的实时写入
• 需要立即反馈的操作
• 事务性要求严格的场景

小批量数据：
• 少于1000条记录
• 频繁的小批量更新
• 交互式数据录入

复杂业务逻辑：
• 需要复杂计算的数据
• 多表关联插入
• 复杂的数据验证逻辑
```

---

## 2. 📋 完整语法结构详解


### 2.1 LOAD DATA语法全貌


**🔸 完整语法结构**
```sql
LOAD DATA 
    [LOW_PRIORITY | CONCURRENT] 
    [LOCAL] INFILE 'file_name'
    [REPLACE | IGNORE]
    INTO TABLE table_name
    [PARTITION (partition_name [, partition_name] ...)]
    [CHARACTER SET charset_name]
    [FIELDS
        [TERMINATED BY 'string']
        [[OPTIONALLY] ENCLOSED BY 'char']
        [ESCAPED BY 'char']
    ]
    [LINES
        [STARTING BY 'string']
        [TERMINATED BY 'string']
    ]
    [IGNORE number {LINES | ROWS}]
    [(col_name_or_user_var [, col_name_or_user_var] ...)]
    [SET col_name={expr | DEFAULT} [, col_name={expr | DEFAULT}] ...]
```

### 2.2 语法结构分解理解


**🔧 执行优先级和并发控制**
```sql
-- 低优先级执行（避免阻塞其他查询）
LOAD DATA LOW_PRIORITY INFILE '/data/users.txt' INTO TABLE users;

-- 并发执行（MyISAM表支持）
LOAD DATA CONCURRENT INFILE '/data/users.txt' INTO TABLE users;

实际应用理解：
• LOW_PRIORITY：在业务高峰期使用，避免影响正常查询
• CONCURRENT：只对MyISAM有效，InnoDB不支持
• 默认情况：正常优先级，会获取表的写锁
```

**📁 文件位置指定**
```sql
-- 服务器端文件（常用）
LOAD DATA INFILE '/var/lib/mysql-files/data.txt' INTO TABLE users;

-- 客户端文件（LOCAL关键字）
LOAD DATA LOCAL INFILE '/home/user/data.txt' INTO TABLE users;

关键区别：
• 不加LOCAL：文件必须在MySQL服务器上
• 加LOCAL：文件在客户端，通过网络传输
• 安全考虑：LOCAL可能有安全风险
```

### 2.3 基础语法示例


**💻 最简单的使用示例**
```sql
-- 创建测试表
CREATE TABLE users (
    id INT AUTO_INCREMENT PRIMARY KEY,
    name VARCHAR(50) NOT NULL,
    age INT,
    email VARCHAR(100),
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);

-- 准备数据文件 users.txt
-- 内容格式：name,age,email
-- 张三,25,zhangsan@email.com
-- 李四,30,lisi@email.com
-- 王五,28,wangwu@email.com

-- 最基础的导入命令
LOAD DATA INFILE '/var/lib/mysql-files/users.txt'
INTO TABLE users
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
IGNORE 1 LINES
(name, age, email);
```

---

## 3. 🔧 文件格式处理参数


### 3.1 字段分隔符设置详解


**🔸 FIELDS TERMINATED BY 字段分隔符**

这个参数告诉MySQL如何识别文件中不同字段之间的分界线，就像告诉它"看到什么符号就知道一个字段结束了"。

```sql
-- 逗号分隔（CSV格式）
FIELDS TERMINATED BY ','
-- 文件内容：张三,25,北京
-- 解析结果：字段1="张三", 字段2="25", 字段3="北京"

-- 制表符分隔（TSV格式）
FIELDS TERMINATED BY '\t'
-- 文件内容：张三    25    北京
-- 解析结果：字段1="张三", 字段2="25", 字段3="北京"

-- 管道符分隔
FIELDS TERMINATED BY '|'
-- 文件内容：张三|25|北京
-- 解析结果：字段1="张三", 字段2="25", 字段3="北京"

-- 多字符分隔符
FIELDS TERMINATED BY ':::'
-- 文件内容：张三:::25:::北京
-- 解析结果：字段1="张三", 字段2="25", 字段3="北京"
```

**💡 实际使用技巧**
```sql
-- 处理包含逗号的数据
-- 错误示例：张三,工程师,月薪5,000,北京 
-- 会被错误解析为5个字段

-- 正确做法：使用引号包围
FIELDS TERMINATED BY ',' ENCLOSED BY '"'
-- 文件内容："张三","工程师","月薪5,000","北京"
-- 正确解析为4个字段
```

### 3.2 字段包围符配置


**🔸 ENCLOSED BY 字段包围符**

这个参数指定用什么符号把字段"包起来"，主要用于处理字段内容包含分隔符的情况。

```sql
-- 强制包围（所有字段都必须用引号）
FIELDS TERMINATED BY ',' ENCLOSED BY '"'
-- 文件格式："张三","25","engineer@company.com"

-- 可选包围（OPTIONALLY关键字）
FIELDS TERMINATED BY ',' OPTIONALLY ENCLOSED BY '"'
-- 文件格式混合：张三,"软件工程师,高级","user@email.com"
-- 只有包含分隔符的字段才需要引号

实际应用场景：
• Excel导出的CSV文件通常使用双引号包围
• 数据中包含特殊字符时必须使用包围符
• 导入外部系统导出的数据文件
```

**🔧 包围符处理示例**
```sql
-- 创建测试表
CREATE TABLE products (
    id INT AUTO_INCREMENT PRIMARY KEY,
    name VARCHAR(100),
    description TEXT,
    price DECIMAL(10,2)
);

-- 数据文件 products.txt 内容：
-- "iPhone 14","高端智能手机,5G网络,A16芯片","6999.00"
-- "MacBook Pro","专业笔记本电脑,M2芯片,16GB内存","12999.00"

-- 导入命令
LOAD DATA INFILE '/var/lib/mysql-files/products.txt'
INTO TABLE products
FIELDS 
    TERMINATED BY ',' 
    ENCLOSED BY '"'
LINES TERMINATED BY '\n'
(name, description, price);
```

### 3.3 转义字符处理


**🔸 ESCAPED BY 转义字符**

当数据中包含特殊字符（如引号、换行符）时，需要用转义字符来处理。

```sql
-- 反斜杠转义（默认）
FIELDS ESCAPED BY '\\'

文件内容示例：
张三,他说\"Hello World\",25
-- 解析后：字段2的值是：他说"Hello World"

-- 自定义转义字符
FIELDS ESCAPED BY '#'
-- 文件内容：张三,他说#"Hello World#",25

-- 禁用转义
FIELDS ESCAPED BY ''
-- 危险：无法处理包含分隔符的数据
```

**💻 转义字符实际应用**
```sql
-- 处理包含换行符的数据
CREATE TABLE comments (
    id INT AUTO_INCREMENT PRIMARY KEY,
    user_name VARCHAR(50),
    comment TEXT
);

-- 数据文件包含多行注释
-- 张三,"这是第一行\n这是第二行\n这是第三行"
-- 李四,"单行评论"

LOAD DATA INFILE '/var/lib/mysql-files/comments.txt'
INTO TABLE comments
FIELDS 
    TERMINATED BY ',' 
    ENCLOSED BY '"'
    ESCAPED BY '\\'
LINES TERMINATED BY '\n'
(user_name, comment);
```

### 3.4 行终止符配置


**🔸 LINES TERMINATED BY 行分隔符**

这个参数告诉MySQL如何识别文件中一行数据的结束。

```sql
-- Unix/Linux系统（\n）
LINES TERMINATED BY '\n'

-- Windows系统（\r\n）  
LINES TERMINATED BY '\r\n'

-- 旧Mac系统（\r）
LINES TERMINATED BY '\r'

-- 自定义行分隔符
LINES TERMINATED BY '||'
-- 文件内容：张三,25,北京||李四,30,上海||
```

**🔸 LINES STARTING BY 行前缀**

有些文件每行都有固定的前缀，可以用这个参数处理。

```sql
-- 处理带前缀的数据
LINES STARTING BY 'DATA:'
-- 文件内容：
-- DATA:张三,25,北京
-- DATA:李四,30,上海
-- COMMENT:这是注释行，会被忽略
-- DATA:王五,28,深圳

LOAD DATA INFILE '/var/lib/mysql-files/prefixed_data.txt'
INTO TABLE users
FIELDS TERMINATED BY ','
LINES 
    STARTING BY 'DATA:'
    TERMINATED BY '\n'
(name, age, city);
```

### 3.5 字符集处理


**🔸 CHARACTER SET 字符集转换**

当文件的字符编码与数据库不同时，需要指定字符集进行转换。

```sql
-- UTF-8文件导入
LOAD DATA INFILE '/data/users.txt'
INTO TABLE users
CHARACTER SET utf8mb4
FIELDS TERMINATED BY ','
(name, age, email);

-- GBK编码文件导入
LOAD DATA INFILE '/data/users_gbk.txt'
INTO TABLE users  
CHARACTER SET gbk
FIELDS TERMINATED BY ','
(name, age, email);

-- 常见字符集对照
utf8mb4  ← 推荐使用，支持完整Unicode
utf8     ← 旧版本，不支持emoji
gbk      ← 中文简体编码
latin1   ← 西欧字符集
```

**⚠️ 字符集问题排查**
```sql
-- 检查表的字符集
SHOW CREATE TABLE users;

-- 检查数据库字符集
SELECT DEFAULT_CHARACTER_SET_NAME 
FROM information_schema.SCHEMATA 
WHERE SCHEMA_NAME = 'your_database';

-- 检查连接字符集
SHOW VARIABLES LIKE 'character_set%';

常见问题：
• 中文显示乱码 → 检查文件编码和数据库字符集
• 导入失败 → 文件编码与指定字符集不匹配
• 数据截断 → 字符集转换时长度溢出
```

---

## 4. 🎯 数据导入模式控制


### 4.1 IGNORE模式 - 忽略错误


**🔸 IGNORE关键字的作用**

当导入的数据与现有数据冲突时，IGNORE模式会跳过有问题的行，继续处理后续数据。

```sql
-- 使用IGNORE模式
LOAD DATA INFILE '/data/users.txt'
IGNORE
INTO TABLE users
FIELDS TERMINATED BY ','
(name, age, email);

处理逻辑：
• 主键冲突 → 跳过这行，继续下一行
• 数据格式错误 → 跳过这行，继续下一行  
• 约束违反 → 跳过这行，继续下一行
• 正常数据 → 正常插入

结果：部分数据导入成功，错误数据被忽略
```

**💡 IGNORE模式实际应用**
```sql
-- 创建带约束的测试表
CREATE TABLE products (
    id INT AUTO_INCREMENT PRIMARY KEY,
    sku VARCHAR(20) UNIQUE NOT NULL,
    name VARCHAR(100) NOT NULL,
    price DECIMAL(10,2) CHECK (price > 0),
    category_id INT NOT NULL
);

-- 测试数据文件 products.txt：
-- SKU001,iPhone 14,6999.00,1
-- SKU002,iPad Pro,4999.00,2  
-- SKU001,重复SKU商品,7999.00,1  ← 违反唯一约束
-- SKU003,MacBook,-1000.00,3     ← 违反CHECK约束
-- SKU004,AirPods,1299.00,4

-- 使用IGNORE导入
LOAD DATA INFILE '/var/lib/mysql-files/products.txt'
IGNORE
INTO TABLE products
FIELDS TERMINATED BY ','
(sku, name, price, category_id);

-- 结果：导入3行成功，2行被忽略
-- 查看导入统计
SHOW WARNINGS;
```

### 4.2 REPLACE模式 - 替换冲突数据


**🔸 REPLACE关键字的作用**

当遇到主键或唯一键冲突时，REPLACE模式会删除旧记录，插入新记录。

```sql
-- 使用REPLACE模式
LOAD DATA INFILE '/data/users.txt'
REPLACE
INTO TABLE users
FIELDS TERMINATED BY ','
(id, name, age, email);

处理逻辑：
• 无冲突 → 直接插入新记录
• 主键冲突 → 删除旧记录，插入新记录
• 唯一键冲突 → 删除旧记录，插入新记录
• 其他约束违反 → 报错停止
```

**⚡ REPLACE vs IGNORE 对比**
```sql
-- 假设表中已存在：id=1, name='张三', age=25

-- 使用IGNORE模式
-- 文件内容：1,张三更新,30
LOAD DATA INFILE '/data/update.txt' IGNORE INTO TABLE users;
-- 结果：记录保持不变（id=1, name='张三', age=25）

-- 使用REPLACE模式  
-- 文件内容：1,张三更新,30
LOAD DATA INFILE '/data/update.txt' REPLACE INTO TABLE users;
-- 结果：记录被更新（id=1, name='张三更新', age=30）

-- 不使用任何模式
-- 文件内容：1,张三更新,30
LOAD DATA INFILE '/data/update.txt' INTO TABLE users;
-- 结果：报错停止，整个导入失败
```

### 4.3 错误处理策略


**📊 三种模式对比表**

| 导入模式 | **遇到冲突时** | **继续执行** | **适用场景** | **风险级别** |
|---------|--------------|-------------|-------------|-------------|
| **默认模式** | `报错停止` | `否` | `数据完整性要求高` | `低` |
| **IGNORE** | `跳过记录` | `是` | `允许丢失部分数据` | `中` |
| **REPLACE** | `覆盖旧数据` | `是` | `需要更新已有数据` | `高` |

**🔧 错误信息查看**
```sql
-- 导入后查看警告信息
SHOW WARNINGS;

-- 常见警告类型：
-- Warning | 1265 | Data truncated for column 'age'
-- Warning | 1062 | Duplicate entry '1' for key 'PRIMARY'
-- Warning | 1366 | Incorrect integer value

-- 查看详细的导入统计
SELECT ROW_COUNT() as '影响行数';

-- 在导入前清空警告
RESET QUERY CACHE;
```

---

## 5. 🔄 字段转换与处理


### 5.1 字段映射和跳过


**🔸 字段顺序映射**

文件中的字段顺序不一定要与表结构一致，可以通过字段列表指定映射关系。

```sql
-- 表结构：id, name, age, email, created_at
-- 文件字段顺序：name, email, age

LOAD DATA INFILE '/data/users.txt'
INTO TABLE users
FIELDS TERMINATED BY ','
(name, email, age);  -- 指定字段映射顺序

-- MySQL会自动处理：
-- id字段：使用AUTO_INCREMENT自动生成
-- created_at字段：使用DEFAULT值
```

**🔧 跳过不需要的字段**
```sql
-- 文件有5个字段，但只需要导入3个
-- 文件内容：name,age,phone,email,address

LOAD DATA INFILE '/data/mixed_data.txt'
INTO TABLE users
FIELDS TERMINATED BY ','
(name, age, @dummy_phone, email, @dummy_address);
-- 使用@dummy_变量接收不需要的字段

-- 或者更简洁的方式
(name, age, @skip, email, @skip);
```

### 5.2 SET子句字段转换


**🔸 SET子句的强大功能**

SET子句允许在导入时对数据进行计算和转换，这是LOAD DATA最灵活的特性之一。

```sql
-- 基础字段转换
LOAD DATA INFILE '/data/users.txt'
INTO TABLE users
FIELDS TERMINATED BY ','
(name, @age_str, email)
SET 
    age = CAST(@age_str AS UNSIGNED),
    created_at = NOW(),
    status = 'active';

-- 复杂数据转换示例
LOAD DATA INFILE '/data/sales.txt'
INTO TABLE sales_records
FIELDS TERMINATED BY ','
(@date_str, @amount_str, customer_name, @tax_rate)
SET 
    sale_date = STR_TO_DATE(@date_str, '%Y-%m-%d'),
    amount = CAST(@amount_str AS DECIMAL(10,2)),
    tax_amount = CAST(@amount_str AS DECIMAL(10,2)) * CAST(@tax_rate AS DECIMAL(3,2)) / 100,
    total_amount = amount + tax_amount,
    created_at = NOW();
```

**💡 SET子句实用技巧**
```sql
-- 处理空值和默认值
(name, @age, @email)
SET 
    age = IF(@age = '', NULL, @age),
    email = IF(@email = '', NULL, @email),
    status = 'imported';

-- 数据清洗和标准化
(@phone)
SET 
    phone = REGEXP_REPLACE(@phone, '[^0-9]', '');  -- 只保留数字

-- 条件字段赋值
(@gender)
SET 
    gender = CASE 
        WHEN @gender IN ('男', 'M', 'Male') THEN 'M'
        WHEN @gender IN ('女', 'F', 'Female') THEN 'F'
        ELSE 'U'
    END;

-- 生成衍生字段
(first_name, last_name)
SET 
    full_name = CONCAT(first_name, ' ', last_name),
    name_length = LENGTH(CONCAT(first_name, last_name));
```

### 5.3 忽略文件头部行


**🔸 IGNORE LINES 跳过行数**

很多数据文件都有标题行或说明行，需要跳过这些非数据行。

```sql
-- 跳过CSV文件的标题行
IGNORE 1 LINES

-- 跳过多行说明
IGNORE 3 LINES

-- 实际应用示例
-- 文件内容 users.csv：
-- name,age,email,phone          ← 标题行，需要跳过
-- 张三,25,zhang@email.com,13800138001
-- 李四,30,li@email.com,13900139002

LOAD DATA INFILE '/var/lib/mysql-files/users.csv'
INTO TABLE users
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n'
IGNORE 1 LINES                   -- 跳过标题行
(name, age, email, phone);
```

---

## 6. 🔐 安全性与权限控制


### 6.1 LOCAL关键字安全风险


**⚠️ LOCAL的安全隐患**

LOCAL关键字允许客户端读取本地文件并传输到服务器，这可能带来严重的安全风险。

```sql
-- 危险示例：恶意客户端可能读取敏感文件
LOAD DATA LOCAL INFILE '/etc/passwd'
INTO TABLE temp_table
FIELDS TERMINATED BY ':';

-- 攻击者可能获取：
• 系统用户信息
• 应用配置文件
• 其他敏感数据
```

**🔒 LOCAL安全防护措施**
```sql
-- 1. 服务器端禁用LOCAL（推荐）
-- 在my.cnf中配置：
[mysqld]
local-infile = 0

-- 2. 客户端连接时禁用
mysql --local-infile=0 -u user -p

-- 3. 应用程序中禁用
-- JDBC连接字符串：
jdbc:mysql://localhost:3306/mydb?allowLoadLocalInfile=false

-- 4. 检查当前设置
SHOW GLOBAL VARIABLES LIKE 'local_infile';
```

### 6.2 文件权限和路径安全


**🔸 secure_file_priv 安全目录**

MySQL通过`secure_file_priv`变量限制文件操作的目录，防止任意文件访问。

```sql
-- 查看安全目录设置
SHOW VARIABLES LIKE 'secure_file_priv';

-- 可能的返回值：
-- 1. 空值：可以访问任意目录（不安全）
-- 2. NULL：禁止文件操作
-- 3. 目录路径：只能访问指定目录

-- 安全配置示例（my.cnf）：
[mysqld]
secure_file_priv = /var/lib/mysql-files/
```

**📁 文件路径安全实践**
```bash
# 1. 创建专用的导入目录
sudo mkdir -p /var/lib/mysql-files
sudo chown mysql:mysql /var/lib/mysql-files
sudo chmod 750 /var/lib/mysql-files

# 2. 复制文件到安全目录
sudo cp /home/user/data.txt /var/lib/mysql-files/
sudo chown mysql:mysql /var/lib/mysql-files/data.txt
sudo chmod 644 /var/lib/mysql-files/data.txt

# 3. 验证文件权限
ls -la /var/lib/mysql-files/

# 4. 执行安全的导入
mysql -u root -p << EOF
LOAD DATA INFILE '/var/lib/mysql-files/data.txt'
INTO TABLE users
FIELDS TERMINATED BY ',';
EOF
```

### 6.3 权限控制最佳实践


**🔑 用户权限配置**
```sql
-- 为数据导入创建专用用户
CREATE USER 'data_import'@'localhost' IDENTIFIED BY 'secure_password';

-- 授予必要的权限
GRANT INSERT, FILE ON *.* TO 'data_import'@'localhost';
GRANT SELECT, INSERT, UPDATE, DELETE ON mydb.* TO 'data_import'@'localhost';

-- 权限说明：
-- FILE权限：执行LOAD DATA INFILE必需
-- INSERT权限：导入数据到表中
-- UPDATE权限：REPLACE模式需要
-- DELETE权限：REPLACE模式需要

-- 检查用户权限
SHOW GRANTS FOR 'data_import'@'localhost';
```

**🛡️ 安全检查清单**
```sql
-- 安全配置验证脚本
SELECT 
    '安全配置检查' as '检查项',
    CASE 
        WHEN $$secure_file_priv IS NULL THEN '❌ 文件操作被禁用'
        WHEN $$secure_file_priv = '' THEN '⚠️ 可访问任意目录（危险）'
        ELSE CONCAT('✅ 限制目录: ', $$secure_file_priv)
    END as '状态';

SELECT 
    'LOCAL INFILE设置' as '检查项',
    CASE 
        WHEN $$local_infile = 1 THEN '⚠️ LOCAL导入已启用'
        ELSE '✅ LOCAL导入已禁用'
    END as '状态';

-- 检查当前用户权限
SELECT 
    '当前用户FILE权限' as '检查项',
    IF(
        EXISTS(
            SELECT 1 FROM information_schema.user_privileges 
            WHERE grantee = CONCAT("'", USER(), "'") 
            AND privilege_type = 'FILE'
        ),
        '✅ 有FILE权限',
        '❌ 无FILE权限'
    ) as '状态';
```

---

## 7. 💼 实际应用案例


### 7.1 CSV文件导入完整案例


**📊 电商订单数据导入**
```sql
-- 1. 创建订单表
CREATE TABLE orders (
    order_id INT AUTO_INCREMENT PRIMARY KEY,
    order_no VARCHAR(32) UNIQUE NOT NULL,
    customer_name VARCHAR(100) NOT NULL,
    product_name VARCHAR(200),
    quantity INT CHECK (quantity > 0),
    unit_price DECIMAL(10,2),
    total_amount DECIMAL(12,2),
    order_date DATE,
    status ENUM('pending', 'paid', 'shipped', 'completed') DEFAULT 'pending',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP
);

-- 2. 数据文件 orders.csv 示例内容：
-- 订单编号,客户姓名,商品名称,数量,单价,订单日期
-- ORD20250101001,张三,iPhone 14 Pro,1,7999.00,2025-01-01
-- ORD20250101002,李四,MacBook Pro 14寸,2,12999.00,2025-01-01  
-- ORD20250101003,王五,AirPods Pro,3,1499.00,2025-01-02

-- 3. 完整的导入命令
LOAD DATA INFILE '/var/lib/mysql-files/orders.csv'
INTO TABLE orders
CHARACTER SET utf8mb4
FIELDS 
    TERMINATED BY ','
    OPTIONALLY ENCLOSED BY '"'
    ESCAPED BY '\\'
LINES TERMINATED BY '\n'
IGNORE 1 LINES  -- 跳过标题行
(order_no, customer_name, product_name, quantity, unit_price, @order_date_str)
SET 
    total_amount = quantity * unit_price,
    order_date = STR_TO_DATE(@order_date_str, '%Y-%m-%d'),
    status = 'pending';

-- 4. 验证导入结果
SELECT 
    COUNT(*) as '导入记录数',
    MIN(order_date) as '最早订单日期',
    MAX(order_date) as '最晚订单日期',
    SUM(total_amount) as '订单总金额'
FROM orders;
```

### 7.2 复杂数据格式处理


**🔧 处理特殊格式的数据文件**
```sql
-- 1. 日志文件导入示例
CREATE TABLE access_logs (
    id INT AUTO_INCREMENT PRIMARY KEY,
    ip_address VARCHAR(45),
    request_time DATETIME,
    http_method VARCHAR(10),
    url VARCHAR(500),
    http_status INT,
    response_size BIGINT,
    user_agent TEXT,
    referer VARCHAR(500)
);

-- 2. Apache访问日志格式：
-- 192.168.1.1 - - [01/Jan/2025:10:30:45 +0800] "GET /api/users HTTP/1.1" 200 1234 "https://example.com" "Mozilla/5.0 ..."

-- 3. 处理复杂格式的导入
LOAD DATA INFILE '/var/lib/mysql-files/access.log'
INTO TABLE access_logs
FIELDS TERMINATED BY ' '
LINES TERMINATED BY '\n'
(ip_address, @skip1, @skip2, @datetime_str, @request_str, http_status, response_size, @referer_str, @useragent_str)
SET 
    request_time = STR_TO_DATE(
        TRIM(BOTH '"' FROM SUBSTRING(@datetime_str, 2, LENGTH(@datetime_str)-2)), 
        '%d/%b/%Y:%H:%i:%s'
    ),
    http_method = TRIM(BOTH '"' FROM SUBSTRING_INDEX(@request_str, ' ', 1)),
    url = TRIM(BOTH '"' FROM SUBSTRING_INDEX(SUBSTRING_INDEX(@request_str, ' ', 2), ' ', -1)),
    referer = TRIM(BOTH '"' FROM @referer_str),
    user_agent = TRIM(BOTH '"' FROM @useragent_str);
```

### 7.3 批量数据更新案例


**🔄 使用LOAD DATA更新现有数据**
```sql
-- 场景：批量更新商品价格
-- 1. 创建临时表
CREATE TABLE temp_price_update (
    sku VARCHAR(20),
    new_price DECIMAL(10,2),
    effective_date DATE
);

-- 2. 导入价格更新数据
LOAD DATA INFILE '/var/lib/mysql-files/price_update.txt'
INTO TABLE temp_price_update
FIELDS TERMINATED BY ','
(sku, new_price, @date_str)
SET effective_date = STR_TO_DATE(@date_str, '%Y-%m-%d');

-- 3. 使用JOIN更新主表
UPDATE products p
JOIN temp_price_update t ON p.sku = t.sku
SET 
    p.price = t.new_price,
    p.price_update_date = t.effective_date,
    p.updated_at = NOW()
WHERE t.effective_date <= CURDATE();

-- 4. 清理临时表
DROP TABLE temp_price_update;
```

### 7.4 多文件批量导入


**📁 批量处理多个文件**
```bash
#!/bin/bash
# batch_import.sh - 批量导入工具

MYSQL_USER="import_user"
MYSQL_PASSWORD="secure_password"
MYSQL_DATABASE="mydb"
DATA_DIR="/var/lib/mysql-files"

# 批量导入函数
batch_import() {
    local table_name=$1
    local file_pattern=$2
    
    echo "开始批量导入到表: $table_name"
    
    # 查找匹配的文件
    for file in $DATA_DIR/$file_pattern; do
        if [ -f "$file" ]; then
            echo "正在导入文件: $(basename $file)"
            
            # 执行导入
            mysql -u$MYSQL_USER -p$MYSQL_PASSWORD $MYSQL_DATABASE << EOF
LOAD DATA INFILE '$file'
IGNORE
INTO TABLE $table_name
CHARACTER SET utf8mb4
FIELDS TERMINATED BY ','
OPTIONALLY ENCLOSED BY '"'
LINES TERMINATED BY '\n'
IGNORE 1 LINES;

-- 显示导入统计
SELECT ROW_COUNT() as '本次导入行数', NOW() as '导入时间';
EOF
            
            # 移动已处理的文件
            mv "$file" "$DATA_DIR/processed/"
            echo "文件处理完成: $(basename $file)"
        fi
    done
    
    echo "批量导入完成"
}

# 使用示例
batch_import "users" "users_*.csv"
batch_import "orders" "order_data_*.txt"
```

### 7.5 导入性能优化


**⚡ 大数据量导入优化策略**
```sql
-- 1. 导入前的优化设置
SET autocommit = 0;              -- 禁用自动提交
SET unique_checks = 0;           -- 临时禁用唯一性检查
SET foreign_key_checks = 0;      -- 临时禁用外键检查
SET sql_log_bin = 0;            -- 禁用二进制日志（从服务器）

-- 2. 执行导入
LOAD DATA INFILE '/var/lib/mysql-files/big_data.txt'
INTO TABLE large_table
FIELDS TERMINATED BY '\t'
LINES TERMINATED BY '\n';

-- 3. 恢复设置
COMMIT;                         -- 提交事务
SET autocommit = 1;
SET unique_checks = 1;
SET foreign_key_checks = 1;
SET sql_log_bin = 1;

-- 4. 重建索引（如果需要）
ALTER TABLE large_table ENABLE KEYS;
ANALYZE TABLE large_table;
```

**🔧 分块导入策略**
```bash
#!/bin/bash
# split_import.sh - 大文件分块导入

SOURCE_FILE="/data/huge_data.txt"
CHUNK_SIZE=100000  # 每块10万行
TABLE_NAME="large_table"

# 分割大文件
split -l $CHUNK_SIZE $SOURCE_FILE chunk_

# 逐块导入
for chunk_file in chunk_*; do
    echo "导入文件块: $chunk_file"
    
    mysql -u$MYSQL_USER -p$MYSQL_PASSWORD $MYSQL_DATABASE << EOF
LOAD DATA INFILE '/var/lib/mysql-files/$chunk_file'
INTO TABLE $TABLE_NAME
FIELDS TERMINATED BY ','
LINES TERMINATED BY '\n';
EOF
    
    # 检查导入结果
    if [ $? -eq 0 ]; then
        echo "✅ $chunk_file 导入成功"
        rm $chunk_file  # 删除已处理的块文件
    else
        echo "❌ $chunk_file 导入失败"
        exit 1
    fi
done

echo "🎉 所有文件块导入完成"
```

---

## 8. 📋 核心要点总结


### 8.1 必须掌握的核心概念


**🔸 LOAD DATA的本质理解**
```
什么是LOAD DATA：
• MySQL专门的批量导入工具
• 直接操作存储引擎，性能极高
• 支持复杂的文件格式处理
• 比普通INSERT快10-20倍

为什么这么快：
• 绕过SQL解析开销
• 批量操作减少网络传输
• 存储引擎级别的优化
• 最小化事务开销
```

**🔸 语法结构的逻辑**
```
语法组成部分：
文件位置 → 导入模式 → 目标表 → 格式定义 → 字段处理

记忆顺序：
1. 从哪里读（INFILE）
2. 怎么处理冲突（IGNORE/REPLACE）  
3. 导入到哪里（INTO TABLE）
4. 文件格式如何（FIELDS/LINES）
5. 数据如何转换（SET子句）
```

### 8.2 关键实践要点


**🔹 文件格式处理的核心逻辑**
```
分隔符设置思路：
• TERMINATED BY：字段之间用什么分开
• ENCLOSED BY：字段内容用什么包围
• ESCAPED BY：特殊字符怎么转义
• LINES：每行数据怎么识别

实际应用口诀：
"分隔包围转义行，四个参数要配齐"
```

**🔹 错误处理的选择策略**
```
模式选择判断：
数据质量高，不允许错误 → 默认模式（遇错即停）
允许部分数据丢失 → IGNORE模式（跳过错误）
需要更新已有数据 → REPLACE模式（覆盖冲突）

风险控制：
• 生产环境先用小文件测试
• 重要数据导入前先备份
• 使用事务包装，出错可回滚
```

**🔹 安全性的重要认知**
```
LOCAL的风险：
• 客户端文件读取权限过大
• 可能被恶意利用读取敏感文件
• 生产环境建议禁用

安全措施：
• 使用secure_file_priv限制目录
• 创建专用的导入用户
• 最小权限原则
• 定期审计文件操作日志
```

### 8.3 性能优化核心策略


**⚡ 导入速度优化**
```
影响性能的因素：
• 索引数量：索引越多，导入越慢
• 约束检查：外键和唯一性检查耗时
• 事务大小：过大过小都不好
• 存储引擎：InnoDB vs MyISAM特性不同

优化策略：
1. 导入前删除非主键索引，导入后重建
2. 临时禁用约束检查
3. 适当调整批次大小
4. 使用专用的导入实例
```

**📊 实际性能测试数据**
```
测试环境：4核8GB MySQL 8.0

100万条记录导入对比：
• 普通INSERT：15分钟
• LOAD DATA（有索引）：3分钟  
• LOAD DATA（无索引）：1分钟
• LOAD DATA（优化后）：30秒

关键优化效果：
• 禁用索引检查：速度提升50%
• 调整缓冲池：速度提升20%
• 禁用二进制日志：速度提升30%
```

### 8.4 实际应用指导


**🎯 使用决策流程**
```
数据量判断
    ↓
< 1万条 → 使用普通INSERT
    ↓
1万-10万条 → 考虑LOAD DATA  
    ↓
> 10万条 → 强烈推荐LOAD DATA
    ↓
> 100万条 → 必须优化+分块导入
```

**🔧 实施步骤标准化**
```
导入前准备：
1. 检查文件格式和编码
2. 验证表结构和约束
3. 备份现有数据
4. 设置安全目录和权限

导入执行：
1. 复制文件到安全目录
2. 测试小批量导入
3. 执行完整导入
4. 验证导入结果

导入后处理：
1. 检查数据完整性
2. 重建索引和统计信息
3. 清理临时文件
4. 记录导入日志
```

**💡 常见问题解决**
```
问题1：中文乱码
解决：检查文件编码，指定正确的CHARACTER SET

问题2：导入失败权限不足
解决：检查FILE权限和secure_file_priv设置

问题3：部分数据导入失败
解决：查看SHOW WARNINGS了解具体错误

问题4：导入速度慢
解决：临时禁用索引和约束检查

问题5：数据格式不匹配
解决：使用SET子句进行数据转换
```

**核心记忆**：
- LOAD DATA是批量导入的最佳选择，性能比INSERT快十几倍
- 文件格式参数要根据实际数据调整，分隔符、包围符、转义符缺一不可
- 安全性要重视，LOCAL关键字有风险，生产环境要限制文件目录
- 错误处理模式要根据业务需求选择，IGNORE跳过错误，REPLACE覆盖冲突
- SET子句是数据转换的利器，可以在导入时进行复杂的字段计算