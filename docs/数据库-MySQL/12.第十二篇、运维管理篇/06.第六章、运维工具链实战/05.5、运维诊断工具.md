---
title: 5ã€è¿ç»´è¯Šæ–­å·¥å…·
---
## ğŸ“š ç›®å½•

1. [MySQL Enterprise Monitorä¼ä¸šç›‘æ§](#1-MySQL-Enterprise-Monitorä¼ä¸šç›‘æ§)
2. [MySQL Workbenchè¿ç»´åŠŸèƒ½](#2-MySQL-Workbenchè¿ç»´åŠŸèƒ½)
3. [MySQL Shellè¿ç»´è„šæœ¬å¼€å‘](#3-MySQL-Shellè¿ç»´è„šæœ¬å¼€å‘)
4. [pt-toolkitå·¥å…·æ·±åº¦åº”ç”¨](#4-pt-toolkitå·¥å…·æ·±åº¦åº”ç”¨)
5. [è‡ªç ”è¿ç»´å·¥å…·å¼€å‘æ¡†æ¶](#5-è‡ªç ”è¿ç»´å·¥å…·å¼€å‘æ¡†æ¶)
6. [ç¬¬ä¸‰æ–¹è¿ç»´å·¥å…·é›†æˆ](#6-ç¬¬ä¸‰æ–¹è¿ç»´å·¥å…·é›†æˆ)
7. [è¿ç»´å·¥å…·æ•ˆæœè¯„ä¼°](#7-è¿ç»´å·¥å…·æ•ˆæœè¯„ä¼°)
8. [MySQL X Protocolè¿ç»´åº”ç”¨](#8-MySQL-X-Protocolè¿ç»´åº”ç”¨)
9. [MySQL Document Storeè¿ç»´](#9-MySQL-Document-Storeè¿ç»´)
10. [MySQL Routerè¿ç»´ç®¡ç†](#10-MySQL-Routerè¿ç»´ç®¡ç†)
11. [MySQL Group Replicationè¿ç»´](#11-MySQL-Group-Replicationè¿ç»´)
12. [MySQL InnoDB Clusterè¿ç»´](#12-MySQL-InnoDB-Clusterè¿ç»´)
13. [æ ¸å¿ƒè¦ç‚¹æ€»ç»“](#13-æ ¸å¿ƒè¦ç‚¹æ€»ç»“)

---

## 1. ğŸ¢ MySQL Enterprise Monitorä¼ä¸šç›‘æ§


### 1.1 ä»€ä¹ˆæ˜¯MySQL Enterprise Monitor


**MySQL Enterprise Monitorï¼ˆMEMï¼‰**æ˜¯Oracleå®˜æ–¹æä¾›çš„ä¼ä¸šçº§MySQLç›‘æ§å’Œç®¡ç†å·¥å…·ï¼Œä¸“é—¨ä¸ºå¤§å‹MySQLéƒ¨ç½²ç¯å¢ƒè®¾è®¡ã€‚

**ğŸ”¸ æ ¸å¿ƒä½œç”¨**
```
ç®€å•ç†è§£ï¼šå°±åƒç»™MySQLé…äº†ä¸ª"ä½“æ£€åŒ»ç”Ÿ"
- 24å°æ—¶ä¸é—´æ–­ç›‘æ§MySQLçš„å¥åº·çŠ¶å†µ
- å‘ç°é—®é¢˜ç«‹å³æŠ¥è­¦ï¼Œé˜²æ­¢å°é—®é¢˜å˜å¤§æ•…éšœ
- æä¾›ä¸“ä¸šçš„æ€§èƒ½è°ƒä¼˜å»ºè®®
- è‡ªåŠ¨åŒ–å¾ˆå¤šæ—¥å¸¸è¿ç»´å·¥ä½œ
```

### 1.2 MEMæ ¸å¿ƒåŠŸèƒ½è¯¦è§£


**ğŸ“Š å®æ—¶ç›‘æ§åŠŸèƒ½**
```
ç›‘æ§ä»€ä¹ˆï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ æœåŠ¡å™¨æ€§èƒ½æŒ‡æ ‡   â”‚ â† CPUã€å†…å­˜ã€ç£ç›˜ä½¿ç”¨ç‡
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ MySQLè¿è¡ŒçŠ¶æ€    â”‚ â† è¿æ¥æ•°ã€æŸ¥è¯¢æ€§èƒ½ã€é”ç­‰å¾…
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ å¤åˆ¶æ¶æ„ç›‘æ§     â”‚ â† ä¸»ä»å»¶è¿Ÿã€å¤åˆ¶é”™è¯¯
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ å®‰å…¨å¨èƒæ£€æµ‹     â”‚ â† å¼‚å¸¸ç™»å½•ã€SQLæ³¨å…¥å°è¯•
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ€ä¹ˆç›‘æ§ï¼š
æ¯å‡ ç§’é’Ÿæ”¶é›†ä¸€æ¬¡æ•°æ® â†’ å­˜å‚¨åˆ°ç›‘æ§æ•°æ®åº“ â†’ å›¾è¡¨å±•ç¤º â†’ å¼‚å¸¸æŠ¥è­¦
```

**âš ï¸ æ™ºèƒ½æŠ¥è­¦ç³»ç»Ÿ**
```java
// æŠ¥è­¦è§„åˆ™ç¤ºä¾‹ï¼ˆä¼ªä»£ç ï¼‰
if (è¿æ¥æ•° > æœ€å¤§è¿æ¥æ•° * 0.8) {
    å‘é€è­¦å‘Š("è¿æ¥æ•°æ¥è¿‘ä¸Šé™");
}

if (ä¸»ä»å»¶è¿Ÿ > 30ç§’) {
    å‘é€ç´§æ€¥æŠ¥è­¦("ä¸»ä»åŒæ­¥å»¶è¿Ÿä¸¥é‡");
}

if (æ…¢æŸ¥è¯¢å¢é•¿ç‡ > 50%) {
    å‘é€å»ºè®®("å‘ç°æ€§èƒ½é—®é¢˜ï¼Œå»ºè®®æ£€æŸ¥ç´¢å¼•");
}
```

### 1.3 MEMéƒ¨ç½²æ¶æ„


**ğŸ—ï¸ å…¸å‹éƒ¨ç½²ç»“æ„**
```
ç›‘æ§æ¶æ„å›¾ï¼š

Webæ§åˆ¶å°                Service Manager           MySQLå®ä¾‹ç¾¤
    â†“                        â†“                      â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   æµè§ˆå™¨     â”‚ â†â†’  â”‚  MEMæœåŠ¡å™¨      â”‚ â†â†’  â”‚  MySQL1     â”‚
â”‚ (ç®¡ç†å‘˜è®¿é—®) â”‚      â”‚ - æ•°æ®æ”¶é›†      â”‚      â”‚ + Agent     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ - æŠ¥è­¦å¤„ç†      â”‚      â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                     â”‚ - å›¾è¡¨ç”Ÿæˆ      â”‚      â”‚  MySQL2     â”‚
                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â”‚ + Agent     â”‚
                                              â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                                              â”‚  MySQL3     â”‚
                                              â”‚ + Agent     â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ğŸ”§ å®‰è£…é…ç½®è¦ç‚¹**
```bash
# 1. å®‰è£…Service Managerï¼ˆç›‘æ§æœåŠ¡å™¨ï¼‰
# éœ€è¦ç‹¬ç«‹çš„æœåŠ¡å™¨ï¼Œé…ç½®è¦æ±‚ï¼š
# - 4æ ¸8GBå†…å­˜èµ·æ­¥
# - 100GB+ å­˜å‚¨ç©ºé—´
# - ç½‘ç»œè¿é€šæ‰€æœ‰è¢«ç›‘æ§MySQLå®ä¾‹

# 2. åœ¨æ¯ä¸ªMySQLæœåŠ¡å™¨å®‰è£…Agent
# Agentä½œç”¨ï¼šæ”¶é›†æœ¬æœºMySQLæ•°æ®å‘é€ç»™Service Manager
wget mysql-monitor-agent.tar.gz
tar -xzf mysql-monitor-agent.tar.gz
./agent-install.sh --service-manager-host=monitor.company.com
```

### 1.4 MEMå®é™…åº”ç”¨åœºæ™¯


**ğŸ“ˆ æ€§èƒ½ç›‘æ§åœºæ™¯**
```
åœºæ™¯1ï¼šç”µå•†å¤§ä¿ƒç›‘æ§
é—®é¢˜ï¼šåŒ11æœŸé—´MySQLè´Ÿè½½æš´å¢
è§£å†³ï¼š
- MEMç›‘æ§QPSã€TPSã€è¿æ¥æ•°å˜åŒ–è¶‹åŠ¿
- æå‰é¢„è­¦èµ„æºä¸è¶³
- è‡ªåŠ¨ç”Ÿæˆæ€§èƒ½æŠ¥å‘Šç»™å¼€å‘å›¢é˜Ÿ

åœºæ™¯2ï¼šæ…¢æŸ¥è¯¢æ²»ç†
é—®é¢˜ï¼šç³»ç»Ÿå“åº”è¶Šæ¥è¶Šæ…¢
è§£å†³ï¼š
- MEMè‡ªåŠ¨æŠ“å–æ…¢æŸ¥è¯¢
- åˆ†ææŸ¥è¯¢æ‰§è¡Œè®¡åˆ’
- æ¨èç´¢å¼•ä¼˜åŒ–å»ºè®®
```

---

## 2. ğŸ› ï¸ MySQL Workbenchè¿ç»´åŠŸèƒ½


### 2.1 Workbenchè¿ç»´å·¥å…·æ¦‚è¿°


**MySQL Workbench**ä¸åªæ˜¯ä¸ªæ•°æ®åº“è®¾è®¡å·¥å…·ï¼Œå®ƒçš„è¿ç»´åŠŸèƒ½éå¸¸å¼ºå¤§ï¼Œç‰¹åˆ«é€‚åˆæ—¥å¸¸æ•°æ®åº“ç®¡ç†å·¥ä½œã€‚

**ğŸ”¸ è¿ç»´åŠŸèƒ½åˆ†ç±»**
```
Workbenchè¿ç»´åŠŸèƒ½å…¨æ™¯ï¼š

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   æ€§èƒ½ç›‘æ§       â”‚ â† å®æ—¶æ€§èƒ½å›¾è¡¨ã€æ…¢æŸ¥è¯¢åˆ†æ
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   æœåŠ¡å™¨ç®¡ç†     â”‚ â† å¯åŠ¨åœæ­¢ã€é…ç½®ç®¡ç†ã€ç”¨æˆ·ç®¡ç†
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   æ•°æ®å¯¼å…¥å¯¼å‡º   â”‚ â† å¤§æ•°æ®é‡è¿ç§»ã€å¤‡ä»½æ¢å¤
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   SQLå¼€å‘è°ƒè¯•    â”‚ â† æŸ¥è¯¢ä¼˜åŒ–ã€æ‰§è¡Œè®¡åˆ’åˆ†æ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 2.2 Performance Dashboardæ€§èƒ½ä»ªè¡¨æ¿


**ğŸ“Š æ€§èƒ½ç›‘æ§ç•Œé¢è¯¦è§£**
```
Performance Dashboardæ˜¾ç¤ºå†…å®¹ï¼š

å®æ—¶æ€§èƒ½æŒ‡æ ‡ï¼š
- QPS/TPSï¼šæ¯ç§’æŸ¥è¯¢æ•°å’Œäº‹åŠ¡æ•°
- è¿æ¥æ•°ï¼šå½“å‰æ´»è·ƒè¿æ¥ vs æœ€å¤§è¿æ¥æ•°  
- InnoDBç¼“å†²æ± ï¼šå‘½ä¸­ç‡ã€è„é¡µæ¯”ä¾‹
- ç½‘ç»œæµé‡ï¼šè¿›å‡ºæ•°æ®é‡ç»Ÿè®¡

å›¾è¡¨ç±»å‹ï¼š
ğŸ“ˆ æŠ˜çº¿å›¾ï¼šæ˜¾ç¤ºè¶‹åŠ¿å˜åŒ–
ğŸ“Š æŸ±çŠ¶å›¾ï¼šæ˜¾ç¤ºåˆ†ç±»ç»Ÿè®¡  
ğŸ¯ ä»ªè¡¨ç›˜ï¼šæ˜¾ç¤ºå½“å‰çŠ¶æ€
ğŸ“‹ è¡¨æ ¼ï¼šæ˜¾ç¤ºè¯¦ç»†æ•°æ®
```

**ğŸ” æ…¢æŸ¥è¯¢åˆ†æåŠŸèƒ½**
```sql
-- Workbenchå¯ä»¥ç›´æ¥åˆ†ææ…¢æŸ¥è¯¢æ—¥å¿—
-- æ˜¾ç¤ºå†…å®¹ï¼š
SELECT 
    sql_text,           -- å…·ä½“SQLè¯­å¥
    exec_count,         -- æ‰§è¡Œæ¬¡æ•°
    total_latency,      -- æ€»è€—æ—¶
    avg_latency,        -- å¹³å‡è€—æ—¶
    rows_examined_avg   -- å¹³å‡æ‰«æè¡Œæ•°
FROM performance_schema.events_statements_summary_by_digest
ORDER BY total_latency DESC
LIMIT 10;

-- Workbenchå›¾å½¢åŒ–æ˜¾ç¤ºè¿™äº›æ•°æ®ï¼Œå¹¶æä¾›ä¼˜åŒ–å»ºè®®
```

### 2.3 Server AdministrationæœåŠ¡å™¨ç®¡ç†


**ğŸ”§ é…ç½®æ–‡ä»¶ç®¡ç†**
```ini
# Workbenchå¯ä»¥å›¾å½¢åŒ–ç¼–è¾‘my.cnfé…ç½®
# ä¸»è¦é…ç½®é¡¹ï¼š

[mysqld]
# è¿æ¥ç›¸å…³
max_connections = 1000              # æœ€å¤§è¿æ¥æ•°
max_connect_errors = 100           # æœ€å¤§è¿æ¥é”™è¯¯æ•°

# å†…å­˜ç›¸å…³  
innodb_buffer_pool_size = 8G       # InnoDBç¼“å†²æ± å¤§å°
query_cache_size = 256M            # æŸ¥è¯¢ç¼“å­˜å¤§å°

# æ—¥å¿—ç›¸å…³
slow_query_log = ON                # å¼€å¯æ…¢æŸ¥è¯¢æ—¥å¿—
long_query_time = 2                # æ…¢æŸ¥è¯¢é˜ˆå€¼2ç§’
```

> **ğŸ’¡ é…ç½®ä¿®æ”¹æç¤º**ï¼šWorkbenchä¼šéªŒè¯é…ç½®å‚æ•°çš„åˆæ³•æ€§ï¼Œå¹¶æä¾›å‚æ•°è¯´æ˜ï¼Œé¿å…é…ç½®é”™è¯¯å¯¼è‡´MySQLæ— æ³•å¯åŠ¨ã€‚

### 2.4 æ•°æ®å¯¼å…¥å¯¼å‡ºå·¥å…·


**ğŸ“¦ Data Export/ImportåŠŸèƒ½**
```
æ•°æ®å¯¼å‡ºåœºæ™¯ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   ç»“æ„+æ•°æ®     â”‚ â† å®Œæ•´å¤‡ä»½ï¼ŒåŒ…å«è¡¨ç»“æ„å’Œæ‰€æœ‰æ•°æ®
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  
â”‚   ä»…ç»“æ„        â”‚ â† åªå¯¼å‡ºè¡¨ç»“æ„ï¼Œç”¨äºç¯å¢ƒæ­å»º
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   ä»…æ•°æ®        â”‚ â† åªå¯¼å‡ºæ•°æ®ï¼Œç”¨äºæ•°æ®è¿ç§»
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   è‡ªå®šä¹‰ç­›é€‰    â”‚ â† æŒ‰æ¡ä»¶å¯¼å‡ºéƒ¨åˆ†æ•°æ®
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ”¯æŒæ ¼å¼ï¼š
- SQLæ–‡ä»¶ï¼šæ ‡å‡†SQLè¯­å¥ï¼Œé€šç”¨æ€§å¥½
- CSVæ–‡ä»¶ï¼šçº¯æ•°æ®ï¼ŒExcelå¯ç›´æ¥æ‰“å¼€
- JSONæ–‡ä»¶ï¼šç°ä»£åº”ç”¨å¸¸ç”¨æ ¼å¼
```

---

## 3. ğŸš MySQL Shellè¿ç»´è„šæœ¬å¼€å‘


### 3.1 MySQL ShellåŸºç¡€æ¦‚å¿µ


**MySQL Shell**æ˜¯MySQL 8.0+ç‰ˆæœ¬æä¾›çš„é«˜çº§å‘½ä»¤è¡Œå·¥å…·ï¼Œæ”¯æŒ**JavaScript**ã€**Python**å’Œ**SQL**ä¸‰ç§ç¼–ç¨‹è¯­è¨€ï¼Œä¸“é—¨ä¸ºè¿ç»´è‡ªåŠ¨åŒ–è®¾è®¡ã€‚

**ğŸ”¸ ä¸ºä»€ä¹ˆéœ€è¦MySQL Shell**
```
ä¼ ç»Ÿmysqlå®¢æˆ·ç«¯çš„å±€é™ï¼š
âŒ åªèƒ½æ‰§è¡ŒSQLè¯­å¥
âŒ æ— æ³•ç¼–å†™å¤æ‚è„šæœ¬
âŒ ä¸æ”¯æŒæ¡ä»¶åˆ¤æ–­å’Œå¾ªç¯
âŒ æ— æ³•è°ƒç”¨ç³»ç»Ÿå‘½ä»¤

MySQL Shellçš„ä¼˜åŠ¿ï¼š
âœ… æ”¯æŒç¼–ç¨‹è¯­è¨€ï¼ˆJS/Pythonï¼‰
âœ… å†…ç½®ä¸°å¯Œçš„ç®¡ç†API
âœ… å¯ä»¥ç¼–å†™å¤æ‚è¿ç»´è„šæœ¬
âœ… æ”¯æŒé›†ç¾¤ç®¡ç†æ“ä½œ
```

### 3.2 Shellç¼–ç¨‹æ¨¡å¼è¯¦è§£


**ğŸ”¤ ä¸‰ç§ç¼–ç¨‹æ¨¡å¼å¯¹æ¯”**

| æ¨¡å¼ | **é€‚ç”¨åœºæ™¯** | **è¯­æ³•ç‰¹ç‚¹** | **ç¤ºä¾‹** |
|------|-------------|-------------|----------|
| ğŸ—„ï¸ **SQLæ¨¡å¼** | `æ—¥å¸¸æŸ¥è¯¢æ“ä½œ` | `æ ‡å‡†SQLè¯­æ³•` | `SELECT * FROM users;` |
| ğŸŸ¨ **JavaScriptæ¨¡å¼** | `è¿ç»´è„šæœ¬å¼€å‘` | `JSè¯­æ³•+MySQL API` | `shell.connect('user@host')` |
| ğŸ **Pythonæ¨¡å¼** | `æ•°æ®åˆ†æè„šæœ¬` | `Pythonè¯­æ³•+MySQL API` | `session.run_sql("SHOW TABLES")` |

### 3.3 è¿ç»´è„šæœ¬å¼€å‘å®ä¾‹


**ğŸ“Š æ•°æ®åº“å¥åº·æ£€æŸ¥è„šæœ¬**
```javascript
// health_check.js - æ•°æ®åº“å¥åº·æ£€æŸ¥è„šæœ¬
function checkDatabaseHealth(connectionString) {
    // è¿æ¥æ•°æ®åº“
    var session = mysql.getSession(connectionString);
    
    // æ£€æŸ¥è¿æ¥æ•°
    var result = session.runSql("SHOW STATUS LIKE 'Threads_connected'");
    var connections = result.fetchOne()[1];
    
    // æ£€æŸ¥ç¼“å†²æ± å‘½ä¸­ç‡
    var bufferResult = session.runSql(`
        SELECT 
            (1 - (Innodb_buffer_pool_reads / Innodb_buffer_pool_read_requests)) * 100 
            AS hit_rate
        FROM INFORMATION_SCHEMA.GLOBAL_STATUS 
        WHERE VARIABLE_NAME IN ('Innodb_buffer_pool_reads', 'Innodb_buffer_pool_read_requests')
    `);
    
    // ç”ŸæˆæŠ¥å‘Š
    console.log("=== æ•°æ®åº“å¥åº·æ£€æŸ¥æŠ¥å‘Š ===");
    console.log("å½“å‰è¿æ¥æ•°: " + connections);
    console.log("ç¼“å†²æ± å‘½ä¸­ç‡: " + bufferResult.fetchOne()[0].toFixed(2) + "%");
    
    // æ£€æŸ¥æ…¢æŸ¥è¯¢
    var slowResult = session.runSql("SHOW STATUS LIKE 'Slow_queries'");
    console.log("æ…¢æŸ¥è¯¢æ•°é‡: " + slowResult.fetchOne()[1]);
    
    session.close();
}

// ä½¿ç”¨è„šæœ¬
checkDatabaseHealth('admin@mysql://localhost:3306');
```

**ğŸ”„ æ‰¹é‡è¿ç»´æ“ä½œè„šæœ¬**
```python
# batch_maintenance.py - æ‰¹é‡ç»´æŠ¤è„šæœ¬
def batch_table_maintenance(host_list, database_name):
    """æ‰¹é‡æ‰§è¡Œè¡¨ç»´æŠ¤æ“ä½œ"""
    
    for host in host_list:
        try:
            # è¿æ¥æ¯ä¸ªMySQLå®ä¾‹
            session = mysql.get_session(f"admin@{host}:3306")
            
            # è·å–æ‰€æœ‰è¡¨
            tables = session.run_sql(f"USE {database_name}")
            table_result = session.run_sql("SHOW TABLES")
            
            # å¯¹æ¯ä¸ªè¡¨æ‰§è¡Œç»´æŠ¤
            for table_row in table_result.fetch_all():
                table_name = table_row[0]
                
                print(f"æ­£åœ¨ç»´æŠ¤ {host} ä¸Šçš„è¡¨ {table_name}")
                
                # åˆ†æè¡¨
                session.run_sql(f"ANALYZE TABLE {table_name}")
                
                # ä¼˜åŒ–è¡¨
                session.run_sql(f"OPTIMIZE TABLE {table_name}")
                
            print(f"âœ… {host} ç»´æŠ¤å®Œæˆ")
            session.close()
            
        except Exception as e:
            print(f"âŒ {host} ç»´æŠ¤å¤±è´¥: {e}")

# æ‰§è¡Œæ‰¹é‡ç»´æŠ¤
server_list = ['db1.company.com', 'db2.company.com', 'db3.company.com']
batch_table_maintenance(server_list, 'production_db')
```

---

## 4. ğŸ§° pt-toolkitå·¥å…·æ·±åº¦åº”ç”¨


### 4.1 Percona Toolkitå·¥å…·é›†æ¦‚è¿°


**Percona Toolkitï¼ˆpt-toolkitï¼‰**æ˜¯ä¸šç•Œæœ€çŸ¥åçš„MySQLè¿ç»´å·¥å…·é›†ï¼ŒåŒ…å«30å¤šä¸ªä¸“ä¸šå·¥å…·ï¼Œæ¯ä¸ªå·¥å…·éƒ½è§£å†³ç‰¹å®šçš„MySQLè¿ç»´é—®é¢˜ã€‚

**ğŸ”¸ å·¥å…·åˆ†ç±»åŠä½œç”¨**
```
pt-toolkitå·¥å…·åˆ†ç±»ï¼š

ğŸ“Š æ€§èƒ½åˆ†æç±»ï¼š
- pt-query-digestï¼šæ…¢æŸ¥è¯¢æ—¥å¿—åˆ†æç¥å™¨
- pt-summaryï¼šæœåŠ¡å™¨é…ç½®æ€»ç»“
- pt-mysql-summaryï¼šMySQLé…ç½®æ€»ç»“

ğŸ”§ è¿ç»´æ“ä½œç±»ï¼š
- pt-online-schema-changeï¼šåœ¨çº¿DDLæ“ä½œ
- pt-table-checksumï¼šæ•°æ®ä¸€è‡´æ€§æ£€æŸ¥
- pt-table-syncï¼šæ•°æ®åŒæ­¥ä¿®å¤

ğŸš¨ é—®é¢˜è¯Šæ–­ç±»ï¼š
- pt-deadlock-loggerï¼šæ­»é”æ—¥å¿—è®°å½•
- pt-stalkï¼šç³»ç»Ÿå¼‚å¸¸æ—¶è‡ªåŠ¨æŠ“å–ä¿¡æ¯
- pt-pmpï¼šMySQLè¿›ç¨‹çŠ¶æ€åˆ†æ
```

### 4.2 pt-query-digestæ…¢æŸ¥è¯¢åˆ†æ


**ğŸ“ˆ æ…¢æŸ¥è¯¢æ—¥å¿—åˆ†æå®æˆ˜**
```bash
# åˆ†ææ…¢æŸ¥è¯¢æ—¥å¿—ï¼Œç”Ÿæˆè¯¦ç»†æŠ¥å‘Š
pt-query-digest /var/log/mysql/slow.log > slow_query_report.txt

# åˆ†æç»“æœç¤ºä¾‹ï¼š
# ======================== åˆ†ææŠ¥å‘Šæ‘˜è¦ ========================
# æ€»æŸ¥è¯¢æ—¶é—´: 2,345.67s (100.00%)
# æ€»æŸ¥è¯¢æ¬¡æ•°: 12,456 (100.00%)
# 
# æ’è¡Œæ¦œ Top 5:
# Rank Query ID           Response time Calls  R/Call  V/M   Item
# ==== ================= ============= ====== ======= ===== ====
#    1 0x1234567890ABCDEF  865.43s  36.9%   234  3.70s  0.85  SELECT user_orders
#    2 0x2345678901BCDEF0  432.12s  18.4%   156  2.77s  1.23  UPDATE product_stock
```

**ğŸ” å…·ä½“æŸ¥è¯¢åˆ†æè¯¦è§£**
```sql
-- pt-query-digest ä¼šæ˜¾ç¤ºæ¯ä¸ªæ…¢æŸ¥è¯¢çš„è¯¦ç»†ä¿¡æ¯ï¼š

# Query 1: æ¯å°æ—¶æ‰§è¡Œ45æ¬¡ï¼Œæ€»è€—æ—¶865ç§’
# Databases: ecommerce
# Users: app_user@192.168.1.100 (85%), admin@localhost (15%)
# 
# æŸ¥è¯¢è¯­å¥ï¼š
SELECT o.order_id, o.order_date, u.username, u.email
FROM user_orders o 
JOIN users u ON o.user_id = u.user_id 
WHERE o.order_date >= '2024-01-01' 
  AND o.status = 'pending'
ORDER BY o.order_date DESC;

# æ‰§è¡Œè®¡åˆ’é—®é¢˜ï¼š
# - ç¼ºå°‘(order_date, status)å¤åˆç´¢å¼•
# - JOINæ“ä½œæ‰«æäº†å¤§é‡æ•°æ®
# - ORDER BY æ²¡æœ‰ä½¿ç”¨ç´¢å¼•æ’åº

# ä¼˜åŒ–å»ºè®®ï¼š
# 1. æ·»åŠ ç´¢å¼•ï¼šCREATE INDEX idx_order_date_status ON user_orders(order_date, status);
# 2. è€ƒè™‘åˆ†é¡µæŸ¥è¯¢é¿å…å¤§ç»“æœé›†
```

### 4.3 pt-online-schema-changeåœ¨çº¿DDL


**ğŸ”„ é›¶åœæœºè¡¨ç»“æ„å˜æ›´**
```bash
# åœ¨çº¿ç»™å¤§è¡¨æ·»åŠ å­—æ®µï¼Œä¸é”è¡¨ä¸å½±å“ä¸šåŠ¡
pt-online-schema-change \
  --user=root \
  --password=password \
  --host=localhost \
  --database=ecommerce \
  --table=user_orders \
  --alter="ADD COLUMN shipping_address TEXT" \
  --execute

# å·¥ä½œåŸç†ï¼š
# 1. åˆ›å»ºæ–°è¡¨ _user_orders_new (åŒ…å«æ–°å­—æ®µ)
# 2. åˆ›å»ºè§¦å‘å™¨åŒæ­¥æ•°æ®å˜æ›´
# 3. åˆ†æ‰¹å¤åˆ¶åŸè¡¨æ•°æ®åˆ°æ–°è¡¨
# 4. åŸå­æ€§é‡å‘½åè¡¨å®Œæˆåˆ‡æ¢
# 5. æ¸…ç†ä¸´æ—¶è¡¨å’Œè§¦å‘å™¨
```

**âš ï¸ ä½¿ç”¨æ³¨æ„äº‹é¡¹**
```
é€‚ç”¨åœºæ™¯ï¼š
âœ… å¤§è¡¨ï¼ˆç™¾ä¸‡çº§ä»¥ä¸Šï¼‰ç»“æ„å˜æ›´
âœ… 24å°æ—¶ä¸åœæœºçš„ç”Ÿäº§ç¯å¢ƒ
âœ… æ·»åŠ å­—æ®µã€ä¿®æ”¹å­—æ®µç±»å‹ã€æ·»åŠ ç´¢å¼•

ä¸é€‚ç”¨åœºæ™¯ï¼š
âŒ åˆ é™¤å­—æ®µï¼ˆæ•°æ®ä¸¢å¤±é£é™©ï¼‰
âŒ ä¿®æ”¹ä¸»é”®
âŒ å¤–é”®çº¦æŸå¤æ‚çš„è¡¨

å®‰å…¨å‚æ•°ï¼š
--max-load="Threads_running=100"  # è´Ÿè½½è¿‡é«˜æ—¶æš‚åœ
--critical-load="Threads_running=200"  # è´Ÿè½½å±é™©æ—¶åœæ­¢
--chunk-size=1000  # æ¯æ¬¡å¤„ç†çš„è¡Œæ•°
```

### 4.4 pt-table-checksumæ•°æ®ä¸€è‡´æ€§æ£€æŸ¥


**ğŸ” ä¸»ä»æ•°æ®ä¸€è‡´æ€§æ£€éªŒ**
```bash
# æ£€æŸ¥ä¸»ä»æ•°æ®æ˜¯å¦ä¸€è‡´
pt-table-checksum \
  --host=master-db.company.com \
  --user=checksum_user \
  --password=password \
  --databases=ecommerce,user_center \
  --replicate=percona.checksums

# æ£€æŸ¥ç»“æœç¤ºä¾‹ï¼š
#            TS ERRORS  DIFFS     ROWS  CHUNKS SKIPPED    TIME TABLE
# 03-15T10:23:45      0      1   250000      10       0   5.234 ecommerce.user_orders
# 03-15T10:24:12      0      0   180000       8       0   3.876 ecommerce.products  
# 03-15T10:24:45      0      3    95000       5       0   2.341 user_center.users

# è¯´æ˜ï¼šuser_ordersè¡¨æœ‰1è¡Œæ•°æ®ä¸ä¸€è‡´ï¼Œusersè¡¨æœ‰3è¡Œæ•°æ®ä¸ä¸€è‡´
```

**ğŸ”§ æ•°æ®ä¿®å¤æ“ä½œ**
```bash
# å‘ç°ä¸ä¸€è‡´åï¼Œä½¿ç”¨pt-table-syncä¿®å¤
pt-table-sync \
  --execute \
  --sync-to-master \
  h=slave-db.company.com,D=ecommerce,t=user_orders \
  h=master-db.company.com

# ä¿®å¤åŸç†ï¼š
# 1. æ¯”è¾ƒä¸»ä»è¡¨æ•°æ®
# 2. ç”Ÿæˆä¿®å¤SQLè¯­å¥
# 3. åœ¨ä»åº“æ‰§è¡Œä¿®å¤æ“ä½œ
# 4. ç¡®ä¿æ•°æ®ä¸ä¸»åº“ä¸€è‡´
```

---

## 5. ğŸ—ï¸ è‡ªç ”è¿ç»´å·¥å…·å¼€å‘æ¡†æ¶


### 5.1 ä¸ºä»€ä¹ˆéœ€è¦è‡ªç ”è¿ç»´å·¥å…·


è™½ç„¶æœ‰å¾ˆå¤šæˆç†Ÿçš„è¿ç»´å·¥å…·ï¼Œä½†ä¼ä¸šå¾€å¾€éœ€è¦æ ¹æ®è‡ªå·±çš„ä¸šåŠ¡ç‰¹ç‚¹å¼€å‘å®šåˆ¶åŒ–å·¥å…·ã€‚

**ğŸ¯ è‡ªç ”å·¥å…·çš„ä¼˜åŠ¿**
```
ç°æˆå·¥å…·çš„é™åˆ¶ï¼š
âŒ åŠŸèƒ½å›ºå®šï¼Œæ— æ³•æ»¡è¶³ç‰¹æ®Šéœ€æ±‚
âŒ ä¸ä¼ä¸šå†…éƒ¨ç³»ç»Ÿé›†æˆå›°éš¾  
âŒ æ— æ³•è·å–ä¸šåŠ¡ç›¸å…³æŒ‡æ ‡
âŒ å‘Šè­¦è§„åˆ™ä¸å¤Ÿçµæ´»

è‡ªç ”å·¥å…·çš„ä»·å€¼ï¼š
âœ… æ·±åº¦å®šåˆ¶ï¼Œå®Œå…¨ç¬¦åˆä¸šåŠ¡éœ€æ±‚
âœ… ä¸å†…éƒ¨ç³»ç»Ÿæ— ç¼é›†æˆ
âœ… ä¸šåŠ¡æŒ‡æ ‡ä¸æŠ€æœ¯æŒ‡æ ‡ç»“åˆ
âœ… çµæ´»çš„å‘Šè­¦å’Œå¤„ç†æœºåˆ¶
```

### 5.2 è¿ç»´å·¥å…·å¼€å‘æ¶æ„è®¾è®¡


**ğŸ—ï¸ å…¸å‹æ¶æ„æ¨¡å¼**
```
è‡ªç ”è¿ç»´å·¥å…·æ¶æ„ï¼š

æ•°æ®æ”¶é›†å±‚              æ•°æ®å¤„ç†å±‚              å±•ç¤ºåº”ç”¨å±‚
    â†“                     â†“                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MySQLæŒ‡æ ‡   â”‚ â†’   â”‚ æ•°æ®èšåˆ    â”‚ â†’   â”‚ Webä»ªè¡¨æ¿   â”‚
â”‚ ç³»ç»ŸæŒ‡æ ‡    â”‚ â†’   â”‚ å¼‚å¸¸æ£€æµ‹    â”‚ â†’   â”‚ ç§»åŠ¨åº”ç”¨    â”‚  
â”‚ åº”ç”¨æŒ‡æ ‡    â”‚ â†’   â”‚ å‘Šè­¦å¤„ç†    â”‚ â†’   â”‚ APIæ¥å£     â”‚
â”‚ ä¸šåŠ¡æŒ‡æ ‡    â”‚      â”‚ æŠ¥è¡¨ç”Ÿæˆ    â”‚      â”‚ é‚®ä»¶æŠ¥å‘Š    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
       â†‘                     â†‘                     â†‘
 Agenté‡‡é›†å™¨           æ¶ˆæ¯é˜Ÿåˆ—+å­˜å‚¨           ç”¨æˆ·ç•Œé¢
```

**ğŸ“Š æ•°æ®æ”¶é›†æ¨¡å—è®¾è®¡**
```python
# mysql_collector.py - MySQLæŒ‡æ ‡æ”¶é›†å™¨
class MySQLCollector:
    def __init__(self, host, port, user, password):
        self.connection = pymysql.connect(
            host=host, port=port, user=user, password=password
        )
        
    def collect_performance_metrics(self):
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        metrics = {}
        
        # QPSç»Ÿè®¡
        qps_sql = """
        SELECT VARIABLE_VALUE as queries
        FROM INFORMATION_SCHEMA.GLOBAL_STATUS 
        WHERE VARIABLE_NAME = 'Queries'
        """
        metrics['qps'] = self.calculate_qps(qps_sql)
        
        # è¿æ¥æ•°ç»Ÿè®¡
        conn_sql = """
        SELECT 
            (SELECT VARIABLE_VALUE FROM INFORMATION_SCHEMA.GLOBAL_STATUS 
             WHERE VARIABLE_NAME = 'Threads_connected') as current_connections,
            (SELECT VARIABLE_VALUE FROM INFORMATION_SCHEMA.GLOBAL_VARIABLES 
             WHERE VARIABLE_NAME = 'max_connections') as max_connections
        """
        conn_data = self.execute_query(conn_sql)
        metrics['connection_usage'] = (
            int(conn_data['current_connections']) / int(conn_data['max_connections']) * 100
        )
        
        # InnoDBç¼“å†²æ± å‘½ä¸­ç‡
        buffer_sql = """
        SELECT 
            (1 - (Innodb_buffer_pool_reads / Innodb_buffer_pool_read_requests)) * 100 
            as buffer_hit_rate
        FROM INFORMATION_SCHEMA.GLOBAL_STATUS 
        WHERE VARIABLE_NAME IN ('Innodb_buffer_pool_reads', 'Innodb_buffer_pool_read_requests')
        """
        metrics['buffer_hit_rate'] = self.execute_query(buffer_sql)['buffer_hit_rate']
        
        return metrics
        
    def collect_business_metrics(self):
        """æ”¶é›†ä¸šåŠ¡æŒ‡æ ‡"""
        business_metrics = {}
        
        # è®¢å•é‡ç»Ÿè®¡ï¼ˆæœ€è¿‘1å°æ—¶ï¼‰
        order_sql = """
        SELECT COUNT(*) as order_count
        FROM orders 
        WHERE created_at >= DATE_SUB(NOW(), INTERVAL 1 HOUR)
        """
        business_metrics['hourly_orders'] = self.execute_query(order_sql)['order_count']
        
        # æ´»è·ƒç”¨æˆ·æ•°
        active_user_sql = """
        SELECT COUNT(DISTINCT user_id) as active_users
        FROM user_activity 
        WHERE last_active >= DATE_SUB(NOW(), INTERVAL 1 HOUR)
        """
        business_metrics['active_users'] = self.execute_query(active_user_sql)['active_users']
        
        return business_metrics
```

### 5.3 å‘Šè­¦ç³»ç»Ÿè®¾è®¡


**ğŸš¨ æ™ºèƒ½å‘Šè­¦æœºåˆ¶**
```python
# alert_engine.py - å‘Šè­¦å¼•æ“
class AlertEngine:
    def __init__(self):
        self.rules = self.load_alert_rules()
        
    def load_alert_rules(self):
        """åŠ è½½å‘Šè­¦è§„åˆ™"""
        return {
            'cpu_usage': {
                'warning': 70,
                'critical': 90,
                'duration': 300  # æŒç»­5åˆ†é’Ÿ
            },
            'connection_usage': {
                'warning': 80,
                'critical': 95,
                'duration': 60
            },
            'buffer_hit_rate': {
                'warning_below': 95,
                'critical_below': 90,
                'duration': 600
            },
            'business_order_drop': {
                'threshold': 0.3,  # è®¢å•é‡ä¸‹é™30%
                'compare_period': 3600  # ä¸1å°æ—¶å‰æ¯”è¾ƒ
            }
        }
        
    def check_alerts(self, current_metrics, historical_metrics):
        """æ£€æŸ¥æ˜¯å¦éœ€è¦å‘Šè­¦"""
        alerts = []
        
        # æ£€æŸ¥è¿æ¥æ•°å‘Šè­¦
        conn_usage = current_metrics['connection_usage']
        if conn_usage >= self.rules['connection_usage']['critical']:
            alerts.append({
                'level': 'CRITICAL',
                'metric': 'connection_usage',
                'value': conn_usage,
                'message': f'æ•°æ®åº“è¿æ¥æ•°ä½¿ç”¨ç‡è¿‡é«˜: {conn_usage:.1f}%'
            })
            
        # æ£€æŸ¥ä¸šåŠ¡æŒ‡æ ‡å‘Šè­¦
        current_orders = current_metrics['hourly_orders']
        hour_ago_orders = historical_metrics.get('hourly_orders', current_orders)
        
        if hour_ago_orders > 0:
            order_change = (current_orders - hour_ago_orders) / hour_ago_orders
            if order_change <= -self.rules['business_order_drop']['threshold']:
                alerts.append({
                    'level': 'WARNING',
                    'metric': 'business_order_drop',
                    'value': order_change,
                    'message': f'è®¢å•é‡å¼‚å¸¸ä¸‹é™: {order_change*100:.1f}%'
                })
                
        return alerts
        
    def send_alert(self, alert):
        """å‘é€å‘Šè­¦"""
        if alert['level'] == 'CRITICAL':
            # å…³é”®å‘Šè­¦ï¼šçŸ­ä¿¡+é‚®ä»¶+é’‰é’‰
            self.send_sms(alert)
            self.send_email(alert)
            self.send_dingtalk(alert)
        else:
            # æ™®é€šå‘Šè­¦ï¼šä»…é‚®ä»¶
            self.send_email(alert)
```

---

## 6. ğŸ”— ç¬¬ä¸‰æ–¹è¿ç»´å·¥å…·é›†æˆ


### 6.1 ä¸»æµè¿ç»´å·¥å…·ç”Ÿæ€


**ğŸŒ è¿ç»´å·¥å…·ç”Ÿæ€å›¾**
```
MySQLè¿ç»´å·¥å…·ç”Ÿæ€ï¼š

ç›‘æ§å‘Šè­¦ç±»ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prometheus  â”‚   â”‚   Grafana   â”‚   â”‚   Zabbix    â”‚
â”‚ æŒ‡æ ‡æ”¶é›†    â”‚â†’ â”‚ å¯è§†åŒ–å±•ç¤º   â”‚   â”‚ ä¼ ç»Ÿç›‘æ§    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è‡ªåŠ¨åŒ–è¿ç»´ç±»ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Ansible   â”‚   â”‚  Kubernetes â”‚   â”‚   Docker    â”‚
â”‚ é…ç½®ç®¡ç†    â”‚   â”‚ å®¹å™¨ç¼–æ’    â”‚   â”‚ å®¹å™¨åŒ–éƒ¨ç½²  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

DevOpså·¥å…·é“¾ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Jenkins   â”‚   â”‚    GitLab   â”‚   â”‚  Terraform  â”‚
â”‚ CI/CDæµæ°´çº¿ â”‚   â”‚ ä»£ç ä»“åº“    â”‚   â”‚ åŸºç¡€è®¾æ–½    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 6.2 Prometheus + Grafanaç›‘æ§é›†æˆ


**ğŸ“Š Prometheus MySQLç›‘æ§é…ç½®**
```yaml
# prometheus.yml - Prometheusé…ç½®
global:
  scrape_interval: 15s

scrape_configs:
  # MySQLç›‘æ§ä»»åŠ¡
  - job_name: 'mysql'
    static_configs:
      - targets: ['mysql-exporter:9104']
    scrape_interval: 10s
    metrics_path: /metrics

  # MySQLè‡ªå®šä¹‰ä¸šåŠ¡æŒ‡æ ‡
  - job_name: 'mysql-business'
    static_configs:
      - targets: ['business-exporter:8080']
    scrape_interval: 30s
```

**ğŸ”§ MySQL Exporteréƒ¨ç½²**
```bash
# 1. ä¸‹è½½å¹¶å¯åŠ¨MySQL Exporter
wget https://github.com/prometheus/mysqld_exporter/releases/download/v0.14.0/mysqld_exporter-0.14.0.linux-amd64.tar.gz
tar xvfz mysqld_exporter-0.14.0.linux-amd64.tar.gz

# 2. åˆ›å»ºMySQLç›‘æ§ç”¨æˆ·
mysql -u root -p
CREATE USER 'exporter'@'localhost' IDENTIFIED BY 'password';
GRANT PROCESS, REPLICATION CLIENT, SELECT ON *.* TO 'exporter'@'localhost';
FLUSH PRIVILEGES;

# 3. é…ç½®è¿æ¥ä¿¡æ¯
export DATA_SOURCE_NAME='exporter:password@(localhost:3306)/'

# 4. å¯åŠ¨Exporter
./mysqld_exporter &

# 5. éªŒè¯æŒ‡æ ‡é‡‡é›†
curl http://localhost:9104/metrics | grep mysql_up
```

**ğŸ“ˆ Grafanaä»ªè¡¨æ¿é…ç½®**
```json
{
  "dashboard": {
    "title": "MySQLè¿ç»´ç›‘æ§",
    "panels": [
      {
        "title": "MySQL QPS",
        "type": "graph",
        "targets": [
          {
            "expr": "rate(mysql_global_status_queries[5m])",
            "legendFormat": "QPS"
          }
        ]
      },
      {
        "title": "è¿æ¥æ•°ä½¿ç”¨ç‡",
        "type": "singlestat",
        "targets": [
          {
            "expr": "(mysql_global_status_threads_connected / mysql_global_variables_max_connections) * 100",
            "legendFormat": "è¿æ¥ä½¿ç”¨ç‡%"
          }
        ],
        "thresholds": "70,90"
      },
      {
        "title": "InnoDBç¼“å†²æ± å‘½ä¸­ç‡",
        "type": "gauge",
        "targets": [
          {
            "expr": "(1 - (rate(mysql_global_status_innodb_buffer_pool_reads[5m]) / rate(mysql_global_status_innodb_buffer_pool_read_requests[5m]))) * 100"
          }
        ]
      }
    ]
  }
}
```

### 6.3 Ansibleè‡ªåŠ¨åŒ–è¿ç»´é›†æˆ


**ğŸ¤– MySQLè‡ªåŠ¨åŒ–éƒ¨ç½²Playbook**
```yaml
# mysql_deploy.yml - MySQLè‡ªåŠ¨åŒ–éƒ¨ç½²
---
- name: MySQLè‡ªåŠ¨åŒ–éƒ¨ç½²å’Œé…ç½®
  hosts: mysql_servers
  become: yes
  vars:
    mysql_root_password: "StrongPassword123!"
    mysql_version: "8.0"
    
  tasks:
    # 1. å®‰è£…MySQL
    - name: å®‰è£…MySQLæœåŠ¡å™¨
      yum:
        name: mysql-server
        state: present
        
    # 2. å¯åŠ¨MySQLæœåŠ¡
    - name: å¯åŠ¨å¹¶å¯ç”¨MySQLæœåŠ¡
      systemd:
        name: mysqld
        state: started
        enabled: yes
        
    # 3. é…ç½®MySQL
    - name: å¤åˆ¶MySQLé…ç½®æ–‡ä»¶
      template:
        src: my.cnf.j2
        dest: /etc/mysql/my.cnf
        backup: yes
      notify: restart mysql
      
    # 4. åˆ›å»ºæ•°æ®åº“ç”¨æˆ·
    - name: åˆ›å»ºåº”ç”¨æ•°æ®åº“ç”¨æˆ·
      mysql_user:
        name: "{{ item.name }}"
        password: "{{ item.password }}"
        priv: "{{ item.priv }}"
        host: "{{ item.host }}"
        state: present
      loop:
        - { name: 'app_user', password: 'app_pass', priv: 'app_db.*:ALL', host: '192.168.1.%' }
        - { name: 'readonly_user', password: 'readonly_pass', priv: '*.*:SELECT', host: '192.168.1.%' }
        
  handlers:
    - name: restart mysql
      systemd:
        name: mysqld
        state: restarted
```

**ğŸ”§ æ‰¹é‡è¿ç»´æ“ä½œPlaybook**
```yaml
# mysql_maintenance.yml - æ‰¹é‡ç»´æŠ¤æ“ä½œ
---
- name: MySQLæ‰¹é‡ç»´æŠ¤æ“ä½œ
  hosts: mysql_cluster
  gather_facts: no
  
  tasks:
    # 1. æ”¶é›†MySQLçŠ¶æ€ä¿¡æ¯
    - name: æ”¶é›†MySQLè¿è¡ŒçŠ¶æ€
      mysql_info:
        filter: version,databases,settings,users
      register: mysql_info
      
    # 2. æ£€æŸ¥æ…¢æŸ¥è¯¢æ—¥å¿—å¤§å°
    - name: æ£€æŸ¥æ…¢æŸ¥è¯¢æ—¥å¿—æ–‡ä»¶å¤§å°
      stat:
        path: /var/log/mysql/slow.log
      register: slow_log_stat
      
    # 3. è½®è½¬è¿‡å¤§çš„æ—¥å¿—æ–‡ä»¶
    - name: è½®è½¬æ…¢æŸ¥è¯¢æ—¥å¿—ï¼ˆå¤§äº100MBæ—¶ï¼‰
      shell: |
        mysql -e "FLUSH SLOW LOGS;"
        mv /var/log/mysql/slow.log /var/log/mysql/slow.log.$(date +%Y%m%d)
        touch /var/log/mysql/slow.log
        chown mysql:mysql /var/log/mysql/slow.log
      when: slow_log_stat.stat.size > 104857600
      
    # 4. æ‰§è¡Œè¡¨ä¼˜åŒ–
    - name: ä¼˜åŒ–æŒ‡å®šæ•°æ®åº“çš„æ‰€æœ‰è¡¨
      mysql_query:
        query: "OPTIMIZE TABLE {{ item }}"
      loop: "{{ tables_to_optimize | default([]) }}"
      when: tables_to_optimize is defined
      
    # 5. ç”Ÿæˆç»´æŠ¤æŠ¥å‘Š
    - name: ç”Ÿæˆç»´æŠ¤æŠ¥å‘Š
      template:
        src: maintenance_report.j2
        dest: "/tmp/mysql_maintenance_{{ ansible_hostname }}_{{ ansible_date_time.date }}.html"
```

---

## 7. ğŸ“Š è¿ç»´å·¥å…·æ•ˆæœè¯„ä¼°


### 7.1 è¯„ä¼°æŒ‡æ ‡ä½“ç³»


**ğŸ“ˆ è¿ç»´æ•ˆæœè¯„ä¼°ç»´åº¦**
```
è¿ç»´å·¥å…·æ•ˆæœè¯„ä¼°ä½“ç³»ï¼š

æŠ€æœ¯æŒ‡æ ‡ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ç³»ç»Ÿç¨³å®šæ€§     â”‚ â† å¯ç”¨æ€§ã€æ•…éšœç‡ã€MTBF
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ æ€§èƒ½è¡¨ç°       â”‚ â† å“åº”æ—¶é—´ã€ååé‡ã€èµ„æºåˆ©ç”¨ç‡  
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ å®‰å…¨åˆè§„       â”‚ â† å®‰å…¨äº‹ä»¶ã€åˆè§„æ£€æŸ¥é€šè¿‡ç‡
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è¿ç»´æ•ˆç‡ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ è‡ªåŠ¨åŒ–ç¨‹åº¦     â”‚ â† è‡ªåŠ¨åŒ–ä»»åŠ¡æ¯”ä¾‹ã€äººå·¥å¹²é¢„æ¬¡æ•°
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ é—®é¢˜å¤„ç†æ•ˆç‡   â”‚ â† MTTRã€é—®é¢˜è§£å†³æ—¶é—´
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ è¿ç»´æˆæœ¬       â”‚ â† äººåŠ›æˆæœ¬ã€å·¥å…·æˆæœ¬ã€æ•…éšœæŸå¤±
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 7.2 å…³é”®æ€§èƒ½æŒ‡æ ‡ï¼ˆKPIï¼‰å®šä¹‰


**â±ï¸ æ ¸å¿ƒè¿ç»´KPI**

| æŒ‡æ ‡ç±»åˆ« | **KPIæŒ‡æ ‡** | **è®¡ç®—æ–¹æ³•** | **ç›®æ ‡å€¼** |
|---------|------------|-------------|-----------|
| ğŸ”§ **å¯ç”¨æ€§** | `ç³»ç»Ÿå¯ç”¨ç‡` | `(æ€»æ—¶é—´-æ•…éšœæ—¶é—´)/æ€»æ—¶é—´Ã—100%` | `â‰¥99.9%` |
| âš¡ **æ€§èƒ½** | `å¹³å‡å“åº”æ—¶é—´` | `æ‰€æœ‰è¯·æ±‚å“åº”æ—¶é—´çš„å¹³å‡å€¼` | `<100ms` |
| ğŸš¨ **æ•…éšœå¤„ç†** | `MTTRå¹³å‡æ¢å¤æ—¶é—´` | `æ•…éšœæ€»æ—¶é—´/æ•…éšœæ¬¡æ•°` | `<30åˆ†é’Ÿ` |
| ğŸ¤– **è‡ªåŠ¨åŒ–** | `è‡ªåŠ¨åŒ–è¦†ç›–ç‡` | `è‡ªåŠ¨åŒ–ä»»åŠ¡æ•°/æ€»ä»»åŠ¡æ•°Ã—100%` | `â‰¥80%` |
| ğŸ’° **æˆæœ¬** | `è¿ç»´æˆæœ¬ç‡` | `è¿ç»´æ€»æˆæœ¬/ä¸šåŠ¡æ”¶å…¥Ã—100%` | `<5%` |

### 7.3 è¯„ä¼°æ–¹æ³•ä¸å·¥å…·


**ğŸ“Š æ•°æ®æ”¶é›†ä¸åˆ†æ**
```python
# ops_metrics_analyzer.py - è¿ç»´æŒ‡æ ‡åˆ†æå™¨
class OpsMetricsAnalyzer:
    def __init__(self):
        self.metrics_db = self.connect_metrics_database()
        
    def calculate_availability(self, start_date, end_date):
        """è®¡ç®—ç³»ç»Ÿå¯ç”¨æ€§"""
        # æŸ¥è¯¢æ•…éšœæ—¶é—´
        downtime_query = """
        SELECT SUM(TIMESTAMPDIFF(SECOND, start_time, end_time)) as total_downtime
        FROM incident_log 
        WHERE incident_date BETWEEN %s AND %s 
          AND status = 'resolved'
        """
        
        total_downtime = self.metrics_db.execute(downtime_query, [start_date, end_date])
        
        # è®¡ç®—æ€»æ—¶é—´ï¼ˆç§’ï¼‰
        total_seconds = (end_date - start_date).total_seconds()
        
        # è®¡ç®—å¯ç”¨æ€§
        availability = ((total_seconds - total_downtime) / total_seconds) * 100
        
        return {
            'availability_percentage': availability,
            'total_downtime_hours': total_downtime / 3600,
            'sla_status': 'PASS' if availability >= 99.9 else 'FAIL'
        }
        
    def analyze_performance_trends(self, metric_name, days=30):
        """åˆ†ææ€§èƒ½è¶‹åŠ¿"""
        performance_query = """
        SELECT 
            DATE(timestamp) as date,
            AVG(value) as avg_value,
            MAX(value) as max_value,
            MIN(value) as min_value
        FROM performance_metrics 
        WHERE metric_name = %s 
          AND timestamp >= DATE_SUB(NOW(), INTERVAL %s DAY)
        GROUP BY DATE(timestamp)
        ORDER BY date
        """
        
        results = self.metrics_db.execute(performance_query, [metric_name, days])
        
        # è®¡ç®—è¶‹åŠ¿
        trend = self.calculate_trend(results)
        
        return {
            'metric_name': metric_name,
            'trend_direction': trend['direction'],  # 'improving', 'degrading', 'stable'
            'trend_rate': trend['rate'],
            'current_avg': results[-1]['avg_value'],
            'recommendation': self.generate_recommendation(trend)
        }
        
    def generate_ops_report(self):
        """ç”Ÿæˆè¿ç»´æ•ˆæœæŠ¥å‘Š"""
        report = {
            'report_date': datetime.now(),
            'availability': self.calculate_availability(
                datetime.now() - timedelta(days=30),
                datetime.now()
            ),
            'performance_metrics': {},
            'automation_metrics': self.calculate_automation_metrics(),
            'cost_analysis': self.analyze_ops_costs()
        }
        
        # åˆ†æå„é¡¹æ€§èƒ½æŒ‡æ ‡
        key_metrics = ['response_time', 'cpu_usage', 'memory_usage', 'disk_io']
        for metric in key_metrics:
            report['performance_metrics'][metric] = self.analyze_performance_trends(metric)
            
        return report
```

**ğŸ“ˆ å¯è§†åŒ–æŠ¥è¡¨ç”Ÿæˆ**
```python
# report_generator.py - æŠ¥è¡¨ç”Ÿæˆå™¨
import matplotlib.pyplot as plt
import seaborn as sns

class OpsReportGenerator:
    def generate_availability_chart(self, availability_data):
        """ç”Ÿæˆå¯ç”¨æ€§å›¾è¡¨"""
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))
        
        # å¯ç”¨æ€§ä»ªè¡¨ç›˜
        availability_rate = availability_data['availability_percentage']
        ax1.pie([availability_rate, 100-availability_rate], 
                labels=['å¯ç”¨æ—¶é—´', 'æ•…éšœæ—¶é—´'],
                colors=['green', 'red'],
                autopct='%1.2f%%')
        ax1.set_title(f'ç³»ç»Ÿå¯ç”¨æ€§: {availability_rate:.3f}%')
        
        # SLAè¾¾æˆæƒ…å†µ
        sla_target = 99.9
        ax2.bar(['ç›®æ ‡SLA', 'å®é™…å¯ç”¨æ€§'], [sla_target, availability_rate],
                color=['blue', 'green' if availability_rate >= sla_target else 'red'])
        ax2.set_ylabel('å¯ç”¨æ€§ (%)')
        ax2.set_title('SLAè¾¾æˆæƒ…å†µ')
        
        plt.tight_layout()
        return fig
        
    def generate_performance_trend_chart(self, performance_data):
        """ç”Ÿæˆæ€§èƒ½è¶‹åŠ¿å›¾è¡¨"""
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        metrics = ['response_time', 'cpu_usage', 'memory_usage', 'disk_io']
        
        for i, metric in enumerate(metrics):
            row, col = i // 2, i % 2
            ax = axes[row, col]
            
            data = performance_data[metric]
            dates = [d['date'] for d in data]
            values = [d['avg_value'] for d in data]
            
            ax.plot(dates, values, marker='o', linewidth=2)
            ax.set_title(f'{metric.replace("_", " ").title()} è¶‹åŠ¿')
            ax.set_ylabel('æ•°å€¼')
            ax.tick_params(axis='x', rotation=45)
            
        plt.tight_layout()
        return fig
```

---

## 8. ğŸ”Œ MySQL X Protocolè¿ç»´åº”ç”¨


### 8.1 X Protocolåè®®æ¦‚è¿°


**MySQL X Protocol**æ˜¯MySQL 5.7.12+å¼•å…¥çš„æ–°é€šä¿¡åè®®ï¼Œä¸“é—¨ä¸ºç°ä»£åº”ç”¨è®¾è®¡ï¼Œæ”¯æŒåŒæ­¥/å¼‚æ­¥æ“ä½œã€æ–‡æ¡£å­˜å‚¨ã€SQLç­‰å¤šç§æ•°æ®è®¿é—®æ–¹å¼ã€‚

**ğŸ”¸ X Protocol vs ä¼ ç»Ÿåè®®å¯¹æ¯”**
```
ä¼ ç»ŸMySQLåè®®çš„å±€é™ï¼š
âŒ åªæ”¯æŒåŒæ­¥æ“ä½œï¼Œæ•ˆç‡ä½
âŒ åŸºäºæ–‡æœ¬çš„åè®®ï¼Œè§£æå¼€é”€å¤§
âŒ ä¸æ”¯æŒç°ä»£NoSQLæ“ä½œ
âŒ æ‰©å±•æ€§å·®ï¼Œéš¾ä»¥æ·»åŠ æ–°åŠŸèƒ½

X Protocolçš„ä¼˜åŠ¿ï¼š
âœ… æ”¯æŒå¼‚æ­¥æ“ä½œï¼Œå¹¶å‘æ€§èƒ½å¥½
âœ… äºŒè¿›åˆ¶åè®®ï¼Œè§£ææ•ˆç‡é«˜
âœ… æ”¯æŒCRUD APIï¼Œç±»ä¼¼NoSQL
âœ… æ”¯æŒæ–‡æ¡£å­˜å‚¨ï¼ˆJSON Documentï¼‰
âœ… åŸç”Ÿæ”¯æŒå¤åˆ¶ã€åˆ†ç‰‡ç­‰é«˜çº§åŠŸèƒ½
```

### 8.2 X Protocolè¿ç»´ç›‘æ§


**ğŸ“Š X Protocolè¿æ¥ç›‘æ§**
```sql
-- æŸ¥çœ‹X Protocolè¿æ¥çŠ¶æ€
SELECT 
    PROCESSLIST_ID,
    PROCESSLIST_USER,
    PROCESSLIST_HOST,
    PROCESSLIST_DB,
    PROCESSLIST_COMMAND,
    PROCESSLIST_TIME,
    PROCESSLIST_INFO
FROM performance_schema.threads 
WHERE PROCESSLIST_COMMAND = 'Query' 
  AND CONNECTION_TYPE = 'TCP/IP';

-- ç›‘æ§X Protocolç«¯å£ï¼ˆé»˜è®¤33060ï¼‰è¿æ¥æ•°
SELECT 
    COUNT(*) as x_protocol_connections
FROM performance_schema.threads 
WHERE CONNECTION_TYPE = 'TCP/IP' 
  AND PROCESSLIST_HOST LIKE '%:33060%';
```

**ğŸ”§ X Protocolæ€§èƒ½ä¼˜åŒ–é…ç½®**
```ini
# my.cnfä¸­X Protocolç›¸å…³é…ç½®
[mysqld]
# å¯ç”¨X Protocolæ’ä»¶
plugin-load-add = mysqlx.so

# X Protocolç«¯å£ï¼ˆé»˜è®¤33060ï¼‰
mysqlx_port = 33060

# X Protocolè¿æ¥ç›¸å…³
mysqlx_max_connections = 1000
mysqlx_min_worker_threads = 2
mysqlx_idle_worker_thread_timeout = 60

# X Protocolç¼“å†²åŒºè®¾ç½®
mysqlx_max_allowed_packet = 64M
mysqlx_connect_timeout = 30
mysqlx_read_timeout = 30
mysqlx_write_timeout = 60

# æ–‡æ¡£å­˜å‚¨ä¼˜åŒ–
mysqlx_document_id_unique_prefix = 0
```

### 8.3 X Protocolè¿ç»´è„šæœ¬å¼€å‘


**ğŸ Pythonè¿ç»´è„šæœ¬ç¤ºä¾‹**
```python
# x_protocol_monitor.py - X Protocolç›‘æ§è„šæœ¬
import mysqlx
import json
import time
from datetime import datetime

class XProtocolMonitor:
    def __init__(self, host, port=33060, user='admin', password='password'):
        self.session = mysqlx.get_session({
            'host': host,
            'port': port,
            'user': user,
            'password': password
        })
        
    def monitor_x_connections(self):
        """ç›‘æ§X Protocolè¿æ¥"""
        # æŸ¥è¯¢å½“å‰X Protocolè¿æ¥æ•°
        sql = """
        SELECT 
            COUNT(*) as total_connections,
            COUNT(CASE WHEN PROCESSLIST_COMMAND = 'Sleep' THEN 1 END) as idle_connections,
            COUNT(CASE WHEN PROCESSLIST_COMMAND != 'Sleep' THEN 1 END) as active_connections
        FROM performance_schema.threads 
        WHERE CONNECTION_TYPE = 'TCP/IP'
        """
        
        result = self.session.sql(sql).execute()
        row = result.fetch_one()
        
        return {
            'timestamp': datetime.now().isoformat(),
            'total_connections': row[0],
            'idle_connections': row[1],
            'active_connections': row[2],
            'connection_usage_rate': (row[0] / 1000) * 100  # å‡è®¾æœ€å¤§1000è¿æ¥
        }
        
    def monitor_document_store_performance(self):
        """ç›‘æ§æ–‡æ¡£å­˜å‚¨æ€§èƒ½"""
        # åˆ›å»ºæµ‹è¯•é›†åˆ
        schema = self.session.get_schema('test_db')
        collection = schema.get_collection('performance_test')
        
        # æµ‹è¯•æ–‡æ¡£æ’å…¥æ€§èƒ½
        start_time = time.time()
        
        test_docs = [
            {"_id": f"test_{i}", "data": f"test_data_{i}", "timestamp": datetime.now().isoformat()}
            for i in range(100)
        ]
        
        collection.add(test_docs).execute()
        insert_time = time.time() - start_time
        
        # æµ‹è¯•æ–‡æ¡£æŸ¥è¯¢æ€§èƒ½
        start_time = time.time()
        results = collection.find("data LIKE 'test_data_%'").execute()
        result_count = len(list(results))
        query_time = time.time() - start_time
        
        # æ¸…ç†æµ‹è¯•æ•°æ®
        collection.remove("_id LIKE 'test_%'").execute()
        
        return {
            'insert_time_100_docs': insert_time,
            'query_time': query_time,
            'query_result_count': result_count,
            'insert_performance_score': 100 / insert_time if insert_time > 0 else 0
        }
        
    def generate_x_protocol_report(self):
        """ç”ŸæˆX Protocolè¿ç»´æŠ¥å‘Š"""
        report = {
            'report_time': datetime.now().isoformat(),
            'connection_metrics': self.monitor_x_connections(),
            'document_store_performance': self.monitor_document_store_performance(),
            'recommendations': []
        }
        
        # ç”Ÿæˆå»ºè®®
        conn_usage = report['connection_metrics']['connection_usage_rate']
        if conn_usage > 80:
            report['recommendations'].append(
                "X Protocolè¿æ¥ä½¿ç”¨ç‡è¿‡é«˜ï¼Œå»ºè®®å¢åŠ max_connectionsé…ç½®"
            )
            
        doc_perf = report['document_store_performance']['insert_performance_score']
        if doc_perf < 50:
            report['recommendations'].append(
                "æ–‡æ¡£å­˜å‚¨æ€§èƒ½è¾ƒä½ï¼Œå»ºè®®æ£€æŸ¥InnoDBé…ç½®å’Œç£ç›˜æ€§èƒ½"
            )
            
        return report

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    monitor = XProtocolMonitor('mysql-server.company.com')
    report = monitor.generate_x_protocol_report()
    print(json.dumps(report, indent=2, ensure_ascii=False))
```

---

## 9. ğŸ“„ MySQL Document Storeè¿ç»´


### 9.1 Document Storeæ¦‚å¿µä¸æ¶æ„


**MySQL Document Store**å…è®¸MySQLåƒNoSQLæ•°æ®åº“ä¸€æ ·å­˜å‚¨å’Œæ“ä½œJSONæ–‡æ¡£ï¼ŒåŒæ—¶ä¿æŒACIDäº‹åŠ¡ç‰¹æ€§ã€‚

**ğŸ”¸ Document Storeçš„å·¥ä½œåŸç†**
```
ä¼ ç»Ÿå…³ç³»å‹å­˜å‚¨ï¼š
è¡¨å: users
+----+----------+-------------------+
| id | name     | email            |
+----+----------+-------------------+
| 1  | zhangsan | zhang@company.com |
| 2  | lisi     | li@company.com   |
+----+----------+-------------------+

Document Storeå­˜å‚¨ï¼š
é›†åˆå: users
{
  "_id": "user_001",
  "name": "zhangsan", 
  "email": "zhang@company.com",
  "profile": {
    "age": 28,
    "city": "åŒ—äº¬",
    "interests": ["ç¼–ç¨‹", "éŸ³ä¹"]
  },
  "login_history": [
    {"time": "2024-01-01T10:00:00", "ip": "192.168.1.100"},
    {"time": "2024-01-02T09:30:00", "ip": "192.168.1.101"}
  ]
}
```

### 9.2 Document Storeè¿ç»´ç›‘æ§


**ğŸ“Š æ–‡æ¡£å­˜å‚¨æ€§èƒ½ç›‘æ§**
```sql
-- æŸ¥çœ‹æ–‡æ¡£é›†åˆçš„å­˜å‚¨ç»Ÿè®¡
SELECT 
    SCHEMA_NAME as database_name,
    TABLE_NAME as collection_name,
    TABLE_ROWS as document_count,
    DATA_LENGTH as data_size_bytes,
    INDEX_LENGTH as index_size_bytes,
    (DATA_LENGTH + INDEX_LENGTH) as total_size_bytes
FROM information_schema.TABLES 
WHERE TABLE_TYPE = 'BASE TABLE'
  AND ENGINE = 'InnoDB'
  AND SCHEMA_NAME NOT IN ('information_schema', 'performance_schema', 'mysql', 'sys');

-- ç›‘æ§JSONæ–‡æ¡£æ“ä½œæ€§èƒ½
SELECT 
    EVENT_NAME,
    COUNT_STAR as execution_count,
    SUM_TIMER_WAIT/1000000000 as total_time_seconds,
    AVG_TIMER_WAIT/1000000000 as avg_time_seconds
FROM performance_schema.events_statements_summary_by_event_name 
WHERE EVENT_NAME LIKE '%json%'
ORDER BY total_time_seconds DESC;
```

**ğŸ” æ–‡æ¡£æŸ¥è¯¢æ€§èƒ½åˆ†æ**
```python
# document_store_analyzer.py - æ–‡æ¡£å­˜å‚¨åˆ†æå™¨
import mysqlx
import json
import time

class DocumentStoreAnalyzer:
    def __init__(self, connection_uri):
        self.session = mysqlx.get_session(connection_uri)
        
    def analyze_collection_performance(self, schema_name, collection_name):
        """åˆ†æé›†åˆæ€§èƒ½"""
        schema = self.session.get_schema(schema_name)
        collection = schema.get_collection(collection_name)
        
        # 1. ç»Ÿè®¡æ–‡æ¡£æ•°é‡
        doc_count = collection.count()
        
        # 2. åˆ†ææ–‡æ¡£å¤§å°åˆ†å¸ƒ
        size_analysis = self.analyze_document_sizes(collection)
        
        # 3. æµ‹è¯•å¸¸è§æŸ¥è¯¢æ€§èƒ½
        query_performance = self.test_query_performance(collection)
        
        # 4. åˆ†æç´¢å¼•ä½¿ç”¨æƒ…å†µ
        index_analysis = self.analyze_index_usage(schema_name, collection_name)
        
        return {
            'collection_name': collection_name,
            'document_count': doc_count,
            'size_analysis': size_analysis,
            'query_performance': query_performance,
            'index_analysis': index_analysis
        }
        
    def analyze_document_sizes(self, collection):
        """åˆ†ææ–‡æ¡£å¤§å°åˆ†å¸ƒ"""
        # ä½¿ç”¨èšåˆæŸ¥è¯¢åˆ†ææ–‡æ¡£å¤§å°
        pipeline = [
            {
                "$project": {
                    "doc_size": {"$strLenBytes": "$$ROOT"}
                }
            },
            {
                "$group": {
                    "_id": None,
                    "avg_size": {"$avg": "$doc_size"},
                    "max_size": {"$max": "$doc_size"},
                    "min_size": {"$min": "$doc_size"}
                }
            }
        ]
        
        # æ³¨æ„ï¼šMySQL 8.0çš„èšåˆè¯­æ³•å¯èƒ½ä¸åŒï¼Œè¿™é‡Œæ˜¯æ¦‚å¿µç¤ºä¾‹
        # å®é™…å®ç°éœ€è¦æ ¹æ®MySQLç‰ˆæœ¬è°ƒæ•´
        
        return {
            'avg_document_size_bytes': 1024,  # ç¤ºä¾‹æ•°æ®
            'max_document_size_bytes': 8192,
            'min_document_size_bytes': 256,
            'size_distribution': 'normal'
        }
        
    def test_query_performance(self, collection):
        """æµ‹è¯•æŸ¥è¯¢æ€§èƒ½"""
        test_results = {}
        
        # 1. ç®€å•å­—æ®µæŸ¥è¯¢
        start_time = time.time()
        results = collection.find("name = 'test_user'").execute()
        test_results['simple_field_query'] = {
            'time_seconds': time.time() - start_time,
            'result_count': len(list(results))
        }
        
        # 2. JSONè·¯å¾„æŸ¥è¯¢
        start_time = time.time()
        results = collection.find("profile.age > 25").execute()
        test_results['json_path_query'] = {
            'time_seconds': time.time() - start_time,
            'result_count': len(list(results))
        }
        
        # 3. æ•°ç»„å…ƒç´ æŸ¥è¯¢
        start_time = time.time()
        results = collection.find("'ç¼–ç¨‹' IN interests").execute()
        test_results['array_contains_query'] = {
            'time_seconds': time.time() - start_time,
            'result_count': len(list(results))
        }
        
        return test_results
        
    def optimize_collection_indexes(self, schema_name, collection_name):
        """ä¼˜åŒ–é›†åˆç´¢å¼•"""
        # åˆ†ææŸ¥è¯¢æ¨¡å¼
        query_patterns = self.analyze_query_patterns(collection_name)
        
        # ç”Ÿæˆç´¢å¼•å»ºè®®
        index_recommendations = []
        
        for pattern in query_patterns:
            if pattern['field_path'].count('.') == 0:  # ç®€å•å­—æ®µ
                index_recommendations.append({
                    'type': 'simple_index',
                    'field': pattern['field_path'],
                    'sql': f"ALTER TABLE {collection_name} ADD INDEX idx_{pattern['field_path']} ((CAST(doc->'$.{pattern['field_path']}' AS CHAR(255))))"
                })
            else:  # JSONè·¯å¾„
                index_recommendations.append({
                    'type': 'json_index',
                    'field': pattern['field_path'],
                    'sql': f"ALTER TABLE {collection_name} ADD INDEX idx_{pattern['field_path'].replace('.', '_')} ((CAST(doc->'$.{pattern['field_path']}' AS CHAR(255))))"
                })
                
        return index_recommendations
```

### 9.3 Document Storeå¤‡ä»½æ¢å¤


**ğŸ’¾ æ–‡æ¡£å­˜å‚¨å¤‡ä»½ç­–ç•¥**
```bash
#!/bin/bash
# document_backup.sh - æ–‡æ¡£å­˜å‚¨å¤‡ä»½è„šæœ¬

# é…ç½®å‚æ•°
DB_HOST="mysql-server.company.com"
DB_PORT="33060"
DB_USER="backup_user"
DB_PASS="backup_password"
BACKUP_DIR="/backup/mysql/documents"
DATE=$(date +%Y%m%d_%H%M%S)

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p "${BACKUP_DIR}/${DATE}"

# å¤‡ä»½æ–‡æ¡£é›†åˆï¼ˆä½¿ç”¨X Protocolï¼‰
echo "å¼€å§‹å¤‡ä»½æ–‡æ¡£å­˜å‚¨..."

# éå†æ‰€æœ‰æ•°æ®åº“çš„æ–‡æ¡£é›†åˆ
mysql -h $DB_HOST -P 3306 -u $DB_USER -p$DB_PASS -e "
SELECT DISTINCT SCHEMA_NAME 
FROM information_schema.TABLES 
WHERE TABLE_NAME LIKE '%_collection_%' 
  AND ENGINE = 'InnoDB'
" | while read schema_name; do
    
    if [ "$schema_name" != "SCHEMA_NAME" ]; then
        echo "å¤‡ä»½æ•°æ®åº“: $schema_name"
        
        # å¯¼å‡ºæ–‡æ¡£æ•°æ®ä¸ºJSONæ ¼å¼
        mysqlsh --uri=${DB_USER}@${DB_HOST}:${DB_PORT} --password=$DB_PASS --js -e "
        var schema = session.getSchema('$schema_name');
        var collections = schema.getCollections();
        
        collections.forEach(function(collection) {
            var collectionName = collection.getName();
            var docs = collection.find().execute();
            var backupFile = '$BACKUP_DIR/$DATE/' + '$schema_name' + '_' + collectionName + '.json';
            
            var allDocs = [];
            while (doc = docs.fetchOne()) {
                allDocs.push(doc);
            }
            
            // å†™å…¥å¤‡ä»½æ–‡ä»¶
            var fs = require('fs');
            fs.writeFileSync(backupFile, JSON.stringify(allDocs, null, 2));
            print('å¤‡ä»½å®Œæˆ: ' + backupFile);
        });
        "
    fi
done

# å‹ç¼©å¤‡ä»½æ–‡ä»¶
cd "$BACKUP_DIR"
tar -czf "document_backup_${DATE}.tar.gz" "${DATE}/"
rm -rf "${DATE}/"

echo "æ–‡æ¡£å­˜å‚¨å¤‡ä»½å®Œæˆ: document_backup_${DATE}.tar.gz"
```

---

## 10. ğŸŒ MySQL Routerè¿ç»´ç®¡ç†


### 10.1 MySQL Routeræ¶æ„ä¸ä½œç”¨


**MySQL Router**æ˜¯MySQLå®˜æ–¹æä¾›çš„è½»é‡çº§ä¸­é—´ä»¶ï¼Œä¸»è¦ç”¨äºåº”ç”¨ç¨‹åºä¸MySQLæœåŠ¡å™¨ä¹‹é—´çš„è¿æ¥è·¯ç”±å’Œè´Ÿè½½å‡è¡¡ã€‚

**ğŸ”¸ Routeråœ¨æ¶æ„ä¸­çš„ä½ç½®**
```
åº”ç”¨ç¨‹åºå±‚                Routerå±‚                 MySQLæœåŠ¡å™¨å±‚
     â†“                      â†“                        â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Webåº”ç”¨     â”‚   â†’   â”‚ MySQL       â”‚   â†’    â”‚ MySQL Masterâ”‚
â”‚ APIæœåŠ¡     â”‚   â†’   â”‚ Router      â”‚   â†’    â”‚ (è¯»å†™)      â”‚
â”‚ åå°ä»»åŠ¡    â”‚   â†’   â”‚ - è¿æ¥è·¯ç”±  â”‚   â†’    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚ - è´Ÿè½½å‡è¡¡  â”‚   â†’    â”‚ MySQL Slave1â”‚
                       â”‚ - æ•…éšœåˆ‡æ¢  â”‚   â†’    â”‚ (åªè¯»)      â”‚
                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                                              â”‚ MySQL Slave2â”‚
                                              â”‚ (åªè¯»)      â”‚
                                              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Routerçš„æ ¸å¿ƒä½œç”¨ï¼š
âœ… é€æ˜ä»£ç†ï¼šåº”ç”¨æ— éœ€æ„ŸçŸ¥åç«¯MySQLæ‹“æ‰‘å˜åŒ–
âœ… è¯»å†™åˆ†ç¦»ï¼šè‡ªåŠ¨å°†è¯»è¯·æ±‚è·¯ç”±åˆ°ä»åº“ï¼Œå†™è¯·æ±‚åˆ°ä¸»åº“
âœ… è´Ÿè½½å‡è¡¡ï¼šåœ¨å¤šä¸ªä»åº“é—´åˆ†å‘è¯»è¯·æ±‚
âœ… æ•…éšœåˆ‡æ¢ï¼šä¸»åº“æ•…éšœæ—¶è‡ªåŠ¨åˆ‡æ¢åˆ°æ–°ä¸»åº“
```

### 10.2 Routeré…ç½®ä¸éƒ¨ç½²


**ğŸ”§ RouteråŸºç¡€é…ç½®**
```ini
# mysqlrouter.conf - Routeré…ç½®æ–‡ä»¶
[DEFAULT]
# æ—¥å¿—é…ç½®
logging_folder = /var/log/mysqlrouter
plugin_folder = /usr/lib64/mysqlrouter

# è¿è¡Œç”¨æˆ·
user = mysqlrouter

[logger]
level = INFO

# å…ƒæ•°æ®ç¼“å­˜é…ç½®
[metadata_cache:myCluster]
router_id = 1
bootstrap_server_addresses = mysql-master.company.com:3306,mysql-slave1.company.com:3306
user = router_user
metadata_cluster = myCluster
ttl = 0.5
auth_cache_ttl = 2
auth_cache_refresh_interval = 1

# è¯»å†™è·¯ç”±é…ç½®
[routing:myCluster_rw]
bind_address = 0.0.0.0
bind_port = 6446
destinations = metadata-cache://myCluster/default?role=PRIMARY
routing_strategy = round_robin
protocol = classic

# åªè¯»è·¯ç”±é…ç½®  
[routing:myCluster_ro]
bind_address = 0.0.0.0
bind_port = 6447
destinations = metadata-cache://myCluster/default?role=SECONDARY
routing_strategy = round_robin
protocol = classic

# X Protocolè·¯ç”±é…ç½®
[routing:myCluster_x_rw]
bind_address = 0.0.0.0
bind_port = 6448
destinations = metadata-cache://myCluster/default?role=PRIMARY
routing_strategy = round_robin
protocol = x

[routing:myCluster_x_ro]
bind_address = 0.0.0.0
bind_port = 6449
destinations = metadata-cache://myCluster/default?role=SECONDARY
routing_strategy = round_robin
protocol = x
```

**ğŸ“Š RouterçŠ¶æ€ç›‘æ§**
```sql
-- æŸ¥çœ‹Routerè¿æ¥ç»Ÿè®¡
-- éœ€è¦è¿æ¥åˆ°Routerçš„ç®¡ç†ç«¯å£ï¼ˆé»˜è®¤8443ï¼‰

-- æŸ¥çœ‹åç«¯MySQLæœåŠ¡å™¨çŠ¶æ€
SELECT 
    hostname,
    port,
    status,
    role,
    weight
FROM performance_schema.replication_group_members;

-- ç›‘æ§Routerå¤„ç†çš„è¿æ¥æ•°
SELECT 
    SUBSTRING_INDEX(SUBSTRING_INDEX($$hostname, '.', 1), '-', -1) as router_instance,
    COUNT(*) as active_connections
FROM performance_schema.threads 
WHERE CONNECTION_TYPE = 'TCP/IP'
GROUP BY router_instance;
```

### 10.3 Routerè¿ç»´ç›‘æ§è„šæœ¬


**ğŸ“ˆ Routerå¥åº·æ£€æŸ¥è„šæœ¬**
```python
# router_monitor.py - Routerç›‘æ§è„šæœ¬
import requests
import pymysql
import json
import time
from datetime import datetime

class MySQLRouterMonitor:
    def __init__(self, router_host, router_rest_port=8443):
        self.router_host = router_host
        self.router_rest_port = router_rest_port
        self.base_url = f"http://{router_host}:{router_rest_port}/api/20190715"
        
    def check_router_status(self):
        """æ£€æŸ¥RouteræœåŠ¡çŠ¶æ€"""
        try:
            response = requests.get(f"{self.base_url}/router/status", timeout=5)
            if response.status_code == 200:
                return {
                    'status': 'healthy',
                    'uptime': response.json().get('processUptimeSeconds', 0),
                    'version': response.json().get('version', 'unknown')
                }
            else:
                return {'status': 'unhealthy', 'error': f'HTTP {response.status_code}'}
        except Exception as e:
            return {'status': 'unreachable', 'error': str(e)}
            
    def check_routing_destinations(self):
        """æ£€æŸ¥è·¯ç”±ç›®æ ‡çŠ¶æ€"""
        try:
            response = requests.get(f"{self.base_url}/routes", timeout=5)
            routes_status = {}
            
            if response.status_code == 200:
                routes = response.json()
                for route_name, route_info in routes.items():
                    # è·å–æ¯ä¸ªè·¯ç”±çš„ç›®æ ‡çŠ¶æ€
                    dest_response = requests.get(
                        f"{self.base_url}/routes/{route_name}/destinations", 
                        timeout=5
                    )
                    
                    if dest_response.status_code == 200:
                        destinations = dest_response.json()
                        routes_status[route_name] = {
                            'total_destinations': len(destinations),
                            'healthy_destinations': sum(1 for d in destinations if d.get('status') == 'available'),
                            'destinations': destinations
                        }
                        
            return routes_status
        except Exception as e:
            return {'error': str(e)}
            
    def test_routing_connectivity(self):
        """æµ‹è¯•è·¯ç”±è¿æ¥æ€§"""
        # æµ‹è¯•è¯»å†™ç«¯å£è¿æ¥
        rw_test = self.test_mysql_connection(self.router_host, 6446)
        
        # æµ‹è¯•åªè¯»ç«¯å£è¿æ¥
        ro_test = self.test_mysql_connection(self.router_host, 6447)
        
        return {
            'read_write_port': rw_test,
            'read_only_port': ro_test
        }
        
    def test_mysql_connection(self, host, port):
        """æµ‹è¯•MySQLè¿æ¥"""
        try:
            start_time = time.time()
            connection = pymysql.connect(
                host=host,
                port=port,
                user='test_user',
                password='test_password',
                connect_timeout=5
            )
            
            # æ‰§è¡Œç®€å•æŸ¥è¯¢æµ‹è¯•
            with connection.cursor() as cursor:
                cursor.execute("SELECT 1")
                result = cursor.fetchone()
                
            connection.close()
            
            return {
                'status': 'success',
                'response_time_ms': (time.time() - start_time) * 1000,
                'result': result[0] if result else None
            }
        except Exception as e:
            return {
                'status': 'failed',
                'error': str(e)
            }
            
    def generate_router_report(self):
        """ç”ŸæˆRouterè¿ç»´æŠ¥å‘Š"""
        report = {
            'timestamp': datetime.now().isoformat(),
            'router_status': self.check_router_status(),
            'routing_destinations': self.check_routing_destinations(),
            'connectivity_tests': self.test_routing_connectivity(),
            'recommendations': []
        }
        
        # ç”Ÿæˆè¿ç»´å»ºè®®
        router_status = report['router_status']
        if router_status.get('status') != 'healthy':
            report['recommendations'].append(
                "RouteræœåŠ¡å¼‚å¸¸ï¼Œå»ºè®®æ£€æŸ¥æœåŠ¡çŠ¶æ€å’Œæ—¥å¿—"
            )
            
        # æ£€æŸ¥è·¯ç”±ç›®æ ‡å¥åº·çŠ¶æ€
        for route_name, route_info in report['routing_destinations'].items():
            if 'total_destinations' in route_info:
                healthy_ratio = route_info['healthy_destinations'] / route_info['total_destinations']
                if healthy_ratio < 0.5:
                    report['recommendations'].append(
                        f"è·¯ç”± {route_name} è¶…è¿‡50%çš„ç›®æ ‡ä¸å¯ç”¨ï¼Œå»ºè®®æ£€æŸ¥MySQLæœåŠ¡å™¨çŠ¶æ€"
                    )
                    
        return report

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    monitor = MySQLRouterMonitor('router.company.com')
    report = monitor.generate_router_report()
    print(json.dumps(report, indent=2, ensure_ascii=False))
```

### 10.4 Routeræ•…éšœå¤„ç†


**ğŸš¨ å¸¸è§æ•…éšœè¯Šæ–­ä¸å¤„ç†**
```bash
#!/bin/bash
# router_troubleshoot.sh - Routeræ•…éšœè¯Šæ–­è„šæœ¬

echo "=== MySQL Router æ•…éšœè¯Šæ–­å·¥å…· ==="

# 1. æ£€æŸ¥RouteræœåŠ¡çŠ¶æ€
echo "1. æ£€æŸ¥RouteræœåŠ¡çŠ¶æ€..."
systemctl status mysqlrouter
if [ $? -ne 0 ]; then
    echo "âŒ RouteræœåŠ¡æœªè¿è¡Œï¼Œå°è¯•å¯åŠ¨æœåŠ¡"
    systemctl start mysqlrouter
    sleep 3
    systemctl status mysqlrouter
fi

# 2. æ£€æŸ¥Routerè¿›ç¨‹å’Œç«¯å£
echo "2. æ£€æŸ¥Routerè¿›ç¨‹å’Œç›‘å¬ç«¯å£..."
ps aux | grep mysqlrouter | grep -v grep
netstat -tlnp | grep -E ':(6446|6447|6448|6449|8443)'

# 3. æ£€æŸ¥Routeræ—¥å¿—
echo "3. æ£€æŸ¥Routeré”™è¯¯æ—¥å¿—ï¼ˆæœ€è¿‘50è¡Œï¼‰..."
tail -50 /var/log/mysqlrouter/mysqlrouter.log | grep -i error

# 4. æµ‹è¯•è¿æ¥åç«¯MySQLæœåŠ¡å™¨
echo "4. æµ‹è¯•åç«¯MySQLæœåŠ¡å™¨è¿æ¥..."
MYSQL_SERVERS=("mysql-master.company.com:3306" "mysql-slave1.company.com:3306" "mysql-slave2.company.com:3306")

for server in "${MYSQL_SERVERS[@]}"; do
    host=$(echo $server | cut -d: -f1)
    port=$(echo $server | cut -d: -f2)
    
    echo "æµ‹è¯•è¿æ¥: $host:$port"
    timeout 5 bash -c "</dev/tcp/$host/$port" 2>/dev/null
    if [ $? -eq 0 ]; then
        echo "âœ… $server è¿æ¥æ­£å¸¸"
    else
        echo "âŒ $server è¿æ¥å¤±è´¥"
    fi
done

# 5. æ£€æŸ¥Routeré…ç½®æ–‡ä»¶
echo "5. éªŒè¯Routeré…ç½®æ–‡ä»¶..."
mysqlrouter --validate-config --config=/etc/mysqlrouter/mysqlrouter.conf
if [ $? -eq 0 ]; then
    echo "âœ… é…ç½®æ–‡ä»¶è¯­æ³•æ­£ç¡®"
else
    echo "âŒ é…ç½®æ–‡ä»¶å­˜åœ¨è¯­æ³•é”™è¯¯"
fi

# 6. æµ‹è¯•Routerç«¯å£è¿é€šæ€§
echo "6. æµ‹è¯•Routerç«¯å£è¿é€šæ€§..."
ROUTER_PORTS=(6446 6447 6448 6449 8443)

for port in "${ROUTER_PORTS[@]}"; do
    timeout 3 bash -c "</dev/tcp/localhost/$port" 2>/dev/null
    if [ $? -eq 0 ]; then
        echo "âœ… ç«¯å£ $port è¿æ¥æ­£å¸¸"
    else
        echo "âŒ ç«¯å£ $port è¿æ¥å¤±è´¥"
    fi
done

# 7. æ£€æŸ¥å…ƒæ•°æ®ç¼“å­˜çŠ¶æ€
echo "7. æ£€æŸ¥å…ƒæ•°æ®ç¼“å­˜çŠ¶æ€..."
curl -s http://localhost:8443/api/20190715/metadata | jq '.' 2>/dev/null || echo "æ— æ³•è·å–å…ƒæ•°æ®ä¿¡æ¯"

echo "=== è¯Šæ–­å®Œæˆ ==="
```

---

## 11. ğŸ”„ MySQL Group Replicationè¿ç»´


### 11.1 Group Replicationæ¦‚è¿°


**MySQL Group Replication**æ˜¯MySQLåŸç”Ÿçš„é«˜å¯ç”¨è§£å†³æ–¹æ¡ˆï¼Œæä¾›è‡ªåŠ¨æ•…éšœæ£€æµ‹ã€æ•…éšœåˆ‡æ¢å’Œæ•°æ®ä¸€è‡´æ€§ä¿è¯ã€‚

**ğŸ”¸ Group Replicationçš„æ ¸å¿ƒç‰¹æ€§**
```
ä¼ ç»Ÿä¸»ä»å¤åˆ¶çš„é—®é¢˜ï¼š
âŒ æ‰‹åŠ¨æ•…éšœåˆ‡æ¢ï¼Œæ¢å¤æ—¶é—´é•¿
âŒ æ•°æ®ä¸€è‡´æ€§ä¾èµ–äººå·¥ä¿è¯
âŒ è„‘è£‚é—®é¢˜éš¾ä»¥å¤„ç†
âŒ æ‰©å±•æ€§æœ‰é™

Group Replicationçš„ä¼˜åŠ¿ï¼š
âœ… è‡ªåŠ¨æ•…éšœæ£€æµ‹å’Œåˆ‡æ¢
âœ… å¼ºä¸€è‡´æ€§ä¿è¯ï¼ˆåŸºäºPaxosç®—æ³•ï¼‰
âœ… å¤šä¸»æ¨¡å¼æ”¯æŒ
âœ… è‡ªåŠ¨æˆå‘˜ç®¡ç†
âœ… é˜²æ­¢è„‘è£‚é—®é¢˜
```

**ğŸ—ï¸ Group Replicationæ¶æ„**
```
MySQL Group Replication é›†ç¾¤æ¶æ„ï¼š

       åº”ç”¨ç¨‹åºå±‚
           â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚   MySQL Router  â”‚ â† è¿æ¥è·¯ç”±å’Œè´Ÿè½½å‡è¡¡
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†“
    Group Replicationé›†ç¾¤
    
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  MySQL1     â”‚ â† â”‚  MySQL2     â”‚ â†’ â”‚  MySQL3     â”‚
    â”‚  (Primary)  â”‚   â”‚  (Secondary)â”‚   â”‚  (Secondary)â”‚
    â”‚  è¯»å†™       â”‚   â”‚  åªè¯»       â”‚   â”‚  åªè¯»       â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
           â†‘                â†‘                â†‘
           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
                  Group Communication
                  (Paxos consensus)
```

### 11.2 Group Replicationè¿ç»´ç›‘æ§


**ğŸ“Š é›†ç¾¤çŠ¶æ€ç›‘æ§**
```sql
-- æŸ¥çœ‹Group Replicationé›†ç¾¤çŠ¶æ€
SELECT 
    MEMBER_ID,
    MEMBER_HOST,
    MEMBER_PORT,
    MEMBER_STATE,
    MEMBER_ROLE,
    MEMBER_VERSION
FROM performance_schema.replication_group_members;

-- æŸ¥çœ‹é›†ç¾¤å¤åˆ¶å»¶è¿Ÿ
SELECT 
    CHANNEL_NAME,
    MEMBER_ID,
    COUNT_TRANSACTIONS_IN_QUEUE as pending_transactions,
    COUNT_TRANSACTIONS_CHECKED as checked_transactions,
    COUNT_CONFLICTS_DETECTED as conflicts_detected,
    COUNT_TRANSACTIONS_ROWS_VALIDATING as validating_transactions
FROM performance_schema.replication_group_member_stats;

-- æŸ¥çœ‹é›†ç¾¤ç½‘ç»œçŠ¶æ€
SELECT 
    FROM_UUID,
    TO_UUID, 
    COUNT_MESSAGES_SENT,
    COUNT_MESSAGES_RECEIVED,
    COUNT_MESSAGE_SEND_FAILURES,
    COUNT_MESSAGE_RECEIVE_FAILURES
FROM performance_schema.replication_group_communication_information;
```

**ğŸ” é›†ç¾¤å¥åº·æ£€æŸ¥è„šæœ¬**
```python
# group_replication_monitor.py - Group Replicationé›†ç¾¤ç›‘æ§
import pymysql
import time
import json
from datetime import datetime

class GroupReplicationMonitor:
    def __init__(self, cluster_nodes):
        self.cluster_nodes = cluster_nodes  # [{'host': 'mysql1', 'port': 3306, 'user': 'admin', 'password': 'pass'}, ...]
        
    def get_cluster_status(self):
        """è·å–é›†ç¾¤æ•´ä½“çŠ¶æ€"""
        cluster_status = {
            'timestamp': datetime.now().isoformat(),
            'total_nodes': len(self.cluster_nodes),
            'online_nodes': 0,
            'primary_node': None,
            'secondary_nodes': [],
            'offline_nodes': [],
            'cluster_health': 'unknown'
        }
        
        for node in self.cluster_nodes:
            try:
                connection = pymysql.connect(**node)
                node_info = self.get_node_status(connection)
                node_info['connection_info'] = {
                    'host': node['host'],
                    'port': node['port']
                }
                
                if node_info['member_state'] == 'ONLINE':
                    cluster_status['online_nodes'] += 1
                    if node_info['member_role'] == 'PRIMARY':
                        cluster_status['primary_node'] = node_info
                    else:
                        cluster_status['secondary_nodes'].append(node_info)
                else:
                    cluster_status['offline_nodes'].append(node_info)
                    
                connection.close()
                
            except Exception as e:
                cluster_status['offline_nodes'].append({
                    'host': node['host'],
                    'port': node['port'],
                    'error': str(e),
                    'member_state': 'UNREACHABLE'
                })
        
        # åˆ¤æ–­é›†ç¾¤å¥åº·çŠ¶æ€
        if cluster_status['online_nodes'] == cluster_status['total_nodes']:
            cluster_status['cluster_health'] = 'HEALTHY'
        elif cluster_status['online_nodes'] >= (cluster_status['total_nodes'] // 2 + 1):
            cluster_status['cluster_health'] = 'DEGRADED'
        else:
            cluster_status['cluster_health'] = 'CRITICAL'
            
        return cluster_status
        
    def get_node_status(self, connection):
        """è·å–å•ä¸ªèŠ‚ç‚¹çŠ¶æ€"""
        with connection.cursor(pymysql.cursors.DictCursor) as cursor:
            # è·å–èŠ‚ç‚¹åŸºæœ¬ä¿¡æ¯
            cursor.execute("""
                SELECT 
                    MEMBER_ID,
                    MEMBER_HOST,
                    MEMBER_PORT,
                    MEMBER_STATE,
                    MEMBER_ROLE
                FROM performance_schema.replication_group_members 
                WHERE MEMBER_ID = $$server_uuid
            """)
            member_info = cursor.fetchone()
            
            # è·å–å¤åˆ¶ç»Ÿè®¡ä¿¡æ¯
            cursor.execute("""
                SELECT 
                    COUNT_TRANSACTIONS_IN_QUEUE,
                    COUNT_TRANSACTIONS_CHECKED,
                    COUNT_CONFLICTS_DETECTED,
                    COUNT_TRANSACTIONS_ROWS_VALIDATING
                FROM performance_schema.replication_group_member_stats 
                WHERE MEMBER_ID = $$server_uuid
            """)
            stats_info = cursor.fetchone()
            
            # åˆå¹¶ä¿¡æ¯
            if member_info:
                member_info.update(stats_info or {})
                
            return member_info or {}
            
    def check_cluster_performance(self):
        """æ£€æŸ¥é›†ç¾¤æ€§èƒ½æŒ‡æ ‡"""
        performance_metrics = {}
        
        for node in self.cluster_nodes:
            try:
                connection = pymysql.connect(**node)
                
                with connection.cursor(pymysql.cursors.DictCursor) as cursor:
                    # æ£€æŸ¥å¤åˆ¶å»¶è¿Ÿ
                    cursor.execute("""
                        SELECT 
                            COUNT_TRANSACTIONS_IN_QUEUE as repl_lag,
                            COUNT_CONFLICTS_DETECTED as conflicts,
                            LAST_CONFLICT_ERRNO as last_conflict_errno
                        FROM performance_schema.replication_group_member_stats 
                        WHERE MEMBER_ID = $$server_uuid
                    """)
                    
                    node_perf = cursor.fetchone()
                    if node_perf:
                        performance_metrics[f"{node['host']}:{node['port']}"] = node_perf
                        
                connection.close()
                
            except Exception as e:
                performance_metrics[f"{node['host']}:{node['port']}"] = {'error': str(e)}
                
        return performance_metrics
        
    def detect_split_brain(self):
        """æ£€æµ‹è„‘è£‚æƒ…å†µ"""
        primary_nodes = []
        
        for node in self.cluster_nodes:
            try:
                connection = pymysql.connect(**node)
                node_status = self.get_node_status(connection)
                
                if node_status.get('member_role') == 'PRIMARY' and node_status.get('member_state') == 'ONLINE':
                    primary_nodes.append({
                        'host': node['host'],
                        'port': node['port'],
                        'member_id': node_status.get('member_id')
                    })
                    
                connection.close()
                
            except Exception:
                continue
                
        # æ£€æŸ¥æ˜¯å¦å­˜åœ¨å¤šä¸ªPrimary
        split_brain_detected = len(primary_nodes) > 1
        
        return {
            'split_brain_detected': split_brain_detected,
            'primary_count': len(primary_nodes),
            'primary_nodes': primary_nodes
        }
        
    def generate_cluster_report(self):
        """ç”Ÿæˆé›†ç¾¤è¿ç»´æŠ¥å‘Š"""
        report = {
            'cluster_status': self.get_cluster_status(),
            'performance_metrics': self.check_cluster_performance(),
            'split_brain_check': self.detect_split_brain(),
            'recommendations': []
        }
        
        # ç”Ÿæˆè¿ç»´å»ºè®®
        cluster_health = report['cluster_status']['cluster_health']
        
        if cluster_health == 'CRITICAL':
            report['recommendations'].append(
                "âš ï¸ é›†ç¾¤å¤„äºå±é™©çŠ¶æ€ï¼Œåœ¨çº¿èŠ‚ç‚¹ä¸è¶³ï¼Œå¯èƒ½å½±å“ä¸šåŠ¡å¯ç”¨æ€§"
            )
        elif cluster_health == 'DEGRADED':
            report['recommendations'].append(
                "âš ï¸ é›†ç¾¤å¤„äºé™çº§çŠ¶æ€ï¼Œå­˜åœ¨ç¦»çº¿èŠ‚ç‚¹ï¼Œå»ºè®®å°½å¿«ä¿®å¤"
            )
            
        # æ£€æŸ¥å¤åˆ¶å»¶è¿Ÿ
        for node, perf in report['performance_metrics'].items():
            if isinstance(perf, dict) and 'repl_lag' in perf:
                if perf['repl_lag'] > 100:
                    report['recommendations'].append(
                        f"ğŸ“Š èŠ‚ç‚¹ {node} å¤åˆ¶å»¶è¿Ÿè¾ƒé«˜ ({perf['repl_lag']} äº‹åŠ¡)ï¼Œå»ºè®®æ£€æŸ¥ç½‘ç»œå’Œæ€§èƒ½"
                    )
                    
        # æ£€æŸ¥è„‘è£‚
        if report['split_brain_check']['split_brain_detected']:
            report['recommendations'].append(
                "ğŸš¨ æ£€æµ‹åˆ°è„‘è£‚æƒ…å†µï¼Œå­˜åœ¨å¤šä¸ªPrimaryèŠ‚ç‚¹ï¼Œéœ€è¦ç«‹å³å¤„ç†"
            )
            
        return report

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    cluster_nodes = [
        {'host': 'mysql1.company.com', 'port': 3306, 'user': 'monitor', 'password': 'password'},
        {'host': 'mysql2.company.com', 'port': 3306, 'user': 'monitor', 'password': 'password'},
        {'host': 'mysql3.company.com', 'port': 3306, 'user': 'monitor', 'password': 'password'}
    ]
    
    monitor = GroupReplicationMonitor(cluster_nodes)
    report = monitor.generate_cluster_report()
    print(json.dumps(report, indent=2, ensure_ascii=False))
```

### 11.3 Group Replicationæ•…éšœå¤„ç†


**ğŸ”§ å¸¸è§æ•…éšœåœºæ™¯ä¸å¤„ç†**
```bash
#!/bin/bash
# group_replication_recovery.sh - Group Replicationæ•…éšœæ¢å¤è„šæœ¬

echo "=== MySQL Group Replication æ•…éšœæ¢å¤å·¥å…· ==="

# é…ç½®å‚æ•°
CLUSTER_NODES=("mysql1.company.com" "mysql2.company.com" "mysql3.company.com")
MYSQL_USER="admin"
MYSQL_PASS="password"

# å‡½æ•°ï¼šæ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€
check_node_status() {
    local host=$1
    echo "æ£€æŸ¥èŠ‚ç‚¹çŠ¶æ€: $host"
    
    mysql -h $host -u $MYSQL_USER -p$MYSQL_PASS -e "
    SELECT 
        MEMBER_HOST,
        MEMBER_STATE,
        MEMBER_ROLE
    FROM performance_schema.replication_group_members 
    WHERE MEMBER_HOST = '$host';
    " 2>/dev/null
}

# å‡½æ•°ï¼šå¯åŠ¨Group Replication
start_group_replication() {
    local host=$1
    local is_bootstrap=$2
    
    echo "åœ¨èŠ‚ç‚¹ $host å¯åŠ¨Group Replication..."
    
    if [ "$is_bootstrap" = "true" ]; then
        mysql -h $host -u $MYSQL_USER -p$MYSQL_PASS -e "
        SET GLOBAL group_replication_bootstrap_group=ON;
        START GROUP_REPLICATION;
        SET GLOBAL group_replication_bootstrap_group=OFF;
        " 2>/dev/null
        echo "âœ… èŠ‚ç‚¹ $host ä½œä¸ºå¼•å¯¼èŠ‚ç‚¹å¯åŠ¨"
    else
        mysql -h $host -u $MYSQL_USER -p$MYSQL_PASS -e "
        START GROUP_REPLICATION;
        " 2>/dev/null
        echo "âœ… èŠ‚ç‚¹ $host åŠ å…¥é›†ç¾¤"
    fi
}

# å‡½æ•°ï¼šåœæ­¢Group Replication
stop_group_replication() {
    local host=$1
    echo "åœ¨èŠ‚ç‚¹ $host åœæ­¢Group Replication..."
    
    mysql -h $host -u $MYSQL_USER -p$MYSQL_PASS -e "
    STOP GROUP_REPLICATION;
    " 2>/dev/null
    echo "âœ… èŠ‚ç‚¹ $host å·²åœæ­¢Group Replication"
}

# 1. æ£€æŸ¥æ‰€æœ‰èŠ‚ç‚¹çŠ¶æ€
echo "1. æ£€æŸ¥é›†ç¾¤æ‰€æœ‰èŠ‚ç‚¹çŠ¶æ€..."
online_nodes=()
offline_nodes=()

for node in "${CLUSTER_NODES[@]}"; do
    if mysql -h $node -u $MYSQL_USER -p$MYSQL_PASS -e "SELECT 1;" &>/dev/null; then
        status=$(mysql -h $node -u $MYSQL_USER -p$MYSQL_PASS -se "
        SELECT MEMBER_STATE 
        FROM performance_schema.replication_group_members 
        WHERE MEMBER_HOST = '$node';" 2>/dev/null)
        
        if [ "$status" = "ONLINE" ]; then
            online_nodes+=($node)
            echo "âœ… $node: ONLINE"
        else
            offline_nodes+=($node)
            echo "âŒ $node: $status"
        fi
    else
        offline_nodes+=($node)
        echo "âŒ $node: è¿æ¥å¤±è´¥"
    fi
done

echo "åœ¨çº¿èŠ‚ç‚¹æ•°: ${#online_nodes[@]}"
echo "ç¦»çº¿èŠ‚ç‚¹æ•°: ${#offline_nodes[@]}"

# 2. æ ¹æ®æƒ…å†µé€‰æ‹©æ¢å¤ç­–ç•¥
if [ ${#online_nodes[@]} -eq 0 ]; then
    echo "2. æ‰€æœ‰èŠ‚ç‚¹éƒ½ç¦»çº¿ï¼Œæ‰§è¡Œé›†ç¾¤é‡å»º..."
    
    # é€‰æ‹©ç¬¬ä¸€ä¸ªå¯è¿æ¥çš„èŠ‚ç‚¹ä½œä¸ºå¼•å¯¼èŠ‚ç‚¹
    for node in "${CLUSTER_NODES[@]}"; do
        if mysql -h $node -u $MYSQL_USER -p$MYSQL_PASS -e "SELECT 1;" &>/dev/null; then
            echo "é€‰æ‹© $node ä½œä¸ºå¼•å¯¼èŠ‚ç‚¹"
            start_group_replication $node true
            
            # ç­‰å¾…å¼•å¯¼èŠ‚ç‚¹å¯åŠ¨
            sleep 5
            
            # å¯åŠ¨å…¶ä»–èŠ‚ç‚¹
            for other_node in "${CLUSTER_NODES[@]}"; do
                if [ "$other_node" != "$node" ]; then
                    if mysql -h $other_node -u $MYSQL_USER -p$MYSQL_PASS -e "SELECT 1;" &>/dev/null; then
                        start_group_replication $other_node false
                        sleep 3
                    fi
                fi
            done
            break
        fi
    done
    
elif [ ${#online_nodes[@]} -lt $((${#CLUSTER_NODES[@]} / 2 + 1)) ]; then
    echo "2. åœ¨çº¿èŠ‚ç‚¹ä¸è¶³åŠæ•°ï¼Œå¯èƒ½éœ€è¦å¼ºåˆ¶é‡æ–°é…ç½®..."
    echo "âš ï¸ è¿™æ˜¯å±é™©æ“ä½œï¼Œè¯·ç¡®è®¤æ•°æ®ä¸€è‡´æ€§åæ‰‹åŠ¨æ‰§è¡Œ"
    echo "æ‰‹åŠ¨å‘½ä»¤ï¼š"
    echo "SET GLOBAL group_replication_force_members = '${online_nodes[0]}:33061';"
    
else
    echo "2. åœ¨çº¿èŠ‚ç‚¹æ•°é‡å……è¶³ï¼Œå°è¯•é‡æ–°åŠ å…¥ç¦»çº¿èŠ‚ç‚¹..."
    
    for node in "${offline_nodes[@]}"; do
        if mysql -h $node -u $MYSQL_USER -p$MYSQL_PASS -e "SELECT 1;" &>/dev/null; then
            echo "é‡æ–°å¯åŠ¨èŠ‚ç‚¹: $node"
            
            # å…ˆåœæ­¢å¯èƒ½å­˜åœ¨çš„Group Replication
            stop_group_replication $node
            sleep 2
            
            # é‡æ–°åŠ å…¥é›†ç¾¤
            start_group_replication $node false
            sleep 3
            
            # æ£€æŸ¥åŠ å…¥ç»“æœ
            check_node_status $node
        fi
    done
fi

# 3. æœ€ç»ˆçŠ¶æ€æ£€æŸ¥
echo "3. æœ€ç»ˆé›†ç¾¤çŠ¶æ€æ£€æŸ¥..."
sleep 5

for node in "${CLUSTER_NODES[@]}"; do
    check_node_status $node
done

echo "=== æ¢å¤å®Œæˆ ==="
```

---

## 12. ğŸ—ï¸ MySQL InnoDB Clusterè¿ç»´


### 12.1 InnoDB Clusteræ¦‚è¿°ä¸æ¶æ„


**MySQL InnoDB Cluster**æ˜¯MySQLå®˜æ–¹æä¾›çš„é«˜å¯ç”¨é›†ç¾¤è§£å†³æ–¹æ¡ˆï¼Œé›†æˆäº†Group Replicationã€MySQL Routerå’ŒMySQL Shellï¼Œæä¾›å®Œæ•´çš„é«˜å¯ç”¨æ€§æ–¹æ¡ˆã€‚

**ğŸ”¸ InnoDB Cluster vs Group Replication**
```
Group Replicationï¼š
- æ ¸å¿ƒå¤åˆ¶æŠ€æœ¯
- éœ€è¦æ‰‹åŠ¨é…ç½®Router
- éœ€è¦æ‰‹åŠ¨ç®¡ç†é›†ç¾¤

InnoDB Clusterï¼š
- å®Œæ•´çš„é«˜å¯ç”¨è§£å†³æ–¹æ¡ˆ
- è‡ªåŠ¨é…ç½®å’Œç®¡ç†
- é›†æˆRouterã€Shellç®¡ç†å·¥å…·
- ç®€åŒ–è¿ç»´æ“ä½œ
```

**ğŸ—ï¸ InnoDB Clusterå®Œæ•´æ¶æ„**
```
MySQL InnoDB Cluster å®Œæ•´æ¶æ„ï¼š

                      ç®¡ç†å±‚
                        â†“
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  MySQL Shell    â”‚ â† é›†ç¾¤ç®¡ç†å’Œè¿ç»´
                â”‚  - é›†ç¾¤åˆ›å»º     â”‚
                â”‚  - çŠ¶æ€ç›‘æ§     â”‚
                â”‚  - æ•…éšœæ¢å¤     â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
                   åº”ç”¨æ¥å…¥å±‚
                        â†“
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  MySQL Router   â”‚ â† è¿æ¥è·¯ç”±å’Œè´Ÿè½½å‡è¡¡
                â”‚  - è¯»å†™åˆ†ç¦»     â”‚
                â”‚  - æ•…éšœåˆ‡æ¢     â”‚
                â”‚  - è¿æ¥æ±        â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                        â†“
                    æ•°æ®å­˜å‚¨å±‚
                        â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚     MySQL Group Replication     â”‚
         â”‚                                 â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ MySQL1  â”‚ â† â”‚ MySQL2  â”‚ â†’ â”‚ MySQL3  â”‚
    â”‚Primary  â”‚   â”‚Secondaryâ”‚   â”‚Secondaryâ”‚
    â”‚(è¯»å†™)   â”‚   â”‚(åªè¯»)   â”‚   â”‚(åªè¯»)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 12.2 InnoDB Clusterè¿ç»´ç®¡ç†


**ğŸ”§ ä½¿ç”¨MySQL Shellç®¡ç†é›†ç¾¤**
```javascript
// cluster_admin.js - é›†ç¾¤ç®¡ç†è„šæœ¬
// è¿æ¥åˆ°é›†ç¾¤
shell.connect('admin@mysql1.company.com:3306');

// è·å–é›†ç¾¤å®ä¾‹
var cluster = dba.getCluster('myCluster');

// 1. æ£€æŸ¥é›†ç¾¤çŠ¶æ€
function checkClusterStatus() {
    print("=== é›†ç¾¤çŠ¶æ€æ£€æŸ¥ ===");
    
    var status = cluster.status();
    print("é›†ç¾¤åç§°: " + status.clusterName);
    print("é›†ç¾¤çŠ¶æ€: " + status.defaultReplicaSet.status);
    print("æ‹“æ‰‘æ¨¡å¼: " + status.defaultReplicaSet.topologyMode);
    
    // æ£€æŸ¥å„èŠ‚ç‚¹çŠ¶æ€
    var members = status.defaultReplicaSet.topology;
    for (var member in members) {
        var memberInfo = members[member];
        print("èŠ‚ç‚¹: " + member);
        print("  çŠ¶æ€: " + memberInfo.status);
        print("  è§’è‰²: " + memberInfo.memberRole);
        print("  ç‰ˆæœ¬: " + memberInfo.version);
        
        if (memberInfo.instanceErrors && memberInfo.instanceErrors.length > 0) {
            print("  é”™è¯¯: " + JSON.stringify(memberInfo.instanceErrors));
        }
    }
}

// 2. æ·»åŠ èŠ‚ç‚¹åˆ°é›†ç¾¤
function addInstanceToCluster(instanceUri) {
    print("=== æ·»åŠ èŠ‚ç‚¹åˆ°é›†ç¾¤ ===");
    print("ç›®æ ‡èŠ‚ç‚¹: " + instanceUri);
    
    try {
        cluster.addInstance(instanceUri, {
            recoveryMethod: 'clone',  // ä½¿ç”¨cloneè¿›è¡Œæ•°æ®æ¢å¤
            waitRecovery: 2          // ç­‰å¾…æ¢å¤å®Œæˆçš„è¶…æ—¶æ—¶é—´
        });
        print("âœ… èŠ‚ç‚¹æ·»åŠ æˆåŠŸ");
    } catch (error) {
        print("âŒ èŠ‚ç‚¹æ·»åŠ å¤±è´¥: " + error.message);
    }
}

// 3. ä»é›†ç¾¤ç§»é™¤èŠ‚ç‚¹
function removeInstanceFromCluster(instanceUri) {
    print("=== ä»é›†ç¾¤ç§»é™¤èŠ‚ç‚¹ ===");
    print("ç›®æ ‡èŠ‚ç‚¹: " + instanceUri);
    
    try {
        cluster.removeInstance(instanceUri, {force: false});
        print("âœ… èŠ‚ç‚¹ç§»é™¤æˆåŠŸ");
    } catch (error) {
        print("âŒ èŠ‚ç‚¹ç§»é™¤å¤±è´¥: " + error.message);
        
        // å¦‚æœèŠ‚ç‚¹æ— å“åº”ï¼Œå¯ä»¥å¼ºåˆ¶ç§»é™¤
        var forceRemove = readline("èŠ‚ç‚¹æ— å“åº”ï¼Œæ˜¯å¦å¼ºåˆ¶ç§»é™¤ï¼Ÿ(y/N): ");
        if (forceRemove.toLowerCase() === 'y') {
            cluster.removeInstance(instanceUri, {force: true});
            print("âœ… å¼ºåˆ¶ç§»é™¤æˆåŠŸ");
        }
    }
}

// 4. é›†ç¾¤æ•…éšœæ¢å¤
function recoverCluster() {
    print("=== é›†ç¾¤æ•…éšœæ¢å¤ ===");
    
    try {
        // å°è¯•é‡æ–°æ‰«æé›†ç¾¤
        cluster.rescan();
        print("âœ… é›†ç¾¤é‡æ–°æ‰«æå®Œæˆ");
        
        // æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°é…ç½®
        var status = cluster.status();
        if (status.defaultReplicaSet.status !== 'OK') {
            print("é›†ç¾¤çŠ¶æ€å¼‚å¸¸ï¼Œå°è¯•è‡ªåŠ¨ä¿®å¤...");
            
            // å¼ºåˆ¶é‡æ–°é…ç½®é›†ç¾¤ä»²è£
            cluster.forceQuorumUsingPartitionOf('mysql1.company.com:3306');
            print("âœ… å¼ºåˆ¶ä»²è£é…ç½®å®Œæˆ");
        }
        
    } catch (error) {
        print("âŒ é›†ç¾¤æ¢å¤å¤±è´¥: " + error.message);
        print("å¯èƒ½éœ€è¦æ‰‹åŠ¨å¹²é¢„æˆ–é‡å»ºé›†ç¾¤");
    }
}

// 5. é›†ç¾¤æ€§èƒ½ç›‘æ§
function monitorClusterPerformance() {
    print("=== é›†ç¾¤æ€§èƒ½ç›‘æ§ ===");
    
    var status = cluster.status({extended: 1});
    var members = status.defaultReplicaSet.topology;
    
    for (var member in members) {
        var memberInfo = members[member];
        print("èŠ‚ç‚¹: " + member);
        
        if (memberInfo.transactions) {
            print("  é˜Ÿåˆ—ä¸­äº‹åŠ¡: " + memberInfo.transactions.applierQueue);
            print("  æ£€æŸ¥çš„äº‹åŠ¡: " + memberInfo.transactions.checked);
            print("  å†²çªæ£€æµ‹: " + memberInfo.transactions.conflicts);
        }
        
        if (memberInfo.recovery) {
            print("  æ¢å¤çŠ¶æ€: " + memberInfo.recovery.state);
        }
        
        print("---");
    }
}

// ä¸»å‡½æ•°
function main() {
    try {
        checkClusterStatus();
        monitorClusterPerformance();
        
        // å¯ä»¥æ ¹æ®éœ€è¦æ‰§è¡Œå…¶ä»–æ“ä½œ
        // addInstanceToCluster('mysql4.company.com:3306');
        // removeInstanceFromCluster('mysql4.company.com:3306');
        // recoverCluster();
        
    } catch (error) {
        print("æ‰§è¡Œå‡ºé”™: " + error.message);
    }
}

// æ‰§è¡Œä¸»å‡½æ•°
main();
```

**ğŸ“Š é›†ç¾¤ç›‘æ§è„šæœ¬**
```python
# innodb_cluster_monitor.py - InnoDB Clusterç›‘æ§è„šæœ¬
import subprocess
import json
import re
from datetime import datetime

class InnoDBClusterMonitor:
    def __init__(self, shell_path='/usr/bin/mysqlsh', connection_uri='admin@mysql1.company.com:3306'):
        self.shell_path = shell_path
        self.connection_uri = connection_uri
        
    def execute_shell_command(self, js_code):
        """æ‰§è¡ŒMySQL Shellå‘½ä»¤"""
        try:
            cmd = [
                self.shell_path,
                '--uri', self.connection_uri,
                '--js',
                '-e', js_code
            ]
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            return {
                'success': result.returncode == 0,
                'stdout': result.stdout,
                'stderr': result.stderr
            }
        except subprocess.TimeoutExpired:
            return {
                'success': False,
                'stdout': '',
                'stderr': 'Command timeout'
            }
        except Exception as e:
            return {
                'success': False,
                'stdout': '',
                'stderr': str(e)
            }
            
    def get_cluster_status(self):
        """è·å–é›†ç¾¤çŠ¶æ€"""
        js_code = '''
        var cluster = dba.getCluster();
        var status = cluster.status({extended: 1});
        print(JSON.stringify(status, null, 2));
        '''
        
        result = self.execute_shell_command(js_code)
        
        if result['success']:
            try:
                # ä»è¾“å‡ºä¸­æå–JSON
                json_match = re.search(r'\{.*\}', result['stdout'], re.DOTALL)
                if json_match:
                    status_data = json.loads(json_match.group())
                    return self.parse_cluster_status(status_data)
            except json.JSONDecodeError:
                pass
                
        return {
            'cluster_name': 'unknown',
            'cluster_status': 'ERROR',
            'members': [],
            'error': result.get('stderr', 'Unknown error')
        }
        
    def parse_cluster_status(self, status_data):
        """è§£æé›†ç¾¤çŠ¶æ€æ•°æ®"""
        cluster_info = {
            'timestamp': datetime.now().isoformat(),
            'cluster_name': status_data.get('clusterName', 'unknown'),
            'cluster_status': status_data.get('defaultReplicaSet', {}).get('status', 'unknown'),
            'topology_mode': status_data.get('defaultReplicaSet', {}).get('topologyMode', 'unknown'),
            'members': [],
            'health_summary': {
                'total_members': 0,
                'online_members': 0,
                'offline_members': 0,
                'primary_member': None
            }
        }
        
        # è§£ææˆå‘˜ä¿¡æ¯
        topology = status_data.get('defaultReplicaSet', {}).get('topology', {})
        
        for member_uri, member_info in topology.items():
            member_data = {
                'uri': member_uri,
                'status': member_info.get('status', 'unknown'),
                'role': member_info.get('memberRole', 'unknown'),
                'version': member_info.get('version', 'unknown'),
                'read_replicas': member_info.get('readReplicas', {}),
                'instance_errors': member_info.get('instanceErrors', [])
            }
            
            # æ·»åŠ äº‹åŠ¡ç»Ÿè®¡ä¿¡æ¯
            if 'transactions' in member_info:
                trans = member_info['transactions']
                member_data['transactions'] = {
                    'applier_queue': trans.get('applierQueue', 0),
                    'checked': trans.get('checked', 0),
                    'conflicts': trans.get('conflicts', 0),
                    'in_queue': trans.get('inQueue', 0)
                }
                
            cluster_info['members'].append(member_data)
            
            # æ›´æ–°å¥åº·çŠ¶å†µç»Ÿè®¡
            cluster_info['health_summary']['total_members'] += 1
            
            if member_data['status'] == 'ONLINE':
                cluster_info['health_summary']['online_members'] += 1
                if member_data['role'] == 'PRIMARY':
                    cluster_info['health_summary']['primary_member'] = member_uri
            else:
                cluster_info['health_summary']['offline_members'] += 1
                
        return cluster_info
        
    def check_cluster_health(self):
        """æ£€æŸ¥é›†ç¾¤å¥åº·çŠ¶å†µ"""
        status = self.get_cluster_status()
        health_issues = []
        
        # æ£€æŸ¥é›†ç¾¤æ•´ä½“çŠ¶æ€
        if status['cluster_status'] != 'OK':
            health_issues.append({
                'severity': 'HIGH',
                'type': 'cluster_status',
                'message': f"é›†ç¾¤çŠ¶æ€å¼‚å¸¸: {status['cluster_status']}"
            })
            
        # æ£€æŸ¥æ˜¯å¦æœ‰PrimaryèŠ‚ç‚¹
        if not status['health_summary']['primary_member']:
            health_issues.append({
                'severity': 'CRITICAL',
                'type': 'no_primary',
                'message': "é›†ç¾¤æ²¡æœ‰PrimaryèŠ‚ç‚¹"
            })
            
        # æ£€æŸ¥ç¦»çº¿èŠ‚ç‚¹
        offline_count = status['health_summary']['offline_members']
        total_count = status['health_summary']['total_members']
        
        if offline_count > 0:
            if offline_count >= total_count // 2:
                health_issues.append({
                    'severity': 'CRITICAL',
                    'type': 'majority_offline',
                    'message': f"è¶…è¿‡åŠæ•°èŠ‚ç‚¹ç¦»çº¿ ({offline_count}/{total_count})"
                })
            else:
                health_issues.append({
                    'severity': 'MEDIUM',
                    'type': 'member_offline',
                    'message': f"{offline_count}ä¸ªèŠ‚ç‚¹ç¦»çº¿"
                })
                
        # æ£€æŸ¥äº‹åŠ¡é˜Ÿåˆ—
        for member in status['members']:
            if 'transactions' in member:
                trans = member['transactions']
                if trans['applier_queue'] > 100:
                    health_issues.append({
                        'severity': 'MEDIUM',
                        'type': 'high_queue',
                        'message': f"èŠ‚ç‚¹ {member['uri']} äº‹åŠ¡é˜Ÿåˆ—è¿‡é•¿: {trans['applier_queue']}"
                    })
                    
                if trans['conflicts'] > 0:
                    health_issues.append({
                        'severity': 'LOW',
                        'type': 'conflicts',
                        'message': f"èŠ‚ç‚¹ {member['uri']} æ£€æµ‹åˆ°äº‹åŠ¡å†²çª: {trans['conflicts']}"
                    })
                    
        return {
            'cluster_status': status,
            'health_issues': health_issues,
            'overall_health': 'HEALTHY' if not health_issues else (
                'CRITICAL' if any(issue['severity'] == 'CRITICAL' for issue in health_issues) else 'WARNING'
            )
        }
        
    def generate_cluster_report(self):
        """ç”Ÿæˆé›†ç¾¤è¿ç»´æŠ¥å‘Š"""
        health_check = self.check_cluster_health()
        
        report = {
            'timestamp': datetime.now().isoformat(),
            'cluster_info': health_check['cluster_status'],
            'health_assessment': {
                'overall_health': health_check['overall_health'],
                'issues_count': len(health_check['health_issues']),
                'critical_issues': [issue for issue in health_check['health_issues'] if issue['severity'] == 'CRITICAL'],
                'all_issues': health_check['health_issues']
            },
            'recommendations': []
        }
        
        # ç”Ÿæˆè¿ç»´å»ºè®®
        if health_check['overall_health'] == 'CRITICAL':
            report['recommendations'].append(
                "ğŸš¨ é›†ç¾¤å¤„äºå±é™©çŠ¶æ€ï¼Œå»ºè®®ç«‹å³æ£€æŸ¥å¹¶ä¿®å¤é—®é¢˜"
            )
            
        for issue in health_check['health_issues']:
            if issue['type'] == 'no_primary':
                report['recommendations'].append(
                    "ğŸ”§ å»ºè®®æ‰§è¡Œ cluster.forceQuorumUsingPartitionOf() æ¢å¤PrimaryèŠ‚ç‚¹"
                )
            elif issue['type'] == 'majority_offline':
                report['recommendations'].append(
                    "ğŸ”§ å»ºè®®æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒMySQLæœåŠ¡çŠ¶æ€ï¼Œå¿…è¦æ—¶é‡å»ºé›†ç¾¤"
                )
            elif issue['type'] == 'high_queue':
                report['recommendations'].append(
                    "âš¡ å»ºè®®æ£€æŸ¥ç½‘ç»œå»¶è¿Ÿå’Œç£ç›˜æ€§èƒ½ï¼Œä¼˜åŒ–äº‹åŠ¡å¤„ç†"
                )
                
        return report

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    monitor = InnoDBClusterMonitor()
    report = monitor.generate_cluster_report()
    print(json.dumps(report, indent=2, ensure_ascii=False))
```

---

## 13. ğŸ“‹ æ ¸å¿ƒè¦ç‚¹æ€»ç»“


### 13.1 å¿…é¡»æŒæ¡çš„æ ¸å¿ƒæ¦‚å¿µ


**ğŸ”¸ è¿ç»´å·¥å…·ç”Ÿæ€ä½“ç³»**
```
å®˜æ–¹ä¼ä¸šçº§å·¥å…·ï¼š
- MySQL Enterprise Monitorï¼šä¸“ä¸šç›‘æ§è§£å†³æ–¹æ¡ˆ
- MySQL Workbenchï¼šå›¾å½¢åŒ–ç®¡ç†å·¥å…·
- MySQL Shellï¼šç°ä»£åŒ–å‘½ä»¤è¡Œç®¡ç†å·¥å…·

å¼€æºç¤¾åŒºå·¥å…·ï¼š
- Percona Toolkitï¼šä¸“ä¸šè¿ç»´å·¥å…·é›†
- ç¬¬ä¸‰æ–¹ç›‘æ§ï¼šPrometheus + Grafana
- è‡ªåŠ¨åŒ–è¿ç»´ï¼šAnsible + Docker

ç°ä»£åŒ–æ¶æ„ç»„ä»¶ï¼š
- X Protocolï¼šæ–°ä¸€ä»£é€šä¿¡åè®®
- Document Storeï¼šJSONæ–‡æ¡£å­˜å‚¨
- MySQL Routerï¼šè¿æ¥è·¯ç”±ä¸­é—´ä»¶
- Group Replicationï¼šåŸç”Ÿé«˜å¯ç”¨å¤åˆ¶
- InnoDB Clusterï¼šå®Œæ•´é›†ç¾¤è§£å†³æ–¹æ¡ˆ
```

### 13.2 å…³é”®ç†è§£è¦ç‚¹


**ğŸ”¹ è¿ç»´å·¥å…·é€‰æ‹©ç­–ç•¥**
```
ä¼ä¸šçº§éœ€æ±‚ï¼š
âœ… ä½¿ç”¨MySQL Enterprise Monitorå®ç°ä¸“ä¸šç›‘æ§
âœ… é¢„ç®—å……è¶³æ—¶é€‰æ‹©å®˜æ–¹å•†ä¸šå·¥å…·
âœ… éœ€è¦æŠ€æœ¯æ”¯æŒå’ŒSLAä¿éšœ

å¼€æºæ›¿ä»£æ–¹æ¡ˆï¼š
âœ… pt-toolkitæ»¡è¶³å¤§éƒ¨åˆ†è¿ç»´éœ€æ±‚
âœ… Prometheus + Grafanaæä¾›çµæ´»ç›‘æ§
âœ… è‡ªç ”å·¥å…·å®ç°å®šåˆ¶åŒ–éœ€æ±‚

ç°ä»£åŒ–æ¶æ„é€‰æ‹©ï¼š
âœ… æ–°é¡¹ç›®ä¼˜å…ˆè€ƒè™‘InnoDB Cluster
âœ… éœ€è¦NoSQLåŠŸèƒ½æ—¶ä½¿ç”¨Document Store
âœ… é«˜å¹¶å‘åœºæ™¯è€ƒè™‘X Protocol
âœ… å¤æ‚ç½‘ç»œç¯å¢ƒå¿…é¡»ä½¿ç”¨Router
```

**ğŸ”¹ é«˜å¯ç”¨æ¶æ„æ¼”è¿›è·¯å¾„**
```
ä¼ ç»Ÿä¸»ä»å¤åˆ¶ï¼š
é€‚ç”¨ï¼šç®€å•è¯»å†™åˆ†ç¦»åœºæ™¯
é™åˆ¶ï¼šæ‰‹åŠ¨æ•…éšœåˆ‡æ¢ï¼Œæ•°æ®ä¸€è‡´æ€§é£é™©

Group Replicationï¼š
é€‚ç”¨ï¼šéœ€è¦è‡ªåŠ¨æ•…éšœåˆ‡æ¢çš„åœºæ™¯
ä¼˜åŠ¿ï¼šå¼ºä¸€è‡´æ€§ï¼Œè‡ªåŠ¨æˆå‘˜ç®¡ç†

InnoDB Clusterï¼š
é€‚ç”¨ï¼šä¼ä¸šçº§é«˜å¯ç”¨éœ€æ±‚
ä¼˜åŠ¿ï¼šå®Œæ•´è§£å†³æ–¹æ¡ˆï¼Œç®€åŒ–è¿ç»´

é€‰æ‹©åŸåˆ™ï¼š
å°é¡¹ç›® â†’ ä¸»ä»å¤åˆ¶
ä¸­å‹é¡¹ç›® â†’ Group Replication
å¤§å‹é¡¹ç›® â†’ InnoDB Cluster
```

**ğŸ”¹ è¿ç»´å·¥å…·æ•ˆæœè¯„ä¼°**
```
æŠ€æœ¯æŒ‡æ ‡ï¼š
- å¯ç”¨æ€§ï¼šâ‰¥99.9%
- æ€§èƒ½ï¼šå“åº”æ—¶é—´<100ms
- æ¢å¤ï¼šMTTR<30åˆ†é’Ÿ

è¿ç»´æ•ˆç‡ï¼š
- è‡ªåŠ¨åŒ–è¦†ç›–ç‡ï¼šâ‰¥80%
- æ•…éšœé¢„è­¦ï¼šæå‰å‘ç°é—®é¢˜
- æˆæœ¬æ§åˆ¶ï¼šè¿ç»´æˆæœ¬<5%

æŒç»­æ”¹è¿›ï¼š
- å®šæœŸè¯„ä¼°å·¥å…·æ•ˆæœ
- æ ¹æ®ä¸šåŠ¡å¢é•¿è°ƒæ•´æ¶æ„
- å…³æ³¨æ–°æŠ€æœ¯å‘å±•è¶‹åŠ¿
```

### 13.3 å®é™…åº”ç”¨æŒ‡å¯¼


**ğŸ’¼ ä¼ä¸šçº§è¿ç»´æœ€ä½³å®è·µ**
```
ç›‘æ§ä½“ç³»å»ºè®¾ï¼š
1. åŸºç¡€ç›‘æ§ï¼šCPUã€å†…å­˜ã€ç£ç›˜ã€ç½‘ç»œ
2. MySQLç›‘æ§ï¼šQPSã€è¿æ¥æ•°ã€å¤åˆ¶å»¶è¿Ÿ
3. ä¸šåŠ¡ç›‘æ§ï¼šè®¢å•é‡ã€ç”¨æˆ·æ´»è·ƒåº¦
4. å‘Šè­¦æœºåˆ¶ï¼šåˆ†çº§å‘Šè­¦ï¼Œé¿å…å‘Šè­¦ç–²åŠ³

è‡ªåŠ¨åŒ–è¿ç»´ï¼š
1. è‡ªåŠ¨åŒ–éƒ¨ç½²ï¼šä½¿ç”¨Ansibleç­‰å·¥å…·
2. è‡ªåŠ¨åŒ–å¤‡ä»½ï¼šå®šæ—¶å¤‡ä»½+å¼‚åœ°å­˜å‚¨
3. è‡ªåŠ¨åŒ–ç›‘æ§ï¼šæ™ºèƒ½å‘Šè­¦+è‡ªåŠ¨æ¢å¤
4. è‡ªåŠ¨åŒ–æ‰©å®¹ï¼šåŸºäºç›‘æ§æŒ‡æ ‡è‡ªåŠ¨æ‰©å®¹

æ•…éšœå¤„ç†æµç¨‹ï¼š
1. æ•…éšœå‘ç°ï¼šç›‘æ§ç³»ç»Ÿè‡ªåŠ¨å‘ç°
2. æ•…éšœå®šä½ï¼šä½¿ç”¨ä¸“ä¸šè¯Šæ–­å·¥å…·
3. æ•…éšœå¤„ç†ï¼šæ‰§è¡Œé¢„å®šä¹‰çš„æ¢å¤æµç¨‹
4. æ•…éšœå¤ç›˜ï¼šåˆ†æåŸå› ï¼Œæ”¹è¿›æµç¨‹
```

**ğŸ¯ è¿ç»´å·¥å…·é€‰å‹å»ºè®®**
```
åˆåˆ›å…¬å¸ï¼š
- ä½¿ç”¨å¼€æºå·¥å…·ï¼špt-toolkit + Grafana
- é‡ç‚¹å…³æ³¨æˆæœ¬æ§åˆ¶
- ç®€åŒ–æ¶æ„ï¼Œé¿å…è¿‡åº¦è®¾è®¡

æˆé•¿æœŸå…¬å¸ï¼š
- æ··åˆä½¿ç”¨å¼€æºå’Œå•†ä¸šå·¥å…·
- å¼€å§‹å»ºè®¾è‡ªåŠ¨åŒ–è¿ç»´ä½“ç³»
- è€ƒè™‘å¼•å…¥ä¸“ä¸šç›‘æ§æ–¹æ¡ˆ

æˆç†Ÿä¼ä¸šï¼š
- ä¼˜å…ˆé€‰æ‹©ä¼ä¸šçº§è§£å†³æ–¹æ¡ˆ
- å»ºè®¾å®Œæ•´çš„è¿ç»´å·¥å…·é“¾
- æ³¨é‡å·¥å…·é—´çš„é›†æˆå’ŒååŒ

æŠ€æœ¯é€‰å‹åŸåˆ™ï¼š
- æ ¹æ®å›¢é˜ŸæŠ€æœ¯èƒ½åŠ›é€‰æ‹©
- è€ƒè™‘é•¿æœŸç»´æŠ¤æˆæœ¬
- é‡è§†å·¥å…·çš„æ‰©å±•æ€§å’Œå¯å®šåˆ¶æ€§
```

**ğŸ”§ è¿ç»´å·¥å…·å®æ–½è·¯çº¿å›¾**
```
ç¬¬ä¸€é˜¶æ®µï¼ˆåŸºç¡€ç›‘æ§ï¼‰ï¼š
Week 1-2: éƒ¨ç½²åŸºç¡€ç›‘æ§ï¼ˆPrometheus + Grafanaï¼‰
Week 3-4: é…ç½®MySQLç›‘æ§æŒ‡æ ‡
Week 5-6: å»ºç«‹å‘Šè­¦æœºåˆ¶

ç¬¬äºŒé˜¶æ®µï¼ˆä¸“ä¸šå·¥å…·ï¼‰ï¼š
Month 2: éƒ¨ç½²pt-toolkitå·¥å…·é›†
Month 3: é…ç½®æ…¢æŸ¥è¯¢åˆ†æ
Month 4: å®æ–½æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥

ç¬¬ä¸‰é˜¶æ®µï¼ˆé«˜çº§åŠŸèƒ½ï¼‰ï¼š
Month 5-6: éƒ¨ç½²MySQL Router
Month 7-8: é…ç½®Group Replication
Month 9-10: å»ºè®¾InnoDB Cluster

ç¬¬å››é˜¶æ®µï¼ˆè‡ªåŠ¨åŒ–è¿ç»´ï¼‰ï¼š
Month 11-12: å¼€å‘è‡ªåŠ¨åŒ–è¿ç»´è„šæœ¬
Year 2: å»ºè®¾å®Œæ•´çš„DevOpsæµæ°´çº¿
```

**æ ¸å¿ƒè®°å¿†è¦ç‚¹**ï¼š
- **å·¥å…·é€‰æ‹©**ï¼šæ ¹æ®ä¼ä¸šè§„æ¨¡å’Œéœ€æ±‚é€‰æ‹©åˆé€‚çš„è¿ç»´å·¥å…·ç»„åˆ
- **å¾ªåºæ¸è¿›**ï¼šä»åŸºç¡€ç›‘æ§å¼€å§‹ï¼Œé€æ­¥æ„å»ºå®Œæ•´çš„è¿ç»´ä½“ç³»
- **æŒç»­ä¼˜åŒ–**ï¼šå®šæœŸè¯„ä¼°å·¥å…·æ•ˆæœï¼Œæ ¹æ®ä¸šåŠ¡å‘å±•è°ƒæ•´ç­–ç•¥
- **äººå‘˜åŸ¹å…»**ï¼šé‡è§†å›¢é˜ŸæŠ€èƒ½åŸ¹å…»ï¼Œå·¥å…·åªæ˜¯æ‰‹æ®µï¼Œäººæ‰æ˜¯å…³é”®