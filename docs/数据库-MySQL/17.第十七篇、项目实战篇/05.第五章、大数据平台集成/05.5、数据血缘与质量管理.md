---
title: 5、数据血缘与质量管理
---
## 📚 目录

1. [数据血缘基础概念](#1-数据血缘基础概念)
2. [数据血缘追踪实现](#2-数据血缘追踪实现)
3. [数据质量管理体系](#3-数据质量管理体系)
4. [自动化血缘发现](#4-自动化血缘发现)
5. [数据质量监控与告警](#5-数据质量监控与告警)
6. [智能化数据治理](#6-智能化数据治理)
7. [核心要点总结](#7-核心要点总结)

---

## 1. 🔍 数据血缘基础概念


### 1.1 什么是数据血缘


**🔸 数据血缘定义**
> 数据血缘就像家族族谱一样，记录数据从哪里来、经过哪些处理、最终到哪里去的完整路径

```
简单理解：
原始数据 → 清洗处理 → 业务计算 → 报表展示
    ↓        ↓         ↓         ↓
  来源表   → 临时表  → 汇总表  → 视图表

每一步都有记录，就是数据血缘
```

**🔹 核心作用**
- **影响分析**：某个表结构变化会影响哪些下游应用
- **根因定位**：报表数据异常时快速找到问题源头  
- **依赖关系**：了解系统各部分的数据依赖关系
- **合规审计**：满足数据治理和合规要求

### 1.2 数据血缘的层次结构


```
┌─────────────────── 业务血缘层 ──────────────────┐
│ 销售报表 ← 订单汇总 ← 订单明细 ← 原始订单        │
├─────────────────── 技术血缘层 ──────────────────┤  
│ report.sales ← dwm.order_sum ← ods.order_detail │
├─────────────────── 字段血缘层 ──────────────────┤
│ total_amount = sum(price * quantity)            │
└────────────────────────────────────────────────┘

🔴 **业务血缘**：从业务角度看数据流动
🟡 **技术血缘**：从技术实现看表与表关系  
🟢 **字段血缘**：具体到每个字段的计算逻辑
```

### 1.3 血缘追踪的挑战


**⚠️ 常见难题**
```
复杂SQL解析：
• 多层子查询嵌套
• 动态SQL生成
• 存储过程调用

跨系统数据流：
• MySQL → Kafka → Spark → Hive
• 数据经过多个系统处理

实时性要求：
• 血缘信息要及时更新
• 变更影响要快速分析
```

---

## 2. 📊 数据血缘追踪实现


### 2.1 基于日志解析的血缘追踪


**🔸 MySQL日志解析方案**

```sql
-- 1. 开启MySQL审计日志
SET GLOBAL general_log = 'ON';
SET GLOBAL log_output = 'TABLE';

-- 2. 创建血缘记录表
CREATE TABLE data_lineage (
    id BIGINT AUTO_INCREMENT PRIMARY KEY,
    source_db VARCHAR(64) NOT NULL COMMENT '源数据库',
    source_table VARCHAR(64) NOT NULL COMMENT '源表名',
    target_db VARCHAR(64) NOT NULL COMMENT '目标数据库', 
    target_table VARCHAR(64) NOT NULL COMMENT '目标表名',
    operation_type ENUM('SELECT','INSERT','UPDATE','DELETE') COMMENT '操作类型',
    sql_text TEXT COMMENT 'SQL语句',
    created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP COMMENT '记录时间',
    INDEX idx_source (source_db, source_table),
    INDEX idx_target (target_db, target_table)
) COMMENT '数据血缘追踪表';
```

**🔹 血缘解析实现**

```python
import re
import pymysql

class LineageParser:
    def __init__(self, connection):
        self.conn = connection
    
    def parse_sql_lineage(self, sql_text):
        """解析SQL语句中的表依赖关系"""
        # 简化的SQL解析逻辑
        source_tables = []
        target_table = None
        
        # 提取INSERT/UPDATE的目标表
        insert_pattern = r'INSERT\s+INTO\s+(\w+\.\w+|\w+)'
        update_pattern = r'UPDATE\s+(\w+\.\w+|\w+)'
        
        # 提取FROM/JOIN的源表
        from_pattern = r'FROM\s+(\w+\.\w+|\w+)'
        join_pattern = r'JOIN\s+(\w+\.\w+|\w+)'
        
        # 实际项目中需要更复杂的SQL解析器
        if 'INSERT' in sql_text.upper():
            target_match = re.search(insert_pattern, sql_text, re.IGNORECASE)
            if target_match:
                target_table = target_match.group(1)
        
        # 提取源表
        for pattern in [from_pattern, join_pattern]:
            matches = re.findall(pattern, sql_text, re.IGNORECASE)
            source_tables.extend(matches)
        
        return {
            'source_tables': source_tables,
            'target_table': target_table,
            'sql': sql_text
        }
    
    def record_lineage(self, lineage_info):
        """记录血缘关系到数据库"""
        sql = """
        INSERT INTO data_lineage 
        (source_db, source_table, target_db, target_table, operation_type, sql_text)
        VALUES (%s, %s, %s, %s, %s, %s)
        """
        # 实现具体的插入逻辑
        pass
```

### 2.2 血缘关系图构建


**🔸 血缘图数据结构**

```sql
-- 血缘节点表
CREATE TABLE lineage_nodes (
    node_id VARCHAR(128) PRIMARY KEY,
    node_type ENUM('table','view','procedure') NOT NULL,
    db_name VARCHAR(64) NOT NULL,
    table_name VARCHAR(64) NOT NULL,
    node_level INT DEFAULT 0 COMMENT '层次级别',
    created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
) COMMENT '血缘图节点表';

-- 血缘边表  
CREATE TABLE lineage_edges (
    edge_id BIGINT AUTO_INCREMENT PRIMARY KEY,
    source_node VARCHAR(128) NOT NULL,
    target_node VARCHAR(128) NOT NULL,
    edge_type ENUM('direct','derived') DEFAULT 'direct',
    transformation_logic TEXT COMMENT '转换逻辑',
    created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    FOREIGN KEY (source_node) REFERENCES lineage_nodes(node_id),
    FOREIGN KEY (target_node) REFERENCES lineage_nodes(node_id)
) COMMENT '血缘图边表';
```

**🔹 血缘查询功能**

```sql
-- 查询某表的上游依赖
WITH RECURSIVE upstream AS (
    SELECT source_node, target_node, 1 as level
    FROM lineage_edges 
    WHERE target_node = 'db1.user_orders'
    
    UNION ALL
    
    SELECT e.source_node, e.target_node, u.level + 1
    FROM lineage_edges e
    JOIN upstream u ON e.target_node = u.source_node
    WHERE u.level < 10  -- 防止循环依赖
)
SELECT DISTINCT source_node as upstream_table, level
FROM upstream
ORDER BY level;

-- 查询某表的下游影响
WITH RECURSIVE downstream AS (
    SELECT source_node, target_node, 1 as level
    FROM lineage_edges
    WHERE source_node = 'db1.user_profile'
    
    UNION ALL
    
    SELECT e.source_node, e.target_node, d.level + 1  
    FROM lineage_edges e
    JOIN downstream d ON e.source_node = d.target_node
    WHERE d.level < 10
)
SELECT DISTINCT target_node as downstream_table, level
FROM downstream  
ORDER BY level;
```

---

## 3. 🎯 数据质量管理体系


### 3.1 数据质量维度


**🔸 质量评估六大维度**

```
┌─────────────── 数据质量维度 ───────────────┐
│                                            │
│ 🔴 完整性(Completeness)   🟡 准确性(Accuracy) │
│     ↓                        ↓            │
│   缺失值检查               数据正确性检查     │
│                                            │
│ 🟢 一致性(Consistency)    🔵 及时性(Timeliness)│
│     ↓                        ↓            │
│   格式统一检查             数据时效性检查     │
│                                            │
│ 🟣 有效性(Validity)       🟠 唯一性(Uniqueness)│
│     ↓                        ↓            │
│   业务规则检查             重复数据检查       │
│                                            │
└────────────────────────────────────────────┘
```

### 3.2 质量规则配置


**🔸 质量规则定义表**

```sql
CREATE TABLE data_quality_rules (
    rule_id BIGINT AUTO_INCREMENT PRIMARY KEY,
    rule_name VARCHAR(128) NOT NULL COMMENT '规则名称',
    table_name VARCHAR(128) NOT NULL COMMENT '检查表名',
    column_name VARCHAR(64) COMMENT '检查字段',
    rule_type ENUM('completeness','accuracy','consistency','validity','uniqueness','timeliness') NOT NULL,
    rule_expression TEXT NOT NULL COMMENT '规则表达式',
    threshold_value DECIMAL(10,4) DEFAULT 0.95 COMMENT '阈值',
    severity ENUM('critical','major','minor') DEFAULT 'major',
    is_active TINYINT(1) DEFAULT 1,
    created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_table (table_name),
    INDEX idx_type (rule_type)
) COMMENT '数据质量规则配置表';

-- 质量检查结果表
CREATE TABLE data_quality_results (
    result_id BIGINT AUTO_INCREMENT PRIMARY KEY,
    rule_id BIGINT NOT NULL,
    check_time TIMESTAMP NOT NULL,
    total_records BIGINT NOT NULL COMMENT '总记录数',
    passed_records BIGINT NOT NULL COMMENT '通过记录数',
    failed_records BIGINT NOT NULL COMMENT '失败记录数', 
    quality_score DECIMAL(6,4) NOT NULL COMMENT '质量得分',
    status ENUM('pass','fail','warning') NOT NULL,
    error_details TEXT COMMENT '错误详情',
    FOREIGN KEY (rule_id) REFERENCES data_quality_rules(rule_id)
) COMMENT '质量检查结果表';
```

**🔹 质量检查实现**

```sql
-- 完整性检查示例
DELIMITER $$
CREATE PROCEDURE check_completeness(
    IN p_table_name VARCHAR(128),
    IN p_column_name VARCHAR(64),
    IN p_rule_id BIGINT
)
BEGIN
    DECLARE v_total_count BIGINT DEFAULT 0;
    DECLARE v_null_count BIGINT DEFAULT 0;
    DECLARE v_completeness_score DECIMAL(6,4);
    DECLARE v_sql TEXT;
    
    -- 动态构建SQL
    SET v_sql = CONCAT('SELECT COUNT(*) FROM ', p_table_name);
    SET @sql = v_sql;
    PREPARE stmt FROM @sql;
    EXECUTE stmt;
    DEALLOCATE PREPARE stmt;
    
    -- 计算空值数量
    SET v_sql = CONCAT('SELECT COUNT(*) FROM ', p_table_name, 
                      ' WHERE ', p_column_name, ' IS NULL');
    SET @sql = v_sql;
    PREPARE stmt FROM @sql;
    EXECUTE stmt;
    DEALLOCATE PREPARE stmt;
    
    -- 计算完整性得分
    SET v_completeness_score = (v_total_count - v_null_count) / v_total_count;
    
    -- 插入检查结果
    INSERT INTO data_quality_results 
    (rule_id, check_time, total_records, passed_records, failed_records, quality_score, status)
    VALUES 
    (p_rule_id, NOW(), v_total_count, v_total_count - v_null_count, v_null_count, 
     v_completeness_score, IF(v_completeness_score >= 0.95, 'pass', 'fail'));
     
END$$
DELIMITER ;
```

### 3.3 数据剖析功能


**🔸 数据剖析统计**

```sql
-- 数据剖析结果表
CREATE TABLE data_profiling_results (
    profile_id BIGINT AUTO_INCREMENT PRIMARY KEY,
    table_name VARCHAR(128) NOT NULL,
    column_name VARCHAR(64) NOT NULL,
    data_type VARCHAR(32) NOT NULL,
    total_rows BIGINT NOT NULL COMMENT '总行数',
    null_count BIGINT DEFAULT 0 COMMENT '空值数量',
    distinct_count BIGINT DEFAULT 0 COMMENT '去重数量',
    min_value VARCHAR(255) COMMENT '最小值',
    max_value VARCHAR(255) COMMENT '最大值', 
    avg_length DECIMAL(10,2) COMMENT '平均长度',
    profiling_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    INDEX idx_table_column (table_name, column_name)
) COMMENT '数据剖析结果表';

-- 自动数据剖析存储过程
DELIMITER $$
CREATE PROCEDURE profile_table_data(IN p_table_name VARCHAR(128))
BEGIN
    DECLARE done INT DEFAULT FALSE;
    DECLARE v_column_name VARCHAR(64);
    DECLARE v_data_type VARCHAR(32);
    
    -- 声明游标获取表字段信息
    DECLARE column_cursor CURSOR FOR
        SELECT COLUMN_NAME, DATA_TYPE
        FROM INFORMATION_SCHEMA.COLUMNS
        WHERE TABLE_SCHEMA = DATABASE()
        AND TABLE_NAME = p_table_name;
    
    DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE;
    
    OPEN column_cursor;
    
    read_loop: LOOP
        FETCH column_cursor INTO v_column_name, v_data_type;
        IF done THEN
            LEAVE read_loop;
        END IF;
        
        -- 调用具体字段剖析
        CALL profile_column_data(p_table_name, v_column_name, v_data_type);
        
    END LOOP;
    
    CLOSE column_cursor;
END$$
DELIMITER ;
```

---

## 4. 🤖 自动化血缘发现


### 4.1 SQL解析引擎


**🔸 智能SQL解析**

```python
class AutoLineageDiscovery:
    def __init__(self, db_config):
        self.db_config = db_config
        self.parser = SQLParser()
    
    def discover_lineage(self, sql_logs):
        """自动发现数据血缘关系"""
        lineage_map = {}
        
        for sql_log in sql_logs:
            try:
                # 解析SQL语句
                parsed_result = self.parser.parse(sql_log['sql_text'])
                
                # 提取表依赖关系
                dependencies = self.extract_dependencies(parsed_result)
                
                # 构建血缘关系
                for dep in dependencies:
                    source_table = dep['source']
                    target_table = dep['target']
                    
                    if target_table not in lineage_map:
                        lineage_map[target_table] = []
                    
                    lineage_map[target_table].append({
                        'source': source_table,
                        'transformation': dep['transformation'],
                        'sql': sql_log['sql_text']
                    })
                    
            except Exception as e:
                self.log_error(f"解析SQL失败: {sql_log['sql_text']}", e)
        
        return lineage_map
    
    def extract_dependencies(self, parsed_sql):
        """提取SQL中的表依赖关系"""
        dependencies = []
        
        # 简化的依赖提取逻辑
        if parsed_sql.get('operation') == 'INSERT':
            target = parsed_sql['target_table']
            sources = parsed_sql.get('source_tables', [])
            
            for source in sources:
                dependencies.append({
                    'source': source,
                    'target': target,
                    'transformation': self.extract_transformation_logic(parsed_sql)
                })
        
        return dependencies
```

### 4.2 血缘关系自动更新


**🔸 实时血缘更新**

```sql
-- 血缘变更日志表
CREATE TABLE lineage_change_log (
    change_id BIGINT AUTO_INCREMENT PRIMARY KEY,
    change_type ENUM('create','update','delete') NOT NULL,
    source_table VARCHAR(128),
    target_table VARCHAR(128), 
    change_description TEXT,
    sql_statement TEXT,
    change_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    processed TINYINT(1) DEFAULT 0 COMMENT '是否已处理'
) COMMENT '血缘关系变更日志';

-- 触发器：自动记录表结构变更
DELIMITER $$
CREATE TRIGGER tr_ddl_lineage_tracking
AFTER CREATE ON DATABASE
FOR EACH ROW
BEGIN
    INSERT INTO lineage_change_log 
    (change_type, target_table, change_description, sql_statement)
    VALUES 
    ('create', NEW.table_name, '新建表', '触发器记录');
END$$

CREATE TRIGGER tr_ddl_lineage_update  
AFTER ALTER ON DATABASE
FOR EACH ROW
BEGIN
    INSERT INTO lineage_change_log
    (change_type, target_table, change_description, sql_statement)
    VALUES
    ('update', NEW.table_name, '表结构变更', '触发器记录');
END$$
DELIMITER ;
```

### 4.3 影响分析功能


**🔸 变更影响评估**

```sql
-- 影响分析视图
CREATE VIEW v_impact_analysis AS
SELECT 
    n1.node_id as source_table,
    n2.node_id as affected_table,
    e.edge_type,
    COUNT(*) OVER (PARTITION BY n1.node_id) as total_affected_count,
    n2.node_level - n1.node_level as impact_depth
FROM lineage_nodes n1
JOIN lineage_edges e ON n1.node_id = e.source_node  
JOIN lineage_nodes n2 ON e.target_node = n2.node_id;

-- 影响分析存储过程
DELIMITER $$
CREATE PROCEDURE analyze_change_impact(
    IN p_table_name VARCHAR(128),
    IN p_change_type VARCHAR(32)
)
BEGIN
    DECLARE v_impact_count INT DEFAULT 0;
    
    -- 统计受影响的下游表数量
    SELECT COUNT(DISTINCT target_node) INTO v_impact_count
    FROM lineage_edges 
    WHERE source_node = p_table_name;
    
    -- 生成影响分析报告
    SELECT 
        '影响评估报告' as report_type,
        p_table_name as changed_table,
        p_change_type as change_type,
        v_impact_count as affected_tables_count,
        NOW() as analysis_time;
        
    -- 详细影响列表
    SELECT 
        e.target_node as affected_table,
        n.node_type as table_type,
        e.edge_type as dependency_type,
        '需要验证数据一致性' as recommendation
    FROM lineage_edges e
    JOIN lineage_nodes n ON e.target_node = n.node_id
    WHERE e.source_node = p_table_name
    ORDER BY n.node_level;
    
END$$
DELIMITER ;
```

---

## 5. 📈 数据质量监控与告警


### 5.1 质量监控仪表板


**🔸 质量指标统计**

```sql
-- 质量监控汇总表
CREATE TABLE quality_monitoring_summary (
    summary_id BIGINT AUTO_INCREMENT PRIMARY KEY,
    table_name VARCHAR(128) NOT NULL,
    monitor_date DATE NOT NULL,
    total_records BIGINT NOT NULL,
    quality_score DECIMAL(6,4) NOT NULL COMMENT '综合质量得分',
    completeness_score DECIMAL(6,4) DEFAULT NULL,
    accuracy_score DECIMAL(6,4) DEFAULT NULL,
    consistency_score DECIMAL(6,4) DEFAULT NULL,
    timeliness_score DECIMAL(6,4) DEFAULT NULL,
    critical_issues INT DEFAULT 0,
    major_issues INT DEFAULT 0, 
    minor_issues INT DEFAULT 0,
    UNIQUE KEY uk_table_date (table_name, monitor_date)
) COMMENT '质量监控汇总表';

-- 质量趋势分析视图
CREATE VIEW v_quality_trend AS
SELECT 
    table_name,
    monitor_date,
    quality_score,
    quality_score - LAG(quality_score) OVER (
        PARTITION BY table_name 
        ORDER BY monitor_date
    ) as score_change,
    CASE 
        WHEN quality_score >= 0.95 THEN '优秀'
        WHEN quality_score >= 0.85 THEN '良好'  
        WHEN quality_score >= 0.70 THEN '一般'
        ELSE '较差'
    END as quality_level
FROM quality_monitoring_summary
ORDER BY table_name, monitor_date;
```

**🔹 质量评分算法**

```sql
DELIMITER $$
CREATE FUNCTION calculate_dq_score(
    p_completeness DECIMAL(6,4),
    p_accuracy DECIMAL(6,4), 
    p_consistency DECIMAL(6,4),
    p_timeliness DECIMAL(6,4)
) RETURNS DECIMAL(6,4)
READS SQL DATA
DETERMINISTIC
BEGIN
    DECLARE v_weighted_score DECIMAL(6,4);
    
    -- 加权计算综合质量得分
    -- 完整性30% + 准确性40% + 一致性20% + 及时性10%
    SET v_weighted_score = 
        COALESCE(p_completeness, 1.0) * 0.30 +
        COALESCE(p_accuracy, 1.0) * 0.40 +  
        COALESCE(p_consistency, 1.0) * 0.20 +
        COALESCE(p_timeliness, 1.0) * 0.10;
    
    RETURN LEAST(v_weighted_score, 1.0000);
END$$
DELIMITER ;
```

### 5.2 异常检测算法


**🔸 统计异常检测**

```sql
-- 异常检测规则表
CREATE TABLE anomaly_detection_rules (
    rule_id BIGINT AUTO_INCREMENT PRIMARY KEY,
    rule_name VARCHAR(128) NOT NULL,
    table_name VARCHAR(128) NOT NULL,
    column_name VARCHAR(64),
    detection_method ENUM('statistical','threshold','pattern','ml') NOT NULL,
    rule_config JSON COMMENT '检测规则配置',
    sensitivity ENUM('high','medium','low') DEFAULT 'medium',
    is_active TINYINT(1) DEFAULT 1,
    created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
) COMMENT '异常检测规则表';

-- 异常检测结果表  
CREATE TABLE anomaly_detection_results (
    anomaly_id BIGINT AUTO_INCREMENT PRIMARY KEY,
    rule_id BIGINT NOT NULL,
    detection_time TIMESTAMP NOT NULL,
    anomaly_type VARCHAR(64) NOT NULL COMMENT '异常类型',
    anomaly_value TEXT COMMENT '异常值',
    confidence_score DECIMAL(4,2) COMMENT '置信度',
    severity ENUM('critical','major','minor') NOT NULL,
    status ENUM('new','investigating','resolved','ignored') DEFAULT 'new',
    description TEXT COMMENT '异常描述',
    FOREIGN KEY (rule_id) REFERENCES anomaly_detection_rules(rule_id)
) COMMENT '异常检测结果表';

-- 统计异常检测存储过程
DELIMITER $$
CREATE PROCEDURE detect_statistical_anomaly(
    IN p_table_name VARCHAR(128),
    IN p_column_name VARCHAR(64),
    IN p_rule_id BIGINT
)
BEGIN
    DECLARE v_avg_value DECIMAL(15,4);
    DECLARE v_std_dev DECIMAL(15,4);
    DECLARE v_threshold_upper DECIMAL(15,4);
    DECLARE v_threshold_lower DECIMAL(15,4);
    DECLARE v_anomaly_count INT DEFAULT 0;
    
    -- 计算统计指标
    SET @sql = CONCAT('SELECT AVG(', p_column_name, '), STDDEV(', p_column_name, ') FROM ', p_table_name);
    PREPARE stmt FROM @sql;
    EXECUTE stmt;
    DEALLOCATE PREPARE stmt;
    
    -- 3σ原则确定异常阈值
    SET v_threshold_upper = v_avg_value + 3 * v_std_dev;
    SET v_threshold_lower = v_avg_value - 3 * v_std_dev;
    
    -- 检测异常数据
    SET @sql = CONCAT('SELECT COUNT(*) FROM ', p_table_name, 
                      ' WHERE ', p_column_name, ' > ', v_threshold_upper,
                      ' OR ', p_column_name, ' < ', v_threshold_lower);
    PREPARE stmt FROM @sql;
    EXECUTE stmt;
    DEALLOCATE PREPARE stmt;
    
    -- 记录异常检测结果
    IF v_anomaly_count > 0 THEN
        INSERT INTO anomaly_detection_results
        (rule_id, detection_time, anomaly_type, anomaly_value, confidence_score, severity, description)
        VALUES
        (p_rule_id, NOW(), '统计异常', v_anomaly_count, 0.85, 'major', 
         CONCAT('发现', v_anomaly_count, '个统计异常值'));
    END IF;
    
END$$
DELIMITER ;
```

### 5.3 智能告警系统


**🔸 告警规则配置**

```sql
-- 告警规则表
CREATE TABLE alert_rules (
    rule_id BIGINT AUTO_INCREMENT PRIMARY KEY,
    rule_name VARCHAR(128) NOT NULL,
    trigger_condition TEXT NOT NULL COMMENT '触发条件',
    alert_level ENUM('critical','warning','info') NOT NULL,
    notification_channels JSON COMMENT '通知渠道配置',
    cooldown_minutes INT DEFAULT 60 COMMENT '告警冷却时间',
    is_active TINYINT(1) DEFAULT 1,
    created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
) COMMENT '告警规则表';

-- 告警历史表
CREATE TABLE alert_history (
    alert_id BIGINT AUTO_INCREMENT PRIMARY KEY,
    rule_id BIGINT NOT NULL,
    alert_time TIMESTAMP NOT NULL,
    alert_level ENUM('critical','warning','info') NOT NULL,
    alert_title VARCHAR(255) NOT NULL,
    alert_content TEXT,
    trigger_data JSON COMMENT '触发数据',
    status ENUM('firing','resolved','suppressed') DEFAULT 'firing',
    resolved_time TIMESTAMP NULL,
    FOREIGN KEY (rule_id) REFERENCES alert_rules(rule_id)
) COMMENT '告警历史表';
```

---

## 6. 🧠 智能化数据治理


### 6.1 自动化数据修复


**🔸 数据修复建议引擎**

```sql
-- 修复建议表
CREATE TABLE data_repair_suggestions (
    suggestion_id BIGINT AUTO_INCREMENT PRIMARY KEY,
    table_name VARCHAR(128) NOT NULL,
    column_name VARCHAR(64),
    issue_type VARCHAR(64) NOT NULL COMMENT '问题类型',
    issue_description TEXT COMMENT '问题描述',
    repair_method ENUM('auto','semi_auto','manual') NOT NULL,
    repair_sql TEXT COMMENT '修复SQL',
    confidence_score DECIMAL(4,2) COMMENT '修复可信度',
    estimated_affected_rows BIGINT COMMENT '预计影响行数',
    status ENUM('pending','approved','applied','rejected') DEFAULT 'pending',
    created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP
) COMMENT '数据修复建议表';

-- 自动修复规则引擎
DELIMITER $$
CREATE PROCEDURE generate_repair_suggestions(
    IN p_table_name VARCHAR(128)
)
BEGIN
    DECLARE done INT DEFAULT FALSE;
    DECLARE v_column_name VARCHAR(64);
    DECLARE v_null_count BIGINT;
    DECLARE v_total_count BIGINT;
    DECLARE v_null_rate DECIMAL(6,4);
    
    -- 获取表的所有字段
    DECLARE column_cursor CURSOR FOR
        SELECT COLUMN_NAME
        FROM INFORMATION_SCHEMA.COLUMNS
        WHERE TABLE_SCHEMA = DATABASE()
        AND TABLE_NAME = p_table_name
        AND IS_NULLABLE = 'YES';
    
    DECLARE CONTINUE HANDLER FOR NOT FOUND SET done = TRUE;
    
    -- 获取总记录数
    SET @sql = CONCAT('SELECT COUNT(*) INTO @total_count FROM ', p_table_name);
    PREPARE stmt FROM @sql;
    EXECUTE stmt;
    DEALLOCATE PREPARE stmt;
    SET v_total_count = @total_count;
    
    OPEN column_cursor;
    
    read_loop: LOOP
        FETCH column_cursor INTO v_column_name;
        IF done THEN
            LEAVE read_loop;
        END IF;
        
        -- 计算空值率
        SET @sql = CONCAT('SELECT COUNT(*) INTO @null_count FROM ', p_table_name, 
                         ' WHERE ', v_column_name, ' IS NULL');
        PREPARE stmt FROM @sql;
        EXECUTE stmt;
        DEALLOCATE PREPARE stmt;
        SET v_null_count = @null_count;
        SET v_null_rate = v_null_count / v_total_count;
        
        -- 生成修复建议
        IF v_null_rate > 0.05 THEN  -- 空值率超过5%
            INSERT INTO data_repair_suggestions
            (table_name, column_name, issue_type, issue_description, repair_method, repair_sql, confidence_score, estimated_affected_rows)
            VALUES
            (p_table_name, v_column_name, '数据缺失', 
             CONCAT('字段空值率为', ROUND(v_null_rate * 100, 2), '%'),
             'semi_auto',
             CONCAT('-- 建议使用默认值或均值填充\n',
                   'UPDATE ', p_table_name, ' SET ', v_column_name, ' = [默认值] WHERE ', v_column_name, ' IS NULL;'),
             0.75, v_null_count);
        END IF;
        
    END LOOP;
    
    CLOSE column_cursor;
END$$
DELIMITER ;
```

### 6.2 质量SLA管理


**🔸 SLA指标定义**

```sql
-- SLA配置表
CREATE TABLE data_quality_sla (
    sla_id BIGINT AUTO_INCREMENT PRIMARY KEY,
    table_name VARCHAR(128) NOT NULL,
    metric_name VARCHAR(64) NOT NULL COMMENT 'SLA指标名称',
    target_value DECIMAL(6,4) NOT NULL COMMENT '目标值',
    warning_threshold DECIMAL(6,4) COMMENT '预警阈值',
    critical_threshold DECIMAL(6,4) COMMENT '严重阈值',
    measurement_window ENUM('daily','weekly','monthly') DEFAULT 'daily',
    is_active TINYINT(1) DEFAULT 1,
    owner VARCHAR(64) COMMENT '责任人',
    created_time TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    UNIQUE KEY uk_table_metric (table_name, metric_name)
) COMMENT 'SLA配置表';

-- SLA监控结果表
CREATE TABLE sla_monitoring_results (
    result_id BIGINT AUTO_INCREMENT PRIMARY KEY,
    sla_id BIGINT NOT NULL,
    measurement_date DATE NOT NULL,
    actual_value DECIMAL(6,4) NOT NULL COMMENT '实际值',
    target_value DECIMAL(6,4) NOT NULL COMMENT '目标值',
    achievement_rate DECIMAL(6,4) NOT NULL COMMENT '达成率',
    sla_status ENUM('met','warning','breach') NOT NULL,
    breach_duration_hours INT DEFAULT 0 COMMENT '违约持续时间',
    FOREIGN KEY (sla_id) REFERENCES data_quality_sla(sla_id),
    UNIQUE KEY uk_sla_date (sla_id, measurement_date)  
) COMMENT 'SLA监控结果表';
```

### 6.3 智能化治理效果评估


**🔸 治理效果评估**

```sql
-- 治理效果评估视图
CREATE VIEW v_governance_effectiveness AS
SELECT 
    t.table_name,
    -- 质量改善情况
    AVG(CASE WHEN qms.monitor_date >= DATE_SUB(CURDATE(), INTERVAL 30 DAY) 
             THEN qms.quality_score END) as recent_avg_score,
    AVG(CASE WHEN qms.monitor_date < DATE_SUB(CURDATE(), INTERVAL 30 DAY)
             AND qms.monitor_date >= DATE_SUB(CURDATE(), INTERVAL 60 DAY)
             THEN qms.quality_score END) as previous_avg_score,
    -- SLA达成情况  
    COUNT(CASE WHEN smr.sla_status = 'met' THEN 1 END) * 100.0 / COUNT(smr.sla_status) as sla_achievement_rate,
    -- 异常趋势
    COUNT(CASE WHEN adr.detection_time >= DATE_SUB(CURDATE(), INTERVAL 30 DAY)
               THEN 1 END) as recent_anomaly_count,
    COUNT(CASE WHEN adr.detection_time < DATE_SUB(CURDATE(), INTERVAL 30 DAY)
               AND adr.detection_time >= DATE_SUB(CURDATE(), INTERVAL 60 DAY)
               THEN 1 END) as previous_anomaly_count
FROM (SELECT DISTINCT table_name FROM quality_monitoring_summary) t
LEFT JOIN quality_monitoring_summary qms ON t.table_name = qms.table_name
LEFT JOIN data_quality_sla dqs ON t.table_name = dqs.table_name  
LEFT JOIN sla_monitoring_results smr ON dqs.sla_id = smr.sla_id
LEFT JOIN anomaly_detection_rules adr_rules ON t.table_name = adr_rules.table_name
LEFT JOIN anomaly_detection_results adr ON adr_rules.rule_id = adr.rule_id
GROUP BY t.table_name;

-- 治理ROI计算
DELIMITER $$
CREATE FUNCTION calculate_governance_roi(
    p_table_name VARCHAR(128),
    p_period_days INT
) RETURNS DECIMAL(10,2)
READS SQL DATA
DETERMINISTIC  
BEGIN
    DECLARE v_quality_improvement DECIMAL(6,4);
    DECLARE v_anomaly_reduction DECIMAL(6,4); 
    DECLARE v_governance_cost DECIMAL(10,2);
    DECLARE v_business_value DECIMAL(10,2);
    DECLARE v_roi DECIMAL(10,2);
    
    -- 计算质量提升幅度
    SELECT 
        recent_avg_score - previous_avg_score,
        (previous_anomaly_count - recent_anomaly_count) / previous_anomaly_count
    INTO v_quality_improvement, v_anomaly_reduction
    FROM v_governance_effectiveness
    WHERE table_name = p_table_name;
    
    -- 简化的ROI计算逻辑
    SET v_business_value = (COALESCE(v_quality_improvement, 0) * 10000) + 
                          (COALESCE(v_anomaly_reduction, 0) * 5000);
    SET v_governance_cost = 1000; -- 假设的治理成本
    
    SET v_roi = (v_business_value - v_governance_cost) / v_governance_cost * 100;
    
    RETURN COALESCE(v_roi, 0);
END$$
DELIMITER ;
```

---

## 7. 📋 核心要点总结


### 7.1 数据血缘核心价值


> 🎯 **一句话精华**  
> 数据血缘就像GPS导航，帮你快速找到数据从哪来、到哪去，出问题时马上定位根源

**🔴 必须掌握**：
- **血缘追踪**：记录数据流转的完整路径
- **影响分析**：变更前评估影响范围，避免意外破坏
- **根因定位**：数据异常时快速找到问题源头

**🟡 关键理解**：
```
血缘关系三层次：
业务层 → 看懂数据业务逻辑
技术层 → 了解表与表关系  
字段层 → 掌握具体计算逻辑
```

### 7.2 数据质量管理要点


**🔸 六大质量维度**：
- **完整性**：数据不能有缺失
- **准确性**：数据必须正确无误
- **一致性**：格式标准要统一
- **及时性**：数据要保持时效
- **有效性**：符合业务规则
- **唯一性**：避免重复数据

**🔸 质量管理闭环**：
```
规则定义 → 自动检测 → 异常告警 → 问题修复 → 效果评估
    ↑                                              ↓
    └──────────── 持续优化 ←──────────────────────────┘
```

### 7.3 实施关键步骤


**✅ 实施检查清单**：
- [ ] 建立血缘追踪机制
- [ ] 配置核心质量规则  
- [ ] 部署异常监控系统
- [ ] 设置告警通知机制
- [ ] 建立数据修复流程
- [ ] 制定SLA考核标准

### 7.4 最佳实践建议


**🚀 实用建议**：

1. **从核心业务表开始**：不要一开始就覆盖所有表
2. **质量规则要实用**：设置合理阈值，避免告警过多
3. **自动化是关键**：尽量减少人工干预
4. **重视业务理解**：技术手段要服务业务需求

**💡 记忆锚点**：
```
血缘追踪像族谱，数据家族关系清
质量管理设门槛，异常数据不放行
自动发现很重要，智能治理提效率
```

**🔧 核心技术栈**：
- SQL日志解析 + 血缘构建
- 质量规则引擎 + 异常检测  
- 监控告警系统 + 自动修复
- SLA管理 + 效果评估

通过数据血缘和质量管理，让数据变得可信、可追溯、可控制，为业务决策提供坚实的数据基础！