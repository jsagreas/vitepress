---
title: 5ã€ç´¢å¼•é€‰æ‹©æ€§åˆ†æ
---
## ğŸ“š ç›®å½•

1. [ç´¢å¼•é€‰æ‹©æ€§åŸºç¡€æ¦‚å¿µ](#1-ç´¢å¼•é€‰æ‹©æ€§åŸºç¡€æ¦‚å¿µ)
2. [åŸºæ•°è¯„ä¼°æ–¹æ³•è¯¦è§£](#2-åŸºæ•°è¯„ä¼°æ–¹æ³•è¯¦è§£)
3. [åŸºæ•°ç»Ÿè®¡ç®—æ³•åŸç†](#3-åŸºæ•°ç»Ÿè®¡ç®—æ³•åŸç†)
4. [é€‰æ‹©æ€§è®¡ç®—ä¸ä¼˜åŒ–](#4-é€‰æ‹©æ€§è®¡ç®—ä¸ä¼˜åŒ–)
5. [æ•°æ®åˆ†å¸ƒå€¾æ–œå¤„ç†](#5-æ•°æ®åˆ†å¸ƒå€¾æ–œå¤„ç†)
6. [åŠ¨æ€é€‰æ‹©æ€§ç›‘æ§](#6-åŠ¨æ€é€‰æ‹©æ€§ç›‘æ§)
7. [å¤šç»´é€‰æ‹©æ€§åˆ†æ](#7-å¤šç»´é€‰æ‹©æ€§åˆ†æ)
8. [ä½é€‰æ‹©æ€§ç´¢å¼•æ›¿ä»£ç­–ç•¥](#8-ä½é€‰æ‹©æ€§ç´¢å¼•æ›¿ä»£ç­–ç•¥)
9. [æ ¸å¿ƒè¦ç‚¹æ€»ç»“](#9-æ ¸å¿ƒè¦ç‚¹æ€»ç»“)

---

## 1. ğŸ¯ ç´¢å¼•é€‰æ‹©æ€§åŸºç¡€æ¦‚å¿µ


### 1.1 é€‰æ‹©æ€§çš„æœ¬è´¨å«ä¹‰


**ğŸ”¸ é€‰æ‹©æ€§å®šä¹‰**
é€‰æ‹©æ€§ï¼ˆSelectivityï¼‰æ˜¯è¡¡é‡ç´¢å¼•è¿‡æ»¤èƒ½åŠ›çš„å…³é”®æŒ‡æ ‡ï¼Œè¡¨ç¤ºç´¢å¼•èƒ½å¤Ÿç­›é€‰å‡ºå¤šå°‘æ¯”ä¾‹çš„æ•°æ®ã€‚

```
é€‰æ‹©æ€§è®¡ç®—å…¬å¼ï¼š
é€‰æ‹©æ€§ = ä¸é‡å¤å€¼æ•°é‡ / æ€»è®°å½•æ•°

ç¤ºä¾‹ç†è§£ï¼š
è¡¨ä¸­æœ‰100ä¸‡æ¡è®°å½•
- æ€§åˆ«å­—æ®µï¼šåªæœ‰2ä¸ªä¸é‡å¤å€¼ï¼ˆç”·/å¥³ï¼‰
  é€‰æ‹©æ€§ = 2 / 1,000,000 = 0.000002 (æä½é€‰æ‹©æ€§)
  
- èº«ä»½è¯å·ï¼šæœ‰99.9ä¸‡ä¸ªä¸é‡å¤å€¼  
  é€‰æ‹©æ€§ = 999,000 / 1,000,000 = 0.999 (æé«˜é€‰æ‹©æ€§)
```

**ğŸ’¡ é€‰æ‹©æ€§ä¸æŸ¥è¯¢æ•ˆç‡çš„å…³ç³»**
```
é€‰æ‹©æ€§è¶Šé«˜ = ç´¢å¼•è¿‡æ»¤æ•ˆæœè¶Šå¥½ = æŸ¥è¯¢æ•ˆç‡è¶Šé«˜

é«˜é€‰æ‹©æ€§ç´¢å¼•ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  100ä¸‡è®°å½•  â”‚ â”€â”€ç´¢å¼•è¿‡æ»¤â”€â”€â–¶ 1æ¡è®°å½• (æ•ˆæœå¥½)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

ä½é€‰æ‹©æ€§ç´¢å¼•ï¼š  
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  100ä¸‡è®°å½•  â”‚ â”€â”€ç´¢å¼•è¿‡æ»¤â”€â”€â–¶ 50ä¸‡æ¡è®°å½• (æ•ˆæœå·®)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 é€‰æ‹©æ€§åˆ†ç±»æ ‡å‡†


**ğŸ“Š é€‰æ‹©æ€§ç­‰çº§åˆ’åˆ†**

| é€‰æ‹©æ€§èŒƒå›´ | **ç­‰çº§** | **è¿‡æ»¤æ•ˆæœ** | **æ˜¯å¦å»ºè®®å»ºç´¢å¼•** | **å…¸å‹åœºæ™¯** |
|-----------|----------|-------------|------------------|-------------|
| `0.9 - 1.0` | `ğŸŸ¢ æé«˜` | `ä¼˜ç§€` | `å¼ºçƒˆæ¨è` | ä¸»é”®ã€å”¯ä¸€é”® |
| `0.7 - 0.9` | `ğŸŸ¡ é«˜` | `è‰¯å¥½` | `æ¨è` | ç”¨æˆ·IDã€è®¢å•å· |
| `0.3 - 0.7` | `ğŸŸ  ä¸­ç­‰` | `ä¸€èˆ¬` | `éœ€è¦è¯„ä¼°` | æ—¥æœŸã€çŠ¶æ€ç»„åˆ |
| `0.1 - 0.3` | `ğŸ”´ ä½` | `è¾ƒå·®` | `è°¨æ…è€ƒè™‘` | ç±»å‹ã€çŠ¶æ€å­—æ®µ |
| `< 0.1` | `ğŸš« æä½` | `å¾ˆå·®` | `ä¸æ¨è` | æ€§åˆ«ã€å¸ƒå°”å­—æ®µ |

### 1.3 é€‰æ‹©æ€§è¯„ä¼°çš„é‡è¦æ„ä¹‰


**ğŸ¯ ä¸ºä»€ä¹ˆè¦åˆ†æé€‰æ‹©æ€§**
```
ç´¢å¼•åˆ›å»ºå†³ç­–ï¼š
â”œâ”€ é«˜é€‰æ‹©æ€§å­—æ®µï¼šä¼˜å…ˆåˆ›å»ºç´¢å¼•
â”œâ”€ ä½é€‰æ‹©æ€§å­—æ®µï¼šé¿å…æ— æ•ˆç´¢å¼•
â””â”€ ç»„åˆç´¢å¼•ï¼šé€šè¿‡é€‰æ‹©æ€§ä¼˜åŒ–å­—æ®µé¡ºåº

æŸ¥è¯¢ä¼˜åŒ–å™¨ï¼š
â”œâ”€ åŸºäºé€‰æ‹©æ€§é€‰æ‹©ç´¢å¼•
â”œâ”€ ä¼°ç®—æŸ¥è¯¢æˆæœ¬
â””â”€ å†³å®šæ‰§è¡Œè®¡åˆ’

å­˜å‚¨ç©ºé—´ä¼˜åŒ–ï¼š
â”œâ”€ é¿å…åˆ›å»ºä½æ•ˆç´¢å¼•
â”œâ”€ å‡å°‘å­˜å‚¨å¼€é”€
â””â”€ æå‡æ•´ä½“æ€§èƒ½
```

---

## 2. ğŸ“ˆ åŸºæ•°è¯„ä¼°æ–¹æ³•è¯¦è§£


### 2.1 ç²¾ç¡®åŸºæ•°è®¡ç®—æ–¹æ³•


**ğŸ” COUNT DISTINCTæ–¹æ³•**
```sql
-- åŸºç¡€é€‰æ‹©æ€§è®¡ç®—
SELECT 
    'user_id' as column_name,
    COUNT(DISTINCT user_id) as cardinality,
    COUNT(*) as total_rows,
    ROUND(COUNT(DISTINCT user_id) / COUNT(*), 4) as selectivity
FROM users
UNION ALL
SELECT 
    'gender',
    COUNT(DISTINCT gender),
    COUNT(*),
    ROUND(COUNT(DISTINCT gender) / COUNT(*), 4)
FROM users;
```

**âš¡ é‡‡æ ·ä¼°ç®—æ–¹æ³•**
```sql
-- å¤§è¡¨é‡‡æ ·ä¼°ç®—åŸºæ•°
SELECT 
    COUNT(DISTINCT user_id) * 100 as estimated_cardinality,
    COUNT(*) * 100 as estimated_total,
    COUNT(DISTINCT user_id) / COUNT(*) as sample_selectivity
FROM (
    SELECT user_id 
    FROM large_table 
    WHERE RAND() < 0.01  -- 1%éšæœºé‡‡æ ·
) sample_data;
```

### 2.2 MySQLå†…ç½®ç»Ÿè®¡ä¿¡æ¯


**ğŸ“Š ä½¿ç”¨INFORMATION_SCHEMAè·å–ç»Ÿè®¡**
```sql
-- æŸ¥çœ‹ç´¢å¼•åŸºæ•°ä¿¡æ¯
SELECT 
    INDEX_NAME,
    COLUMN_NAME,
    CARDINALITY,
    CARDINALITY / (
        SELECT TABLE_ROWS 
        FROM INFORMATION_SCHEMA.TABLES 
        WHERE TABLE_SCHEMA = s.TABLE_SCHEMA 
        AND TABLE_NAME = s.TABLE_NAME
    ) as estimated_selectivity
FROM INFORMATION_SCHEMA.STATISTICS s
WHERE TABLE_SCHEMA = 'your_database'
AND TABLE_NAME = 'your_table';

-- æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
ANALYZE TABLE table_name;
```

### 2.3 é‡å¤å€¼åˆ†å¸ƒåˆ†æ


**ğŸ” æ•°æ®åˆ†å¸ƒåˆ†æ**
```sql
-- åˆ†æå­—æ®µå€¼åˆ†å¸ƒ
SELECT 
    status as value,
    COUNT(*) as frequency,
    COUNT(*) * 100.0 / total.total_rows as percentage,
    CASE 
        WHEN COUNT(*) * 100.0 / total.total_rows > 50 THEN 'ä¸¥é‡å€¾æ–œ'
        WHEN COUNT(*) * 100.0 / total.total_rows > 20 THEN 'ä¸­åº¦å€¾æ–œ'
        ELSE 'åˆ†å¸ƒå‡åŒ€'
    END as skew_level
FROM orders
CROSS JOIN (SELECT COUNT(*) as total_rows FROM orders) total
GROUP BY status, total_rows
ORDER BY frequency DESC;
```

---

## 3. âš™ï¸ åŸºæ•°ç»Ÿè®¡ç®—æ³•åŸç†


### 3.1 HyperLogLogåŸºæ•°ä¼°ç®—ç®—æ³•


**ğŸ§® HyperLogLogç®—æ³•æ ¸å¿ƒæ€æƒ³**
HyperLogLogæ˜¯ä¸€ç§æ¦‚ç‡å‹åŸºæ•°ä¼°ç®—ç®—æ³•ï¼Œèƒ½å¤Ÿåœ¨ä½¿ç”¨æå°‘å†…å­˜çš„æƒ…å†µä¸‹ä¼°ç®—å¤§æ•°æ®é›†çš„åŸºæ•°ã€‚

```
ç®—æ³•åŸç†ï¼š
1. å“ˆå¸Œåˆ†æ¡¶ï¼šå°†æ•°æ®é€šè¿‡å“ˆå¸Œå‡½æ•°æ˜ å°„åˆ°ä¸åŒæ¡¶ä¸­
2. å‰å¯¼é›¶è®¡æ•°ï¼šç»Ÿè®¡æ¯ä¸ªæ¡¶ä¸­å“ˆå¸Œå€¼çš„æœ€å¤§å‰å¯¼é›¶ä¸ªæ•°
3. è°ƒå’Œå¹³å‡ï¼šä½¿ç”¨è°ƒå’Œå¹³å‡æ•°ä¼°ç®—æ€»åŸºæ•°

å†…å­˜ä¼˜åŠ¿ï¼š
- ä¼ ç»Ÿæ–¹æ³•ï¼šéœ€è¦å­˜å‚¨æ‰€æœ‰ä¸é‡å¤å€¼
- HyperLogLogï¼šåªéœ€è¦å›ºå®šå¤§å°çš„è®¡æ•°å™¨æ•°ç»„ï¼ˆé€šå¸¸å‡ KBï¼‰
```

**ğŸ’» ç®€åŒ–å®ç°ç¤ºä¾‹**
```python
import hashlib
import math

class SimpleHyperLogLog:
    def __init__(self, precision=12):
        self.m = 2 ** precision  # æ¡¶æ•°é‡
        self.buckets = [0] * self.m
        
    def add(self, value):
        # è®¡ç®—å“ˆå¸Œå€¼å¹¶åˆ†æ¡¶
        hash_val = int(hashlib.md5(str(value).encode()).hexdigest(), 16)
        bucket_id = hash_val & (self.m - 1)
        
        # è®¡ç®—å‰å¯¼é›¶
        remaining = hash_val >> 12
        leading_zeros = bin(remaining).count('0') if remaining > 0 else 32
        
        # æ›´æ–°æ¡¶ä¸­æœ€å¤§å‰å¯¼é›¶æ•°
        self.buckets[bucket_id] = max(self.buckets[bucket_id], leading_zeros)
    
    def estimate(self):
        # åŸºäºè°ƒå’Œå¹³å‡çš„åŸºæ•°ä¼°ç®—
        raw_estimate = 0.7213 * (self.m ** 2) / sum(2 ** (-x) for x in self.buckets)
        return int(raw_estimate)

# ä½¿ç”¨ç¤ºä¾‹
hll = SimpleHyperLogLog()
for i in range(50000):
    hll.add(f"user_{i}")
print(f"ä¼°ç®—åŸºæ•°: {hll.estimate()}")  # çº¦50000
```

### 3.2 é‡‡æ ·ç»Ÿè®¡æ–¹æ³•


**ğŸ“Š æ°´å¡˜é‡‡æ ·ç®—æ³•**
æ°´å¡˜é‡‡æ ·èƒ½å¤Ÿä»æ•°æ®æµä¸­ç­‰æ¦‚ç‡åœ°é€‰å–å›ºå®šæ•°é‡çš„æ ·æœ¬ï¼Œé€‚ç”¨äºå¤§æ•°æ®é›†çš„åŸºæ•°ä¼°ç®—ã€‚

```python
import random

class ReservoirSampling:
    def __init__(self, sample_size=1000):
        self.sample_size = sample_size
        self.reservoir = []
        self.count = 0
        
    def add(self, item):
        self.count += 1
        if len(self.reservoir) < self.sample_size:
            self.reservoir.append(item)
        else:
            # éšæœºæ›¿æ¢
            j = random.randint(1, self.count)
            if j <= self.sample_size:
                self.reservoir[j-1] = item
    
    def estimate_cardinality(self):
        unique_items = len(set(self.reservoir))
        return unique_items * self.count / len(self.reservoir)

# ä½¿ç”¨ç¤ºä¾‹
sampler = ReservoirSampling(1000)
for i in range(100000):
    sampler.add(f"user_{i % 10000}")  # å®é™…10000ä¸ªä¸é‡å¤
print(f"ä¼°ç®—åŸºæ•°: {sampler.estimate_cardinality()}")
```

### 3.3 ç›´æ–¹å›¾ç»Ÿè®¡Histogram


**ğŸ“ˆ MySQL 8.0ç›´æ–¹å›¾åŠŸèƒ½**
```sql
-- åˆ›å»ºç›´æ–¹å›¾ç»Ÿè®¡
ANALYZE TABLE orders UPDATE HISTOGRAM ON status WITH 10 BUCKETS;

-- æŸ¥çœ‹ç›´æ–¹å›¾ä¿¡æ¯
SELECT 
    COLUMN_NAME,
    JSON_EXTRACT(HISTOGRAM, '$.histogram-type') as type,
    JSON_LENGTH(JSON_EXTRACT(HISTOGRAM, '$.buckets')) as bucket_count
FROM information_schema.COLUMN_STATISTICS
WHERE TABLE_NAME = 'orders';
```

**ğŸ”§ è‡ªå®šä¹‰ç›´æ–¹å›¾æ„å»º**
```sql
-- ç­‰å®½ç›´æ–¹å›¾åˆ†æ
WITH price_histogram AS (
    SELECT 
        FLOOR(price / 100) * 100 as price_bucket,
        COUNT(*) as frequency
    FROM products
    GROUP BY FLOOR(price / 100)
)
SELECT 
    CONCAT(price_bucket, '-', price_bucket + 99) as price_range,
    frequency,
    ROUND(frequency * 100.0 / SUM(frequency) OVER(), 2) as percentage
FROM price_histogram
ORDER BY price_bucket;
```

---

## 4. ğŸ“Š é€‰æ‹©æ€§è®¡ç®—ä¸ä¼˜åŒ–


### 4.1 é€‰æ‹©æ€§è®¡ç®—ä¼˜åŒ–ç®—æ³•


**âš¡ å¢é‡é€‰æ‹©æ€§æ›´æ–°**
```python
class SelectivityTracker:
    def __init__(self):
        self.total_rows = 0
        self.distinct_values = set()
        
    def add_value(self, value):
        self.total_rows += 1
        self.distinct_values.add(value)
    
    def get_selectivity(self):
        if self.total_rows == 0:
            return 0
        return len(self.distinct_values) / self.total_rows
    
    def get_stats(self):
        return {
            'total_rows': self.total_rows,
            'distinct_count': len(self.distinct_values),
            'selectivity': self.get_selectivity()
        }

# ä½¿ç”¨ç¤ºä¾‹
tracker = SelectivityTracker()
for i in range(1000):
    status = ['active', 'inactive', 'pending'][i % 3]
    tracker.add_value(status)

stats = tracker.get_stats()
print(f"é€‰æ‹©æ€§ç»Ÿè®¡: {stats}")
```

### 4.2 é€‰æ‹©æ€§é˜ˆå€¼åŠ¨æ€è°ƒæ•´


**ğŸ¯ è‡ªé€‚åº”é˜ˆå€¼è°ƒæ•´ç®—æ³•**
```python
class AdaptiveThreshold:
    def __init__(self, initial_threshold=0.3):
        self.threshold = initial_threshold
        self.performance_history = []
        
    def evaluate_query(self, selectivity, query_time, result_count):
        """è¯„ä¼°æŸ¥è¯¢æ€§èƒ½å¹¶è°ƒæ•´é˜ˆå€¼"""
        efficiency = result_count / query_time if query_time > 0 else 0
        
        self.performance_history.append({
            'selectivity': selectivity,
            'efficiency': efficiency
        })
        
        # ä¿æŒæœ€è¿‘20æ¡è®°å½•
        if len(self.performance_history) > 20:
            self.performance_history.pop(0)
        
        return self._adjust_threshold()
    
    def _adjust_threshold(self):
        if len(self.performance_history) < 10:
            return self.threshold
            
        recent = self.performance_history[-10:]
        high_sel = [p for p in recent if p['selectivity'] > self.threshold]
        low_sel = [p for p in recent if p['selectivity'] <= self.threshold]
        
        if high_sel and low_sel:
            avg_high = sum(p['efficiency'] for p in high_sel) / len(high_sel)
            avg_low = sum(p['efficiency'] for p in low_sel) / len(low_sel)
            
            if avg_low > avg_high * 1.2:
                self.threshold = max(0.1, self.threshold - 0.05)
            elif avg_high > avg_low * 1.2:
                self.threshold = min(0.8, self.threshold + 0.05)
        
        return self.threshold
```

### 4.3 é€‰æ‹©æ€§é¢„æµ‹æ¨¡å‹


**ğŸ”® è¶‹åŠ¿é¢„æµ‹ç®—æ³•**
```python
import numpy as np

class SelectivityPredictor:
    def __init__(self, window_size=30):
        self.history = []
        self.window_size = window_size
        
    def record(self, timestamp, selectivity):
        self.history.append((timestamp, selectivity))
        if len(self.history) > self.window_size:
            self.history.pop(0)
    
    def predict_trend(self):
        if len(self.history) < 5:
            return "æ•°æ®ä¸è¶³"
        
        values = [s for _, s in self.history]
        
        # ç®€å•çº¿æ€§è¶‹åŠ¿åˆ†æ
        x = np.arange(len(values))
        slope = np.polyfit(x, values, 1)[0]
        
        if slope > 0.01:
            return "ä¸Šå‡è¶‹åŠ¿"
        elif slope < -0.01:
            return "ä¸‹é™è¶‹åŠ¿"
        else:
            return "ç¨³å®š"
    
    def get_stability_score(self):
        if len(self.history) < 3:
            return 1.0
        
        values = [s for _, s in self.history]
        std_dev = np.std(values)
        mean_val = np.mean(values)
        
        # å˜å¼‚ç³»æ•°è¶Šå°è¶Šç¨³å®š
        cv = std_dev / mean_val if mean_val > 0 else 1
        return max(0, 1 - cv)
```

---

## 5. ğŸ“ˆ æ•°æ®åˆ†å¸ƒå€¾æ–œå¤„ç†


### 5.1 å€¾æ–œè¯†åˆ«ä¸é‡åŒ–


**ğŸ“Š å€¾æ–œåº¦è¯†åˆ«ç®—æ³•**
```sql
-- æ•°æ®å€¾æ–œåˆ†æ
WITH value_stats AS (
    SELECT 
        status,
        COUNT(*) as frequency,
        COUNT(*) * 100.0 / SUM(COUNT(*)) OVER() as percentage
    FROM orders
    GROUP BY status
)
SELECT 
    status,
    frequency,
    ROUND(percentage, 2) as percentage,
    CASE 
        WHEN percentage > 80 THEN 'ä¸¥é‡å€¾æ–œ'
        WHEN percentage > 60 THEN 'ä¸­åº¦å€¾æ–œ'
        WHEN percentage > 40 THEN 'è½»åº¦å€¾æ–œ'
        ELSE 'åˆ†å¸ƒå‡åŒ€'
    END as skew_level,
    -- å¸•ç´¯æ‰˜åˆ†æ
    CASE WHEN SUM(frequency) OVER (ORDER BY frequency DESC) <= 
              SUM(frequency) OVER () * 0.8 THEN 'çƒ­ç‚¹æ•°æ®'
         ELSE 'é•¿å°¾æ•°æ®' 
    END as data_category
FROM value_stats
ORDER BY frequency DESC;
```

### 5.2 å€¾æ–œæ•°æ®ç´¢å¼•ç­–ç•¥


**ğŸ¯ åˆ†å±‚ç´¢å¼•ç­–ç•¥**
```sql
-- ç­–ç•¥1ï¼šä¸ºçƒ­ç‚¹æ•°æ®åˆ›å»ºéƒ¨åˆ†ç´¢å¼•
CREATE INDEX idx_orders_active 
ON orders(user_id, created_date) 
WHERE status = 'active';  -- åªä¸ºactiveçŠ¶æ€åˆ›å»º

-- ç­–ç•¥2ï¼šç»„åˆç´¢å¼•ä¼˜åŒ–å­—æ®µé¡ºåº
-- é”™è¯¯ï¼šä½é€‰æ‹©æ€§å­—æ®µåœ¨å‰
-- CREATE INDEX idx_bad ON orders(status, user_id);

-- æ­£ç¡®ï¼šé«˜é€‰æ‹©æ€§å­—æ®µåœ¨å‰
CREATE INDEX idx_good ON orders(user_id, status);

-- ç­–ç•¥3ï¼šå“ˆå¸Œåˆ†æ¡¶å¤„ç†
CREATE INDEX idx_user_bucket 
ON orders((user_id % 10), created_date)
WHERE user_id % 10 = 0;
```

### 5.3 å€¾æ–œå¤„ç†ä¼˜åŒ–æŠ€æœ¯


**ğŸ”§ æ•°æ®é‡ç»„ä¼˜åŒ–**
```python
class SkewOptimizer:
    def analyze_skew(self, data, column):
        """åˆ†ææ•°æ®å€¾æ–œç¨‹åº¦"""
        value_counts = {}
        total = len(data)
        
        for row in data:
            value = row[column]
            value_counts[value] = value_counts.get(value, 0) + 1
        
        max_count = max(value_counts.values())
        max_percentage = max_count / total
        
        if max_percentage > 0.8:
            return 'severe_skew'
        elif max_percentage > 0.6:
            return 'moderate_skew'
        else:
            return 'balanced'
    
    def suggest_strategy(self, skew_level):
        """å»ºè®®ä¼˜åŒ–ç­–ç•¥"""
        strategies = {
            'severe_skew': [
                'ä¸å»ºè®®å•ç‹¬å»ºç´¢å¼•',
                'è€ƒè™‘éƒ¨åˆ†ç´¢å¼•',
                'ä½¿ç”¨æ•°æ®åˆ†åŒº'
            ],
            'moderate_skew': [
                'åˆ›å»ºç»„åˆç´¢å¼•',
                'è°ƒæ•´å­—æ®µé¡ºåº'
            ],
            'balanced': ['æ­£å¸¸åˆ›å»ºç´¢å¼•']
        }
        return strategies.get(skew_level, ['éœ€è¦è¿›ä¸€æ­¥åˆ†æ'])
```

---

## 6. ğŸ“¡ åŠ¨æ€é€‰æ‹©æ€§ç›‘æ§


### 6.1 å®æ—¶ç›‘æ§ç³»ç»Ÿ


**âš¡ é€‰æ‹©æ€§ç›‘æ§æ¡†æ¶**
```python
import time
import threading
from collections import deque

class SelectivityMonitor:
    def __init__(self, table_name, column_name, interval=300):
        self.table_name = table_name
        self.column_name = column_name
        self.interval = interval
        self.history = deque(maxlen=288)  # 24å°æ—¶æ•°æ®
        self.alerts = []
        self.running = False
        
    def start_monitoring(self):
        self.running = True
        thread = threading.Thread(target=self._monitor_loop)
        thread.daemon = True
        thread.start()
        
    def _monitor_loop(self):
        while self.running:
            try:
                selectivity = self._get_current_selectivity()
                self._record_data(selectivity)
                self._check_alerts(selectivity)
                time.sleep(self.interval)
            except Exception as e:
                print(f"ç›‘æ§å¼‚å¸¸: {e}")
                time.sleep(60)
    
    def _get_current_selectivity(self):
        # å®é™…å®ç°ä¸­è¿æ¥æ•°æ®åº“æŸ¥è¯¢
        # SELECT COUNT(DISTINCT col)/COUNT(*) FROM table
        import random
        return random.uniform(0.1, 0.9)
    
    def _record_data(self, selectivity):
        self.history.append({
            'timestamp': int(time.time()),
            'selectivity': selectivity
        })
    
    def _check_alerts(self, current):
        if len(self.history) < 5:
            return
            
        recent = [h['selectivity'] for h in list(self.history)[-5:]]
        avg_recent = sum(recent) / len(recent)
        
        # é€‰æ‹©æ€§æ€¥å‰§ä¸‹é™å‘Šè­¦
        if current < avg_recent * 0.5:
            alert = f"{self.column_name}é€‰æ‹©æ€§æ€¥å‰§ä¸‹é™: {current:.4f}"
            self.alerts.append(alert)
            print(f"ğŸš¨ {alert}")
    
    def get_stats(self):
        if not self.history:
            return {}
        
        values = [h['selectivity'] for h in self.history]
        return {
            'current': values[-1],
            'average': sum(values) / len(values),
            'min': min(values),
            'max': max(values)
        }
```

### 6.2 è¶‹åŠ¿åˆ†æä¸é¢„è­¦


**ğŸ“ˆ è¶‹åŠ¿åˆ†æç®—æ³•**
```python
import numpy as np

class TrendAnalyzer:
    def analyze_trend(self, time_series_data):
        """åˆ†æé€‰æ‹©æ€§å˜åŒ–è¶‹åŠ¿"""
        if len(time_series_data) < 10:
            return {"error": "æ•°æ®ä¸è¶³"}
        
        values = [item['selectivity'] for item in time_series_data]
        
        # è¶‹åŠ¿åˆ†æ
        x = np.arange(len(values))
        slope = np.polyfit(x, values, 1)[0]
        
        # å¼‚å¸¸æ£€æµ‹
        mean_val = np.mean(values)
        std_val = np.std(values)
        anomalies = [i for i, v in enumerate(values) 
                    if abs(v - mean_val) > 2 * std_val]
        
        # ç¨³å®šæ€§è¯„åˆ†
        cv = std_val / mean_val if mean_val > 0 else 1
        stability = max(0, 1 - cv)
        
        return {
            'trend': 'increasing' if slope > 0.01 else 
                    'decreasing' if slope < -0.01 else 'stable',
            'slope': slope,
            'anomaly_count': len(anomalies),
            'stability_score': stability
        }
```

### 6.3 è‡ªåŠ¨ç´¢å¼•æ¨è


**ğŸ¤– æ™ºèƒ½æ¨èç³»ç»Ÿ**
```python
class IndexRecommender:
    def __init__(self):
        self.rules = {
            'high_selectivity': 0.7,
            'medium_selectivity': 0.3
        }
    
    def analyze_column(self, column_stats, query_patterns):
        """åˆ†æåˆ—å¹¶ç”Ÿæˆç´¢å¼•æ¨è"""
        selectivity = column_stats['selectivity']
        query_freq = query_patterns.get('frequency', 0)
        
        recommendation = {
            'column': column_stats['column_name'],
            'selectivity': selectivity,
            'action': 'none',
            'confidence': 0,
            'reason': ''
        }
        
        if selectivity >= self.rules['high_selectivity'] and query_freq > 100:
            recommendation.update({
                'action': 'create_index',
                'confidence': 0.9,
                'reason': 'é«˜é€‰æ‹©æ€§ä¸”æŸ¥è¯¢é¢‘ç¹',
                'sql': f"CREATE INDEX idx_{column_stats['column_name']} "
                      f"ON {column_stats['table_name']}({column_stats['column_name']})"
            })
        elif selectivity >= self.rules['medium_selectivity']:
            recommendation.update({
                'action': 'consider_composite',
                'confidence': 0.7,
                'reason': 'ä¸­ç­‰é€‰æ‹©æ€§ï¼Œå»ºè®®ç»„åˆç´¢å¼•'
            })
        else:
            recommendation.update({
                'action': 'not_recommended',
                'confidence': 0.8,
                'reason': 'é€‰æ‹©æ€§è¿‡ä½ï¼Œä¸å»ºè®®å•ç‹¬å»ºç´¢å¼•'
            })
        
        return recommendation
    
    def generate_maintenance_plan(self, recommendations):
        """ç”Ÿæˆç´¢å¼•ç»´æŠ¤è®¡åˆ’"""
        plan = {
            'create': [],
            'consider': [],
            'avoid': []
        }
        
        for rec in recommendations:
            if rec['action'] == 'create_index':
                plan['create'].append(rec)
            elif rec['action'] == 'consider_composite':
                plan['consider'].append(rec)
            else:
                plan['avoid'].append(rec)
        
        return plan
```

---

## 7. ğŸ¯ å¤šç»´é€‰æ‹©æ€§åˆ†æ


### 7.1 å¤šå­—æ®µé€‰æ‹©æ€§ç»„åˆè¯„ä¼°


**ğŸ” ç»„åˆé€‰æ‹©æ€§è®¡ç®—**
```sql
-- å¤šå­—æ®µç»„åˆé€‰æ‹©æ€§åˆ†æ
WITH combination_analysis AS (
    SELECT 
        'user_id' as fields,
        COUNT(DISTINCT user_id) as distinct_count,
        COUNT(*) as total_count,
        COUNT(DISTINCT user_id) * 1.0 / COUNT(*) as selectivity
    FROM orders
    UNION ALL
    SELECT 
        'user_id + status',
        COUNT(DISTINCT CONCAT(user_id, '|', status)),
        COUNT(*),
        COUNT(DISTINCT CONCAT(user_id, '|', status)) * 1.0 / COUNT(*)
    FROM orders
    UNION ALL
    SELECT 
        'user_id + status + created_date',
        COUNT(DISTINCT CONCAT(user_id, '|', status, '|', DATE(created_at))),
        COUNT(*),
        COUNT(DISTINCT CONCAT(user_id, '|', status, '|', DATE(created_at))) * 1.0 / COUNT(*)
    FROM orders
)
SELECT 
    fields,
    distinct_count,
    ROUND(selectivity, 4) as selectivity,
    CASE 
        WHEN selectivity >= 0.8 THEN 'ä¼˜ç§€'
        WHEN selectivity >= 0.5 THEN 'è‰¯å¥½'
        WHEN selectivity >= 0.2 THEN 'ä¸€èˆ¬'
        ELSE 'å·®'
    END as rating
FROM combination_analysis
ORDER BY selectivity DESC;
```

### 7.2 ç»„åˆç´¢å¼•ä¼˜åŒ–ç­–ç•¥


**âš¡ å­—æ®µé¡ºåºä¼˜åŒ–ç®—æ³•**
```python
import itertools

class ComboAnalyzer:
    def analyze_combinations(self, field_stats, max_fields=3):
        """åˆ†æå­—æ®µç»„åˆçš„æœ€ä½³é¡ºåº"""
        fields = list(field_stats.keys())
        best_combinations = []
        
        for size in range(2, min(max_fields + 1, len(fields) + 1)):
            for combo in itertools.combinations(fields, size):
                # æŒ‰é€‰æ‹©æ€§æ’åºå­—æ®µ
                sorted_combo = sorted(combo, 
                                    key=lambda f: field_stats[f]['selectivity'], 
                                    reverse=True)
                
                # ä¼°ç®—ç»„åˆé€‰æ‹©æ€§
                combined_selectivity = min(1.0, 
                    sum(field_stats[f]['selectivity'] for f in combo))
                
                best_combinations.append({
                    'fields': sorted_combo,
                    'estimated_selectivity': combined_selectivity,
                    'field_count': len(combo)
                })
        
        return sorted(best_combinations, 
                     key=lambda x: x['estimated_selectivity'], 
                     reverse=True)
    
    def recommend_index_order(self, combination):
        """æ¨èç´¢å¼•å­—æ®µé¡ºåº"""
        return {
            'recommended_order': combination['fields'],
            'sql': f"CREATE INDEX idx_combo ON table_name({', '.join(combination['fields'])})",
            'expected_selectivity': combination['estimated_selectivity']
        }

# ä½¿ç”¨ç¤ºä¾‹
analyzer = ComboAnalyzer()
field_stats = {
    'user_id': {'selectivity': 0.9},
    'status': {'selectivity': 0.1},
    'created_date': {'selectivity': 0.8}
}

combinations = analyzer.analyze_combinations(field_stats)
for combo in combinations[:3]:  # æ˜¾ç¤ºå‰3ä¸ªæ¨è
    recommendation = analyzer.recommend_index_order(combo)
    print(f"æ¨è: {recommendation}")
```

### 7.3 é€‰æ‹©æ€§ç›¸å…³æ€§åˆ†æ


**ğŸ“Š å­—æ®µç›¸å…³æ€§è¯„ä¼°**
```python
import numpy as np

class SelectivityCorrelation:
    def calculate_field_correlation(self, data, field1, field2):
        """è®¡ç®—ä¸¤ä¸ªå­—æ®µçš„é€‰æ‹©æ€§ç›¸å…³æ€§"""
        # æ„å»ºè”åˆåˆ†å¸ƒ
        joint_values = set()
        field1_values = set()
        field2_values = set()
        
        for row in data:
            val1 = row[field1]
            val2 = row[field2]
            joint_values.add(f"{val1}|{val2}")
            field1_values.add(val1)
            field2_values.add(val2)
        
        total_rows = len(data)
        
        # è®¡ç®—å„ç§é€‰æ‹©æ€§
        joint_selectivity = len(joint_values) / total_rows
        field1_selectivity = len(field1_values) / total_rows
        field2_selectivity = len(field2_values) / total_rows
        
        # è®¡ç®—ç›¸å…³æ€§æŒ‡æ ‡
        expected_joint = field1_selectivity * field2_selectivity
        correlation = joint_selectivity / expected_joint if expected_joint > 0 else 0
        
        return {
            'field1_selectivity': field1_selectivity,
            'field2_selectivity': field2_selectivity,
            'joint_selectivity': joint_selectivity,
            'correlation_factor': correlation,
            'independence': abs(1 - correlation) < 0.1  # æ¥è¿‘1è¡¨ç¤ºç‹¬ç«‹
        }
    
    def analyze_all_correlations(self, data, fields):
        """åˆ†ææ‰€æœ‰å­—æ®µå¯¹çš„ç›¸å…³æ€§"""
        correlations = []
        
        for i in range(len(fields)):
            for j in range(i + 1, len(fields)):
                field1, field2 = fields[i], fields[j]
                corr = self.calculate_field_correlation(data, field1, field2)
                corr.update({
                    'field1': field1,
                    'field2': field2
                })
                correlations.append(corr)
        
        return sorted(correlations, 
                     key=lambda x: x['correlation_factor'], 
                     reverse=True)
```

---

## 8. ğŸ› ï¸ ä½é€‰æ‹©æ€§ç´¢å¼•æ›¿ä»£ç­–ç•¥


### 8.1 éƒ¨åˆ†ç´¢å¼•ç­–ç•¥


**ğŸ¯ æœ‰æ¡ä»¶çš„ç´¢å¼•åˆ›å»º**
```sql
-- éƒ¨åˆ†ç´¢å¼•ï¼šåªä¸ºç‰¹å®šæ¡ä»¶åˆ›å»ºç´¢å¼•
-- åœºæ™¯ï¼šè®¢å•çŠ¶æ€ä¸­ï¼Œåªå…³æ³¨æ´»è·ƒè®¢å•
CREATE INDEX idx_active_orders 
ON orders(user_id, created_at) 
WHERE status = 'active';

-- å¸ƒå°”å­—æ®µçš„éƒ¨åˆ†ç´¢å¼•
-- åªä¸ºtrueå€¼åˆ›å»ºç´¢å¼•ï¼ˆé€šå¸¸æ˜¯å°‘æ•°ï¼‰
CREATE INDEX idx_premium_users 
ON users(created_at, last_login) 
WHERE is_premium = true;

-- åŸºäºæ—¶é—´èŒƒå›´çš„éƒ¨åˆ†ç´¢å¼•
CREATE INDEX idx_recent_orders 
ON orders(user_id, amount) 
WHERE created_at >= DATE_SUB(NOW(), INTERVAL 30 DAY);
```

### 8.2 ç´¢å¼•åˆå¹¶ç­–ç•¥


**ğŸ”„ å¤šä¸ªå•åˆ—ç´¢å¼•åˆå¹¶**
```sql
-- åœºæ™¯ï¼šå¤šä¸ªä½é€‰æ‹©æ€§å­—æ®µç»„åˆæŸ¥è¯¢
-- åˆ›å»ºå¤šä¸ªå•åˆ—ç´¢å¼•è®©MySQLè‡ªåŠ¨åˆå¹¶

-- å•åˆ—ç´¢å¼•
CREATE INDEX idx_status ON orders(status);
CREATE INDEX idx_priority ON orders(priority); 
CREATE INDEX idx_type ON orders(order_type);

-- MySQLä¼šåœ¨æŸ¥è¯¢æ—¶è‡ªåŠ¨åˆå¹¶
SELECT * FROM orders 
WHERE status = 'pending' 
  AND priority = 'high' 
  AND order_type = 'express';
  
-- æŸ¥çœ‹æ‰§è¡Œè®¡åˆ’éªŒè¯ç´¢å¼•åˆå¹¶
EXPLAIN SELECT * FROM orders 
WHERE status = 'pending' AND priority = 'high';
```

### 8.3 å‡½æ•°ç´¢å¼•ä¸è¡¨è¾¾å¼ç´¢å¼•


**âš¡ åŸºäºè®¡ç®—çš„ç´¢å¼•**
```sql
-- å‡½æ•°ç´¢å¼•ï¼šä¸ºè®¡ç®—ç»“æœåˆ›å»ºç´¢å¼•
-- MySQL 8.0æ”¯æŒå‡½æ•°ç´¢å¼•
CREATE INDEX idx_order_year 
ON orders((YEAR(created_at)));

-- è¡¨è¾¾å¼ç´¢å¼•ï¼šç»„åˆå¤šä¸ªä½é€‰æ‹©æ€§å­—æ®µ
CREATE INDEX idx_status_priority 
ON orders((CONCAT(status, '_', priority)));

-- å“ˆå¸Œç´¢å¼•ï¼šå¤„ç†é•¿å­—ç¬¦ä¸²
CREATE INDEX idx_description_hash 
ON products((CRC32(description)));

-- å‰ç¼€ç´¢å¼•ï¼šå¤„ç†é•¿å­—ç¬¦ä¸²å­—æ®µ
CREATE INDEX idx_email_prefix 
ON users(email(10));  -- åªç´¢å¼•å‰10ä¸ªå­—ç¬¦
```

### 8.4 æ›¿ä»£æŸ¥è¯¢ä¼˜åŒ–æŠ€æœ¯


**ğŸ”§ æŸ¥è¯¢é‡æ„ç­–ç•¥**
```python
class QueryOptimizer:
    def optimize_low_selectivity_query(self, original_query, field_stats):
        """ä¼˜åŒ–ä½é€‰æ‹©æ€§å­—æ®µçš„æŸ¥è¯¢"""
        suggestions = []
        
        for field, stats in field_stats.items():
            selectivity = stats['selectivity']
            
            if selectivity < 0.1:  # æä½é€‰æ‹©æ€§
                suggestions.append({
                    'field': field,
                    'problem': f'{field}é€‰æ‹©æ€§è¿‡ä½({selectivity:.4f})',
                    'solutions': [
                        'ä½¿ç”¨LIMITé™åˆ¶ç»“æœé›†',
                        'æ·»åŠ å…¶ä»–è¿‡æ»¤æ¡ä»¶',
                        'è€ƒè™‘è¡¨åˆ†åŒº',
                        'ä½¿ç”¨è¦†ç›–ç´¢å¼•'
                    ]
                })
            elif selectivity < 0.3:  # ä½é€‰æ‹©æ€§
                suggestions.append({
                    'field': field,
                    'problem': f'{field}é€‰æ‹©æ€§è¾ƒä½({selectivity:.4f})',
                    'solutions': [
                        'åˆ›å»ºç»„åˆç´¢å¼•',
                        'è°ƒæ•´æŸ¥è¯¢é¡ºåº',
                        'ä½¿ç”¨EXISTSæ›¿ä»£IN'
                    ]
                })
        
        return suggestions
    
    def suggest_query_rewrite(self, query_pattern):
        """å»ºè®®æŸ¥è¯¢é‡å†™æ–¹æ¡ˆ"""
        rewrite_rules = {
            'low_selectivity_first': {
                'problem': 'ä½é€‰æ‹©æ€§å­—æ®µä½œä¸ºä¸»è¦è¿‡æ»¤æ¡ä»¶',
                'solution': 'è°ƒæ•´WHEREæ¡ä»¶é¡ºåºï¼Œé«˜é€‰æ‹©æ€§å­—æ®µä¼˜å…ˆ'
            },
            'missing_limit': {
                'problem': 'å¯èƒ½è¿”å›å¤§é‡ç»“æœ',
                'solution': 'æ·»åŠ LIMITå­å¥é™åˆ¶ç»“æœæ•°é‡'
            },
            'inefficient_like': {
                'problem': 'ä½¿ç”¨å‰ç¼€é€šé…ç¬¦LIKEæŸ¥è¯¢',
                'solution': 'ä½¿ç”¨å…¨æ–‡ç´¢å¼•æˆ–é‡æ„æŸ¥è¯¢é€»è¾‘'
            }
        }
        return rewrite_rules

# ä½¿ç”¨ç¤ºä¾‹
optimizer = QueryOptimizer()
field_stats = {
    'status': {'selectivity': 0.05},
    'user_id': {'selectivity': 0.95},
    'created_date': {'selectivity': 0.80}
}

suggestions = optimizer.optimize_low_selectivity_query(
    "SELECT * FROM orders WHERE status = ?", 
    field_stats
)

for suggestion in suggestions:
    print(f"å­—æ®µ: {suggestion['field']}")
    print(f"é—®é¢˜: {suggestion['problem']}")
    for solution in suggestion['solutions']:
        print(f"  è§£å†³æ–¹æ¡ˆ: {solution}")
```

### 8.5 æ•°æ®åˆ†åŒºæ›¿ä»£æ–¹æ¡ˆ


**ğŸ“Š åŸºäºä½é€‰æ‹©æ€§å­—æ®µçš„åˆ†åŒº**
```sql
-- æŒ‰ä½é€‰æ‹©æ€§å­—æ®µåˆ†åŒº
-- è™½ç„¶å­—æ®µé€‰æ‹©æ€§ä½ï¼Œä½†å¯ä»¥é€šè¿‡åˆ†åŒºæå‡æŸ¥è¯¢æ•ˆç‡

-- æŒ‰çŠ¶æ€åˆ†åŒº
CREATE TABLE orders_partitioned (
    id INT AUTO_INCREMENT,
    user_id INT,
    status ENUM('active', 'inactive', 'pending', 'cancelled'),
    created_at DATETIME,
    amount DECIMAL(10,2),
    PRIMARY KEY (id, status)
) PARTITION BY LIST COLUMNS(status) (
    PARTITION p_active VALUES IN ('active'),
    PARTITION p_inactive VALUES IN ('inactive'), 
    PARTITION p_others VALUES IN ('pending', 'cancelled')
);

-- ä¸ºæ¯ä¸ªåˆ†åŒºåˆ›å»ºåˆé€‚çš„ç´¢å¼•
ALTER TABLE orders_partitioned 
ADD INDEX idx_active_user_date (user_id, created_at) PARTITION p_active;

ALTER TABLE orders_partitioned 
ADD INDEX idx_inactive_user (user_id) PARTITION p_inactive;

-- æŸ¥è¯¢æ—¶MySQLä¼šè‡ªåŠ¨å‰ªæï¼Œåªæ‰«æç›¸å…³åˆ†åŒº
SELECT * FROM orders_partitioned 
WHERE status = 'active' AND user_id = 12345;
-- åªä¼šæ‰«æp_activeåˆ†åŒº
```

---

## 9. ğŸ“‹ æ ¸å¿ƒè¦ç‚¹æ€»ç»“


### 9.1 é€‰æ‹©æ€§åˆ†æå†³ç­–æµç¨‹


**ğŸ”„ é€‰æ‹©æ€§åˆ†æå®Œæ•´æµç¨‹**
```
æ•°æ®æ”¶é›†é˜¶æ®µï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ç»Ÿè®¡åŸºç¡€ä¿¡æ¯   â”‚ â†’ COUNT DISTINCT / COUNT(*)
â”‚  åˆ†ææ•°æ®åˆ†å¸ƒ   â”‚ â†’ è¯†åˆ«çƒ­ç‚¹å’Œé•¿å°¾æ•°æ®  
â”‚  è¯„ä¼°æŸ¥è¯¢æ¨¡å¼   â”‚ â†’ æŸ¥è¯¢é¢‘ç‡å’Œè¿‡æ»¤ä½¿ç”¨ç‡
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
é€‰æ‹©æ€§è®¡ç®—é˜¶æ®µï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  å•å­—æ®µé€‰æ‹©æ€§   â”‚ â†’ åŸºç¡€é€‰æ‹©æ€§æŒ‡æ ‡
â”‚  ç»„åˆå­—æ®µé€‰æ‹©æ€§ â”‚ â†’ å¤šç»´é€‰æ‹©æ€§è¯„ä¼°
â”‚  åŠ¨æ€é€‰æ‹©æ€§ç›‘æ§ â”‚ â†’ è¶‹åŠ¿å˜åŒ–åˆ†æ
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
ç´¢å¼•å†³ç­–é˜¶æ®µï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ é«˜é€‰æ‹©æ€§(>0.7)  â”‚ â†’ åˆ›å»ºå•åˆ—ç´¢å¼•
â”‚ ä¸­é€‰æ‹©æ€§(0.3-0.7)â”‚ â†’ è€ƒè™‘ç»„åˆç´¢å¼•
â”‚ ä½é€‰æ‹©æ€§(<0.3)  â”‚ â†’ æ›¿ä»£ç­–ç•¥
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 9.2 å¿…é¡»æŒæ¡çš„æ ¸å¿ƒæ¦‚å¿µ


```
ğŸ”¸ é€‰æ‹©æ€§å®šä¹‰ï¼šä¸é‡å¤å€¼æ•°é‡/æ€»è®°å½•æ•°ï¼Œè¡¡é‡ç´¢å¼•è¿‡æ»¤èƒ½åŠ›
ğŸ”¸ åŸºæ•°ä¼°ç®—ï¼šHyperLogLogã€é‡‡æ ·ç»Ÿè®¡ã€ç›´æ–¹å›¾ç­‰ç®—æ³•åŸç†
ğŸ”¸ åˆ†å¸ƒåˆ†æï¼šè¯†åˆ«æ•°æ®å€¾æ–œã€çƒ­ç‚¹æ•°æ®ã€é•¿å°¾åˆ†å¸ƒæ¨¡å¼
ğŸ”¸ åŠ¨æ€ç›‘æ§ï¼šå®æ—¶è·Ÿè¸ªé€‰æ‹©æ€§å˜åŒ–ï¼Œé¢„è­¦å¼‚å¸¸æƒ…å†µ
ğŸ”¸ å¤šç»´åˆ†æï¼šç»„åˆå­—æ®µé€‰æ‹©æ€§ã€ç›¸å…³æ€§åˆ†æã€å­—æ®µé¡ºåºä¼˜åŒ–
ğŸ”¸ æ›¿ä»£ç­–ç•¥ï¼šéƒ¨åˆ†ç´¢å¼•ã€ç´¢å¼•åˆå¹¶ã€å‡½æ•°ç´¢å¼•ã€æ•°æ®åˆ†åŒº
ğŸ”¸ ä¼˜åŒ–ç®—æ³•ï¼šè‡ªé€‚åº”é˜ˆå€¼ã€è¶‹åŠ¿é¢„æµ‹ã€æ™ºèƒ½æ¨èç³»ç»Ÿ
```

### 9.3 å®é™…åº”ç”¨æŒ‡å¯¼åŸåˆ™


**ğŸ¯ é€‰æ‹©æ€§é˜ˆå€¼å‚è€ƒ**
- **â‰¥0.9**: æé«˜é€‰æ‹©æ€§ï¼Œå¼ºçƒˆæ¨èå»ºç´¢å¼•ï¼ˆä¸»é”®ã€å”¯ä¸€é”®ï¼‰
- **0.7-0.9**: é«˜é€‰æ‹©æ€§ï¼Œæ¨èå»ºç´¢å¼•ï¼ˆç”¨æˆ·IDã€è®¢å•å·ï¼‰
- **0.3-0.7**: ä¸­ç­‰é€‰æ‹©æ€§ï¼Œéœ€è¯„ä¼°æŸ¥è¯¢æ¨¡å¼å†³å®š
- **0.1-0.3**: ä½é€‰æ‹©æ€§ï¼Œè°¨æ…è€ƒè™‘ï¼Œä¼˜å…ˆç»„åˆç´¢å¼•
- **<0.1**: æä½é€‰æ‹©æ€§ï¼Œä¸æ¨èå•ç‹¬å»ºç´¢å¼•

**ğŸ”§ ä¼˜åŒ–ç­–ç•¥é€‰æ‹©**
```
é«˜é€‰æ‹©æ€§å­—æ®µï¼š
â”œâ”€ ç›´æ¥åˆ›å»ºB+æ ‘ç´¢å¼•
â”œâ”€ ä¼˜å…ˆä½œä¸ºç»„åˆç´¢å¼•çš„ç¬¬ä¸€ä¸ªå­—æ®µ
â””â”€ é€‚åˆä½œä¸ºåˆ†åŒºé”®

ä¸­ç­‰é€‰æ‹©æ€§å­—æ®µï¼š
â”œâ”€ ç»“åˆæŸ¥è¯¢é¢‘ç‡å†³å®šæ˜¯å¦å»ºç´¢å¼•
â”œâ”€ è€ƒè™‘ä¸å…¶ä»–å­—æ®µç»„åˆ
â””â”€ ç›‘æ§é€‰æ‹©æ€§å˜åŒ–è¶‹åŠ¿

ä½é€‰æ‹©æ€§å­—æ®µï¼š
â”œâ”€ éƒ¨åˆ†ç´¢å¼•ï¼ˆWHEREæ¡ä»¶è¿‡æ»¤ï¼‰
â”œâ”€ å‡½æ•°ç´¢å¼•ï¼ˆè¡¨è¾¾å¼è®¡ç®—ï¼‰
â”œâ”€ æ•°æ®åˆ†åŒºï¼ˆç‰©ç†éš”ç¦»ï¼‰
â””â”€ æŸ¥è¯¢é‡æ„ï¼ˆä¼˜åŒ–SQLé€»è¾‘ï¼‰
```

**âš ï¸ å¸¸è§è¯¯åŒºé¿å…**
```
è¯¯åŒº1ï¼šåªçœ‹é€‰æ‹©æ€§æ•°å€¼ï¼Œå¿½ç•¥æŸ¥è¯¢æ¨¡å¼
æ­£ç¡®ï¼šç»“åˆæŸ¥è¯¢é¢‘ç‡å’Œä¸šåŠ¡åœºæ™¯ç»¼åˆåˆ¤æ–­

è¯¯åŒº2ï¼šä½é€‰æ‹©æ€§å­—æ®µç»å¯¹ä¸å»ºç´¢å¼•
æ­£ç¡®ï¼šå¯ä»¥è€ƒè™‘éƒ¨åˆ†ç´¢å¼•å’Œç»„åˆç´¢å¼•æ–¹æ¡ˆ

è¯¯åŒº3ï¼šé™æ€åˆ†æä¸€æ¬¡å°±å¤Ÿäº†
æ­£ç¡®ï¼šå»ºç«‹åŠ¨æ€ç›‘æ§ï¼ŒæŒç»­è·Ÿè¸ªé€‰æ‹©æ€§å˜åŒ–

è¯¯åŒº4ï¼šåªå…³æ³¨å•å­—æ®µé€‰æ‹©æ€§
æ­£ç¡®ï¼šé‡è§†å¤šå­—æ®µç»„åˆçš„é€‰æ‹©æ€§åˆ†æ
```

### 9.4 æ€§èƒ½ä¼˜åŒ–æœ€ä½³å®è·µ


**ğŸ’¡ é€‰æ‹©æ€§ä¼˜åŒ–å»ºè®®**
- **æ•°æ®é¢„å¤„ç†**: å®šæœŸæ¸…ç†å†—ä½™æ•°æ®ï¼Œä¿æŒæ•°æ®è´¨é‡
- **ç»Ÿè®¡ä¿¡æ¯æ›´æ–°**: å®šæœŸæ‰§è¡ŒANALYZE TABLEæ›´æ–°ç»Ÿè®¡ä¿¡æ¯
- **ç›‘æ§å‘Šè­¦**: å»ºç«‹é€‰æ‹©æ€§å˜åŒ–ç›‘æ§å’Œå¼‚å¸¸å‘Šè­¦æœºåˆ¶
- **æµ‹è¯•éªŒè¯**: ç´¢å¼•åˆ›å»ºå‰è¿›è¡Œå……åˆ†çš„æ€§èƒ½æµ‹è¯•
- **æŒç»­ä¼˜åŒ–**: æ ¹æ®ä¸šåŠ¡å‘å±•è°ƒæ•´ç´¢å¼•ç­–ç•¥

**ğŸš€ é«˜çº§ä¼˜åŒ–æŠ€å·§**
- **è‡ªé€‚åº”ç´¢å¼•**: åŸºäºæŸ¥è¯¢æ¨¡å¼è‡ªåŠ¨è°ƒæ•´ç´¢å¼•ç­–ç•¥
- **æœºå™¨å­¦ä¹ **: ä½¿ç”¨MLæ¨¡å‹é¢„æµ‹é€‰æ‹©æ€§å˜åŒ–è¶‹åŠ¿  
- **åˆ†å¸ƒå¼ä¼˜åŒ–**: åœ¨åˆ†åº“åˆ†è¡¨ç¯å¢ƒä¸‹çš„é€‰æ‹©æ€§åˆ†æ
- **å®æ—¶è®¡ç®—**: æµå¼è®¡ç®—å®æ—¶æ›´æ–°é€‰æ‹©æ€§ç»Ÿè®¡

**æ ¸å¿ƒè®°å¿†è¦ç‚¹**ï¼š
- é€‰æ‹©æ€§é«˜ä½å†³å®šç´¢å¼•æ•ˆæœå¥½å
- åŠ¨æ€ç›‘æ§èƒœè¿‡é™æ€åˆ†æä¸€æ¬¡
- ç»„åˆåˆ†æä¼˜äºå•ä¸€å­—æ®µåˆ¤æ–­  
- æ›¿ä»£ç­–ç•¥è§£å†³ä½é€‰æ‹©æ€§éš¾é¢˜
- ä¸šåŠ¡åœºæ™¯æ˜¯æœ€ç»ˆå†³ç­–ä¾æ®