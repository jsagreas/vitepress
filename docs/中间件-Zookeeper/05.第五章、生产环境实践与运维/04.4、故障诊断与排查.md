---
title: 4、故障诊断与排查
---
## 📚 目录

1. [故障诊断基础方法论](#1-故障诊断基础方法论)
2. [常见故障场景分析](#2-常见故障场景分析)
3. [连接超时问题排查](#3-连接超时问题排查)
4. [会话过期问题处理](#4-会话过期问题处理)
5. [脑裂问题诊断](#5-脑裂问题诊断)
6. [内存泄漏排查](#6-内存泄漏排查)
7. [网络问题排查](#7-网络问题排查)
8. [综合排查流程](#8-综合排查流程)
9. [预防与监控策略](#9-预防与监控策略)
10. [核心要点总结](#10-核心要点总结)

---

## 1. 🔍 故障诊断基础方法论


### 1.1 故障诊断思路


**🔸 基本诊断原则**
```
系统性分析方法：
1. 现象收集：准确描述故障现象
2. 信息采集：收集相关日志和状态
3. 假设验证：基于经验提出可能原因
4. 逐层排查：从表层到深层依次验证
5. 修复验证：确认问题解决且无副作用

诊断思维模式：
- 从简单到复杂：先检查常见问题
- 从外到内：网络->系统->应用->配置
- 时间轴分析：问题出现前后发生了什么变化
```

### 1.2 故障分类框架


**📋 按影响层面分类**
```
网络层故障：
- 连接中断、延迟过高、丢包严重
- 症状：ping超时、连接拒绝、响应缓慢

系统层故障：
- CPU、内存、磁盘资源耗尽
- 症状：系统卡顿、进程异常退出

应用层故障：
- 程序崩溃、逻辑错误、配置问题
- 症状：服务不响应、功能异常、错误日志

数据层故障：
- 数据库连接问题、数据损坏
- 症状：数据查询失败、事务回滚
```

### 1.3 诊断工具箱


**🛠️ 必备命令工具**
```bash
# 系统状态查看
top/htop          # 实时系统资源监控
ps aux            # 进程状态查看
df -h             # 磁盘空间使用
free -h           # 内存使用情况
iostat            # IO统计信息

# 网络诊断
ping              # 网络连通性测试
netstat -tulpn    # 网络连接状态
ss -tulpn         # 现代版netstat
tcpdump           # 网络包捕获
nmap              # 端口扫描

# 日志分析
tail -f           # 实时查看日志
grep              # 日志搜索过滤
journalctl        # systemd日志查看
dmesg             # 内核消息查看
```

---

## 2. 🎯 常见故障场景分析


### 2.1 服务无响应故障


**❌ 故障现象**
```
用户反馈：
- 网站/应用无法访问
- 请求超时或连接被拒绝
- 间歇性服务中断

初步症状：
- 浏览器显示"无法连接到服务器"
- curl命令返回连接超时
- 负载均衡器显示后端不健康
```

**🔍 诊断步骤**
```bash
# 1. 检查服务进程状态
ps aux | grep nginx
systemctl status nginx

# 2. 检查端口监听情况
netstat -tulpn | grep :80
ss -tulpn | grep :80

# 3. 检查系统资源
top
free -h
df -h

# 4. 查看服务日志
journalctl -u nginx -f
tail -f /var/log/nginx/error.log
```

### 2.2 性能急剧下降


**📉 故障现象**
```
性能表现：
- 响应时间从毫秒级变为秒级
- 并发处理能力大幅下降
- 用户投诉系统卡顿

可能原因：
- 资源瓶颈：CPU、内存、磁盘IO
- 数据库锁等待或慢查询
- 网络带宽饱和
- 缓存失效导致数据库压力激增
```

**⚡ 快速定位方法**
```bash
# 资源使用情况快速检查
iostat 1 5          # IO使用率
sar -u 1 5          # CPU使用率  
sar -r 1 5          # 内存使用率
sar -n DEV 1 5      # 网络流量

# 进程资源占用排序
ps aux --sort=-%cpu | head -10
ps aux --sort=-%mem | head -10

# 数据库连接数检查（以MySQL为例）
mysql -e "SHOW PROCESSLIST;" | wc -l
mysql -e "SHOW STATUS LIKE 'Threads_connected';"
```

### 2.3 间歇性故障


**🔄 故障特征**
```
难点分析：
- 故障不可重现，时好时坏
- 没有明显的触发条件
- 日志信息不完整或不明确

常见原因：
- 资源竞争导致的偶发性问题
- 定时任务或计划任务的影响
- 网络抖动或不稳定
- 内存泄漏导致的周期性问题
```

**📊 监控策略**
```bash
# 建立长期监控
# 创建资源监控脚本
cat > monitor_resources.sh << 'EOF'
#!/bin/bash
while true; do
    echo "$(date): CPU=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}'), MEM=$(free | grep Mem | awk '{printf("%.1f%%", $3/$2 * 100.0)}')" >> /var/log/resource_monitor.log
    sleep 30
done
EOF

# 后台运行监控
nohup bash monitor_resources.sh &

# 分析历史数据找规律
grep "$(date +%H:%M)" /var/log/resource_monitor.log
```

---

## 3. ⏰ 连接超时问题排查


### 3.1 超时问题分类


**🔸 超时类型理解**
```
连接建立超时：
- 现象：客户端无法建立TCP连接
- 原因：服务端未监听、防火墙阻拦、网络不通

读取超时：
- 现象：连接建立成功但读取响应超时  
- 原因：服务端处理慢、网络延迟高

写入超时：
- 现象：无法向服务端发送完整请求
- 原因：网络拥塞、缓冲区满
```

### 3.2 网络层面排查


**🌐 网络连通性测试**
```bash
# 基础网络测试
ping -c 4 target_host

# 检查路由路径
traceroute target_host
mtr target_host  # 更详细的路由分析

# TCP连接测试
telnet target_host 80
nc -zv target_host 80

# 端口扫描确认服务可用性
nmap -p 80,443,22 target_host
```

### 3.3 服务端排查


**🖥️ 服务端状态检查**
```bash
# 检查服务监听状态
netstat -tlnp | grep :80
ss -tlnp | grep :80

# 检查连接数和状态分布
ss -s  # 连接统计概览
netstat -an | grep :80 | wc -l  # 特定端口连接数

# 检查系统连接限制
ulimit -n                    # 进程文件描述符限制
cat /proc/sys/net/core/somaxconn    # 监听队列长度
cat /proc/sys/net/ipv4/tcp_max_syn_backlog  # SYN队列长度
```

### 3.4 超时参数调优


**⚙️ 系统参数优化**
```bash
# TCP连接相关参数查看
cat /proc/sys/net/ipv4/tcp_syn_retries      # SYN重试次数
cat /proc/sys/net/ipv4/tcp_synack_retries   # SYN-ACK重试次数
cat /proc/sys/net/ipv4/tcp_fin_timeout      # FIN_WAIT_2超时时间

# 临时调整参数（重启后失效）
echo 2 > /proc/sys/net/ipv4/tcp_syn_retries
echo 1 > /proc/sys/net/ipv4/tcp_synack_retries

# 永久调整参数
cat >> /etc/sysctl.conf << EOF
net.ipv4.tcp_syn_retries = 2
net.ipv4.tcp_synack_retries = 1
net.ipv4.tcp_fin_timeout = 15
EOF
sysctl -p
```

### 3.5 应用层超时配置


**📝 常见应用超时设置**
```bash
# Nginx超时配置
client_header_timeout 60s;
client_body_timeout 60s;
send_timeout 60s;
proxy_connect_timeout 30s;
proxy_send_timeout 60s;
proxy_read_timeout 60s;

# Apache超时配置
Timeout 300
KeepAliveTimeout 15

# 数据库连接超时
# MySQL
wait_timeout = 28800
interactive_timeout = 28800
connect_timeout = 10
```

---

## 4. 🕒 会话过期问题处理


### 4.1 会话过期现象识别


**🔸 典型症状**
```
用户体验问题：
- 操作过程中突然要求重新登录
- 表单提交时提示会话无效
- AJAX请求返回未认证错误

技术表现：
- Session ID在服务端找不到对应数据
- Cookie过期或被清除
- 负载均衡导致会话丢失
```

### 4.2 会话存储机制诊断


**💾 会话存储位置检查**
```bash
# PHP会话文件存储
ls -la /var/lib/php/sessions/
# 检查会话文件权限和时间戳
ls -lt /var/lib/php/sessions/ | head -10

# Redis会话存储
redis-cli
> KEYS sess_*
> TTL sess_xxxxxx  # 检查会话过期时间
> GET sess_xxxxxx  # 查看会话内容

# 数据库会话存储
mysql -e "SELECT * FROM sessions WHERE id = 'session_id' LIMIT 5;"
```

### 4.3 会话配置检查


**⚙️ 会话参数诊断**
```bash
# PHP会话配置检查
php -i | grep session
# 关键参数：
# session.gc_maxlifetime：会话垃圾回收时间
# session.cookie_lifetime：Cookie生存时间
# session.save_path：会话存储路径

# 检查具体配置值
php -r "echo 'GC MaxLifetime: ' . ini_get('session.gc_maxlifetime') . \"\\n\";"
php -r "echo 'Cookie Lifetime: ' . ini_get('session.cookie_lifetime') . \"\\n\";"

# Web服务器会话配置
# Nginx + PHP-FPM
grep -r "session" /etc/php/*/fpm/pool.d/
grep -r "session" /etc/php/*/fpm/php.ini
```

### 4.4 负载均衡会话问题


**⚖️ 会话粘性配置**
```bash
# Nginx会话粘性配置示例
upstream backend {
    ip_hash;  # 基于客户端IP的会话粘性
    server 192.168.1.10:80;
    server 192.168.1.11:80;
}

# HAProxy会话粘性配置
backend webservers
    balance roundrobin
    cookie SERVERID insert indirect nocache
    server web1 192.168.1.10:80 cookie web1 check
    server web2 192.168.1.11:80 cookie web2 check

# 检查当前负载均衡配置
nginx -t  # 检查Nginx配置语法
haproxy -c -f /etc/haproxy/haproxy.cfg  # 检查HAProxy配置
```

---

## 5. 🧠 脑裂问题诊断


### 5.1 脑裂现象理解


**🔸 脑裂概念解析**
```
脑裂定义：
集群中的节点失去相互通信，但都认为自己是活跃的主节点，
导致出现多个"大脑"同时工作的情况

危害影响：
- 数据不一致：多个节点同时写入相同数据
- 服务冲突：多个节点提供相同服务造成混乱
- 资源竞争：节点争夺共享资源访问权

常见场景：
- 数据库主从复制中的双主问题
- 集群文件系统的多点挂载
- 高可用服务的多活状态
```

### 5.2 脑裂检测方法


**🔍 检测脑裂状态**
```bash
# MySQL主从复制脑裂检查
# 在各个MySQL节点上执行
mysql -e "SHOW VARIABLES LIKE 'read_only';"
mysql -e "SHOW MASTER STATUS;"
mysql -e "SHOW SLAVE STATUS\G;"

# 检查是否有多个节点都显示为Master
# 正常情况下只能有一个节点read_only=OFF

# Keepalived脑裂检查
ip addr show | grep -E "192.168.1.100"  # 虚拟IP检查
# 在所有节点执行，看是否多个节点都有VIP

# 集群状态检查脚本
cat > check_split_brain.sh << 'EOF'
#!/bin/bash
VIP="192.168.1.100"
for host in node1 node2 node3; do
    echo "=== $host ==="
    ssh $host "ip addr show | grep $VIP && echo 'HAS VIP' || echo 'NO VIP'"
done
EOF
```

### 5.3 网络分区诊断


**🌐 网络连通性检查**
```bash
# 节点间网络连通性测试
for node in node1 node2 node3; do
    echo "Testing connectivity to $node:"
    ping -c 3 $node
    telnet $node 22 < /dev/null
done

# 检查网络分区情况
# 创建网络拓扑检测脚本
cat > network_partition_test.sh << 'EOF'
#!/bin/bash
NODES=("node1" "node2" "node3")

echo "网络连通性矩阵："
printf "%-10s" "From\\To"
for node in "${NODES[@]}"; do
    printf "%-10s" "$node"
done
echo

for from_node in "${NODES[@]}"; do
    printf "%-10s" "$from_node"
    for to_node in "${NODES[@]}"; do
        if [ "$from_node" = "$to_node" ]; then
            printf "%-10s" "SELF"
        else
            result=$(ssh $from_node "ping -c 1 -W 2 $to_node >/dev/null 2>&1 && echo OK || echo FAIL")
            printf "%-10s" "$result"
        fi
    done
    echo
done
EOF
```

### 5.4 脑裂预防机制


**🛡️ 预防和恢复策略**
```bash
# Keepalived防脑裂配置
# /etc/keepalived/keepalived.conf
vrrp_script chk_mysql {
    script "/usr/local/bin/check_mysql.sh"
    interval 2
    weight -20
    fall 3
    rise 2
}

vrrp_instance VI_1 {
    state BACKUP          # 所有节点都配置为BACKUP
    interface eth0
    virtual_router_id 51
    priority 100          # 优先级通过其他方式动态调整
    advert_int 1
    
    track_script {
        chk_mysql
    }
}

# MySQL双主防脑裂配置
# 在my.cnf中添加
auto_increment_increment = 2    # 主键自增步长
auto_increment_offset = 1       # 节点1偏移量为1，节点2为2
```

---

## 6. 🧰 内存泄漏排查


### 6.1 内存泄漏识别


**🔸 内存泄漏症状**
```
系统层面表现：
- 可用内存持续减少
- swap使用率不断上升
- 系统响应越来越慢
- 最终导致OOM killer触发

应用层面表现：
- 进程内存占用持续增长
- 垃圾回收频率增加但效果不佳
- 应用性能逐渐下降
```

### 6.2 系统内存使用分析


**📊 内存使用情况监控**
```bash
# 实时内存监控
free -h -s 5  # 每5秒显示一次内存使用情况

# 详细内存信息
cat /proc/meminfo | head -20

# 按内存使用排序进程
ps aux --sort=-%mem | head -10

# 查看系统内存分配详情
cat /proc/buddyinfo     # 伙伴系统内存分配
cat /proc/slabinfo      # slab分配器信息（需要root权限）

# 内存使用趋势监控脚本
cat > memory_monitor.sh << 'EOF'
#!/bin/bash
while true; do
    timestamp=$(date '+%Y-%m-%d %H:%M:%S')
    mem_total=$(free | grep Mem | awk '{print $2}')
    mem_used=$(free | grep Mem | awk '{print $3}')
    mem_free=$(free | grep Mem | awk '{print $4}')
    mem_percent=$(echo "scale=2; $mem_used/$mem_total*100" | bc)
    
    echo "$timestamp,Total:${mem_total}KB,Used:${mem_used}KB,Free:${mem_free}KB,Usage:${mem_percent}%" >> memory_usage.log
    sleep 60
done
EOF
```

### 6.3 进程级内存分析


**🔍 具体进程内存诊断**
```bash
# 查看特定进程的内存映射
PID=$(pgrep nginx | head -1)
cat /proc/$PID/smaps | grep -E "(Size|Rss|Pss)"

# 进程内存使用详情
cat /proc/$PID/status | grep -E "(VmSize|VmRSS|VmHWM|VmData|VmStk|VmLib)"

# 使用valgrind检测内存泄漏（开发环境）
valgrind --tool=memcheck --leak-check=full --show-leak-kinds=all ./your_program

# 使用pmap查看进程内存映射
pmap -d $PID  # 显示详细内存映射信息
```

### 6.4 应用程序内存分析


**💻 不同语言的内存分析工具**
```bash
# Java应用内存分析
jps                    # 列出Java进程
jstat -gc $PID 5s      # 每5秒显示GC情况
jmap -histo $PID       # 显示堆内对象统计
jmap -dump:format=b,file=heap.dump $PID  # 生成堆转储文件

# Python应用内存分析  
# 使用memory_profiler
pip install memory-profiler
python -m memory_profiler your_script.py

# 或在代码中添加内存监控
@profile
def your_function():
    # 函数代码

# C/C++应用内存分析
# 编译时添加调试信息
gcc -g -O0 program.c -o program
# 使用gdb + 内存检查
gdb ./program
(gdb) run
(gdb) info proc mappings  # 查看内存映射
```

### 6.5 内存泄漏修复策略


**🔧 修复和预防措施**
```bash
# 临时缓解措施
# 定期重启有内存泄漏的服务
cat > restart_service.sh << 'EOF'
#!/bin/bash
SERVICE_NAME="nginx"
MEMORY_THRESHOLD=80  # 内存使用率超过80%就重启

current_mem=$(ps -o pid,ppid,cmd,%mem -C $SERVICE_NAME | awk 'NR>1 {sum+=$4} END {print sum}')
if (( $(echo "$current_mem > $MEMORY_THRESHOLD" | bc -l) )); then
    echo "$(date): Memory usage ${current_mem}% exceeds threshold, restarting $SERVICE_NAME"
    systemctl restart $SERVICE_NAME
fi
EOF

# 配置定时任务
echo "0 */4 * * * /path/to/restart_service.sh" | crontab -

# 应用层面的内存管理
# 设置Java堆内存限制
export JAVA_OPTS="-Xmx2g -Xms2g -XX:+UseG1GC"

# 设置PHP内存限制
echo "memory_limit = 256M" >> /etc/php.ini

# 数据库连接池配置
# MySQL连接池设置
max_connections = 100
wait_timeout = 300
interactive_timeout = 300
```

---

## 7. 🌐 网络问题排查


### 7.1 网络问题分类


**🔸 网络故障类型**
```
连通性问题：
- 完全无法连接
- 间歇性连接中断
- 特定端口无法访问

性能问题：
- 网络延迟过高
- 带宽利用率异常
- 丢包率偏高

配置问题：
- 路由配置错误
- 防火墙规则问题
- DNS解析异常
```

### 7.2 基础网络诊断


**🔧 网络连通性测试**
```bash
# 基础连通性测试
ping -c 4 8.8.8.8        # 测试外网连通性
ping -c 4 192.168.1.1    # 测试网关连通性
ping -c 4 localhost      # 测试本机网络栈

# DNS解析测试
nslookup google.com
dig google.com
host google.com

# 路由跟踪
traceroute google.com
mtr --report-cycles=10 google.com  # 更详细的路由分析

# 端口连通性测试
telnet google.com 80
nc -zv google.com 80     # 使用netcat测试
timeout 5 bash -c "</dev/tcp/google.com/80" && echo "Port 80 is open"
```

### 7.3 网络性能分析


**📈 网络性能监控**
```bash
# 网络接口统计信息
cat /proc/net/dev        # 网络接口统计
ip -s link show          # 显示接口统计信息

# 实时网络流量监控
iftop                    # 实时流量监控
nethogs                 # 按进程显示网络使用
ss -i                   # 显示socket统计信息

# 网络带宽测试
# 使用iperf测试带宽
# 服务端：
iperf3 -s
# 客户端：
iperf3 -c server_ip -t 30

# 网络延迟测试
ping -i 0.2 -c 50 target_host | tail -1  # 计算平均延迟
```

### 7.4 防火墙和安全诊断


**🛡️ 防火墙规则检查**
```bash
# iptables规则查看
iptables -L -n -v        # 显示所有规则
iptables -L INPUT -n -v  # 显示INPUT链规则
iptables -L OUTPUT -n -v # 显示OUTPUT链规则

# firewalld状态检查（CentOS/RHEL 7+）
firewall-cmd --state
firewall-cmd --list-all
firewall-cmd --list-ports
firewall-cmd --list-services

# ufw状态检查（Ubuntu）
ufw status verbose
ufw show raw

# 检查是否有封禁IP
iptables -L | grep DROP
fail2ban-client status   # 如果使用fail2ban
```

### 7.5 网络服务诊断


**🔌 网络服务状态检查**
```bash
# 查看监听端口
netstat -tulpn | grep LISTEN
ss -tulpn | grep LISTEN

# 查看网络连接状态分布
netstat -an | awk '/^tcp/ {++state[$NF]} END {for(key in state) print key,"\t",state[key]}'

# 检查网络服务配置
# SSH服务
sshd -T | grep -E "(Port|ListenAddress|PermitRootLogin)"

# Web服务
nginx -t              # 检查Nginx配置
apache2ctl configtest # 检查Apache配置

# 数据库服务
mysql -e "SHOW VARIABLES LIKE 'bind_address';"
mysql -e "SHOW VARIABLES LIKE 'port';"
```

---

## 8. 🔄 综合排查流程


### 8.1 标准排查流程


**📋 系统化排查步骤**
```
第一阶段：问题定义和信息收集
1. 明确故障现象和影响范围
2. 收集用户反馈和错误信息
3. 确定故障发生时间和频率
4. 了解最近的系统变更

第二阶段：快速诊断
1. 检查系统基础状态（CPU、内存、磁盘、网络）
2. 验证服务运行状态
3. 查看最新的系统日志和错误日志
4. 进行基础的连通性测试

第三阶段：深入分析
1. 根据初步诊断结果制定详细排查计划
2. 使用专业工具进行深入分析
3. 复现问题或收集更多证据
4. 分析根本原因

第四阶段：解决和验证
1. 制定解决方案
2. 在测试环境验证修复方案
3. 实施修复措施
4. 验证问题解决且无副作用
```

### 8.2 快速排查清单


**⚡ 5分钟快速检查清单**
```bash
# 创建快速诊断脚本
cat > quick_diagnosis.sh << 'EOF'
#!/bin/bash
echo "=== 系统快速诊断报告 ==="
echo "时间: $(date)"
echo

echo "1. 系统负载:"
uptime
echo

echo "2. 内存使用:"
free -h
echo

echo "3. 磁盘空间:"
df -h | grep -E "(Filesystem|/dev/)"
echo

echo "4. 网络连接数:"
ss -s
echo

echo "5. 最近错误日志 (最后10行):"
journalctl -p err -n 10 --no-pager
echo

echo "6. 进程CPU占用TOP5:"
ps aux --sort=-%cpu | head -6
echo

echo "7. 进程内存占用TOP5:"
ps aux --sort=-%mem | head -6
echo

echo "=== 诊断完成 ==="
EOF

chmod +x quick_diagnosis.sh
```

### 8.3 日志分析策略


**📝 日志分析技巧**
```bash
# 系统日志关键字搜索
journalctl -p err -S today        # 今天的错误日志
journalctl -u nginx -S "1 hour ago"  # 最近1小时nginx日志

# 应用日志分析
# 错误计数统计
grep "ERROR" /var/log/application.log | wc -l

# 按时间段分析错误
grep "$(date '+%Y-%m-%d %H')" /var/log/application.log | grep "ERROR"

# 日志分析脚本
cat > log_analyzer.sh << 'EOF'
#!/bin/bash
LOG_FILE="$1"
if [[ -z "$LOG_FILE" ]]; then
    echo "Usage: $0 <log_file>"
    exit 1
fi

echo "=== 日志文件分析: $LOG_FILE ==="
echo "总行数: $(wc -l < $LOG_FILE)"
echo "错误数量: $(grep -i error $LOG_FILE | wc -l)"
echo "警告数量: $(grep -i warn $LOG_FILE | wc -l)"
echo

echo "最频繁的错误类型:"
grep -i error $LOG_FILE | cut -d' ' -f4- | sort | uniq -c | sort -rn | head -5
echo

echo "最近10条错误:"
grep -i error $LOG_FILE | tail -10
EOF
```

---

## 9. 🛡️ 预防与监控策略


### 9.1 监控系统搭建


**📊 关键指标监控**
```bash
# 系统资源监控脚本
cat > system_monitor.sh << 'EOF'
#!/bin/bash
# 系统监控脚本，收集关键指标

LOGFILE="/var/log/system_monitor.log"
TIMESTAMP=$(date '+%Y-%m-%d %H:%M:%S')

# CPU使用率
CPU_USAGE=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | sed 's/%us,//')

# 内存使用率  
MEM_USAGE=$(free | grep Mem | awk '{printf("%.1f", $3/$2 * 100.0)}')

# 磁盘使用率
DISK_USAGE=$(df -h / | awk 'NR==2 {print $5}' | sed 's/%//')

# 负载平均值
LOAD_AVG=$(uptime | awk -F'load average:' '{print $2}' | sed 's/^ *//')

# 网络连接数
CONN_COUNT=$(ss -s | grep TCP | head -1 | awk '{print $2}')

# 记录到日志
echo "$TIMESTAMP,CPU:${CPU_USAGE}%,MEM:${MEM_USAGE}%,DISK:${DISK_USAGE}%,LOAD:$LOAD_AVG,CONN:$CONN_COUNT" >> $LOGFILE

# 检查阈值并告警
if (( $(echo "$CPU_USAGE > 80" | bc -l) )); then
    echo "$(date): 高CPU使用率告警: ${CPU_USAGE}%" | mail -s "CPU Alert" admin@company.com
fi

if (( $(echo "$MEM_USAGE > 85" | bc -l) )); then
    echo "$(date): 高内存使用率告警: ${MEM_USAGE}%" | mail -s "Memory Alert" admin@company.com
fi
EOF

# 设置定时执行
echo "*/5 * * * * /path/to/system_monitor.sh" | crontab -
```

### 9.2 自动故障处理


**🤖 自动化修复脚本**
```bash
# 服务自动重启脚本
cat > auto_service_recovery.sh << 'EOF'
#!/bin/bash
# 服务健康检查和自动恢复

SERVICES=("nginx" "mysql" "redis")
LOG_FILE="/var/log/service_recovery.log"

check_and_restart() {
    local service=$1
    
    if ! systemctl is-active --quiet $service; then
        echo "$(date): $service is not running, attempting to restart..." >> $LOG_FILE
        
        systemctl start $service
        sleep 5
        
        if systemctl is-active --quiet $service; then
            echo "$(date): $service successfully restarted" >> $LOG_FILE
            # 发送成功通知
            echo "$service has been automatically restarted on $(hostname)" | \
                mail -s "$service Auto-Recovery Success" admin@company.com
        else
            echo "$(date): Failed to restart $service" >> $LOG_FILE
            # 发送失败告警
            echo "CRITICAL: Failed to auto-restart $service on $(hostname)" | \
                mail -s "CRITICAL: $service Auto-Recovery Failed" admin@company.com
        fi
    fi
}

for service in "${SERVICES[@]}"; do
    check_and_restart $service
done
EOF

# 设置定时检查
echo "*/2 * * * * /path/to/auto_service_recovery.sh" | crontab -
```

### 9.3 预警阈值设置


**⚠️ 智能告警配置**
```bash
# 多级告警系统
cat > intelligent_alerting.sh << 'EOF'
#!/bin/bash
# 智能告警系统

# 阈值定义
CPU_WARNING=70
CPU_CRITICAL=85
MEM_WARNING=75  
MEM_CRITICAL=90
DISK_WARNING=80
DISK_CRITICAL=95

# 获取当前状态
CPU_CURRENT=$(top -bn1 | grep "Cpu(s)" | awk '{print $2}' | sed 's/%us,//' | cut -d. -f1)
MEM_CURRENT=$(free | grep Mem | awk '{printf("%.0f", $3/$2 * 100.0)}')
DISK_CURRENT=$(df -h / | awk 'NR==2 {print $5}' | sed 's/%//')

send_alert() {
    local level=$1
    local type=$2
    local current=$3
    local threshold=$4
    
    case $level in
        "WARNING")
            echo "WARNING: $type usage is ${current}% (threshold: ${threshold}%)" | \
                mail -s "[$level] $type Alert - $(hostname)" admin@company.com
            ;;
        "CRITICAL")
            echo "CRITICAL: $type usage is ${current}% (threshold: ${threshold}%)" | \
                mail -s "[$level] $type Alert - $(hostname)" admin@company.com
            # 同时发送短信告警（需要配置短信网关）
            # curl -X POST "http://sms-gateway/send" -d "message=CRITICAL $type alert on $(hostname)"
            ;;
    esac
}

# CPU检查
if [ $CPU_CURRENT -ge $CPU_CRITICAL ]; then
    send_alert "CRITICAL" "CPU" $CPU_CURRENT $CPU_CRITICAL
elif [ $CPU_CURRENT -ge $CPU_WARNING ]; then
    send_alert "WARNING" "CPU" $CPU_CURRENT $CPU_WARNING
fi

# 内存检查  
if [ $MEM_CURRENT -ge $MEM_CRITICAL ]; then
    send_alert "CRITICAL" "Memory" $MEM_CURRENT $MEM_CRITICAL
elif [ $MEM_CURRENT -ge $MEM_WARNING ]; then
    send_alert "WARNING" "Memory" $MEM_CURRENT $MEM_WARNING
fi

# 磁盘检查
if [ $DISK_CURRENT -ge $DISK_CRITICAL ]; then
    send_alert "CRITICAL" "Disk" $DISK_CURRENT $DISK_CRITICAL
elif [ $DISK_CURRENT -ge $DISK_WARNING ]; then
    send_alert "WARNING" "Disk" $DISK_CURRENT $DISK_WARNING
fi
EOF
```

---

## 10. 📋 核心要点总结


### 10.1 必须掌握的排查技能


```
🔸 系统化诊断思路：现象→信息→假设→验证→修复
🔸 基础工具使用：top、ps、netstat、ss、journalctl等
🔸 日志分析能力：快速定位关键错误信息
🔸 网络诊断技能：ping、traceroute、telnet、tcpdump
🔸 性能分析方法：资源瓶颈识别和性能优化
```

### 10.2 故障排查关键原则


**🔹 排查效率原则**
```
优先级排序：
1. 影响用户最多的问题优先
2. 容易修复的问题优先  
3. 根本原因 > 表面症状

信息收集：
- 准确描述问题现象
- 收集完整的错误日志
- 了解问题发生的时间线
- 确认最近的系统变更
```

**🔹 安全第一原则**
```
修复前的准备：
✅ 备份重要配置文件
✅ 确认修复步骤的回退方案
✅ 在测试环境先行验证
✅ 通知相关人员维护窗口

修复过程中：
✅ 一次只改一个配置
✅ 每次修改后都要验证效果
✅ 详细记录所有操作步骤
✅ 保持与团队的沟通
```

### 10.3 常见问题快速定位


**⚡ 问题类型与定位策略**
```
连接超时问题：
检查顺序：网络连通性 → 服务监听状态 → 防火墙规则 → 应用配置

性能下降问题：  
检查顺序：系统资源 → 进程状态 → 网络状况 → 应用日志

间歇性故障：
检查重点：定时任务 → 资源竞争 → 网络抖动 → 内存泄漏

服务无响应：
检查顺序：进程存活 → 端口监听 → 资源充足 → 配置正确
```

### 10.4 预防性维护建议


**💡 故障预防策略**
```
监控体系：
- 建立完善的系统监控
- 设置合理的告警阈值  
- 定期检查监控系统本身

日常维护：
- 定期查看系统日志
- 及时更新系统补丁
- 清理无用文件和日志
- 验证备份的完整性

文档管理：
- 维护系统配置文档
- 记录常见问题的解决方案
- 建立故障处理知识库
- 定期更新操作手册
```

**核心记忆要点**：
- 故障排查需要系统性思维和耐心
- 工具熟练度决定排查效率
- 日志是故障诊断的重要线索
- 预防胜于治疗，监控是预防的基础
- 每次故障都是学习和改进的机会