---
title: 2、性能问题排查
---
## 📚 目录

1. [性能问题排查概述](#1-性能问题排查概述)
2. [响应延迟问题](#2-响应延迟问题)
3. [CPU使用率过高](#3-CPU使用率过高)
4. [内存使用率过高](#4-内存使用率过高)
5. [网络IO瓶颈](#5-网络IO瓶颈)
6. [数据库连接问题](#6-数据库连接问题)
7. [第三方服务依赖问题](#7-第三方服务依赖问题)
8. [核心要点总结](#8-核心要点总结)

---

## 1. 🔍 性能问题排查概述


### 1.1 什么是性能问题


**通俗理解**：就像你去餐厅吃饭，如果上菜很慢、厨房太忙、服务员不够用，这就是"性能问题"。API网关也一样，当它处理请求变慢、资源不够用时，就出现了性能问题。

**专业定义**：
- **性能问题**：系统在处理请求时，响应时间过长、资源利用率异常或吞吐量下降的现象
- **核心指标**：响应时间(RT)、吞吐量(TPS/QPS)、资源使用率(CPU/内存/网络)

### 1.2 性能问题的常见表现


```
用户视角：
👤 "怎么这么慢？" → 响应延迟高
👤 "一直转圈圈"   → 请求超时
👤 "突然就卡住了" → 系统假死

技术视角：
📊 接口响应时间从100ms飙升到5s
📊 CPU使用率持续90%以上
📊 内存占用不断增长
📊 网络连接数异常增多
```

### 1.3 排查问题的基本思路


**🎯 黄金排查法则**：

```
问题定位流程：

1️⃣ 确认现象 → 到底慢在哪里？
   ├─ 是所有接口都慢？
   ├─ 还是特定接口慢？
   └─ 什么时候开始慢的？

2️⃣ 收集证据 → 用数据说话
   ├─ 查看监控指标
   ├─ 分析日志记录
   └─ 抓取性能快照

3️⃣ 分析原因 → 找到根本问题
   ├─ 是资源不足？
   ├─ 是配置不当？
   └─ 是代码问题？

4️⃣ 解决问题 → 对症下药
   └─ 优化配置/扩容/修复代码
```

---

## 2. ⏱️ 响应延迟问题


### 2.1 什么是响应延迟


**生活化理解**：
- 你点外卖，从下单到送达的时间就是"延迟"
- 正常30分钟送到 = 低延迟 ✅
- 等了2小时才送到 = 高延迟 ❌

**技术定义**：
- **响应延迟(Latency)**：从发送请求到收到响应的总耗时
- **正常延迟**：通常在100-500ms以内
- **高延迟**：超过1秒就需要关注了

### 2.2 延迟问题的定位方法


**📊 延迟分析图解**：

```
客户端 → 网关 → 服务A → 数据库
  ↓      ↓      ↓       ↓
 10ms   50ms   800ms   200ms
        └────┬─────┘
          网关延迟 = 50ms
        └──────────┬──────────┘
          后端服务延迟 = 1000ms
└──────────────┬──────────────┘
        总延迟 = 1060ms

问题在哪？→ 服务A太慢了！
```

**🔍 排查步骤**：

**第一步：看监控数据**
```
查看延迟分布：
├─ P50延迟：50%的请求在这个时间内完成
├─ P95延迟：95%的请求在这个时间内完成
└─ P99延迟：99%的请求在这个时间内完成

例如：
P50 = 100ms  → 一半请求很快
P95 = 500ms  → 大部分请求还行
P99 = 3000ms → 少数请求很慢(这是问题！)
```

**第二步：分段测试**

| 测试环节 | 正常耗时 | 实际耗时 | 是否异常 |
|---------|---------|---------|---------|
| **网络传输** | `< 10ms` | `8ms` | ✅ 正常 |
| **网关处理** | `< 50ms` | `45ms` | ✅ 正常 |
| **服务调用** | `< 200ms` | `2500ms` | ❌ **异常** |
| **数据库查询** | `< 100ms` | `95ms` | ✅ 正常 |

**第三步：定位具体原因**

> 💡 **排查技巧**  
> 像剥洋葱一样，一层层排除：先看是不是网关的问题，再看是不是后端服务的问题

### 2.3 常见延迟问题及解决方案


**🔸 问题1：慢SQL查询**

```
现象：某个接口突然变慢
原因：数据库查询没有索引

解决方法：
1. 找出慢查询日志
2. 分析SQL执行计划
3. 添加合适的索引

优化前：SELECT * FROM orders WHERE user_id = 123  (2秒)
优化后：在user_id上建索引  (50ms)
```

**🔸 问题2：外部API调用超时**

```
场景：调用第三方支付接口
问题：第三方接口响应慢

应对策略：
✅ 设置合理超时时间（如3秒）
✅ 实现超时重试机制
✅ 考虑异步处理
✅ 添加熔断保护
```

**🔸 问题3：网关自身处理慢**

```
常见原因：
├─ 过滤器链太长（10+个过滤器）
├─ 日志打印过多
├─ JSON序列化/反序列化慢
└─ 路由规则复杂

优化方向：
├─ 简化过滤器逻辑
├─ 异步处理日志
├─ 使用高效的序列化工具
└─ 优化路由匹配算法
```

---

## 3. 🔥 CPU使用率过高


### 3.1 CPU使用率是什么


**形象比喻**：
- CPU就像餐厅的厨师
- 使用率30% = 厨师很轻松，有空闲
- 使用率80% = 厨师忙碌但应付得来
- 使用率95% = 厨师累瘫了，要出问题

**技术解释**：
- **CPU使用率**：处理器工作时间占总时间的百分比
- **正常范围**：30%-70%
- **警戒值**：持续超过80%需要关注
- **危险值**：持续90%以上系统会变慢

### 3.2 CPU过高的表现


```
系统症状：
🔴 接口响应变慢
🔴 请求排队等待
🔴 系统负载(Load)飙高
🔴 服务器风扇狂转

监控图表：
CPU使用率
100% |         ████████████
 80% |    ████████████████
 60% |  ██████████████████
 40% |████████████████████
 20% |████████████████████
  0% |████████████████████
      └────────────────────
       正常  →  异常时段
```

### 3.3 CPU过高的排查步骤


**🔍 第一步：确认CPU占用高的进程**

```bash
# 查看哪个进程CPU高
top -c

# 输出示例：
PID  USER  CPU  MEM  COMMAND
1234 root  95%  20%  java -jar gateway.jar
```

> ⚠️ **新手提示**  
> `top`命令可以实时显示进程状态，按`P`键按CPU排序，很容易找到"罪魁祸首"

**🔍 第二步：找出CPU高的线程**

```bash
# 1. 找到Java进程ID(假设是1234)
jps

# 2. 查看这个进程的线程CPU使用情况
top -H -p 1234

# 输出：
PID   CPU  
1245  80%  ← 这个线程CPU特别高！
1246  10%
1247  5%
```

**🔍 第三步：分析线程在做什么**

```bash
# 1. 将线程ID转换为16进制
printf "%x\n" 1245
# 输出：4dd

# 2. 查看线程堆栈
jstack 1234 | grep -A 20 4dd

# 看到堆栈信息，就知道这个线程在执行什么代码了
```

### 3.4 常见CPU过高原因


**🔸 原因1：死循环或无限递归**

```java
// ❌ 错误示例：忘记退出条件
public void processData() {
    while(true) {  // 死循环！CPU会100%
        // 没有break或sleep
        doSomething();
    }
}

// ✅ 正确做法：
public void processData() {
    while(!stopped) {
        doSomething();
        Thread.sleep(100); // 让CPU休息一下
    }
}
```

**🔸 原因2：大量正则表达式匹配**

```java
// ❌ 性能杀手
String pattern = "(a+)+b";
// 对复杂字符串匹配时，会导致回溯爆炸

// ✅ 优化方法
// 1. 简化正则表达式
// 2. 预编译Pattern对象
// 3. 考虑用其他方式替代
```

**🔸 原因3：频繁GC（垃圾回收）**

```
现象：CPU使用率高，但应用没干啥活

原因：
├─ 创建了大量临时对象
├─ 内存不足，频繁Full GC
└─ GC线程占用大量CPU

查看GC情况：
jstat -gc 1234 1000

如果看到Full GC频繁发生，就是这个问题！
```

### 3.5 CPU过高的解决方案


| 问题类型 | 解决方案 | 效果 |
|---------|---------|------|
| **死循环** | 修复代码逻辑，添加退出条件 | ⭐⭐⭐⭐⭐ |
| **算法复杂** | 优化算法，降低时间复杂度 | ⭐⭐⭐⭐ |
| **频繁GC** | 优化对象创建，调整堆内存 | ⭐⭐⭐⭐ |
| **并发不足** | 增加线程池大小 | ⭐⭐⭐ |
| **资源不足** | 扩容服务器 | ⭐⭐⭐ |

---

## 4. 💾 内存使用率过高


### 4.1 内存问题基础知识


**生活化类比**：
- 内存就像你的工作桌面
- 桌面大 = 内存充足，可以同时做很多事
- 桌面小 = 内存不足，只能一样一样来
- 桌面堆满垃圾 = 内存泄漏，越用越慢

**Java内存结构图**：

```
JVM内存布局
┌─────────────────────────────┐
│    堆内存 (Heap)              │
│  ┌──────────┬──────────┐    │
│  │ 年轻代    │  老年代   │    │
│  │(新对象)   │ (老对象)  │    │
│  └──────────┴──────────┘    │
├─────────────────────────────┤
│    方法区 (Metaspace)         │
│    (类信息、常量池)           │
├─────────────────────────────┤
│    栈内存 (Stack)             │
│    (方法调用、局部变量)        │
└─────────────────────────────┘
```

### 4.2 内存问题的表现


**🚨 典型症状**：

```
初期症状：
├─ 应用启动慢
├─ 响应时间变长
└─ 偶尔出现卡顿

中期症状：
├─ 频繁Full GC
├─ CPU使用率波动大
└─ 接口超时增多

严重症状：
├─ OutOfMemoryError错误
├─ 应用直接崩溃
└─ 服务器无响应
```

> ⚠️ **危险信号**  
> 如果看到`java.lang.OutOfMemoryError: Java heap space`这个错误，说明内存已经爆了！

### 4.3 内存问题排查工具


**🔧 工具1：查看内存使用情况**

```bash
# 查看JVM内存使用
jmap -heap 1234

# 输出示例：
Heap Usage:
New Generation (Eden + 1 Survivor Space):
   capacity = 512MB
   used     = 480MB  ← 已用内存
   free     = 32MB   ← 剩余很少了！

Old Generation:
   capacity = 1024MB
   used     = 950MB  ← 快满了！
   free     = 74MB
```

**🔧 工具2：生成内存快照**

```bash
# 生成堆转储文件(Heap Dump)
jmap -dump:format=b,file=heap.hprof 1234

# 用工具分析(推荐Eclipse MAT或VisualVM)
# 可以看到哪些对象占用内存最多
```

**🔧 工具3：在线监控内存**

```bash
# 实时监控内存变化
jstat -gcutil 1234 1000

# 输出：
S0    S1    E     O     M      YGC  FGC
0.00  95.2  78.5  89.3  95.6   245  12
                  ↑
            老年代使用率89%，危险！
```

### 4.4 内存泄漏常见场景


**🔸 场景1：集合类没有清理**

```java
// ❌ 内存泄漏代码
public class CacheManager {
    // 这个Map会无限增长！
    private static Map<String, Object> cache = new HashMap<>();
    
    public void addCache(String key, Object value) {
        cache.put(key, value);
        // 永远不删除，内存迟早爆掉
    }
}

// ✅ 正确做法：设置大小限制
private static Map<String, Object> cache = 
    new LinkedHashMap<String, Object>(100, 0.75f, true) {
        protected boolean removeEldestEntry(Map.Entry eldest) {
            return size() > 100; // 超过100个就删除最老的
        }
    };
```

**🔸 场景2：线程池没有关闭**

```java
// ❌ 每次创建新线程池
public void processRequest() {
    ExecutorService executor = Executors.newFixedThreadPool(10);
    executor.execute(() -> doWork());
    // 忘记关闭，线程会一直存在
}

// ✅ 复用线程池
private static ExecutorService executor = 
    Executors.newFixedThreadPool(10);
    
public void processRequest() {
    executor.execute(() -> doWork());
}
```

**🔸 场景3：数据库连接没释放**

```java
// ❌ 连接泄漏
Connection conn = dataSource.getConnection();
// 使用连接...
// 忘记关闭，连接池会耗尽

// ✅ 使用try-with-resources自动关闭
try (Connection conn = dataSource.getConnection()) {
    // 使用连接
} // 自动关闭，不会泄漏
```

### 4.5 内存优化建议


**📊 内存配置优化**：

| 参数 | 说明 | 推荐值 | 理由 |
|-----|------|-------|------|
| `-Xms` | 初始堆内存 | `2G` | 避免频繁扩容 |
| `-Xmx` | 最大堆内存 | `2G` | 与Xms相同，避免动态调整 |
| `-XX:MetaspaceSize` | 元空间初始大小 | `256M` | 够用就行 |
| `-XX:MaxMetaspaceSize` | 元空间最大值 | `512M` | 防止无限增长 |

> 💡 **内存设置经验**  
> 堆内存一般设置为服务器物理内存的60%-70%，留一些给操作系统和其他进程

---

## 5. 🌐 网络IO瓶颈


### 5.1 什么是网络IO瓶颈


**通俗解释**：
- 网络就像高速公路
- IO就是车流量
- 瓶颈就是堵车了

**技术定义**：
- **网络IO**：网络输入输出操作，包括接收和发送数据
- **瓶颈**：网络传输能力无法满足业务需求，导致性能下降
- **表现**：连接数过多、带宽占满、网络延迟高

### 5.2 网络问题的诊断


**🔍 网络状态检查命令**：

```bash
# 查看网络连接状态
netstat -an | grep :8080

# 输出示例：
tcp    0    0  0.0.0.0:8080    0.0.0.0:*    LISTEN
tcp    0    0  192.168.1.1:8080  192.168.1.2:45678  ESTABLISHED
tcp    0    0  192.168.1.1:8080  192.168.1.3:45679  TIME_WAIT

# 统计各种连接状态数量
netstat -an | awk '/^tcp/ {print $6}' | sort | uniq -c

# 输出：
2000 ESTABLISHED  ← 活跃连接
500  TIME_WAIT    ← 等待关闭的连接
50   CLOSE_WAIT   ← 异常状态！太多了
```

**📊 网络连接状态解释**：

| 状态 | 含义 | 正常情况 | 异常情况 |
|-----|------|---------|---------|
| `ESTABLISHED` | 正常连接 | 业务量决定 | 突然暴增 |
| `TIME_WAIT` | 等待关闭 | 少量正常 | 数千个异常 |
| `CLOSE_WAIT` | 等待应用关闭 | 几乎没有 | **出现就是问题** |
| `SYN_RECV` | 等待确认 | 瞬时状态 | 持续存在=被攻击 |

### 5.3 常见网络问题


**🔸 问题1：连接数过多**

```
现象：
├─ ESTABLISHED连接数上万
├─ 新请求无法建立连接
└─ 错误：Too many open files

原因分析：
客户端 → [大量连接] → 网关 → [大量连接] → 后端服务
                        ↑
                    连接没有复用！

解决方案：
✅ 启用HTTP Keep-Alive
✅ 使用连接池
✅ 设置合理的连接超时
```

**🔸 问题2：TIME_WAIT过多**

```
TIME_WAIT状态产生原因：
客户端                  服务端
   │                      │
   │────[FIN]─────────────>│  1. 客户端关闭
   │<────[ACK]─────────────│  2. 服务端确认
   │<────[FIN]─────────────│  3. 服务端关闭  
   │────[ACK]─────────────>│  4. 客户端确认
   │                      │
   └── TIME_WAIT(等2分钟) ─┘  ← 这期间端口被占用

大量TIME_WAIT的危害：
├─ 占用端口资源(每个连接占一个端口)
├─ 新连接无法建立
└─ 系统响应变慢

优化方法：
# 调整系统参数(Linux)
net.ipv4.tcp_tw_reuse = 1     # 复用TIME_WAIT状态的socket
net.ipv4.tcp_fin_timeout = 30  # 减少等待时间
```

**🔸 问题3：带宽打满**

```
带宽监控：
# 查看网卡流量
sar -n DEV 1

# 输出示例：
Interface  rxpck/s  txpck/s  rxkB/s  txkB/s
eth0       50000    50000    90000   90000
                              ↑       ↑
                        接近网卡极限！(100MB带宽)

常见原因：
├─ 返回数据量过大(如大文件、大列表)
├─ 没有启用压缩
└─ 大量并发请求

优化策略：
✅ 启用GZIP压缩
✅ 分页查询，不要一次返回所有数据  
✅ 使用CDN分流静态资源
✅ 考虑升级带宽
```

### 5.4 网络IO优化方案


**🚀 优化1：启用HTTP Keep-Alive**

```yaml
# Spring Cloud Gateway配置
spring:
  cloud:
    gateway:
      httpclient:
        pool:
          type: FIXED  # 使用固定大小连接池
          max-connections: 500  # 最大连接数
          max-idle-time: 30s   # 空闲超时
        wiretap: true  # 启用连接重用
```

**🚀 优化2：响应数据压缩**

```yaml
# 启用GZIP压缩
server:
  compression:
    enabled: true
    min-response-size: 1024  # 大于1KB才压缩
    mime-types:
      - application/json
      - text/html
      - text/plain
```

**🚀 优化3：连接超时设置**

```yaml
spring:
  cloud:
    gateway:
      httpclient:
        connect-timeout: 3000  # 连接超时3秒
        response-timeout: 5s   # 响应超时5秒
```

---

## 6. 🗄️ 数据库连接问题


### 6.1 数据库连接基础


**形象比喻**：
- 数据库连接就像餐厅座位
- 连接池就像等位区
- 座位有限，客人太多就要排队等

**核心概念**：
- **连接池**：预先创建一批数据库连接，重复使用
- **连接数**：同时可以使用的数据库连接数量
- **连接泄漏**：连接用完不归还，导致连接池耗尽

### 6.2 数据库连接池配置


**📊 HikariCP连接池配置详解**：

```yaml
spring:
  datasource:
    hikari:
      # 核心参数
      maximum-pool-size: 20        # 最大连接数
      minimum-idle: 5              # 最小空闲连接
      connection-timeout: 30000    # 获取连接超时(毫秒)
      idle-timeout: 600000         # 空闲连接存活时间(10分钟)
      max-lifetime: 1800000        # 连接最大生命周期(30分钟)
      
      # 性能优化参数
      connection-test-query: SELECT 1  # 连接测试查询
      validation-timeout: 3000     # 验证超时
      leak-detection-threshold: 60000  # 连接泄漏检测(1分钟)
```

**🔧 参数设置原则**：

| 参数 | 推荐值 | 说明 |
|-----|-------|------|
| `maximum-pool-size` | CPU核心数 × 2 | 太大浪费资源，太小影响性能 |
| `minimum-idle` | 5-10 | 保持一定数量避免冷启动 |
| `connection-timeout` | 30秒 | 超时时间不要太长 |
| `max-lifetime` | 30分钟 | 定期更新连接，避免数据库端超时 |

> 💡 **经验值**  
> 4核CPU的服务器，连接池大小设置为8-10比较合适

### 6.3 连接池问题排查


**🚨 问题1：连接池耗尽**

```
错误信息：
HikariPool-1 - Connection is not available, 
request timed out after 30000ms.

这是什么意思？
简单说：连接池里没有可用连接了，等了30秒还是拿不到

可能原因：
┌─────────────────────────────┐
│ 1. 并发请求太多，连接不够用  │
│ 2. SQL执行太慢，连接一直被占 │
│ 3. 连接没有正确释放(泄漏)    │
│ 4. 数据库本身性能问题        │
└─────────────────────────────┘

排查方法：
# 查看连接池状态
监控指标：
├─ active_connections (活跃连接数)
├─ idle_connections (空闲连接数)  
└─ waiting_threads (等待线程数)

如果：
- active = maximum-pool-size (连接全被占用)
- waiting_threads > 0 (有线程在等待)
→ 说明连接池确实不够用了
```

**🔍 定位连接泄漏**

```java
// 启用连接泄漏检测
spring.datasource.hikari.leak-detection-threshold=60000

// 日志会显示：
HikariPool-1 - Connection leak detection triggered, 
stack trace follows:
  at com.example.UserService.getUser(UserService.java:25)
  at com.example.UserController.detail(UserController.java:18)
  ↑
找到这里，就知道是UserService的getUser方法有问题
```

### 6.4 常见数据库问题及解决


**🔸 问题1：慢SQL导致连接堵塞**

```sql
-- ❌ 慢SQL示例：没有索引的模糊查询
SELECT * FROM orders 
WHERE customer_name LIKE '%张%'  -- 执行5秒
AND status = 'PAID';

-- 问题分析：
一个SQL执行5秒，一个连接就被占用5秒
如果有100个这样的请求，需要500秒(8分钟)才能处理完
连接池根本撑不住！

-- ✅ 优化方案：
1. 添加索引
CREATE INDEX idx_customer_status 
ON orders(customer_name, status);

2. 避免前缀模糊查询
WHERE customer_name LIKE '张%'  -- 可以用索引

3. 使用全文搜索
-- 对于复杂模糊查询，考虑使用Elasticsearch
```

**🔸 问题2：数据库死锁**

```
死锁场景示例：

事务A                         事务B
│                            │
├─ 锁住订单表记录1           │
│                            ├─ 锁住订单表记录2
│                            │
├─ 尝试锁住记录2 (等待中)    │
│                            ├─ 尝试锁住记录1 (等待中)
│                            │
└─ 死锁！等待对方释放        └─ 死锁！等待对方释放

数据库检测到死锁后，会强制回滚一个事务

解决方法：
✅ 保持事务简短
✅ 按相同顺序访问表
✅ 使用较低的隔离级别(如READ COMMITTED)
✅ 添加合理的超时设置
```

**🔸 问题3：大事务占用连接**

```java
// ❌ 不好的做法：大事务
@Transactional
public void processOrders() {
    // 查询10000条订单
    List<Order> orders = orderDao.findAll(); 
    
    // 逐个处理，很慢
    for(Order order : orders) {
        // 复杂业务处理...
        processOrder(order);  // 可能调用外部接口
        orderDao.update(order);
    }
    // 事务可能持续几分钟，连接一直被占用！
}

// ✅ 好的做法：分批处理
public void processOrders() {
    int pageSize = 100;
    int page = 0;
    
    while(true) {
        // 小事务，只锁100条数据
        List<Order> orders = processOnePage(page, pageSize);
        if(orders.isEmpty()) break;
        page++;
    }
}

@Transactional
public List<Order> processOnePage(int page, int size) {
    List<Order> orders = orderDao.findByPage(page, size);
    orders.forEach(this::processOrder);
    return orders;
}
```

---

## 7. 🔗 第三方服务依赖问题


### 7.1 第三方依赖的影响


**现实场景**：
```
你的网关依赖的服务：
├─ 支付服务 (第三方)
├─ 短信服务 (第三方)  
├─ 物流查询 (第三方)
└─ 实名认证 (第三方)

如果第三方服务挂了或变慢了：
❌ 不处理 → 你的服务也挂
✅ 正确处理 → 你的服务依然可用
```

**依赖调用链**：

```
用户请求 → 网关 → 下单服务 → 支付服务 → 第三方支付API
                                              ↑
                                        如果这里慢了
                                        整条链路都慢
```

### 7.2 第三方服务常见问题


**🔸 问题1：第三方接口响应慢**

```
场景：调用第三方支付接口
正常：200ms返回
异常：5秒才返回，甚至超时

影响：
客户端                网关                第三方
  │                   │                   │
  │───请求──────────>│                   │
  │                   │───请求─────────>│
  │                   │                  │
  │                   │  (等待中...)     │(慢)
  │  (用户等得心焦)   │                  │
  │                   │<───响应──────────│
  │<──响应────────────│                  │
  
一个慢接口，影响所有用户！

应对措施：
✅ 设置合理超时
✅ 实现熔断机制
✅ 提供降级方案
```

**🔸 问题2：第三方服务不稳定**

```
不稳定表现：
├─ 时快时慢
├─ 间歇性超时
├─ 偶尔返回错误
└─ 限流频繁触发

实际案例：
调用第三方验证码接口
成功率：95% (100次有5次失败)
→ 用户体验很差！

解决方案：
✅ 重试机制 (失败自动重试)
✅ 多服务商(A挂了用B)
✅ 本地缓存(减少调用)
```

### 7.3 防护措施实现


**🛡️ 措施1：超时控制**

```yaml
# Feign客户端超时配置
feign:
  client:
    config:
      payment-service:  # 支付服务
        connectTimeout: 3000  # 连接超时3秒
        readTimeout: 5000     # 读取超时5秒
        
      sms-service:  # 短信服务
        connectTimeout: 2000  # 短信不重要，超时短点
        readTimeout: 3000
```

> 💡 **超时设置技巧**  
> 根据服务重要性设置不同超时时间：核心服务长一点，非核心服务短一点

**🛡️ 措施2：熔断降级**

```java
// 使用Resilience4j实现熔断
@CircuitBreaker(name = "paymentService", fallbackMethod = "paymentFallback")
public PaymentResult callPayment(PaymentRequest request) {
    // 调用第三方支付
    return paymentClient.pay(request);
}

// 降级方法：第三方支付挂了，返回友好提示
public PaymentResult paymentFallback(PaymentRequest request, Exception e) {
    log.error("支付服务调用失败", e);
    
    // 返回降级结果
    return PaymentResult.builder()
        .success(false)
        .message("支付系统繁忙，请稍后重试")
        .build();
}
```

**熔断状态机**：

```
熔断器状态转换：

关闭(CLOSED) ────[失败率>50%]────> 开启(OPEN)
      ↑                               │
      │                               │
      └────[测试成功]──── 半开(HALF_OPEN) <─┘
                            │
                            └─[测试失败]─→ 开启(OPEN)

状态说明：
CLOSED: 正常调用第三方服务
OPEN: 快速失败，直接降级，不调用第三方
HALF_OPEN: 尝试调用，看服务是否恢复
```

**🛡️ 措施3：重试策略**

```java
// 智能重试：只对安全的操作重试
@Retryable(
    value = {TimeoutException.class},  // 只对超时重试
    maxAttempts = 3,                   // 最多3次
    backoff = @Backoff(delay = 1000)   // 间隔1秒
)
public SmsResult sendSms(String phone, String code) {
    return smsClient.send(phone, code);
}

// 注意：支付等关键操作不能无脑重试！
// 可能导致重复扣款
```

**重试注意事项**：

| 操作类型 | 是否可重试 | 说明 |
|---------|-----------|------|
| **查询** | ✅ 可以 | 幂等操作，随便重试 |
| **短信发送** | ✅ 可以 | 最多重复收到短信 |
| **支付扣款** | ❌ 危险 | 可能重复扣款 |
| **订单创建** | ❌ 危险 | 可能重复下单 |

### 7.4 服务依赖监控


**📊 关键监控指标**：

```
第三方服务健康度监控：

1. 响应时间(RT)
   ├─ P50: 100ms
   ├─ P95: 500ms
   └─ P99: 2000ms  ← 超过预期，需要关注

2. 成功率
   ├─ 正常: > 99%
   ├─ 警告: 95%-99%
   └─ 严重: < 95%  ← 考虑降级

3. 超时率  
   ├─ 正常: < 1%
   └─ 异常: > 5%   ← 需要调整超时时间

4. 熔断状态
   ├─ 正常: CLOSED
   └─ 异常: OPEN    ← 说明第三方服务有问题
```

**🔔 告警策略**：

```yaml
# 告警规则示例
alerts:
  - name: 第三方支付服务异常
    condition: |
      payment_service_error_rate > 5%  # 错误率>5%
      OR
      payment_service_p99_latency > 5s  # P99延迟>5秒
    action:
      - 发送钉钉通知
      - 自动开启降级
      
  - name: 短信服务不可用
    condition: |
      sms_service_circuit_breaker == "OPEN"  # 熔断器开启
    action:
      - 切换备用短信服务商
      - 通知运维人员
```

---

## 8. 📋 核心要点总结


### 8.1 性能问题排查核心思路


**🎯 黄金排查三步法**：

```
1️⃣ 看现象 → 问题表现是什么？
   ├─ 慢在哪里？(延迟/CPU/内存/网络)
   ├─ 什么时候慢？(一直慢/偶尔慢/越来越慢)
   └─ 影响范围？(全部/部分接口)

2️⃣ 找证据 → 数据怎么说？
   ├─ 看监控图表
   ├─ 查日志记录
   └─ 抓性能快照

3️⃣ 定根因 → 问题出在哪？
   ├─ 代码问题
   ├─ 配置问题  
   └─ 资源问题
```

### 8.2 各类问题速查表


| 问题类型 | 核心表现 | 快速定位 | 解决方向 |
|---------|---------|---------|---------|
| **响应延迟** | 接口变慢 | 看调用链路追踪 | 优化慢SQL/慢接口 |
| **CPU过高** | 系统卡顿 | top + jstack | 找死循环/优化算法 |
| **内存过高** | OOM错误 | jmap + MAT | 修复内存泄漏 |
| **网络瓶颈** | 连接异常 | netstat | 优化连接复用 |
| **DB连接** | 连接超时 | 连接池监控 | 优化SQL/调整连接池 |
| **第三方** | 调用失败 | 熔断器状态 | 降级/重试/换服务商 |

### 8.3 日常监控要点


> 💡 **预防胜于治疗**  
> 做好日常监控，问题发生前就能发现征兆

**🔍 必看的监控指标**：

```
系统层面：
✅ CPU使用率 (< 70%)
✅ 内存使用率 (< 80%)
✅ 磁盘IO (iowait < 20%)
✅ 网络带宽 (< 80%)

应用层面：
✅ 接口响应时间 (P99 < 1s)
✅ 错误率 (< 1%)
✅ JVM GC时间 (< 5%)
✅ 线程池使用率 (< 80%)

数据库：
✅ 慢SQL数量 (< 10/分钟)
✅ 连接池使用率 (< 80%)
✅ 死锁次数 (= 0)

第三方依赖：
✅ 调用成功率 (> 99%)
✅ 调用延迟 (P95 < 500ms)
✅ 熔断器状态 (CLOSED)
```

### 8.4 应急处理手册


**🚨 问题发生时的处理优先级**：

```
P0 - 立即处理(影响所有用户)：
├─ 服务完全不可用
├─ 大量请求超时
└─ 数据库连接池耗尽
→ 措施：紧急扩容/限流/降级

P1 - 优先处理(影响部分用户)：
├─ 部分接口响应慢
├─ CPU/内存持续高位
└─ 第三方服务异常
→ 措施：开启熔断/优化代码

P2 - 计划处理(性能优化)：
├─ 偶发性能问题
├─ 资源使用率偏高
└─ 慢SQL优化
→ 措施：分析优化/技术改造
```

### 8.5 新手易错点提醒


> ⚠️ **常见坑点**

**坑1：盲目扩容**
```
❌ 错误：性能差就加服务器
✅ 正确：先找到瓶颈再扩容

例子：数据库慢，加再多应用服务器也没用
```

**坑2：忽略日志**
```
❌ 错误：出问题才看日志
✅ 正确：定期巡检日志，提前发现问题

技巧：关注ERROR和WARN级别日志
```

**坑3：过度优化**
```
❌ 错误：所有代码都要极致优化
✅ 正确：优先优化热点代码

原则：20%的代码占80%的执行时间
```

**坑4：不设超时**
```
❌ 错误：等待第三方服务无限期返回
✅ 正确：所有外部调用都要设超时

后果：一个慢接口拖垮整个系统
```

### 8.6 学习建议


**📚 从新手到熟练的路径**：

```
第一阶段：会看问题
├─ 学会看监控图表
├─ 理解各项指标含义
└─ 能发现异常现象

第二阶段：会查问题  
├─ 掌握基本排查工具
├─ 能看懂日志和堆栈
└─ 能定位问题模块

第三阶段：会解决问题
├─ 理解常见问题原理
├─ 掌握优化方法
└─ 能独立解决大部分问题

第四阶段：能预防问题
├─ 建立完善监控体系
├─ 设计容错机制
└─ 形成性能优化意识
```

**🔧 推荐掌握的工具**：

| 工具类型 | 推荐工具 | 用途 |
|---------|---------|------|
| **监控平台** | Prometheus + Grafana | 实时监控 |
| **链路追踪** | Zipkin / SkyWalking | 定位慢请求 |
| **日志分析** | ELK Stack | 日志检索 |
| **性能分析** | JProfiler / VisualVM | 代码分析 |
| **压力测试** | JMeter / Gatling | 性能测试 |

---

**🎯 核心记忆口诀**：
- 问题排查看现象，监控日志找证据
- CPU内存网络库，六大问题要记住
- 延迟慢查先定位，超时熔断保稳定
- 监控预警要到位，防患未然最重要

---

> 💡 **学习提示**  
> 性能问题排查是个熟能生巧的过程，多实践、多总结，慢慢就能形成自己的排查思路和方法论