---
title: 3、云原生服务发现机制
---
## 📚 目录

1. [云原生服务发现基础](#1-云原生服务发现基础)
2. [Kubernetes Service发现机制](#2-kubernetes-service发现机制)
3. [CoreDNS域名解析](#3-coredns域名解析)
4. [Headless Service应用](#4-headless-service应用)
5. [EndPoints资源管理](#5-endpoints资源管理)
6. [Service Mesh服务网格](#6-service-mesh服务网格)
7. [Istio服务发现](#7-istio服务发现)
8. [Envoy代理配置](#8-envoy代理配置)
9. [云原生最佳实践](#9-云原生最佳实践)
10. [核心要点总结](#10-核心要点总结)

---

## 1. 🌥️ 云原生服务发现基础


### 1.1 什么是云原生服务发现


**通俗理解**：想象一个大型购物中心，每个店铺（微服务）的位置可能会变动。传统方式是你记住"A店在3楼302号"，但云原生方式更聪明——你只需要记住"A店"的名字，购物中心的导航系统（K8s）会自动告诉你它现在在哪里。

```
传统服务发现的问题：
用户 → 找Eureka → 查询服务IP → 调用服务
缺点：需要额外的注册中心，增加复杂度

云原生服务发现的优势：
用户 → 直接用服务名 → K8s自动解析 → 调用服务
优点：基础设施自带，无需额外组件
```

**核心概念**：
- **平台级服务发现**：Kubernetes原生提供，不需要Eureka/Consul等组件
- **DNS自动解析**：服务名直接转换为IP地址
- **动态更新**：服务实例变化时自动更新路由
- **零侵入**：应用代码无需感知服务发现逻辑

### 1.2 云原生 vs 传统微服务注册中心


| 对比维度 | 传统注册中心(Eureka/Consul) | 云原生服务发现(K8s) |
|---------|---------------------------|-------------------|
| **部署复杂度** | 需要单独部署注册中心集群 | K8s平台自带，开箱即用 |
| **代码侵入性** | 需要引入客户端依赖 | 完全透明，零侵入 |
| **高可用** | 需要自己实现集群方案 | K8s自动保证 |
| **服务发现方式** | 通过HTTP API查询 | DNS解析 + 环境变量 |
| **适用场景** | 传统虚拟机/容器混合 | 容器化云原生应用 |

**选择建议**：
```
🟢 使用云原生服务发现的场景：
- 完全容器化部署在K8s
- 希望简化架构，减少组件
- 团队熟悉K8s生态

🟡 保留传统注册中心的场景：
- 虚拟机和容器混合部署
- 跨K8s集群服务发现
- 需要更细粒度的服务治理
```

### 1.3 云原生服务发现的工作原理


```
服务发现流程示意：

应用Pod                    Service                  DNS(CoreDNS)
   |                         |                          |
   |--[1]请求my-service----->|                          |
   |                         |--[2]查询DNS------------->|
   |                         |<-[3]返回ClusterIP--------|
   |                         |                          |
   |<--[4]返回Service IP-----|                          |
   |                         |                          |
   |--[5]发送请求到ClusterIP->|                          |
   |                         |--[6]负载均衡到Pod------->|目标Pod
```

**关键组件说明**：
- **Service**：逻辑服务的抽象，提供稳定的访问入口
- **CoreDNS**：K8s内置DNS服务器，负责域名解析
- **kube-proxy**：运行在每个节点，负责流量转发
- **EndPoints**：记录Service背后的真实Pod IP列表

---

## 2. 🎯 Kubernetes Service发现机制


### 2.1 Service的本质是什么


**通俗解释**：Service就像一个"前台接待"，后面的员工（Pod）可能换人，但前台电话号码（Service IP）永远不变。客户打这个号码，前台会自动转接到在岗的员工。

**三种核心访问方式**：

```
方式1：ClusterIP（集群内部访问）
┌──────────────────────────┐
│  Service: my-service     │
│  ClusterIP: 10.96.0.10   │ ← 虚拟IP，K8s自动分配
└──────────┬───────────────┘
           │ 负载均衡
    ┌──────┴──────┬──────────┐
    ▼             ▼          ▼
  Pod1          Pod2       Pod3
  
访问方式：http://my-service:8080
```

```
方式2：NodePort（外部访问）
外部请求 → 任意节点IP:30080 → Service → Pod

┌─────────────────┐
│  任意K8s节点     │
│  NodePort:30080 │ ← 每个节点都监听这个端口
└────────┬────────┘
         ▼
   Service转发到Pod
```

```
方式3：LoadBalancer（云平台负载均衡）
云负载均衡器 → NodePort → Service → Pod

外部IP: 47.52.xxx.xxx → Service → Pod
```

### 2.2 Service配置实战


**最基础的Service配置**：

```yaml
# my-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: user-service          # 服务名，其他Pod通过这个名字访问
spec:
  type: ClusterIP             # 默认类型，集群内访问
  selector:
    app: user-app             # 选择标签为app=user-app的Pod
  ports:
    - port: 8080              # Service暴露的端口
      targetPort: 8080        # Pod实际监听的端口
```

**关键配置说明**：
- `selector`：**最重要**！决定Service关联哪些Pod
- `port`：Service对外暴露的端口号
- `targetPort`：Pod内应用真正监听的端口

**Java应用如何使用**：

```java
// 无需任何特殊代码！直接用服务名访问
@Service
public class OrderService {
    
    @Autowired
    private RestTemplate restTemplate;
    
    public User getUser(Long userId) {
        // 直接用服务名，K8s自动解析为ClusterIP
        String url = "http://user-service:8080/users/" + userId;
        return restTemplate.getForObject(url, User.class);
    }
}
```

### 2.3 Service的负载均衡机制


**kube-proxy的工作原理**：

```
客户端Pod发起请求 → Service ClusterIP(虚拟IP) 
                    ↓
              kube-proxy拦截
                    ↓
         根据负载均衡算法选择Pod
                    ↓
         iptables规则转发到真实PodIP
```

**负载均衡模式**：

| 模式 | 工作原理 | 性能 | 适用场景 |
|-----|---------|------|---------|
| **iptables** | 使用iptables规则转发 | 中等 | 默认模式，兼容性好 |
| **ipvs** | Linux虚拟服务器 | 高 | 大规模集群推荐 |
| **userspace** | 用户态代理转发 | 低 | 已废弃 |

**重要提示**：
> ⚠️ Service的负载均衡是**随机**或**轮询**的，不支持Ribbon那样的复杂策略。如需高级负载均衡，建议使用Service Mesh。

---

## 3. 🔍 CoreDNS域名解析


### 3.1 CoreDNS是什么


**通俗比喻**：CoreDNS就像K8s集群里的"电话簿"，你说一个服务名，它告诉你对应的IP地址。

**工作流程**：

```
Pod内应用发起请求
   |
   | http://user-service:8080
   ▼
查看/etc/resolv.conf（DNS配置）
   |
   | nameserver 10.96.0.10  ← CoreDNS的ClusterIP
   ▼
向CoreDNS查询"user-service"
   |
   ▼
CoreDNS查找Service记录
   |
   ▼
返回ClusterIP: 10.96.1.25
   |
   ▼
应用使用这个IP发起真实请求
```

### 3.2 DNS域名格式规则


**完整DNS名称格式**：
```
<服务名>.<命名空间>.svc.cluster.local

示例：
user-service.default.svc.cluster.local
  ↑        ↑       ↑      ↑
服务名   命名空间   固定   集群域名
```

**同命名空间简化访问**：

```java
// 同命名空间(default)内，直接用服务名
http://user-service:8080        ✅ 最简洁

// 跨命名空间访问，需要加命名空间
http://user-service.prod:8080   ✅ 访问prod命名空间的服务

// 完整域名(FQDN)，任何情况都能用
http://user-service.default.svc.cluster.local:8080  ✅ 但太长
```

### 3.3 DNS解析的实际配置


**Pod内DNS配置自动注入**：

```yaml
# K8s自动为每个Pod配置DNS
# 查看Pod内的/etc/resolv.conf

nameserver 10.96.0.10           # CoreDNS的ClusterIP
search default.svc.cluster.local svc.cluster.local cluster.local
options ndots:5
```

**配置说明**：
- `nameserver`：DNS服务器地址，指向CoreDNS
- `search`：DNS搜索域，简化服务名访问
- `ndots:5`：域名点数少于5个时，自动添加搜索域

**Java应用配置示例**：

```yaml
# application.yml
spring:
  cloud:
    kubernetes:
      discovery:
        enabled: true                    # 启用K8s服务发现
        all-namespaces: false            # 只发现当前命名空间
      config:
        enabled: true
        namespace: default               # 指定命名空间

# 微服务间调用配置
feign:
  client:
    config:
      user-service:                      # 直接用服务名
        url: http://user-service:8080    # DNS自动解析
```

---

## 4. 🎭 Headless Service应用


### 4.1 什么是Headless Service


**通俗解释**：普通Service像"呼叫中心"，你打电话，它随机转接一个客服。Headless Service像"通讯录"，它直接给你所有客服的电话号码，你自己选一个打。

**对比理解**：

```
普通Service：
DNS查询 → 返回ClusterIP(虚拟IP) → kube-proxy转发到某个Pod

Headless Service：
DNS查询 → 直接返回所有Pod的真实IP列表 → 应用自己选择
```

### 4.2 Headless Service配置


**配置方式（关键是clusterIP: None）**：

```yaml
apiVersion: v1
kind: Service
metadata:
  name: mysql-headless
spec:
  clusterIP: None              # 🔥 核心配置：不分配ClusterIP
  selector:
    app: mysql
  ports:
    - port: 3306
      targetPort: 3306
```

**DNS解析结果对比**：

```bash
# 普通Service解析
$ nslookup user-service
Name:    user-service.default.svc.cluster.local
Address: 10.96.1.25            # 返回虚拟ClusterIP

# Headless Service解析
$ nslookup mysql-headless
Name:    mysql-headless.default.svc.cluster.local
Address: 172.17.0.5            # Pod1的真实IP
Address: 172.17.0.6            # Pod2的真实IP
Address: 172.17.0.7            # Pod3的真实IP
```

### 4.3 Headless Service典型应用场景


**场景1：有状态应用（StatefulSet）**

```yaml
# MySQL主从集群
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: mysql
spec:
  serviceName: mysql-headless   # 关联Headless Service
  replicas: 3
  template:
    spec:
      containers:
      - name: mysql
        image: mysql:8.0

# 每个Pod有固定DNS名称
mysql-0.mysql-headless.default.svc.cluster.local  # 主库
mysql-1.mysql-headless.default.svc.cluster.local  # 从库1
mysql-2.mysql-headless.default.svc.cluster.local  # 从库2
```

**场景2：服务自定义负载均衡**

```java
// 应用自己实现负载均衡逻辑
@Service
public class CustomLoadBalancer {
    
    public String selectPod() {
        // 通过DNS获取所有Pod IP
        InetAddress[] addresses = InetAddress.getAllByName("user-service-headless");
        
        // 自定义选择策略（如一致性哈希）
        InetAddress selected = consistentHash(addresses);
        
        return "http://" + selected.getHostAddress() + ":8080";
    }
}
```

**使用建议**：

```
✅ 适合Headless Service的场景：
- 有状态应用需要稳定网络标识
- 需要直连特定Pod（如数据库主从）
- 自定义负载均衡策略

❌ 不适合的场景：
- 无状态服务（用普通Service更简单）
- 不需要知道Pod具体IP的场景
```

---

## 5. 📊 EndPoints资源管理


### 5.1 EndPoints是什么


**通俗解释**：EndPoints是Service和Pod之间的"员工花名册"。Service知道要提供服务，但具体有哪些员工（Pod）能干活，就记录在EndPoints里。

**三者关系**：

```
Service（服务定义）
   ↓ 通过selector选择Pod
EndPoints（Pod IP列表）
   ↓ 记录所有匹配的Pod
Pod（实际工作单元）

示例流程：
1. 创建Service，定义selector: app=user
2. K8s自动创建同名EndPoints
3. EndPoints实时记录所有app=user的Pod IP
4. Pod增减时，EndPoints自动更新
```

### 5.2 查看和理解EndPoints


**查看EndPoints信息**：

```bash
# 创建Service后，自动创建同名EndPoints
$ kubectl get endpoints user-service

NAME           ENDPOINTS                          AGE
user-service   172.17.0.5:8080,172.17.0.6:8080   5m

# 详细信息
$ kubectl describe endpoints user-service

Subsets:
  Addresses:  172.17.0.5,172.17.0.6,172.17.0.7
  Ports:      8080/TCP
```

**EndPoints更新机制**：

```
Pod创建 → 标签匹配Service → 自动加入EndPoints
Pod删除 → 自动从EndPoints移除
Pod不健康 → 健康检查失败 → 从EndPoints移除
Pod恢复 → 重新加入EndPoints
```

### 5.3 手动管理EndPoints


**场景：集成外部服务**

有时候需要把K8s外部的服务也纳入服务发现，这时候就手动创建EndPoints。

```yaml
# 外部MySQL数据库接入K8s服务发现
---
# 创建Service（不带selector）
apiVersion: v1
kind: Service
metadata:
  name: external-mysql
spec:
  ports:
    - port: 3306
  # 注意：没有selector，所以不会自动创建EndPoints

---
# 手动创建EndPoints
apiVersion: v1
kind: Endpoints
metadata:
  name: external-mysql      # 必须与Service同名
subsets:
  - addresses:
      - ip: 192.168.1.100   # 外部MySQL的IP
    ports:
      - port: 3306
```

**Java应用使用**：

```java
// 应用无需关心MySQL在K8s内还是外部
@Configuration
public class DataSourceConfig {
    
    @Bean
    public DataSource dataSource() {
        DriverManagerDataSource ds = new DriverManagerDataSource();
        // 直接用服务名，DNS解析会找到EndPoints配置的外部IP
        ds.setUrl("jdbc:mysql://external-mysql:3306/mydb");
        ds.setUsername("root");
        ds.setPassword("password");
        return ds;
    }
}
```

---

## 6. 🕸️ Service Mesh服务网格


### 6.1 什么是Service Mesh


**通俗比喻**：传统微服务像"快递员直接送货"，Service Mesh像"所有快递先到小区驿站，驿站统一管理配送"。驿站（Sidecar代理）负责加密、限流、监控等功能，快递员（业务代码）只管送货。

**架构对比**：

```
传统微服务架构：
┌─────────────┐         ┌─────────────┐
│  服务A      │         │  服务B      │
│  ├业务逻辑  │◄───────►│  ├业务逻辑  │
│  ├限流      │         │  ├限流      │
│  ├熔断      │         │  ├熔断      │
│  ├监控      │         │  ├监控      │
└─────────────┘         └─────────────┘
问题：每个服务都要实现这些功能

Service Mesh架构：
┌─────────────┐         ┌─────────────┐
│  服务A      │         │  服务B      │
│  ├业务逻辑  │         │  ├业务逻辑  │
└──────┬──────┘         └──────┬──────┘
       │                       │
    ┌──▼───┐               ┌───▼──┐
    │Sidecar│◄─────────────►│Sidecar│
    │代理   │   网格通信    │代理   │
    └───────┘               └──────┘
    限流/熔断/监控等功能统一在代理层实现
```

### 6.2 Service Mesh核心组件


**控制平面 vs 数据平面**：

```
控制平面（Control Plane）
    ├── 服务发现规则下发
    ├── 路由策略配置
    ├── 监控指标收集
    └── 安全策略管理
         ↓ 配置下发
数据平面（Data Plane）
    ├── Sidecar代理（每个Pod一个）
    ├── 拦截所有流量
    ├── 执行流量策略
    └── 上报监控数据
```

**Sidecar模式工作流程**：

```
请求流程：
服务A发起请求 → 本地Sidecar代理拦截 
              → 执行限流/熔断/重试等策略
              → 通过mTLS加密
              → 发送到服务B的Sidecar代理
              → 服务B的Sidecar解密转发
              → 服务B业务逻辑处理
```

### 6.3 Service Mesh的优势


**对比Spring Cloud的变化**：

| 功能 | Spring Cloud方式 | Service Mesh方式 |
|-----|-----------------|-----------------|
| **服务发现** | Eureka客户端SDK | K8s原生 + Mesh增强 |
| **负载均衡** | Ribbon代码实现 | Sidecar自动处理 |
| **熔断限流** | Hystrix/Sentinel | Envoy策略配置 |
| **链路追踪** | Sleuth埋点 | Sidecar自动采集 |
| **代码侵入** | 需引入多个依赖 | 零代码侵入 |

**关键优势**：

```
✅ 业务代码零侵入
- Java应用无需任何SDK，专注业务逻辑
- 非Java服务（Go/Python）也能享受同样能力

✅ 统一流量治理
- 所有服务的限流、熔断策略统一配置
- 不用担心开发团队忘记加限流逻辑

✅ 多语言支持
- 不同语言写的服务享受同样的服务治理能力
```

---

## 7. 🚀 Istio服务发现


### 7.1 Istio服务发现原理


**Istio如何增强K8s服务发现**：

```
K8s原生服务发现：
Service → DNS解析 → ClusterIP → kube-proxy转发 → Pod

Istio增强后的服务发现：
Service → DNS解析 → ClusterIP → Envoy Sidecar智能路由 → Pod
                                      ↑
                                 支持灰度/蓝绿/金丝雀
```

**Istio的服务注册表**：

```
Istio会从多个来源聚合服务信息：
1. K8s Service自动发现
2. 外部服务手动注册（ServiceEntry）
3. 虚拟服务配置（VirtualService）

聚合成统一的服务注册表，供Envoy使用
```

### 7.2 Istio核心资源配置


**VirtualService - 虚拟服务路由**：

```yaml
# 实现金丝雀发布：90%流量到v1，10%到v2
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: user-service
spec:
  hosts:
    - user-service              # 服务名
  http:
    - match:
        - headers:
            user-type:
              exact: vip        # VIP用户路由到v2
      route:
        - destination:
            host: user-service
            subset: v2
    - route:                    # 其他用户90% v1, 10% v2
        - destination:
            host: user-service
            subset: v1
          weight: 90
        - destination:
            host: user-service
            subset: v2
          weight: 10
```

**DestinationRule - 流量策略**：

```yaml
apiVersion: networking.istio.io/v1beta1
kind: DestinationRule
metadata:
  name: user-service
spec:
  host: user-service
  subsets:
    - name: v1
      labels:
        version: v1
    - name: v2
      labels:
        version: v2
  trafficPolicy:
    loadBalancer:
      simple: LEAST_REQUEST     # 最少请求负载均衡
    connectionPool:
      tcp:
        maxConnections: 100     # 最大连接数
      http:
        http1MaxPendingRequests: 50
```

**ServiceEntry - 外部服务接入**：

```yaml
# 把外部API纳入服务网格管理
apiVersion: networking.istio.io/v1beta1
kind: ServiceEntry
metadata:
  name: external-api
spec:
  hosts:
    - api.github.com
  ports:
    - number: 443
      name: https
      protocol: HTTPS
  location: MESH_EXTERNAL
  resolution: DNS
```

### 7.3 Java应用集成Istio


**应用无需任何代码改动**：

```java
// 原来的代码
@Service
public class OrderService {
    
    @Autowired
    private RestTemplate restTemplate;
    
    public User getUser(Long userId) {
        // 只需要用服务名，Istio自动处理路由/限流/熔断
        String url = "http://user-service:8080/users/" + userId;
        return restTemplate.getForObject(url, User.class);
    }
}

// Istio会自动：
// 1. 拦截这个请求
// 2. 根据VirtualService规则路由到v1或v2
// 3. 应用DestinationRule的负载均衡策略
// 4. 执行熔断和重试逻辑
// 5. 收集监控指标
```

**部署配置（自动注入Sidecar）**：

```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: order-service
spec:
  template:
    metadata:
      labels:
        app: order-service
        # 不需要手动配置Sidecar！
    spec:
      containers:
      - name: order-service
        image: order-service:1.0
        ports:
        - containerPort: 8080

# 只需要给命名空间加标签，自动注入Sidecar
# kubectl label namespace default istio-injection=enabled
```

---

## 8. ⚙️ Envoy代理配置


### 8.1 Envoy是什么


**通俗解释**：Envoy是Service Mesh的"快递驿站站长"，负责处理所有进出的包裹（流量）。它比Nginx更智能，专为微服务设计。

**Envoy在Istio中的位置**：

```
应用容器                 Envoy Sidecar容器
┌────────────┐          ┌─────────────────┐
│            │          │   Listener      │ ← 监听端口
│  业务代码  │◄────────►│   Filter Chain  │ ← 处理请求
│            │          │   Cluster       │ ← 后端服务集群
└────────────┘          └─────────────────┘
                               ↑
                        Istio控制面下发配置
```

### 8.2 Envoy核心配置概念


**Listener - 监听器**：

```yaml
# 监听入站流量（简化示例）
listeners:
  - name: listener_0
    address:
      socket_address:
        address: 0.0.0.0
        port_value: 15001      # 拦截所有出站流量
    filter_chains:
      - filters:
          - name: envoy.http_connection_manager
            typed_config:
              # HTTP处理逻辑
```

**Cluster - 上游服务集群**：

```yaml
# 定义后端服务（简化示例）
clusters:
  - name: user-service
    type: STRICT_DNS          # 通过DNS发现服务
    lb_policy: ROUND_ROBIN    # 轮询负载均衡
    load_assignment:
      cluster_name: user-service
      endpoints:
        - lb_endpoints:
            - endpoint:
                address:
                  socket_address:
                    address: user-service.default.svc.cluster.local
                    port_value: 8080
```

**Route - 路由规则**：

```yaml
# 基于HTTP路径路由（简化示例）
routes:
  - match:
      prefix: "/api/v1"
    route:
      cluster: user-service-v1
  - match:
      prefix: "/api/v2"
    route:
      cluster: user-service-v2
```

### 8.3 Envoy实用功能配置


**熔断器配置**：

```yaml
# 防止级联故障
clusters:
  - name: user-service
    circuit_breakers:
      thresholds:
        - priority: DEFAULT
          max_connections: 1000         # 最大连接数
          max_pending_requests: 100     # 最大等待请求
          max_requests: 1000            # 最大并发请求
          max_retries: 3                # 最大重试次数
```

**重试策略**：

```yaml
# 自动重试失败请求
routes:
  - match:
      prefix: "/"
    route:
      cluster: user-service
      retry_policy:
        retry_on: 5xx                   # 5xx错误时重试
        num_retries: 3                  # 最多重试3次
        per_try_timeout: 2s             # 每次尝试超时2秒
```

**超时配置**：

```yaml
routes:
  - match:
      prefix: "/"
    route:
      cluster: user-service
      timeout: 5s                       # 总超时5秒
```

### 8.4 Envoy的优势


**对比传统代理**：

| 特性 | Nginx | Envoy |
|-----|-------|-------|
| **动态配置** | 需要重启 | 热更新配置 |
| **服务发现** | 静态配置 | 动态服务发现 |
| **协议支持** | HTTP/TCP | HTTP/gRPC/MongoDB等 |
| **观测性** | 基础日志 | 详细指标和分布式追踪 |
| **云原生** | 传统工具 | 为微服务设计 |

**Java开发者视角**：

```
使用Envoy后的好处：
1. 不用写Feign的fallback降级逻辑（Envoy自动重试）
2. 不用配置Ribbon超时（Envoy统一管理）
3. 不用手动埋点追踪（Envoy自动生成trace）
4. 不用关心灰度发布逻辑（Envoy根据配置路由）
```

---

## 9. 🎯 云原生最佳实践


### 9.1 服务发现架构选型


**决策树**：

```
是否完全容器化部署？
├─ 是 → 是否需要高级流量治理？
│       ├─ 是 → Service Mesh (Istio)
│       └─ 否 → K8s Service + CoreDNS
│
└─ 否 → 虚拟机和容器混合部署？
        ├─ 是 → 传统注册中心(Eureka) + K8s Service
        └─ 否 → 根据团队技术栈选择
```

**各方案适用场景**：

```
🟢 K8s原生Service发现：
- 简单微服务架构
- 无复杂流量治理需求
- 团队K8s经验丰富

🟡 Service Mesh (Istio)：
- 多语言微服务
- 需要金丝雀/蓝绿部署
- 需要零侵入的流量治理

🔵 传统注册中心：
- 虚拟机容器混合部署
- 遗留系统迁移过渡期
- 跨云/跨集群服务发现
```

### 9.2 命名空间隔离策略


**环境隔离最佳实践**：

```yaml
# 开发环境
apiVersion: v1
kind: Namespace
metadata:
  name: dev
  labels:
    env: development

---
# 测试环境
apiVersion: v1
kind: Namespace
metadata:
  name: test
  labels:
    env: testing

---
# 生产环境
apiVersion: v1
kind: Namespace
metadata:
  name: prod
  labels:
    env: production
    istio-injection: enabled  # 只在生产启用Istio
```

**Java应用配置**：

```yaml
# application-dev.yml
spring:
  cloud:
    kubernetes:
      discovery:
        namespace: dev           # 只发现dev环境服务
        
# application-prod.yml
spring:
  cloud:
    kubernetes:
      discovery:
        namespace: prod          # 只发现prod环境服务
```

### 9.3 健康检查配置


**Liveness探针（存活检查）**：

```yaml
apiVersion: apps/v1
kind: Deployment
spec:
  template:
    spec:
      containers:
      - name: user-service
        livenessProbe:
          httpGet:
            path: /actuator/health/liveness
            port: 8080
          initialDelaySeconds: 60    # 启动60秒后开始检查
          periodSeconds: 10          # 每10秒检查一次
          failureThreshold: 3        # 失败3次重启Pod
```

**Readiness探针（就绪检查）**：

```yaml
readinessProbe:
  httpGet:
    path: /actuator/health/readiness
    port: 8080
  initialDelaySeconds: 30          # 启动30秒后检查
  periodSeconds: 5                 # 每5秒检查一次
  # 失败时从EndPoints移除，不重启Pod
```

**Spring Boot配置**：

```java
// 健康检查端点（Spring Boot Actuator）
management:
  endpoint:
    health:
      probes:
        enabled: true
  health:
    livenessState:
      enabled: true
    readinessState:
      enabled: true
```

### 9.4 性能优化建议


**DNS缓存优化**：

```yaml
# Pod的DNS配置优化
apiVersion: v1
kind: Pod
spec:
  dnsConfig:
    options:
      - name: ndots
        value: "2"               # 减少DNS查询次数
      - name: timeout
        value: "1"               # DNS超时1秒
      - name: attempts
        value: "2"               # DNS重试2次
```

**Service连接池优化**：

```java
// RestTemplate连接池配置
@Bean
public RestTemplate restTemplate() {
    HttpComponentsClientHttpRequestFactory factory = 
        new HttpComponentsClientHttpRequestFactory();
    
    factory.setConnectTimeout(2000);       // 连接超时2秒
    factory.setReadTimeout(5000);          // 读取超时5秒
    
    // 连接池配置
    PoolingHttpClientConnectionManager cm = 
        new PoolingHttpClientConnectionManager();
    cm.setMaxTotal(200);                   // 最大连接数
    cm.setDefaultMaxPerRoute(20);          // 每个路由最大连接
    
    return new RestTemplate(factory);
}
```

### 9.5 监控与故障排查


**关键监控指标**：

```
Service层面：
- Endpoints数量变化（服务实例数）
- DNS解析成功率
- Service请求QPS和延迟

Pod层面：
- 健康检查失败次数
- 重启次数和原因
- 网络流量和错误率

Envoy层面（Istio）：
- 上游服务连接池状态
- 熔断器触发次数
- 请求重试和超时统计
```

**故障排查命令**：

```bash
# 1. 检查Service和EndPoints
kubectl get svc user-service
kubectl get endpoints user-service

# 2. 测试DNS解析
kubectl run -it --rm debug --image=busybox --restart=Never -- sh
nslookup user-service

# 3. 检查Pod网络连通性
kubectl exec -it pod-name -- curl http://user-service:8080/health

# 4. 查看Envoy配置（Istio环境）
kubectl exec pod-name -c istio-proxy -- curl localhost:15000/config_dump

# 5. 查看服务访问日志
kubectl logs pod-name -c istio-proxy --tail=100
```

---

## 10. 📋 核心要点总结


### 10.1 知识脉络梳理


```
云原生服务发现体系
│
├── 基础层：K8s原生能力
│   ├── Service抽象（稳定访问入口）
│   ├── CoreDNS（域名解析）
│   ├── kube-proxy（流量转发）
│   └── EndPoints（Pod IP管理）
│
├── 增强层：Service Mesh
│   ├── Istio控制面（策略管理）
│   ├── Envoy数据面（流量代理）
│   ├── VirtualService（智能路由）
│   └── DestinationRule（流量策略）
│
└── 应用层：Java微服务集成
    ├── 零侵入接入（无需SDK）
    ├── 健康检查配置
    └── 监控与故障排查
```

### 10.2 关键技术对比


| 技术方案 | 优势 | 劣势 | 适用场景 |
|---------|------|------|---------|
| **K8s Service** | 简单、原生、零侵入 | 功能基础、只支持轮询 | 小规模微服务 |
| **Headless Service** | 直接访问Pod、自定义LB | 需要应用感知Pod | 有状态应用 |
| **Service Mesh** | 功能强大、多语言 | 复杂、有性能开销 | 大规模异构服务 |
| **传统注册中心** | 成熟、功能丰富 | 需要额外组件 | 混合云部署 |

### 10.3 核心配置模板


**最小化Service配置**：
```yaml
apiVersion: v1
kind: Service
metadata:
  name: my-service
spec:
  selector:
    app: my-app
  ports:
    - port: 8080
```

**Istio流量治理模板**：
```yaml
# 金丝雀发布
apiVersion: networking.istio.io/v1beta1
kind: VirtualService
metadata:
  name: my-service
spec:
  hosts:
    - my-service
  http:
    - route:
        - destination:
            host: my-service
            subset: v1
          weight: 90
        - destination:
            host: my-service
            subset: v2
          weight: 10
```

### 10.4 快速记忆要点


**服务发现三件套**：
```
1. Service：稳定的访问入口（像门牌号）
2. DNS：服务名到IP的翻译（像地图导航）
3. EndPoints：实际的Pod列表（像具体的房间号）
```

**选型决策口诀**：
```
小项目用Service，够用就好
大项目用Mesh，治理更妙
混合云用注册中心，灵活可靠
有状态用Headless，直连最好
```

**故障排查三板斧**：
```
1. 看Service和EndPoints是否正常
2. 测DNS解析能否返回IP
3. 查Pod网络能否连通服务
```

---

## 🎓 学习建议


**学习路径**：
```
第一步：掌握K8s Service基础
    ↓ 实践创建ClusterIP/NodePort类型Service
    
第二步：理解DNS服务发现
    ↓ 实验不同命名空间的服务调用
    
第三步：了解Headless Service应用
    ↓ 部署StatefulSet应用实践
    
第四步：入门Service Mesh概念
    ↓ 安装Istio，体验流量管理
    
第五步：深入Envoy代理配置
    ↓ 配置熔断、重试、灰度发布
```

**实践建议**：
> 💡 从简单的K8s Service开始，先让服务能互相调用，再逐步引入Istio等高级特性。不要一开始就追求完美架构，根据实际需求演进。

**常见误区**：
> ⚠️ Service Mesh不是银弹！如果你的微服务规模小于20个，使用K8s原生Service发现就足够了。过度设计反而增加复杂度。