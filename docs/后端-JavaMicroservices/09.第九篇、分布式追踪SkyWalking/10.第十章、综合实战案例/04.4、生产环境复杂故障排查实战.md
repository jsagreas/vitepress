---
title: 4、生产环境复杂故障排查实战
---
## 📚 目录

1. [故障排查基础思路](#1-故障排查基础思路)
2. [慢查询问题定位](#2-慢查询问题定位)
3. [服务雪崩分析与处理](#3-服务雪崩分析与处理)
4. [内存泄漏追踪](#4-内存泄漏追踪)
5. [网络延迟排查](#5-网络延迟排查)
6. [数据库瓶颈定位](#6-数据库瓶颈定位)
7. [线程池阻塞问题](#7-线程池阻塞问题)
8. [超时重试问题分析](#8-超时重试问题分析)
9. [故障根因分析方法论](#9-故障根因分析方法论)
10. [核心要点总结](#10-核心要点总结)

---

## 1. 🔍 故障排查基础思路


### 1.1 什么是生产环境故障排查


**通俗理解**：就像医生给病人看病一样，当系统出现问题时，我们需要通过各种"检查手段"找出问题的根源。

```
🏥 医生看病流程          →    系统故障排查流程
   ↓                            ↓
👤 病人描述症状          →    用户反馈问题现象
   ↓                            ↓
🔬 医生检查化验          →    查看监控和日志
   ↓                            ↓
💊 诊断病因开药          →    定位问题并修复
   ↓                            ↓
✅ 观察恢复情况          →    验证修复效果
```

### 1.2 故障排查的黄金法则


**🎯 核心原则**：

| 原则 | 说明 | 实际含义 |
|------|------|----------|
| **先看现象** | 不要急着下结论 | 用户说"系统慢"，可能是网络慢、数据库慢、代码慢... |
| **查看监控** | 数据比猜测可靠 | SkyWalking能告诉你真实的调用情况 |
| **逐层排查** | 从上到下找问题 | 用户→网关→服务→数据库，一层层查 |
| **记录过程** | 避免重复劳动 | 下次遇到类似问题能快速解决 |

### 1.3 SkyWalking在故障排查中的作用


**💡 SkyWalking就像给系统装了"行车记录仪"**：

```
传统排查方式（盲人摸象）：
开发：代码没问题啊 ❌
运维：服务器正常啊 ❌  
DBA：数据库没压力啊 ❌
→ 互相推诿，找不到真相

使用SkyWalking（全程录像）：
👀 看到完整调用链
📊 看到每个环节耗时
🔍 直接定位慢在哪里
→ 一目了然，对症下药
```

---

## 2. 🐌 慢查询问题定位


### 2.1 慢查询是什么


**通俗解释**：就像去银行办业务，正常3分钟能办完，结果等了30分钟，这就是"慢查询"。

**在系统中的表现**：
- 用户点击按钮，页面转圈圈很久才出结果
- 接口响应时间从几十毫秒变成几秒甚至十几秒
- 数据库CPU飙高，但看起来请求量不大

### 2.2 使用SkyWalking定位慢查询


**📍 排查步骤**：

```
Step 1: 发现问题
用户投诉：订单查询很慢 ⏰

Step 2: 打开SkyWalking追踪
查看订单服务的调用链
   ↓
[用户请求] → [订单服务] → [数据库查询]
  100ms        50ms         9800ms ❗
                              ↑
                          问题在这里！

Step 3: 查看SQL详情
点击数据库span，看到具体SQL：
SELECT * FROM orders WHERE user_id = ? 
AND status IN (1,2,3,4,5) 
AND create_time > ?
→ 发现没走索引，全表扫描
```

**🔧 实战案例**：

```
故障现象：
订单列表接口超时，响应时间从200ms变成5秒

SkyWalking追踪显示：
[订单服务] → [MySQL查询订单] 
 总耗时：5200ms
   └─ SQL执行：4900ms ← 这里最慢

点开查看SQL：
SELECT o.*, u.name, p.title 
FROM orders o 
LEFT JOIN users u ON o.user_id = u.id
LEFT JOIN products p ON o.product_id = p.id
WHERE o.create_time > '2024-01-01'

问题分析：
❌ 三表关联查询
❌ create_time字段无索引  
❌ 返回了所有字段（SELECT *）

解决方案：
✅ 给create_time加索引
✅ 只查询需要的字段
✅ 考虑分页查询
```

### 2.3 常见慢查询类型


| 类型 | 表现 | SkyWalking特征 | 解决思路 |
|------|------|----------------|----------|
| **全表扫描** | SQL执行很慢 | 数据库span耗时长 | 添加索引 |
| **N+1查询** | 循环调用数据库 | 大量重复SQL调用 | 改用JOIN或批量查询 |
| **大数据量返回** | 内存占用高 | 单次查询返回几万条 | 分页+只查需要的字段 |
| **锁等待** | 偶尔很慢 | 耗时不稳定 | 优化事务范围 |

---

## 3. ❄️ 服务雪崩分析与处理


### 3.1 什么是服务雪崩


**生活化比喻**：

```
🏔️ 雪崩现象：
山上一块小雪掉下来 → 带动更多雪 → 形成大雪崩

🔗 服务雪崩：
一个服务慢了 → 调用它的服务也变慢 → 整个系统瘫痪

实际场景：
订单服务慢(3秒) 
   ↓
购物车等待订单响应
   ↓  
购物车也变慢(5秒)
   ↓
首页调用购物车
   ↓
首页也卡住(8秒)
   ↓
整个系统不可用 💥
```

### 3.2 用SkyWalking识别雪崩


**🔍 追踪分析过程**：

```
SkyWalking拓扑图显示：
```
     👤用户
      ↓
   [网关] ← 响应慢
      ↓
   [订单服务] ← 大量超时
      ↓
   [库存服务] ← 这里卡住了！💥
      ↓
   [数据库] ← CPU 100%
```

点击库存服务查看：
- 错误率：80% ↑
- 平均响应时间：5000ms ↑  
- 并发请求：500+ (平时50)
→ 这就是雪崩起点！
```

**📊 雪崩的典型特征**：

```
正常情况：
服务A → 服务B → 服务C
50ms    30ms    20ms
总耗时：100ms ✅

雪崩发生：
服务A → 服务B → 服务C(慢)
2000ms  1500ms  500ms
   ↑      ↑       ↑
等待超时 等待B   这里卡住
总耗时：4000ms ❌
```

### 3.3 雪崩应对策略


**🛡️ 防护手段**：

| 策略 | 作用原理 | 实际效果 |
|------|----------|----------|
| **熔断** | 检测到错误多就切断 | 快速失败，不再等待 |
| **限流** | 控制请求数量 | 保护下游服务 |
| **降级** | 返回默认值 | 保证核心功能可用 |
| **超时控制** | 不无限等待 | 及时释放资源 |

**💡 实战示例**：

```
场景：库存服务挂了导致订单服务雪崩

Step 1: SkyWalking发现问题
库存服务错误率90%，响应时间5秒

Step 2: 立即熔断
在订单服务配置：
- 检测到库存服务连续5次失败
- 自动熔断，不再调用
- 返回默认库存信息（显示"有货"）

Step 3: 观察恢复
SkyWalking显示：
订单服务响应时间恢复正常(100ms)
错误率下降到5%
用户可以正常下单了 ✅

Step 4: 修复根本问题
定位库存服务慢的原因并修复
逐步恢复正常调用
```

---

## 4. 💧 内存泄漏追踪


### 4.1 内存泄漏是什么


**通俗比喻**：

```
🚰 水龙头漏水：
打开水龙头用水 → 关闭后还在滴水 → 水池越积越满 → 最终溢出

💾 内存泄漏：
程序申请内存 → 用完忘记释放 → 内存越占越多 → 最终OOM崩溃
```

**在Java中的表现**：

```
服务刚启动：内存占用500MB ✅
运行1小时：内存占用1GB   
运行3小时：内存占用2GB   
运行6小时：内存占用4GB   
→ OutOfMemoryError 💥
```

### 4.2 用SkyWalking辅助定位


**📈 监控指标分析**：

```
Step 1: 观察内存趋势
SkyWalking JVM监控显示：
```
内存使用曲线：
4GB |              /
3GB |            /
2GB |          /
1GB |        /
0GB |______/________时间→
    启动  2h  4h  6h
    
    ↑
持续上升，不回落 = 可能内存泄漏
```

Step 2: 结合调用链分析
查看高峰期的请求：
- 某个接口被频繁调用
- 每次调用后内存增长10MB
- GC后内存不释放
→ 这个接口有问题！
```

**🔎 典型内存泄漏场景**：

| 场景 | 原因 | SkyWalking表现 | 排查方法 |
|------|------|----------------|----------|
| **集合未清理** | List持续添加不删除 | 内存持续增长 | 检查缓存代码 |
| **连接未关闭** | 数据库/HTTP连接泄漏 | 连接数增长 | 查看connection span |
| **线程局部变量** | ThreadLocal未remove | 线程池场景下泄漏 | 检查线程相关代码 |
| **监听器未移除** | 事件监听器累积 | 对象无法GC | 查看监听器注册 |

**🛠️ 实战案例**：

```
问题现象：
订单服务每天凌晨3点必崩溃，重启后正常

SkyWalking分析：
1. 查看内存曲线
   凌晨0点：2GB
   凌晨1点：2.5GB  
   凌晨2点：3GB
   凌晨3点：4GB → OOM 💥

2. 查看凌晨的调用链
   发现"订单导出"接口被频繁调用
   每次调用内存增长50MB

3. 代码检查
   发现问题：
   List<Order> allOrders = new ArrayList<>();
   // 一次性加载10万条订单到内存
   // 导出后忘记清空allOrders
   
4. 解决方案
   改为分批导出：
   - 每次只加载1000条
   - 处理完立即清空
   - 内存占用稳定在2GB ✅
```

---

## 5. 🌐 网络延迟排查


### 5.1 网络延迟的类型


**🔌 常见网络问题**：

```
服务间调用过程：
```
[服务A] --①--> [网络] --②--> [服务B] --③--> [数据库]
         DNS解析   传输     业务处理    查询
         50ms     100ms     30ms       20ms
```

可能慢的地方：
① DNS解析慢
② 网络传输慢（跨地域、带宽小）
③ 服务处理慢
```

### 5.2 SkyWalking网络分析


**📍 定位网络问题**：

```
调用链分析：
```
Trace详情：
┌─[订单服务]──────────────────┐
│ 开始：10:00:00.000         │
│ 调用库存服务               │
│   ├─ 发起调用：10:00:00.000│
│   ├─ 收到响应：10:00:00.500│← 网络耗时500ms
│   └─ 处理耗时：30ms        │
│ 结束：10:00:00.530         │
└────────────────────────────┘

分析：
总耗时：530ms
业务处理：30ms
网络耗时：500ms ← 问题在这！
```

**🔍 网络延迟排查清单**：

```
✅ 检查项目清单：

□ 服务部署位置
  - 同一机房：< 1ms
  - 跨机房：10-50ms  
  - 跨地域：100-300ms
  → 异常高就是网络问题

□ 带宽使用情况
  - 查看网络监控
  - 是否有大文件传输
  
□ 连接池配置
  - HTTP连接池是否够用
  - 是否频繁创建新连接

□ DNS解析时间  
  - 第一次调用特别慢
  - 可能DNS解析慢
```

**💡 实战案例**：

```
问题：
A服务调用B服务，响应时间不稳定
有时200ms，有时2000ms

SkyWalking分析：
1. 查看调用链
   200ms的情况：业务处理200ms，网络0ms
   2000ms的情况：业务处理200ms，网络1800ms

2. 原因定位
   A服务在北京机房
   B服务部分实例在北京，部分在上海
   → 负载均衡随机分配
   → 调用到上海实例就慢

3. 解决方案
   配置就近路由：
   - 北京A服务优先调用北京B服务
   - 响应时间稳定在200ms ✅
```

---

## 6. 🗄️ 数据库瓶颈定位


### 6.1 数据库性能问题的表现


**📊 典型症状**：

```
用户视角：
点击查询按钮 → 转圈5秒 → 才显示结果

服务端日志：
[订单服务] 处理时间: 5200ms
  └─ [数据库查询] 耗时: 5000ms ← 问题在这

数据库监控：
CPU: 90% ↑
慢查询: 500条/分钟 ↑  
连接数: 800/1000 (接近上限)
```

### 6.2 SkyWalking数据库分析


**🔎 定位方法**：

```
Step 1: 查看调用链
```
订单查询接口 Trace：
├─ 订单服务处理：200ms
├─ 查询订单SQL：2000ms ← 这里慢
├─ 查询用户SQL：1500ms ← 这里也慢
└─ 拼装数据：100ms
总耗时：3800ms
```

Step 2: 点击SQL Span查看详情
SQL语句：
SELECT * FROM orders 
WHERE user_id IN (1,2,3...1000个ID)
ORDER BY create_time DESC

执行次数：每秒100次
平均耗时：2000ms
→ 高频慢SQL找到了！

Step 3: 分析问题
❌ IN条件包含1000个ID
❌ ORDER BY字段无索引
❌ 返回所有字段
```

**🛠️ 数据库优化策略**：

| 问题类型 | SkyWalking特征 | 优化方案 |
|----------|----------------|----------|
| **慢SQL** | 单次查询耗时长 | 加索引、优化SQL |
| **高频查询** | 同一SQL大量调用 | 加缓存、批量查询 |
| **连接池耗尽** | 获取连接超时 | 扩大连接池、优化查询 |
| **锁等待** | 耗时不稳定 | 缩短事务、优化索引 |

**💡 实战案例**：

```
场景：用户列表接口越来越慢

Step 1: SkyWalking发现数据库慢
查看Trace：数据库查询从100ms增长到3秒

Step 2: 查看SQL详情
SELECT * FROM users 
WHERE status = 1 
ORDER BY create_time DESC 
LIMIT 20

执行计划：全表扫描（用户表500万数据）

Step 3: 问题分析
- 表数据量大（500万）
- status字段无索引
- create_time无索引

Step 4: 优化方案
1. 给status字段加索引
   CREATE INDEX idx_status ON users(status)
   
2. 给create_time加索引
   CREATE INDEX idx_create_time ON users(create_time)
   
3. 组合索引优化
   CREATE INDEX idx_status_time 
   ON users(status, create_time)

Step 5: 效果验证
优化前：3000ms
优化后：50ms ↓ 提升60倍！
```

---

## 7. 🧵 线程池阻塞问题


### 7.1 线程池阻塞是什么


**生活化比喻**：

```
🍜 餐厅场景：
10个服务员（线程池大小10）
正常：每桌客人10分钟吃完，服务员可以服务下一桌
阻塞：有一桌客人聊天2小时不走，服务员只能等着
结果：只有9个服务员能工作，效率下降

💻 系统中的线程池阻塞：
线程池大小：200
某个慢接口占用线程10秒才释放
结果：线程被耗尽，新请求排队等待
```

### 7.2 用SkyWalking诊断线程池问题


**📈 识别线程池阻塞**：

```
Step 1: 观察服务性能
SkyWalking显示：
- 响应时间突然增长：100ms → 5000ms
- 并发请求不高：只有50个/秒
- 但都在排队等待

Step 2: 查看JVM线程监控
```
线程状态分布：
活跃线程：200/200 (100%) ← 线程池满了
等待线程：150个在排队
阻塞线程：180个在等待某个资源
```

Step 3: 分析调用链
发现：
- 180个请求都卡在"发送短信"接口
- 第三方短信服务响应慢(30秒)
- 线程一直等待，不释放
→ 找到阻塞原因！
```

**🔧 线程池阻塞的典型场景**：

| 场景 | 问题原因 | SkyWalking表现 | 解决方案 |
|------|----------|----------------|----------|
| **外部调用慢** | 第三方接口超时 | 大量线程等待 | 设置超时、异步处理 |
| **数据库锁** | 等待行锁释放 | 线程阻塞在SQL | 优化事务、加索引 |
| **文件IO** | 读写大文件 | IO等待时间长 | 异步处理、分批读取 |
| **死锁** | 互相等待资源 | 线程永久阻塞 | 检查锁顺序、加超时 |

**💡 实战案例**：

```
问题：订单服务突然响应慢，用户无法下单

SkyWalking分析：

1. 性能指标
   响应时间：从100ms增长到30秒
   并发量：正常(50 QPS)
   线程池：200/200使用中

2. 查看调用链
   所有慢请求都卡在：
   订单服务 → 积分服务 → 积分计算
   积分计算耗时：28秒 ← 异常！

3. 深入分析
   积分服务调用链：
   - 查询用户等级：50ms
   - 计算积分规则：27秒 ← 问题在这
   
   原因：
   积分规则需要查询1年的订单历史
   SQL没走索引，全表扫描
   
4. 应急处理
   临时方案：
   - 调整超时时间：30秒→3秒
   - 超时快速失败，释放线程
   - 用户体验：下单成功，积分稍后计算
   
   根本解决：
   - 优化积分计算SQL
   - 改为异步计算积分
   - 线程池隔离：积分服务单独线程池
   
5. 效果
   响应时间恢复：100ms
   线程池使用率：30%
   用户可以正常下单 ✅
```

---

## 8. ⏱️ 超时重试问题分析


### 8.1 超时重试的双刃剑


**⚖️ 重试的利与弊**：

```
✅ 重试的好处：
网络抖动 → 重试成功 → 用户无感知

❌ 重试的风险：
服务慢 → 重试 → 更多请求 → 雪崩 💥
```

**🔄 重试放大效应**：

```
场景：支付服务响应慢（5秒）

没有重试：
用户请求 → 支付服务(5秒) → 返回
1个请求 = 1个压力

配置了重试3次：
用户请求 → 超时(3秒) 
         → 重试1(3秒)
         → 重试2(3秒)  
         → 重试3(3秒)
1个请求 = 4倍压力 💥

结果：
原本50 QPS的压力
变成200 QPS
服务直接崩溃
```

### 8.2 SkyWalking识别重试问题


**🔍 重试问题的特征**：

```
正常调用链：
用户 → 订单服务 → 支付服务
       1次调用     1次响应

重试调用链：
用户 → 订单服务 → 支付服务(超时)
                → 支付服务(重试1)
                → 支付服务(重试2)
                → 支付服务(成功)

SkyWalking显示：
同一个TraceID下：
- 4次支付服务调用
- 前3次超时
- 最后1次成功
→ 重试了3次
```

**📊 重试问题分析步骤**：

```
Step 1: 观察异常现象
支付服务监控：
- 请求量突增：100 QPS → 400 QPS
- 但实际业务量没增长
- 大量超时错误

Step 2: 查看Trace
展开一个慢请求：
```
订单创建 Trace:
├─ 订单服务：处理订单
├─ 调用支付服务(超时3秒)
├─ 调用支付服务(超时3秒) ← 重试1
├─ 调用支付服务(超时3秒) ← 重试2
└─ 调用支付服务(成功2秒) ← 重试3
总耗时：11秒
```

Step 3: 统计重试比例
100个请求中：
- 70个请求重试了3次
- 20个请求重试了2次  
- 10个请求重试了1次
实际请求量放大：
100 → 100 + 70×3 + 20×2 + 10×1 = 360次
```

**🛡️ 合理的重试策略**：

| 策略 | 说明 | 适用场景 |
|------|------|----------|
| **不重试** | 直接失败返回 | 写操作、支付等 |
| **立即重试** | 失败后马上重试 | 网络抖动场景 |
| **延迟重试** | 等待一段时间再重试 | 服务恢复场景 |
| **指数退避** | 重试间隔指数增长 | 保护下游服务 |

**💡 实战案例**：

```
问题：支付服务频繁崩溃，重启后又崩

SkyWalking排查：

1. 发现问题
   支付服务每隔10分钟崩溃一次
   QPS：正常100，崩溃前突增到500

2. 查看调用链
   发现：每个支付请求都重试了5次
   配置：超时2秒，重试5次，立即重试
   
3. 问题分析
   支付服务偶尔慢(3秒)
   → 触发2秒超时
   → 立即重试5次  
   → 6倍请求压力
   → 服务更慢
   → 更多重试
   → 雪崩 💥

4. 优化方案
   调整重试策略：
   ```java
   @Retry(
     maxAttempts = 2,        // 最多重试2次
     backoff = @Backoff(
       delay = 1000,         // 延迟1秒重试
       multiplier = 2        // 每次延迟翻倍
     ),
     include = TimeoutException.class,
     exclude = PaymentException.class  // 支付异常不重试
   )
   ```
   
5. 效果
   - 重试次数下降：5次 → 2次
   - 请求压力正常：不再放大
   - 服务稳定：不再崩溃 ✅
```

---

## 9. 🎯 故障根因分析方法论


### 9.1 5-Why分析法


**🤔 什么是5-Why**：连续问5个"为什么"，找到问题根源。

```
问题现象：用户无法登录

Why1: 为什么无法登录？
答：因为登录接口超时

Why2: 为什么接口超时？
答：因为数据库查询慢

Why3: 为什么数据库慢？
答：因为查询了100万条数据

Why4: 为什么查询100万条？
答：因为没有加分页参数

Why5: 为什么没有分页？
答：因为代码逻辑有bug，忘记加了

根本原因：代码bug → 修复代码即可
```

### 9.2 故障根因分类


**📋 常见根因类型**：

```
🐛 代码问题（60%）
├─ 逻辑bug
├─ 性能问题  
├─ 内存泄漏
└─ 并发问题

⚙️ 配置问题（20%）
├─ 超时配置不合理
├─ 线程池太小
├─ 连接池不足
└─ JVM参数不当

🌐 环境问题（15%）
├─ 网络故障
├─ 磁盘满
├─ CPU/内存不足  
└─ 依赖服务故障

📊 容量问题（5%）
├─ 流量突增
├─ 数据量暴涨
└─ 资源不足
```

### 9.3 SkyWalking辅助根因分析


**🔍 根因分析流程**：

```
Step 1: 收集现象
从用户反馈、监控告警收集信息

Step 2: SkyWalking定位范围
```
拓扑图 → 找到问题服务
   ↓
调用链 → 找到慢的环节
   ↓  
Span详情 → 找到具体操作
```

Step 3: 分析数据
查看：
- 错误日志
- 性能指标  
- 调用关系
- 资源使用

Step 4: 假设验证
提出假设 → 查找证据 → 验证假设

Step 5: 定位根因
找到真正的原因，而不是表象
```

**💡 综合案例分析**：

```
【真实故障】电商大促期间系统崩溃

现象：
- 用户无法下单
- 页面打开很慢
- 服务频繁重启

SkyWalking分析过程：

1️⃣ 拓扑图分析
发现：库存服务响应时间从50ms增长到5秒

2️⃣ 调用链分析
库存服务调用链：
├─ 检查库存：4800ms ← 特别慢
├─ 扣减库存：100ms
└─ 返回结果：100ms

3️⃣ 数据库分析  
点击"检查库存"SQL：
SELECT stock FROM inventory 
WHERE product_id = ? FOR UPDATE
执行次数：2000次/秒
平均耗时：4800ms

4️⃣ 深入分析（5-Why）
Q1: 为什么SQL慢？
A1: 因为有行锁等待

Q2: 为什么有行锁？
A2: 因为FOR UPDATE加了行锁

Q3: 为什么用FOR UPDATE？
A3: 防止超卖，锁定库存

Q4: 为什么会锁这么久？  
A4: 事务范围太大，包含了调用积分服务

Q5: 为什么事务包含调用服务？
A5: 代码设计不合理

5️⃣ 根本原因
代码问题：事务范围过大
```java
@Transactional  // ← 问题：事务包含外部调用
public void createOrder() {
    // 1. 检查库存(加锁)
    checkStock();  
    
    // 2. 调用积分服务(慢，5秒)
    addPoints();   // ← 锁一直不释放
    
    // 3. 扣减库存  
    reduceStock();
    
    // 4. 创建订单
    createOrder();
}
```

6️⃣ 解决方案
优化事务范围：
```java
public void createOrder() {
    // 事务外：调用积分服务
    addPoints();
    
    // 事务内：只包含数据库操作
    @Transactional
    doCreateOrder() {
        checkStock();   // 锁
        reduceStock();  // 操作
    }  // 立即释放锁
}
```

7️⃣ 效果验证
- 库存检查：4800ms → 50ms
- 行锁等待：消除
- 吞吐量：提升100倍
- 大促成功：订单处理正常 ✅
```

---

## 10. 📋 核心要点总结


### 10.1 故障排查核心理念


```
🎯 基本原则：
不猜测 → 用数据说话
不慌乱 → 系统化排查  
不遗漏 → 记录完整过程
不重复 → 建立知识库
```

### 10.2 SkyWalking排查能力对照


| 故障类型 | SkyWalking能看到什么 | 排查重点 |
|----------|---------------------|----------|
| **慢查询** | SQL执行时间、语句内容 | 索引、查询优化 |
| **服务雪崩** | 调用链路、错误传播 | 熔断、限流、降级 |
| **内存泄漏** | JVM内存趋势 | 对象生命周期、GC |
| **网络延迟** | 网络传输耗时 | 部署位置、连接池 |
| **数据库瓶颈** | DB操作详情 | SQL优化、连接数 |
| **线程阻塞** | 线程状态分布 | 超时设置、异步化 |
| **超时重试** | 重试次数、成功率 | 重试策略优化 |

### 10.3 排查工具组合使用


```
完整的排查工具链：

📊 监控层：
- SkyWalking：调用链追踪
- Prometheus：指标监控
- Grafana：可视化展示

🔍 分析层：  
- 日志系统：详细错误信息
- JVM工具：内存、线程分析
- 数据库工具：慢查询、执行计划

🛠️ 诊断层：
- Arthas：在线诊断
- JProfiler：性能分析  
- VisualVM：内存分析
```

### 10.4 故障处理流程


```
🚨 故障发生：

1️⃣ 立即响应（5分钟内）
   - 确认影响范围
   - 启动应急预案
   - 通知相关人员

2️⃣ 快速止损（15分钟内）
   - 降级非核心功能
   - 扩容/重启服务
   - 切换备用方案

3️⃣ 问题定位（30分钟内）
   - 查看SkyWalking调用链
   - 分析监控指标
   - 定位问题环节

4️⃣ 修复验证（1小时内）
   - 实施修复方案
   - 灰度验证效果  
   - 全量发布

5️⃣ 复盘总结（1天内）
   - 分析根本原因
   - 制定改进措施
   - 更新知识库
```

### 10.5 预防故障的最佳实践


```
🛡️ 事前预防：

✅ 代码层面
- 超时控制：所有外部调用设置超时
- 重试策略：合理配置重试参数
- 资源隔离：线程池、连接池分开
- 限流保护：接口级别限流

✅ 架构层面  
- 熔断降级：快速失败机制
- 异步解耦：MQ削峰填谷
- 缓存优化：减少数据库压力
- 监控告警：及时发现问题

✅ 运维层面
- 容量规划：提前扩容
- 灰度发布：降低影响面
- 应急预案：准备应对方案
- 定期演练：提升处理能力
```

**💡 记忆口诀**：

```
🎯 排查故障记口诀：
现象收集要全面
调用链路看仔细  
性能指标细分析
根因分析五个why
应急处理要迅速
复盘总结防复发
```

**🌟 核心心法**：

```
故障排查不是靠运气，而是靠：
1. 完善的监控体系（眼睛要亮）
2. 系统的分析方法（思路要清）
3. 丰富的实战经验（手要稳）
4. 及时的复盘总结（要记住）

有了SkyWalking这双"慧眼"，
再配合科学的排查方法，
任何疑难杂症都能迎刃而解！
```