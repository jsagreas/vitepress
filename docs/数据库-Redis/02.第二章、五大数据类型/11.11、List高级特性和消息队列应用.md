---
title: 11ã€Listé«˜çº§ç‰¹æ€§å’Œæ¶ˆæ¯é˜Ÿåˆ—åº”ç”¨
---
## ğŸ“š ç›®å½•

1. [Listå†…éƒ¨å®ç°æœºåˆ¶](#1-listå†…éƒ¨å®ç°æœºåˆ¶)
2. [é«˜çº§æ¶ˆæ¯é˜Ÿåˆ—è®¾è®¡](#2-é«˜çº§æ¶ˆæ¯é˜Ÿåˆ—è®¾è®¡)
3. [ä¸‰ç§æ¶ˆæ¯æœºåˆ¶å¯¹æ¯”](#3-ä¸‰ç§æ¶ˆæ¯æœºåˆ¶å¯¹æ¯”)
4. [ç”Ÿäº§çº§é˜Ÿåˆ—å®ç°](#4-ç”Ÿäº§çº§é˜Ÿåˆ—å®ç°)
5. [æ ¸å¿ƒè¦ç‚¹æ€»ç»“](#5-æ ¸å¿ƒè¦ç‚¹æ€»ç»“)

---

## 1. ğŸ”§ Listå†…éƒ¨å®ç°æœºåˆ¶


### 1.1 quicklistå¿«é€Ÿåˆ—è¡¨ç»“æ„


**ğŸ§± ä»€ä¹ˆæ˜¯quicklist**
quicklistæ˜¯Redis Listç±»å‹çš„åº•å±‚æ•°æ®ç»“æ„ï¼Œå®ƒåƒ**ç«è½¦**ä¸€æ ·ï¼šæ¯èŠ‚è½¦å¢ï¼ˆziplistï¼‰è£…è½½æ•°æ®ï¼Œå¤šèŠ‚è½¦å¢è¿æˆä¸€åˆ—ç«è½¦ã€‚

```
ä¼ ç»ŸåŒå‘é“¾è¡¨é—®é¢˜ï¼š
èŠ‚ç‚¹1 â†â†’ èŠ‚ç‚¹2 â†â†’ èŠ‚ç‚¹3 â†â†’ èŠ‚ç‚¹4
é—®é¢˜ï¼šæ¯ä¸ªèŠ‚ç‚¹éƒ½æœ‰æŒ‡é’ˆå¼€é”€ï¼Œå†…å­˜ç¢ç‰‡å¤š

quicklistè§£å†³æ–¹æ¡ˆï¼š
è½¦å¢1[æ•°æ®1,æ•°æ®2,æ•°æ®3] â†â†’ è½¦å¢2[æ•°æ®4,æ•°æ®5,æ•°æ®6] â†â†’ è½¦å¢3[æ•°æ®7,æ•°æ®8]
ä¼˜åŠ¿ï¼šå‡å°‘æŒ‡é’ˆå¼€é”€ï¼Œæé«˜å†…å­˜åˆ©ç”¨ç‡
```

**ğŸ—ï¸ quicklistç»“æ„ç»„æˆ**
```
quicklistèŠ‚ç‚¹ç»“æ„ï¼š
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ quicklistNode           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ prev     æŒ‡å‘å‰ä¸€ä¸ªèŠ‚ç‚¹  â”‚
â”‚ next     æŒ‡å‘åä¸€ä¸ªèŠ‚ç‚¹  â”‚
â”‚ zl       æŒ‡å‘ziplist    â”‚ â† å®é™…å­˜å‚¨æ•°æ®çš„å‹ç¼©åˆ—è¡¨
â”‚ sz       ziplistå­—èŠ‚æ•°  â”‚
â”‚ count    å…ƒç´ ä¸ªæ•°        â”‚
â”‚ encoding ç¼–ç ç±»å‹       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

æ•´ä½“ç»“æ„ï¼š
quicklist
â”œâ”€ head: æŒ‡å‘ç¬¬ä¸€ä¸ªèŠ‚ç‚¹
â”œâ”€ tail: æŒ‡å‘æœ€åä¸€ä¸ªèŠ‚ç‚¹  
â”œâ”€ count: æ€»å…ƒç´ æ•°
â”œâ”€ len: èŠ‚ç‚¹æ€»æ•°
â””â”€ fill: æ¯ä¸ªèŠ‚ç‚¹çš„å¡«å……å› å­
```

### 1.2 å‹ç¼©èŠ‚ç‚¹ä¼˜åŒ–


**ğŸ“¦ LZFå‹ç¼©ç®—æ³•**
Redisä½¿ç”¨LZFç®—æ³•å‹ç¼©ziplistèŠ‚ç‚¹ï¼Œå°±åƒ**å‹ç¼©æ–‡ä»¶**ä¸€æ ·èŠ‚çœç©ºé—´ï¼š

```
å‹ç¼©é…ç½®å‚æ•°ï¼š
list-compress-depth 1

å«ä¹‰ï¼š
â€¢ 0ï¼šä¸å‹ç¼©ä»»ä½•èŠ‚ç‚¹
â€¢ 1ï¼šé™¤äº†å¤´å°¾å„1ä¸ªèŠ‚ç‚¹ï¼Œå…¶ä»–éƒ½å‹ç¼©  
â€¢ 2ï¼šé™¤äº†å¤´å°¾å„2ä¸ªèŠ‚ç‚¹ï¼Œå…¶ä»–éƒ½å‹ç¼©
â€¢ Nï¼šé™¤äº†å¤´å°¾å„Nä¸ªèŠ‚ç‚¹ï¼Œå…¶ä»–éƒ½å‹ç¼©

å‹ç¼©ç­–ç•¥ç¤ºæ„ï¼š
ä¸å‹ç¼©: [èŠ‚ç‚¹1] â†â†’ [èŠ‚ç‚¹2] â†â†’ [èŠ‚ç‚¹3] â†â†’ [èŠ‚ç‚¹4] â†â†’ [èŠ‚ç‚¹5]

å‹ç¼©æ·±åº¦1: [èŠ‚ç‚¹1] â†â†’ {å‹ç¼©èŠ‚ç‚¹} â†â†’ {å‹ç¼©èŠ‚ç‚¹} â†â†’ {å‹ç¼©èŠ‚ç‚¹} â†â†’ [èŠ‚ç‚¹5]
           â†‘å¤´éƒ¨ä¿ç•™                                        â†‘å°¾éƒ¨ä¿ç•™
```

**âš¡ ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡**
```
è®¾è®¡æ€è·¯ï¼š
â€¢ å¤´å°¾èŠ‚ç‚¹ï¼šç»å¸¸è¢«LPUSH/RPUSH/LPOP/RPOPè®¿é—®ï¼Œä¸å‹ç¼©
â€¢ ä¸­é—´èŠ‚ç‚¹ï¼šå¾ˆå°‘è®¿é—®ï¼Œå¯ä»¥å‹ç¼©èŠ‚çœå†…å­˜
â€¢ è®¿é—®æ—¶è§£å‹ï¼šéœ€è¦è®¿é—®ä¸­é—´æ•°æ®æ—¶ä¸´æ—¶è§£å‹

æƒè¡¡è€ƒè™‘ï¼š
âœ… ä¼˜åŠ¿ï¼šæ˜¾è‘—èŠ‚çœå†…å­˜ç©ºé—´
âŒ åŠ£åŠ¿ï¼šè®¿é—®ä¸­é—´æ•°æ®éœ€è¦è§£å‹ç¼©æ“ä½œ
ğŸ¯ é€‚åˆï¼šé˜Ÿåˆ—åœºæ™¯ï¼ˆä¸»è¦æ“ä½œå¤´å°¾ï¼‰
```

### 1.3 å†…å­˜ä½¿ç”¨åˆ†æ


**ğŸ“Š å†…å­˜å ç”¨è®¡ç®—**
```
Listå†…å­˜ç»„æˆï¼š
1. quicklistç»“æ„ï¼šå›ºå®š40å­—èŠ‚
2. quicklistNodeï¼šæ¯ä¸ªèŠ‚ç‚¹40å­—èŠ‚  
3. ziplistï¼šå®é™…å­˜å‚¨æ•°æ®
4. å‹ç¼©å¼€é”€ï¼šLZFå‹ç¼©çš„CPUå’Œæ—¶é—´æˆæœ¬

å†…å­˜è®¡ç®—ç¤ºä¾‹ï¼š
å‡è®¾Listæœ‰1000ä¸ªå…ƒç´ ï¼Œæ¯ä¸ªå…ƒç´ 50å­—èŠ‚

ä¸ä½¿ç”¨quicklistï¼ˆçº¯é“¾è¡¨ï¼‰ï¼š
1000 Ã— (50 + 16) = 66KB  # 16å­—èŠ‚æ˜¯é“¾è¡¨èŠ‚ç‚¹æŒ‡é’ˆå¼€é”€

ä½¿ç”¨quicklistï¼ˆæ¯ä¸ªziplistè£…100ä¸ªå…ƒç´ ï¼‰ï¼š  
10ä¸ªèŠ‚ç‚¹ Ã— 40å­—èŠ‚ + 10ä¸ªziplist Ã— (100Ã—50) = 50.4KB
èŠ‚çœå†…å­˜ï¼š(66-50.4)/66 = 23.6%
```

**ğŸ” é…ç½®å‚æ•°å½±å“**
```bash
# Redisé…ç½®æ–‡ä»¶ä¸­çš„Listç›¸å…³è®¾ç½®

list-max-ziplist-size -2
# æ§åˆ¶æ¯ä¸ªziplistçš„å¤§å°
# æ­£æ•°ï¼šé™åˆ¶å…ƒç´ ä¸ªæ•°
# è´Ÿæ•°ï¼šé™åˆ¶å­—èŠ‚æ•°
#   -1: 4KB, -2: 8KB, -3: 16KB, -4: 32KB, -5: 64KB

list-compress-depth 1  
# å‹ç¼©æ·±åº¦ï¼Œ0è¡¨ç¤ºä¸å‹ç¼©

# å®é™…å½±å“ï¼š
-2 (8KB): é€‚ä¸­çš„å†…å­˜å’Œæ€§èƒ½å¹³è¡¡
-1 (4KB): æ›´å¤šèŠ‚ç‚¹ï¼Œæ›´ç»†ç²’åº¦
-5 (64KB): æ›´å°‘èŠ‚ç‚¹ï¼Œå¯èƒ½å½±å“æ€§èƒ½
```

### 1.4 æ€§èƒ½ç‰¹å¾è¯¦è§£


**âš¡ æ“ä½œå¤æ‚åº¦åˆ†æ**
```
å¤´å°¾æ“ä½œï¼šO(1) - æœ€é«˜æ•ˆ
â€¢ LPUSH/RPUSH: ç›´æ¥åœ¨å¤´å°¾ziplistæ’å…¥
â€¢ LPOP/RPOP: ç›´æ¥ä»å¤´å°¾ziplistç§»é™¤
â€¢ LLEN: ç›´æ¥è¿”å›countå­—æ®µ

ç´¢å¼•æ“ä½œï¼šO(N) - éœ€è¦éå†
â€¢ LINDEX: éœ€è¦éå†åˆ°æŒ‡å®šèŠ‚ç‚¹
â€¢ LSET: éœ€è¦å…ˆå®šä½å†ä¿®æ”¹
â€¢ ä¼˜åŒ–ï¼šRedisä¼šè®°ä½æœ€è¿‘è®¿é—®çš„èŠ‚ç‚¹ä½ç½®

èŒƒå›´æ“ä½œï¼šO(N) - ä¸èŒƒå›´å¤§å°ç›¸å…³  
â€¢ LRANGE: éœ€è¦éå†èŒƒå›´å†…çš„èŠ‚ç‚¹
â€¢ LTRIM: éœ€è¦åˆ é™¤èŒƒå›´å¤–çš„èŠ‚ç‚¹
```

**ğŸš€ æ€§èƒ½ä¼˜åŒ–ç‰¹æ€§**
```python
# Rediså†…éƒ¨ä¼˜åŒ–æœºåˆ¶æ¼”ç¤º

# 1. èŠ‚ç‚¹ç¼“å­˜ä¼˜åŒ–
redis_client.lindex('mylist', 100)  # ç¬¬ä¸€æ¬¡è®¿é—®ï¼Œéå†åˆ°èŠ‚ç‚¹
redis_client.lindex('mylist', 101)  # ç¬¬äºŒæ¬¡è®¿é—®ï¼Œä»ç¼“å­˜ä½ç½®å¼€å§‹

# 2. æ‰¹é‡æ“ä½œä¼˜åŒ–
redis_client.lpush('mylist', *[f'item{i}' for i in range(100)])
# ä¸€æ¬¡LPUSHå¤šä¸ªå…ƒç´ æ¯”å¤šæ¬¡LPUSHæ•ˆç‡é«˜

# 3. ç®¡é“ä¼˜åŒ–
pipe = redis_client.pipeline()
for i in range(1000):
    pipe.rpush('mylist', f'data{i}')
pipe.execute()  # æ‰¹é‡æäº¤ï¼Œå‡å°‘ç½‘ç»œå¾€è¿”
```

---

## 2. ğŸ“« é«˜çº§æ¶ˆæ¯é˜Ÿåˆ—è®¾è®¡


### 2.1 æ¶ˆæ¯ç¡®è®¤æœºåˆ¶è®¾è®¡


**âœ… å¯é æ¶ˆæ¯é˜Ÿåˆ—çš„æ ¸å¿ƒé—®é¢˜**
æ™®é€šçš„LPUSH/RPOPå°±åƒ**å¯„ä¿¡**ï¼Œä¿¡å¯„å‡ºå»å°±ä¸ç®¡äº†ã€‚ä½†ç”Ÿäº§ç¯å¢ƒéœ€è¦**æŒ‚å·ä¿¡**ï¼Œç¡®ä¿æ¶ˆæ¯è¢«æ­£ç¡®å¤„ç†ã€‚

```
é—®é¢˜åœºæ™¯ï¼š
1. æ¶ˆè´¹è€…å–å‡ºæ¶ˆæ¯åå´©æºƒäº†ï¼Œæ¶ˆæ¯ä¸¢å¤±
2. ç½‘ç»œä¸­æ–­ï¼Œæ¶ˆæ¯å¤„ç†çŠ¶æ€ä¸æ˜
3. æ¶ˆè´¹è€…å¤„ç†å¤±è´¥ï¼Œæ¶ˆæ¯éœ€è¦é‡æ–°å¤„ç†

è§£å†³æ€è·¯ï¼š
ç”Ÿäº§è€… â†’ [å¾…å¤„ç†é˜Ÿåˆ—] â†’ æ¶ˆè´¹è€… â†’ [å¤„ç†ä¸­é˜Ÿåˆ—] â†’ ç¡®è®¤ â†’ [å®Œæˆåˆ é™¤]
                        â†“ å¤±è´¥
                     [é‡è¯•é˜Ÿåˆ—]
```

**ğŸ”§ åŸºäºRedis Listçš„ç¡®è®¤æœºåˆ¶å®ç°**
```python
import redis
import json
import time
import uuid

class ReliableQueue:
    def __init__(self, redis_client, queue_name):
        self.redis = redis_client
        self.pending_queue = f"{queue_name}:pending"      # å¾…å¤„ç†é˜Ÿåˆ—
        self.processing_queue = f"{queue_name}:processing" # å¤„ç†ä¸­é˜Ÿåˆ—
        self.retry_queue = f"{queue_name}:retry"          # é‡è¯•é˜Ÿåˆ—
        
    def send_message(self, message):
        """å‘é€æ¶ˆæ¯åˆ°é˜Ÿåˆ—"""
        msg_id = str(uuid.uuid4())
        msg_data = {
            'id': msg_id,
            'data': message,
            'timestamp': time.time(),
            'retry_count': 0
        }
        
        # æ·»åŠ åˆ°å¾…å¤„ç†é˜Ÿåˆ—
        self.redis.lpush(self.pending_queue, json.dumps(msg_data))
        return msg_id
    
    def consume_message(self, timeout=10):
        """æ¶ˆè´¹æ¶ˆæ¯ï¼ˆåŸå­æ“ä½œï¼‰"""
        # BRPOPLPUSHï¼šåŸå­åœ°ä»pendingç§»åŠ¨åˆ°processing
        raw_msg = self.redis.brpoplpush(
            self.pending_queue, 
            self.processing_queue, 
            timeout
        )
        
        if raw_msg:
            return json.loads(raw_msg)
        return None
    
    def ack_message(self, message):
        """ç¡®è®¤æ¶ˆæ¯å¤„ç†å®Œæˆ"""
        msg_json = json.dumps(message)
        # ä»å¤„ç†ä¸­é˜Ÿåˆ—ç§»é™¤æ¶ˆæ¯
        removed = self.redis.lrem(self.processing_queue, 1, msg_json)
        return removed > 0
    
    def nack_message(self, message, max_retries=3):
        """æ¶ˆæ¯å¤„ç†å¤±è´¥ï¼Œå†³å®šé‡è¯•æˆ–ä¸¢å¼ƒ"""
        message['retry_count'] += 1
        msg_json = json.dumps(message)
        
        # ä»å¤„ç†ä¸­é˜Ÿåˆ—ç§»é™¤
        self.redis.lrem(self.processing_queue, 1, msg_json)
        
        if message['retry_count'] < max_retries:
            # é‡æ–°åŠ å…¥é‡è¯•é˜Ÿåˆ—
            self.redis.lpush(self.retry_queue, msg_json)
        else:
            # è¶…è¿‡é‡è¯•æ¬¡æ•°ï¼Œè®°å½•åˆ°æ­»ä¿¡é˜Ÿåˆ—
            dead_letter_queue = f"{self.retry_queue}:dead"
            self.redis.lpush(dead_letter_queue, msg_json)

# ä½¿ç”¨ç¤ºä¾‹
redis_client = redis.Redis()
queue = ReliableQueue(redis_client, 'order_queue')

# ç”Ÿäº§è€…å‘é€æ¶ˆæ¯
order_id = queue.send_message({'order_id': 12345, 'amount': 99.99})

# æ¶ˆè´¹è€…å¤„ç†æ¶ˆæ¯
message = queue.consume_message(timeout=30)
if message:
    try:
        # å¤„ç†ä¸šåŠ¡é€»è¾‘
        process_order(message['data'])
        # ç¡®è®¤å¤„ç†æˆåŠŸ
        queue.ack_message(message)
    except Exception as e:
        # å¤„ç†å¤±è´¥ï¼Œæ ‡è®°é‡è¯•
        queue.nack_message(message)
```

### 2.2 æ­»ä¿¡é˜Ÿåˆ—å¤„ç†


**ğŸ’€ æ­»ä¿¡é˜Ÿåˆ—çš„å¿…è¦æ€§**
æ­»ä¿¡é˜Ÿåˆ—å°±åƒåŒ»é™¢çš„**é‡ç—‡ç›‘æŠ¤å®¤**ï¼Œæ”¶æ²»å¤„ç†å¤±è´¥çš„æ¶ˆæ¯ï¼š

```
æ¶ˆæ¯ç”Ÿå‘½å‘¨æœŸï¼š
æ­£å¸¸æ¶ˆæ¯ â†’ å¤„ç†æˆåŠŸ â†’ åˆ é™¤
å¼‚å¸¸æ¶ˆæ¯ â†’ é‡è¯•1æ¬¡ â†’ é‡è¯•2æ¬¡ â†’ é‡è¯•3æ¬¡ â†’ æ­»ä¿¡é˜Ÿåˆ—

æ­»ä¿¡æ¶ˆæ¯ç‰¹å¾ï¼š
â€¢ é‡è¯•æ¬¡æ•°è¶…è¿‡é™åˆ¶
â€¢ æ¶ˆæ¯æ ¼å¼é”™è¯¯æ— æ³•è§£æ
â€¢ ä¸šåŠ¡é€»è¾‘å¤„ç†æ°¸ä¹…æ€§å¤±è´¥
â€¢ æ¶ˆæ¯è¿‡æœŸå¤±æ•ˆ
```

**ğŸ¥ æ­»ä¿¡é˜Ÿåˆ—ç®¡ç†ç³»ç»Ÿ**
```python
class DeadLetterManager:
    def __init__(self, redis_client, queue_name):
        self.redis = redis_client
        self.dead_queue = f"{queue_name}:dead"
        self.analysis_key = f"{queue_name}:dead_stats"
    
    def get_dead_messages(self, limit=100):
        """è·å–æ­»ä¿¡æ¶ˆæ¯åˆ—è¡¨"""
        messages = self.redis.lrange(self.dead_queue, 0, limit-1)
        return [json.loads(msg) for msg in messages]
    
    def analyze_dead_reasons(self):
        """åˆ†ææ­»ä¿¡äº§ç”ŸåŸå› """
        messages = self.get_dead_messages(1000)
        
        analysis = {
            'total_count': len(messages),
            'by_error_type': {},
            'by_retry_count': {},
            'oldest_message': None,
            'newest_message': None
        }
        
        for msg in messages:
            # ç»Ÿè®¡é‡è¯•æ¬¡æ•°åˆ†å¸ƒ
            retry_count = msg.get('retry_count', 0)
            analysis['by_retry_count'][retry_count] = analysis['by_retry_count'].get(retry_count, 0) + 1
            
            # è®°å½•æ—¶é—´èŒƒå›´
            timestamp = msg.get('timestamp', 0)
            if not analysis['oldest_message'] or timestamp < analysis['oldest_message']:
                analysis['oldest_message'] = timestamp
            if not analysis['newest_message'] or timestamp > analysis['newest_message']:
                analysis['newest_message'] = timestamp
        
        return analysis
    
    def resurrect_message(self, message_id, target_queue):
        """å¤æ´»æ­»ä¿¡æ¶ˆæ¯åˆ°æŒ‡å®šé˜Ÿåˆ—"""
        messages = self.get_dead_messages(1000)
        
        for msg in messages:
            if msg.get('id') == message_id:
                # é‡ç½®é‡è¯•è®¡æ•°
                msg['retry_count'] = 0
                msg['resurrected_at'] = time.time()
                
                # ç§»åŠ¨åˆ°ç›®æ ‡é˜Ÿåˆ—
                msg_json = json.dumps(msg)
                self.redis.lrem(self.dead_queue, 1, msg_json)
                self.redis.lpush(target_queue, msg_json)
                return True
        
        return False
    
    def cleanup_old_messages(self, days=30):
        """æ¸…ç†è¿‡æœŸçš„æ­»ä¿¡æ¶ˆæ¯"""
        cutoff_time = time.time() - (days * 24 * 3600)
        messages = self.get_dead_messages(10000)
        
        cleaned_count = 0
        for msg in messages:
            if msg.get('timestamp', 0) < cutoff_time:
                msg_json = json.dumps(msg)
                self.redis.lrem(self.dead_queue, 1, msg_json)
                cleaned_count += 1
        
        return cleaned_count

# æ­»ä¿¡ç›‘æ§å’ŒæŠ¥è­¦
def monitor_dead_letter_queue():
    manager = DeadLetterManager(redis_client, 'order_queue')
    stats = manager.analyze_dead_reasons()
    
    # è®¾ç½®å‘Šè­¦é˜ˆå€¼
    if stats['total_count'] > 100:
        send_alert(f"æ­»ä¿¡é˜Ÿåˆ—æ¶ˆæ¯è¿‡å¤š: {stats['total_count']}")
    
    # å®šæœŸæ¸…ç†
    if stats['total_count'] > 1000:
        cleaned = manager.cleanup_old_messages(7)  # æ¸…ç†7å¤©å‰çš„
        print(f"æ¸…ç†äº† {cleaned} æ¡è¿‡æœŸæ­»ä¿¡æ¶ˆæ¯")
```

### 2.3 æ¶ˆæ¯é‡è¯•æœºåˆ¶


**ğŸ”„ æ™ºèƒ½é‡è¯•ç­–ç•¥**
é‡è¯•å°±åƒ**æ•²é—¨**ï¼Œç¬¬ä¸€æ¬¡æ•²æ²¡äººå¬åˆ°ï¼Œè¿‡ä¸€ä¼šå„¿å†æ•²ï¼Œé—´éš”æ—¶é—´è¦é€æ¸å¢åŠ ï¼š

```
é‡è¯•ç­–ç•¥ç±»å‹ï¼š
1. å›ºå®šé—´éš”ï¼šæ¯æ¬¡éƒ½ç­‰5ç§’ (é€‚åˆä¸´æ—¶æ•…éšœ)
2. æŒ‡æ•°é€€é¿ï¼š1ç§’â†’2ç§’â†’4ç§’â†’8ç§’ (é¿å…ç³»ç»Ÿè¿‡è½½)
3. çº¿æ€§å¢é•¿ï¼š1ç§’â†’2ç§’â†’3ç§’â†’4ç§’ (æ¸è¿›å¼æ¢å¤)
4. éšæœºæŠ–åŠ¨ï¼šåœ¨åŸºç¡€æ—¶é—´ä¸ŠåŠ éšæœºå€¼ (é¿å…æƒŠç¾¤æ•ˆåº”)
```

**âš™ï¸ é‡è¯•æœºåˆ¶å®ç°**
```python
import random
import math

class RetryManager:
    def __init__(self, redis_client, queue_name):
        self.redis = redis_client
        self.retry_queue = f"{queue_name}:retry"
        self.delayed_queue = f"{queue_name}:delayed"
        
    def schedule_retry(self, message, strategy='exponential'):
        """è°ƒåº¦æ¶ˆæ¯é‡è¯•"""
        retry_count = message.get('retry_count', 0)
        
        # è®¡ç®—å»¶è¿Ÿæ—¶é—´
        delay = self._calculate_delay(retry_count, strategy)
        
        # è®¾ç½®é‡è¯•æ—¶é—´
        retry_time = time.time() + delay
        message['retry_at'] = retry_time
        message['retry_delay'] = delay
        
        # æ·»åŠ åˆ°å»¶è¿Ÿé˜Ÿåˆ—ï¼ˆæŒ‰æ—¶é—´æ’åºï¼‰
        self.redis.zadd(
            self.delayed_queue,
            {json.dumps(message): retry_time}
        )
    
    def _calculate_delay(self, retry_count, strategy):
        """è®¡ç®—é‡è¯•å»¶è¿Ÿæ—¶é—´"""
        if strategy == 'fixed':
            return 5  # å›ºå®š5ç§’
            
        elif strategy == 'exponential':
            # æŒ‡æ•°é€€é¿ï¼š2^retry_count ç§’ï¼Œæœ€å¤§300ç§’
            base_delay = min(2 ** retry_count, 300)
            # åŠ å…¥éšæœºæŠ–åŠ¨ï¼ˆÂ±20%ï¼‰
            jitter = base_delay * 0.2 * (random.random() - 0.5)
            return base_delay + jitter
            
        elif strategy == 'linear':
            # çº¿æ€§å¢é•¿ï¼šretry_count * 10 ç§’
            return min(retry_count * 10, 180)
            
        else:
            return 60  # é»˜è®¤1åˆ†é’Ÿ
    
    def process_delayed_messages(self):
        """å¤„ç†åˆ°æœŸçš„å»¶è¿Ÿæ¶ˆæ¯"""
        current_time = time.time()
        
        # è·å–åˆ°æœŸçš„æ¶ˆæ¯ï¼ˆä½¿ç”¨ZRANGEBYSCOREï¼‰
        expired_messages = self.redis.zrangebyscore(
            self.delayed_queue,
            0,
            current_time,
            withscores=True
        )
        
        moved_count = 0
        for msg_json, score in expired_messages:
            # ç§»åŠ¨åˆ°é‡è¯•é˜Ÿåˆ—
            self.redis.lpush(self.retry_queue, msg_json)
            # ä»å»¶è¿Ÿé˜Ÿåˆ—ç§»é™¤
            self.redis.zrem(self.delayed_queue, msg_json)
            moved_count += 1
        
        return moved_count
    
    def get_retry_stats(self):
        """è·å–é‡è¯•ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'retry_queue_length': self.redis.llen(self.retry_queue),
            'delayed_queue_length': self.redis.zcard(self.delayed_queue),
            'next_retry_time': self._get_next_retry_time()
        }
    
    def _get_next_retry_time(self):
        """è·å–ä¸‹ä¸€æ¬¡é‡è¯•æ—¶é—´"""
        result = self.redis.zrange(self.delayed_queue, 0, 0, withscores=True)
        if result:
            return result[0][1]  # è¿”å›æœ€æ—©çš„æ—¶é—´æˆ³
        return None

# åå°é‡è¯•å¤„ç†å™¨
import threading
import time

def retry_processor_daemon():
    """åå°å®ˆæŠ¤è¿›ç¨‹å¤„ç†é‡è¯•"""
    retry_manager = RetryManager(redis_client, 'order_queue')
    
    while True:
        try:
            # å¤„ç†åˆ°æœŸçš„å»¶è¿Ÿæ¶ˆæ¯
            moved = retry_manager.process_delayed_messages()
            if moved > 0:
                print(f"ç§»åŠ¨äº† {moved} æ¡æ¶ˆæ¯åˆ°é‡è¯•é˜Ÿåˆ—")
            
            # æ¯10ç§’æ£€æŸ¥ä¸€æ¬¡
            time.sleep(10)
            
        except Exception as e:
            print(f"é‡è¯•å¤„ç†å™¨é”™è¯¯: {e}")
            time.sleep(30)  # å‡ºé”™åç­‰å¾…30ç§’

# å¯åŠ¨åå°å¤„ç†å™¨
retry_thread = threading.Thread(target=retry_processor_daemon, daemon=True)
retry_thread.start()
```

### 2.4 é˜Ÿåˆ—ç›‘æ§ä½“ç³»


**ğŸ“Š ç›‘æ§æŒ‡æ ‡è®¾è®¡**
```python
class QueueMonitor:
    def __init__(self, redis_client, queue_name):
        self.redis = redis_client
        self.queue_name = queue_name
        self.metrics_key = f"{queue_name}:metrics"
    
    def collect_metrics(self):
        """æ”¶é›†é˜Ÿåˆ—æŒ‡æ ‡"""
        metrics = {
            'timestamp': time.time(),
            'pending_count': self.redis.llen(f"{self.queue_name}:pending"),
            'processing_count': self.redis.llen(f"{self.queue_name}:processing"),
            'retry_count': self.redis.llen(f"{self.queue_name}:retry"),
            'dead_count': self.redis.llen(f"{self.queue_name}:dead"),
            'delayed_count': self.redis.zcard(f"{self.queue_name}:delayed"),
        }
        
        # è®°å½•å†å²æŒ‡æ ‡
        self.redis.lpush(self.metrics_key, json.dumps(metrics))
        # åªä¿ç•™æœ€è¿‘1000æ¡è®°å½•
        self.redis.ltrim(self.metrics_key, 0, 999)
        
        return metrics
    
    def get_throughput_stats(self, hours=1):
        """è®¡ç®—ååé‡ç»Ÿè®¡"""
        cutoff_time = time.time() - (hours * 3600)
        
        # è·å–å†å²æŒ‡æ ‡
        raw_metrics = self.redis.lrange(self.metrics_key, 0, -1)
        metrics_list = []
        
        for raw in raw_metrics:
            metric = json.loads(raw)
            if metric['timestamp'] > cutoff_time:
                metrics_list.append(metric)
        
        if len(metrics_list) < 2:
            return {'error': 'æ•°æ®ä¸è¶³ï¼Œæ— æ³•è®¡ç®—ååé‡'}
        
        # è®¡ç®—å¤„ç†é€Ÿç‡
        first_metric = metrics_list[-1]  # æœ€æ—©çš„
        last_metric = metrics_list[0]    # æœ€æ–°çš„
        
        time_diff = last_metric['timestamp'] - first_metric['timestamp']
        
        if time_diff == 0:
            return {'error': 'æ—¶é—´é—´éš”å¤ªçŸ­'}
        
        # ç®€åŒ–è®¡ç®—ï¼šå‡è®¾processingé˜Ÿåˆ—çš„å˜åŒ–åæ˜ å¤„ç†é‡
        processing_change = (first_metric['processing_count'] - 
                           last_metric['processing_count'])
        
        throughput = processing_change / time_diff * 60  # æ¯åˆ†é’Ÿå¤„ç†é‡
        
        return {
            'throughput_per_minute': throughput,
            'avg_pending': sum(m['pending_count'] for m in metrics_list) / len(metrics_list),
            'max_pending': max(m['pending_count'] for m in metrics_list),
            'error_rate': last_metric['dead_count'] / (last_metric['dead_count'] + processing_change) if processing_change > 0 else 0
        }
    
    def health_check(self):
        """é˜Ÿåˆ—å¥åº·æ£€æŸ¥"""
        metrics = self.collect_metrics()
        health_status = {'status': 'healthy', 'warnings': [], 'errors': []}
        
        # æ£€æŸ¥å¾…å¤„ç†é˜Ÿåˆ—ç§¯å‹
        if metrics['pending_count'] > 1000:
            health_status['warnings'].append(f"å¾…å¤„ç†æ¶ˆæ¯è¿‡å¤š: {metrics['pending_count']}")
        
        # æ£€æŸ¥å¤„ç†ä¸­æ¶ˆæ¯æ˜¯å¦è¿‡å¤šï¼ˆå¯èƒ½æœ‰æ¶ˆè´¹è€…å®•æœºï¼‰
        if metrics['processing_count'] > 100:
            health_status['warnings'].append(f"å¤„ç†ä¸­æ¶ˆæ¯å¼‚å¸¸: {metrics['processing_count']}")
        
        # æ£€æŸ¥æ­»ä¿¡é˜Ÿåˆ—
        if metrics['dead_count'] > 50:
            health_status['errors'].append(f"æ­»ä¿¡æ¶ˆæ¯è¿‡å¤š: {metrics['dead_count']}")
        
        # ç»¼åˆåˆ¤æ–­çŠ¶æ€
        if health_status['errors']:
            health_status['status'] = 'error'
        elif health_status['warnings']:
            health_status['status'] = 'warning'
        
        return health_status

# ç›‘æ§å‘Šè­¦ç¤ºä¾‹
def setup_monitoring():
    monitor = QueueMonitor(redis_client, 'order_queue')
    
    def monitoring_loop():
        while True:
            try:
                # æ”¶é›†æŒ‡æ ‡
                metrics = monitor.collect_metrics()
                
                # å¥åº·æ£€æŸ¥
                health = monitor.health_check()
                
                # å‘é€å‘Šè­¦
                if health['status'] != 'healthy':
                    send_alert(f"é˜Ÿåˆ—çŠ¶æ€å¼‚å¸¸: {health}")
                
                # è®°å½•æ—¥å¿—
                print(f"é˜Ÿåˆ—æŒ‡æ ‡: {metrics}")
                
                time.sleep(60)  # æ¯åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
                
            except Exception as e:
                print(f"ç›‘æ§å¼‚å¸¸: {e}")
                time.sleep(60)
    
    # å¯åŠ¨ç›‘æ§çº¿ç¨‹
    monitor_thread = threading.Thread(target=monitoring_loop, daemon=True)
    monitor_thread.start()

def send_alert(message):
    """å‘é€å‘Šè­¦ï¼ˆç¤ºä¾‹ï¼‰"""
    print(f"ğŸš¨ å‘Šè­¦: {message}")
    # å®é™…å¯ä»¥é›†æˆä¼ä¸šå¾®ä¿¡ã€é’‰é’‰ã€é‚®ä»¶ç­‰å‘Šè­¦æ–¹å¼
```

---

## 3. âš–ï¸ ä¸‰ç§æ¶ˆæ¯æœºåˆ¶å¯¹æ¯”


### 3.1 Redisæ¶ˆæ¯æœºåˆ¶æ¦‚è¿°


**ğŸ“‹ ä¸‰ç§æ¶ˆæ¯ä¼ é€’æ–¹å¼**
Redisæä¾›äº†ä¸‰ç§ä¸»è¦çš„æ¶ˆæ¯ä¼ é€’æœºåˆ¶ï¼Œå°±åƒä¸‰ç§ä¸åŒçš„**é‚®ä»¶ç³»ç»Ÿ**ï¼š

```
Listæ¶ˆæ¯é˜Ÿåˆ—ï¼š
åƒä¼ ç»Ÿé‚®å±€ - æ¶ˆæ¯æŒ‰é¡ºåºå­˜å‚¨ï¼Œå…ˆåˆ°å…ˆå¾—

Pub/Subå‘å¸ƒè®¢é˜…ï¼š
åƒå¹¿æ’­ç”µå° - æ¶ˆæ¯å®æ—¶å¹¿æ’­ï¼Œè®¢é˜…è€…åŒæ—¶æ”¶åˆ°

Streamæµï¼š
åƒç°ä»£å¿«é€’ - æ¶ˆæ¯æœ‰åºå­˜å‚¨ï¼Œæ”¯æŒæ¶ˆè´¹ç»„å’Œç¡®è®¤æœºåˆ¶
```

### 3.2 è¯¦ç»†å¯¹æ¯”åˆ†æ


| **ç‰¹æ€§** | **Listé˜Ÿåˆ—** | **Pub/Sub** | **Stream** |
|---------|-------------|-------------|-----------|
| **æŒä¹…åŒ–** | `âœ… æŒä¹…å­˜å‚¨` | `âŒ ä¸æŒä¹…åŒ–` | `âœ… æŒä¹…å­˜å‚¨` |
| **æ¶ˆæ¯ä¸¢å¤±** | `ä¸ä¼šä¸¢å¤±` | `è®¢é˜…è€…ç¦»çº¿ä¼šä¸¢å¤±` | `ä¸ä¼šä¸¢å¤±` |
| **å¤šæ¶ˆè´¹è€…** | `ç«äº‰æ¶ˆè´¹` | `å¹¿æ’­æ¶ˆè´¹` | `æ¶ˆè´¹ç»„æ¨¡å¼` |
| **æ¶ˆæ¯ç¡®è®¤** | `éœ€æ‰‹åŠ¨å®ç°` | `æ— ç¡®è®¤æœºåˆ¶` | `å†…ç½®ç¡®è®¤æœºåˆ¶` |
| **å†å²æ¶ˆæ¯** | `LRANGEæŸ¥çœ‹` | `æ— æ³•è·å–` | `XRANGEæŸ¥çœ‹` |
| **å†…å­˜ä½¿ç”¨** | `ä¸­ç­‰` | `æœ€å°‘` | `æœ€å¤š` |
| **å¤æ‚åº¦** | `ç®€å•` | `æœ€ç®€å•` | `è¾ƒå¤æ‚` |

### 3.3 ä½¿ç”¨åœºæ™¯åˆ†æ


**ğŸ¯ Listé˜Ÿåˆ—é€‚ç”¨åœºæ™¯**
```
âœ… é€‚åˆï¼š
â€¢ ä»»åŠ¡é˜Ÿåˆ—ï¼šåå°å¤„ç†ä»»åŠ¡
â€¢ æ¶ˆæ¯é˜Ÿåˆ—ï¼šç³»ç»Ÿé—´å¼‚æ­¥é€šä¿¡  
â€¢ å·¥ä½œé˜Ÿåˆ—ï¼šå¤šä¸ªWorkerç«äº‰å¤„ç†
â€¢ éœ€è¦æŒä¹…åŒ–çš„åœºæ™¯

ğŸ”§ å…¸å‹åº”ç”¨ï¼š
â€¢ ç”µå•†è®¢å•å¤„ç†é˜Ÿåˆ—
â€¢ å›¾ç‰‡/è§†é¢‘å¤„ç†ä»»åŠ¡
â€¢ é‚®ä»¶å‘é€é˜Ÿåˆ—
â€¢ æ•°æ®åŒæ­¥ä»»åŠ¡

âš ï¸ æ³¨æ„ï¼š
â€¢ éœ€è¦æ‰‹åŠ¨å®ç°æ¶ˆæ¯ç¡®è®¤
â€¢ å•ç‚¹æ¶ˆè´¹ï¼Œä¸æ”¯æŒå¹¿æ’­
â€¢ ä¸æ”¯æŒæ¶ˆæ¯è·¯ç”±
```

**ğŸ“¡ Pub/Subé€‚ç”¨åœºæ™¯**
```
âœ… é€‚åˆï¼š
â€¢ å®æ—¶é€šçŸ¥ï¼šç³»ç»ŸçŠ¶æ€å˜æ›´é€šçŸ¥
â€¢ èŠå¤©ç³»ç»Ÿï¼šå®æ—¶æ¶ˆæ¯æ¨é€
â€¢ é…ç½®æ›´æ–°ï¼šé…ç½®å˜æ›´å®æ—¶åŒæ­¥
â€¢ ç›‘æ§å‘Šè­¦ï¼šå®æ—¶å‘Šè­¦æ¨é€

ğŸ”§ å…¸å‹åº”ç”¨ï¼š
â€¢ å¾®æœåŠ¡é…ç½®åˆ·æ–°
â€¢ ç¼“å­˜å¤±æ•ˆé€šçŸ¥
â€¢ ç”¨æˆ·åœ¨çº¿çŠ¶æ€åŒæ­¥
â€¢ å®æ—¶æ—¥å¿—æ”¶é›†

âš ï¸ æ³¨æ„ï¼š
â€¢ æ¶ˆæ¯ä¸æŒä¹…åŒ–
â€¢ è®¢é˜…è€…ç¦»çº¿æ¶ˆæ¯ä¸¢å¤±
â€¢ æ— æ³•ç¡®è®¤æ¶ˆæ¯å¤„ç†çŠ¶æ€
```

**ğŸŒŠ Streamé€‚ç”¨åœºæ™¯**
```
âœ… é€‚åˆï¼š
â€¢ äº‹ä»¶æº¯æºï¼šéœ€è¦å†å²äº‹ä»¶è®°å½•
â€¢ æ•°æ®æµå¤„ç†ï¼šå®æ—¶æ•°æ®åˆ†æ
â€¢ æ¶ˆæ¯é˜Ÿåˆ—ï¼šéœ€è¦ç¡®è®¤æœºåˆ¶
â€¢ æ—¥å¿—é‡‡é›†ï¼šç»“æ„åŒ–æ—¥å¿—å­˜å‚¨

ğŸ”§ å…¸å‹åº”ç”¨ï¼š
â€¢ ç”¨æˆ·è¡Œä¸ºäº‹ä»¶æµ
â€¢ ç³»ç»Ÿæ“ä½œæ—¥å¿—
â€¢ ç‰©è”ç½‘æ•°æ®é‡‡é›†
â€¢ é‡‘èäº¤æ˜“è®°å½•

âš ï¸ æ³¨æ„ï¼š
â€¢ Redis 5.0+æ‰æ”¯æŒ
â€¢ åŠŸèƒ½å¤æ‚ï¼Œå­¦ä¹ æˆæœ¬é«˜
â€¢ å†…å­˜å ç”¨ç›¸å¯¹è¾ƒå¤§
```

### 3.4 æ€§èƒ½ç‰¹ç‚¹å¯¹æ¯”


**âš¡ æ€§èƒ½æµ‹è¯•æ•°æ®**
```python
import redis
import time

def benchmark_messaging():
    r = redis.Redis()
    message_count = 10000
    
    # Listé˜Ÿåˆ—æ€§èƒ½æµ‹è¯•
    start_time = time.time()
    for i in range(message_count):
        r.lpush('test:list', f'message_{i}')
    list_send_time = time.time() - start_time
    
    start_time = time.time()
    for i in range(message_count):
        r.rpop('test:list')
    list_recv_time = time.time() - start_time
    
    # Pub/Subæ€§èƒ½æµ‹è¯•
    pubsub = r.pubsub()
    pubsub.subscribe('test:channel')
    
    start_time = time.time()
    for i in range(message_count):
        r.publish('test:channel', f'message_{i}')
    pubsub_send_time = time.time() - start_time
    
    # Streamæ€§èƒ½æµ‹è¯•ï¼ˆRedis 5.0+ï¼‰
    try:
        start_time = time.time()
        for i in range(message_count):
            r.xadd('test:stream', {'data': f'message_{i}'})
        stream_send_time = time.time() - start_time
        
        start_time = time.time()
        messages = r.xrange('test:stream', count=message_count)
        stream_recv_time = time.time() - start_time
    except:
        stream_send_time = stream_recv_time = 0
    
    print(f"""
æ€§èƒ½å¯¹æ¯”ç»“æœï¼ˆ{message_count}æ¡æ¶ˆæ¯ï¼‰ï¼š

Listé˜Ÿåˆ—ï¼š
  å‘é€: {list_send_time:.2f}ç§’ ({message_count/list_send_time:.0f} msg/s)
  æ¥æ”¶: {list_recv_time:.2f}ç§’ ({message_count/list_recv_time:.0f} msg/s)

Pub/Subï¼š
  å‘é€: {pubsub_send_time:.2f}ç§’ ({message_count/pubsub_send_time:.0f} msg/s)
  
Streamï¼š
  å‘é€: {stream_send_time:.2f}ç§’ ({message_count/stream_send_time:.0f} msg/s)
  æ¥æ”¶: {stream_recv_time:.2f}ç§’ ({message_count/stream_recv_time:.0f} msg/s)
    """)

# è¿è¡Œæ€§èƒ½æµ‹è¯•
benchmark_messaging()
```

### 3.5 å¯é æ€§ä¿è¯å¯¹æ¯”


**ğŸ›¡ï¸ æ•°æ®å®‰å…¨æ€§å¯¹æ¯”**
```
Listé˜Ÿåˆ—å¯é æ€§ï¼š
âœ… æ•°æ®æŒä¹…åŒ–ï¼šæ¶ˆæ¯å­˜å‚¨åœ¨ç£ç›˜
âœ… åŸå­æ“ä½œï¼šBRPOPLPUSHä¿è¯æ¶ˆæ¯ä¸ä¸¢å¤±
âš ï¸ éœ€è¦æ‰‹åŠ¨å®ç°ï¼šç¡®è®¤æœºåˆ¶ã€é‡è¯•é€»è¾‘ã€æ­»ä¿¡é˜Ÿåˆ—
âŒ å•ç‚¹æ•…éšœï¼šRediså®•æœºå½±å“æ•´ä¸ªé˜Ÿåˆ—

Pub/Subå¯é æ€§ï¼š
âŒ æ— æŒä¹…åŒ–ï¼šæ¶ˆæ¯åªåœ¨å†…å­˜ä¸­
âŒ æ— ç¡®è®¤æœºåˆ¶ï¼šæ— æ³•çŸ¥é“æ¶ˆæ¯æ˜¯å¦è¢«å¤„ç†
âŒ è®¢é˜…è€…ç¦»çº¿ï¼šæ¶ˆæ¯ç›´æ¥ä¸¢å¤±
âš ï¸ é€‚åˆåœºæ™¯ï¼šå¯¹å¯é æ€§è¦æ±‚ä¸é«˜çš„å®æ—¶é€šçŸ¥

Streamå¯é æ€§ï¼š
âœ… æ•°æ®æŒä¹…åŒ–ï¼šæ¶ˆæ¯æ°¸ä¹…å­˜å‚¨ï¼ˆé™¤éæ‰‹åŠ¨åˆ é™¤ï¼‰
âœ… æ¶ˆè´¹ç»„ç¡®è®¤ï¼šå†…ç½®ACKæœºåˆ¶
âœ… æ¶ˆæ¯é‡è¯•ï¼šæœªç¡®è®¤æ¶ˆæ¯å¯ä»¥é‡æ–°æ¶ˆè´¹
âœ… å†å²æŸ¥è¯¢ï¼šæ”¯æŒæŒ‰æ—¶é—´èŒƒå›´æŸ¥è¯¢
âš ï¸ å¤æ‚æ€§é«˜ï¼šéœ€è¦ç†è§£æ¶ˆè´¹ç»„ã€ç¡®è®¤ç­‰æ¦‚å¿µ
```

**ğŸ”§ å¯é æ€§å®ç°å¯¹æ¯”**
```python
# Listé˜Ÿåˆ—ï¼šæ‰‹åŠ¨å®ç°å¯é æ€§
def reliable_list_consume():
    while True:
        # åŸå­æ“ä½œï¼šä»pendingç§»åŠ¨åˆ°processing
        msg = redis_client.brpoplpush('queue:pending', 'queue:processing', 10)
        if msg:
            try:
                process_message(msg)
                # æ‰‹åŠ¨ç¡®è®¤ï¼šä»processingåˆ é™¤
                redis_client.lrem('queue:processing', 1, msg)
            except:
                # æ‰‹åŠ¨é‡è¯•ï¼šç§»å›pending
                redis_client.lrem('queue:processing', 1, msg)
                redis_client.lpush('queue:retry', msg)

# Streamï¼šå†…ç½®å¯é æ€§
def reliable_stream_consume():
    while True:
        # æ¶ˆè´¹ç»„è‡ªåŠ¨ç®¡ç†
        messages = redis_client.xreadgroup(
            'mygroup', 'consumer1', 
            {'mystream': '>'}, 
            count=1, block=1000
        )
        for stream, msgs in messages:
            for msg_id, fields in msgs:
                try:
                    process_message(fields)
                    # å†…ç½®ç¡®è®¤æœºåˆ¶
                    redis_client.xack('mystream', 'mygroup', msg_id)
                except:
                    # æ¶ˆæ¯ä¼šè‡ªåŠ¨ä¿ç•™åœ¨Pending Listä¸­
                    pass
```

---

## 4. ğŸ­ ç”Ÿäº§çº§é˜Ÿåˆ—å®ç°


### 4.1 é«˜å¯ç”¨é˜Ÿåˆ—è®¾è®¡


**ğŸ—ï¸ åˆ†å¸ƒå¼é˜Ÿåˆ—æ¶æ„**
ç”Ÿäº§ç¯å¢ƒçš„æ¶ˆæ¯é˜Ÿåˆ—å°±åƒ**é«˜é€Ÿå…¬è·¯ç³»ç»Ÿ**ï¼Œéœ€è¦å¤šæ¡è·¯çº¿ã€æ”¶è´¹ç«™ã€ç›‘æ§ä¸­å¿ƒï¼š

```
é«˜å¯ç”¨æ¶æ„è®¾è®¡ï¼š
                    è´Ÿè½½å‡è¡¡å™¨
                         |
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        |                |                |
   Redisä¸»èŠ‚ç‚¹1     Redisä¸»èŠ‚ç‚¹2     Redisä¸»èŠ‚ç‚¹3
        |                |                |
   Redisä»èŠ‚ç‚¹1     Redisä»èŠ‚ç‚¹2     Redisä»èŠ‚ç‚¹3
        |                |                |
    é˜Ÿåˆ—åˆ†ç‰‡1         é˜Ÿåˆ—åˆ†ç‰‡2         é˜Ÿåˆ—åˆ†ç‰‡3
```

**ğŸ”§ é«˜å¯ç”¨é˜Ÿåˆ—å®ç°**
```python
import redis
import redis.sentinel
import hashlib
import random

class HighAvailabilityQueue:
    def __init__(self, sentinel_hosts, service_name, queue_name):
        """
        é«˜å¯ç”¨é˜Ÿåˆ—åˆå§‹åŒ–
        sentinel_hosts: SentinelèŠ‚ç‚¹åˆ—è¡¨
        service_name: RedisæœåŠ¡å
        queue_name: é˜Ÿåˆ—åç§°
        """
        self.queue_name = queue_name
        
        # åˆå§‹åŒ–Sentinel
        self.sentinel = redis.sentinel.Sentinel(sentinel_hosts)
        self.service_name = service_name
        
        # è¿æ¥æ± é…ç½®
        self.connection_pool_kwargs = {
            'max_connections': 20,
            'retry_on_timeout': True,
            'socket_timeout': 30,
            'socket_connect_timeout': 10
        }
    
    def get_master_connection(self):
        """è·å–ä¸»èŠ‚ç‚¹è¿æ¥"""
        return self.sentinel.master_for(
            self.service_name, 
            **self.connection_pool_kwargs
        )
    
    def get_slave_connection(self):
        """è·å–ä»èŠ‚ç‚¹è¿æ¥ï¼ˆç”¨äºè¯»æ“ä½œï¼‰"""
        return self.sentinel.slave_for(
            self.service_name,
            **self.connection_pool_kwargs
        )
    
    def send_message(self, message, partition_key=None):
        """å‘é€æ¶ˆæ¯åˆ°é˜Ÿåˆ—"""
        try:
            master = self.get_master_connection()
            
            # æ¶ˆæ¯åˆ†ç‰‡ï¼šæ ¹æ®partition_keyåˆ†é…åˆ°ä¸åŒé˜Ÿåˆ—
            if partition_key:
                shard = self._get_shard(partition_key)
                queue_key = f"{self.queue_name}:shard:{shard}"
            else:
                queue_key = self.queue_name
            
            # æ·»åŠ å…ƒæ•°æ®
            enriched_message = {
                'id': self._generate_message_id(),
                'data': message,
                'timestamp': time.time(),
                'partition_key': partition_key,
                'retry_count': 0
            }
            
            result = master.lpush(queue_key, json.dumps(enriched_message))
            return enriched_message['id']
            
        except redis.ConnectionError as e:
            # è¿æ¥å¤±è´¥æ—¶çš„é™çº§å¤„ç†
            self._handle_connection_error(e)
            raise
    
    def consume_message(self, consumer_id, timeout=30):
        """æ¶ˆè´¹æ¶ˆæ¯ï¼ˆæ”¯æŒå¤šåˆ†ç‰‡ï¼‰"""
        master = self.get_master_connection()
        
        # æ„å»ºåˆ†ç‰‡é˜Ÿåˆ—åˆ—è¡¨
        shard_queues = self._get_all_shard_queues()
        
        # éšæœºæ‰“æ•£é˜Ÿåˆ—é¡ºåºï¼Œé¿å…çƒ­ç‚¹åˆ†ç‰‡
        random.shuffle(shard_queues)
        
        # è½®è¯¢æ‰€æœ‰åˆ†ç‰‡é˜Ÿåˆ—
        for queue_key in shard_queues:
            processing_queue = f"{queue_key}:processing:{consumer_id}"
            
            try:
                # å°è¯•ä»å½“å‰åˆ†ç‰‡æ¶ˆè´¹æ¶ˆæ¯
                message_json = master.brpoplpush(
                    queue_key, 
                    processing_queue, 
                    timeout=1  # çŸ­è¶…æ—¶ï¼Œå¿«é€Ÿè½®è¯¢
                )
                
                if message_json:
                    return json.loads(message_json), queue_key
                    
            except redis.TimeoutError:
                continue  # ç»§ç»­ä¸‹ä¸€ä¸ªåˆ†ç‰‡
        
        return None, None
    
    def _get_shard(self, partition_key, shard_count=16):
        """æ ¹æ®åˆ†åŒºé”®è®¡ç®—åˆ†ç‰‡ç¼–å·"""
        hash_value = int(hashlib.md5(str(partition_key).encode()).hexdigest(), 16)
        return hash_value % shard_count
    
    def _get_all_shard_queues(self, shard_count=16):
        """è·å–æ‰€æœ‰åˆ†ç‰‡é˜Ÿåˆ—å"""
        queues = [self.queue_name]  # é»˜è®¤é˜Ÿåˆ—
        for i in range(shard_count):
            queues.append(f"{self.queue_name}:shard:{i}")
        return queues
    
    def _generate_message_id(self):
        """ç”Ÿæˆå”¯ä¸€æ¶ˆæ¯ID"""
        import uuid
        return str(uuid.uuid4())
    
    def _handle_connection_error(self, error):
        """å¤„ç†è¿æ¥é”™è¯¯"""
        print(f"Redisè¿æ¥é”™è¯¯: {error}")
        # è¿™é‡Œå¯ä»¥æ·»åŠ å‘Šè­¦ã€æ—¥å¿—è®°å½•ç­‰é€»è¾‘

# ä½¿ç”¨ç¤ºä¾‹
sentinel_hosts = [
    ('sentinel1', 26379),
    ('sentinel2', 26379),
    ('sentinel3', 26379)
]

ha_queue = HighAvailabilityQueue(
    sentinel_hosts=sentinel_hosts,
    service_name='mymaster',
    queue_name='order_queue'
)

# å‘é€æ¶ˆæ¯ï¼ˆæŒ‰ç”¨æˆ·IDåˆ†ç‰‡ï¼‰
ha_queue.send_message(
    message={'order_id': 12345, 'amount': 99.99},
    partition_key='user_1001'  # ç›¸åŒç”¨æˆ·çš„æ¶ˆæ¯ä¼šåˆ†é…åˆ°åŒä¸€åˆ†ç‰‡
)
```

### 4.2 é˜Ÿåˆ—æŒä¹…åŒ–ä¿è¯


**ğŸ’¾ æŒä¹…åŒ–ç­–ç•¥é…ç½®**
```bash
# Redisé…ç½®æ–‡ä»¶å…³é”®è®¾ç½®

# RDBæŒä¹…åŒ–ï¼ˆå¿«ç…§ï¼‰
save 900 1      # 900ç§’å†…è‡³å°‘1ä¸ªkeyå˜åŒ–åˆ™ä¿å­˜
save 300 10     # 300ç§’å†…è‡³å°‘10ä¸ªkeyå˜åŒ–åˆ™ä¿å­˜  
save 60 10000   # 60ç§’å†…è‡³å°‘10000ä¸ªkeyå˜åŒ–åˆ™ä¿å­˜

rdbcompression yes
rdbchecksum yes
dbfilename queue_backup.rdb

# AOFæŒä¹…åŒ–ï¼ˆè¿½åŠ æ—¥å¿—ï¼‰
appendonly yes
appendfilename "queue_operations.aof"
appendfsync everysec    # æ¯ç§’åŒæ­¥ä¸€æ¬¡ï¼ˆæ€§èƒ½å’Œå®‰å…¨çš„å¹³è¡¡ï¼‰

# AOFé‡å†™é…ç½®
auto-aof-rewrite-percentage 100
auto-aof-rewrite-min-size 64mb

# æ··åˆæŒä¹…åŒ–ï¼ˆRedis 4.0+æ¨èï¼‰
aof-use-rdb-preamble yes
```

**ğŸ›¡ï¸ æŒä¹…åŒ–ç›‘æ§å’Œç»´æŠ¤**
```python
class PersistenceMonitor:
    def __init__(self, redis_client):
        self.redis = redis_client
    
    def check_persistence_status(self):
        """æ£€æŸ¥æŒä¹…åŒ–çŠ¶æ€"""
        info = self.redis.info()
        
        persistence_status = {
            'rdb_enabled': info.get('rdb_last_save_time', 0) > 0,
            'aof_enabled': info.get('aof_enabled', 0) == 1,
            'last_rdb_save': info.get('rdb_last_save_time', 0),
            'aof_size': info.get('aof_current_size', 0),
            'rdb_changes_since_save': info.get('rdb_changes_since_last_save', 0)
        }
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦å‘Šè­¦
        warnings = []
        current_time = time.time()
        
        # RDBæ£€æŸ¥ï¼šè¶…è¿‡1å°æ—¶æœªä¿å­˜
        if persistence_status['last_rdb_save'] > 0:
            time_since_save = current_time - persistence_status['last_rdb_save']
            if time_since_save > 3600:  # 1å°æ—¶
                warnings.append(f"RDBè¶…è¿‡{time_since_save/3600:.1f}å°æ—¶æœªä¿å­˜")
        
        # AOFæ£€æŸ¥ï¼šæ–‡ä»¶å¤§å°å¼‚å¸¸
        if persistence_status['aof_size'] > 1024 * 1024 * 1024:  # 1GB
            warnings.append(f"AOFæ–‡ä»¶è¿‡å¤§: {persistence_status['aof_size']/1024/1024:.0f}MB")
        
        return {
            'status': persistence_status,
            'warnings': warnings,
            'health': 'warning' if warnings else 'healthy'
        }
    
    def trigger_manual_save(self):
        """æ‰‹åŠ¨è§¦å‘ä¿å­˜"""
        try:
            # å¼‚æ­¥ä¿å­˜ï¼ˆä¸é˜»å¡ï¼‰
            self.redis.bgsave()
            return {'success': True, 'message': 'åå°ä¿å­˜å·²å¯åŠ¨'}
        except redis.ResponseError as e:
            if 'Background save already in progress' in str(e):
                return {'success': False, 'message': 'åå°ä¿å­˜æ­£åœ¨è¿›è¡Œä¸­'}
            else:
                return {'success': False, 'message': f'ä¿å­˜å¤±è´¥: {e}'}
    
    def optimize_aof(self):
        """ä¼˜åŒ–AOFæ–‡ä»¶"""
        try:
            # è§¦å‘AOFé‡å†™
            self.redis.bgrewriteaof()
            return {'success': True, 'message': 'AOFé‡å†™å·²å¯åŠ¨'}
        except redis.ResponseError as e:
            return {'success': False, 'message': f'AOFé‡å†™å¤±è´¥: {e}'}

# æŒä¹…åŒ–ç›‘æ§ä»»åŠ¡
def persistence_monitoring_task():
    monitor = PersistenceMonitor(redis_client)
    
    while True:
        try:
            status = monitor.check_persistence_status()
            
            if status['warnings']:
                print(f"âš ï¸  æŒä¹…åŒ–è­¦å‘Š: {status['warnings']}")
                
                # è‡ªåŠ¨å¤„ç†ä¸€äº›é—®é¢˜
                if any('AOFæ–‡ä»¶è¿‡å¤§' in w for w in status['warnings']):
                    result = monitor.optimize_aof()
                    print(f"è‡ªåŠ¨ä¼˜åŒ–AOF: {result}")
            
            time.sleep(300)  # æ¯5åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡
            
        except Exception as e:
            print(f"æŒä¹…åŒ–ç›‘æ§å¼‚å¸¸: {e}")
            time.sleep(60)
```

### 4.3 æ¶ˆè´¹è€…è´Ÿè½½å‡è¡¡


**âš–ï¸ æ™ºèƒ½è´Ÿè½½å‡è¡¡ç­–ç•¥**
```python
class ConsumerLoadBalancer:
    def __init__(self, redis_client, queue_name):
        self.redis = redis_client
        self.queue_name = queue_name
        self.consumer_registry = f"{queue_name}:consumers"
        self.metrics_key = f"{queue_name}:consumer_metrics"
        
    def register_consumer(self, consumer_id, capabilities=None):
        """æ³¨å†Œæ¶ˆè´¹è€…"""
        consumer_info = {
            'id': consumer_id,
            'registered_at': time.time(),
            'last_heartbeat': time.time(),
            'capabilities': capabilities or {},
            'processed_count': 0,
            'error_count': 0,
            'status': 'active'
        }
        
        self.redis.hset(
            self.consumer_registry,
            consumer_id,
            json.dumps(consumer_info)
        )
        
        return consumer_id
    
    def heartbeat(self, consumer_id, metrics=None):
        """æ¶ˆè´¹è€…å¿ƒè·³"""
        consumer_data = self.redis.hget(self.consumer_registry, consumer_id)
        if consumer_data:
            consumer_info = json.loads(consumer_data)
            consumer_info['last_heartbeat'] = time.time()
            
            # æ›´æ–°å¤„ç†æŒ‡æ ‡
            if metrics:
                consumer_info.update(metrics)
            
            self.redis.hset(
                self.consumer_registry,
                consumer_id,
                json.dumps(consumer_info)
            )
            
            return True
        return False
    
    def get_optimal_consumer(self, message_requirements=None):
        """è·å–æœ€ä¼˜æ¶ˆè´¹è€…"""
        consumers = self._get_active_consumers()
        
        if not consumers:
            return None
        
        # è¿‡æ»¤æ»¡è¶³è¦æ±‚çš„æ¶ˆè´¹è€…
        suitable_consumers = []
        for consumer in consumers:
            if self._consumer_meets_requirements(consumer, message_requirements):
                suitable_consumers.append(consumer)
        
        if not suitable_consumers:
            return None  # æ²¡æœ‰åˆé€‚çš„æ¶ˆè´¹è€…
        
        # è´Ÿè½½å‡è¡¡ç®—æ³•ï¼šé€‰æ‹©å¤„ç†é‡æœ€å°‘çš„æ¶ˆè´¹è€…
        return min(suitable_consumers, 
                  key=lambda c: c['processed_count'] - c['error_count'])
    
    def _get_active_consumers(self, timeout=60):
        """è·å–æ´»è·ƒçš„æ¶ˆè´¹è€…åˆ—è¡¨"""
        current_time = time.time()
        all_consumers = self.redis.hgetall(self.consumer_registry)
        
        active_consumers = []
        inactive_consumers = []
        
        for consumer_id, consumer_data in all_consumers.items():
            consumer_info = json.loads(consumer_data)
            
            # æ£€æŸ¥å¿ƒè·³è¶…æ—¶
            if (current_time - consumer_info['last_heartbeat']) <= timeout:
                consumer_info['id'] = consumer_id.decode()
                active_consumers.append(consumer_info)
            else:
                inactive_consumers.append(consumer_id.decode())
        
        # æ¸…ç†ä¸æ´»è·ƒçš„æ¶ˆè´¹è€…
        if inactive_consumers:
            self.redis.hdel(self.consumer_registry, *inactive_consumers)
            print(f"æ¸…ç†ä¸æ´»è·ƒæ¶ˆè´¹è€…: {inactive_consumers}")
        
        return active_consumers
    
    def _consumer_meets_requirements(self, consumer, requirements):
        """æ£€æŸ¥æ¶ˆè´¹è€…æ˜¯å¦æ»¡è¶³æ¶ˆæ¯å¤„ç†è¦æ±‚"""
        if not requirements:
            return True
        
        capabilities = consumer.get('capabilities', {})
        
        # æ£€æŸ¥å¤„ç†èƒ½åŠ›
        for req_key, req_value in requirements.items():
            if req_key not in capabilities:
                return False
            if capabilities[req_key] < req_value:
                return False
        
        return True
    
    def distribute_message(self, message):
        """æ™ºèƒ½åˆ†å‘æ¶ˆæ¯"""
        # è§£ææ¶ˆæ¯è¦æ±‚
        requirements = message.get('requirements', {})
        
        # é€‰æ‹©æœ€ä¼˜æ¶ˆè´¹è€…
        optimal_consumer = self.get_optimal_consumer(requirements)
        
        if not optimal_consumer:
            # æ²¡æœ‰åˆé€‚çš„æ¶ˆè´¹è€…ï¼Œæ”¾å…¥é»˜è®¤é˜Ÿåˆ—
            target_queue = self.queue_name
        else:
            # åˆ†å‘ç»™ç‰¹å®šæ¶ˆè´¹è€…
            target_queue = f"{self.queue_name}:consumer:{optimal_consumer['id']}"
        
        # å‘é€æ¶ˆæ¯
        self.redis.lpush(target_queue, json.dumps(message))
        
        return {
            'target_queue': target_queue,
            'consumer_id': optimal_consumer['id'] if optimal_consumer else None
        }

# æ™ºèƒ½æ¶ˆè´¹è€…å®ç°
class SmartConsumer:
    def __init__(self, redis_client, queue_name, consumer_id, capabilities=None):
        self.redis = redis_client
        self.queue_name = queue_name
        self.consumer_id = consumer_id
        self.capabilities = capabilities or {}
        
        self.load_balancer = ConsumerLoadBalancer(redis_client, queue_name)
        self.processed_count = 0
        self.error_count = 0
        
        # æ³¨å†Œæ¶ˆè´¹è€…
        self.load_balancer.register_consumer(consumer_id, capabilities)
    
    def start_consuming(self):
        """å¼€å§‹æ¶ˆè´¹æ¶ˆæ¯"""
        # å¯åŠ¨å¿ƒè·³çº¿ç¨‹
        heartbeat_thread = threading.Thread(target=self._heartbeat_loop, daemon=True)
        heartbeat_thread.start()
        
        # æ¶ˆè´¹å¾ªç¯
        while True:
            try:
                # æ£€æŸ¥ä¸“ç”¨é˜Ÿåˆ—å’Œé€šç”¨é˜Ÿåˆ—
                queues = [
                    f"{self.queue_name}:consumer:{self.consumer_id}",  # ä¸“ç”¨é˜Ÿåˆ—
                    self.queue_name  # é€šç”¨é˜Ÿåˆ—
                ]
                
                # é˜»å¡å¼æ¶ˆè´¹
                result = self.redis.brpop(queues, timeout=10)
                
                if result:
                    queue_name, message_json = result
                    message = json.loads(message_json)
                    
                    success = self._process_message(message)
                    
                    if success:
                        self.processed_count += 1
                    else:
                        self.error_count += 1
                
            except Exception as e:
                print(f"æ¶ˆè´¹å¼‚å¸¸: {e}")
                self.error_count += 1
                time.sleep(1)
    
    def _process_message(self, message):
        """å¤„ç†æ¶ˆæ¯ï¼ˆç”±å­ç±»å®ç°å…·ä½“é€»è¾‘ï¼‰"""
        try:
            # æ¨¡æ‹Ÿæ¶ˆæ¯å¤„ç†
            print(f"æ¶ˆè´¹è€… {self.consumer_id} å¤„ç†æ¶ˆæ¯: {message}")
            time.sleep(0.1)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
            return True
        except Exception as e:
            print(f"æ¶ˆæ¯å¤„ç†å¤±è´¥: {e}")
            return False
    
    def _heartbeat_loop(self):
        """å¿ƒè·³å¾ªç¯"""
        while True:
            try:
                metrics = {
                    'processed_count': self.processed_count,
                    'error_count': self.error_count,
                    'status': 'active'
                }
                
                self.load_balancer.heartbeat(self.consumer_id, metrics)
                time.sleep(30)  # æ¯30ç§’å‘é€å¿ƒè·³
                
            except Exception as e:
                print(f"å¿ƒè·³å¼‚å¸¸: {e}")
                time.sleep(10)

# ä½¿ç”¨ç¤ºä¾‹
# å¯åŠ¨å¤šä¸ªä¸åŒèƒ½åŠ›çš„æ¶ˆè´¹è€…
consumer1 = SmartConsumer(
    redis_client, 
    'order_queue', 
    'consumer_fast',
    capabilities={'max_processing_time': 60, 'memory_limit': 512}
)

consumer2 = SmartConsumer(
    redis_client, 
    'order_queue', 
    'consumer_heavy',
    capabilities={'max_processing_time': 300, 'memory_limit': 2048}
)
```

### 4.4 é˜Ÿåˆ—æ€§èƒ½ç›‘æ§


**ğŸ“Š å…¨æ–¹ä½æ€§èƒ½ç›‘æ§ç³»ç»Ÿ**
```python
class QueuePerformanceMonitor:
    def __init__(self, redis_client, queue_name):
        self.redis = redis_client
        self.queue_name = queue_name
        self.metrics_history = f"{queue_name}:perf_history"
        
    def collect_performance_metrics(self):
        """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
        start_time = time.time()
        
        # åŸºç¡€é˜Ÿåˆ—æŒ‡æ ‡
        basic_metrics = {
            'timestamp': start_time,
            'pending_count': self.redis.llen(f"{self.queue_name}:pending"),
            'processing_count': self._get_total_processing_count(),
            'retry_count': self.redis.llen(f"{self.queue_name}:retry"),
            'dead_count': self.redis.llen(f"{self.queue_name}:dead")
        }
        
        # Redisæ€§èƒ½æŒ‡æ ‡
        redis_info = self.redis.info()
        redis_metrics = {
            'memory_used': redis_info.get('used_memory', 0),
            'memory_peak': redis_info.get('used_memory_peak', 0),
            'connected_clients': redis_info.get('connected_clients', 0),
            'commands_processed': redis_info.get('total_commands_processed', 0),
            'keyspace_hits': redis_info.get('keyspace_hits', 0),
            'keyspace_misses': redis_info.get('keyspace_misses', 0)
        }
        
        # è®¡ç®—è¡ç”ŸæŒ‡æ ‡
        derived_metrics = self._calculate_derived_metrics(basic_metrics, redis_metrics)
        
        # åˆå¹¶æ‰€æœ‰æŒ‡æ ‡
        all_metrics = {
            **basic_metrics,
            **redis_metrics,
            **derived_metrics
        }
        
        # ä¿å­˜å†å²æ•°æ®
        self._save_metrics_history(all_metrics)
        
        return all_metrics
    
    def _get_total_processing_count(self):
        """è·å–æ‰€æœ‰å¤„ç†ä¸­é˜Ÿåˆ—çš„æ¶ˆæ¯æ€»æ•°"""
        pattern = f"{self.queue_name}:processing:*"
        processing_queues = self.redis.keys(pattern)
        
        total = 0
        for queue in processing_queues:
            total += self.redis.llen(queue)
        
        return total
    
    def _calculate_derived_metrics(self, basic_metrics, redis_metrics):
        """è®¡ç®—è¡ç”ŸæŒ‡æ ‡"""
        # è·å–å†å²æ•°æ®ç”¨äºè®¡ç®—å¢é‡
        history = self._get_recent_metrics(1)  # è·å–æœ€è¿‘1æ¡è®°å½•
        
        derived = {}
        
        if history:
            last_metrics = history[0]
            time_diff = basic_metrics['timestamp'] - last_metrics['timestamp']
            
            if time_diff > 0:
                # è®¡ç®—å¤„ç†é€Ÿç‡ï¼ˆæ¯ç§’ï¼‰
                cmd_diff = redis_metrics['commands_processed'] - last_metrics.get('commands_processed', 0)
                derived['commands_per_second'] = cmd_diff / time_diff
                
                # è®¡ç®—ç¼“å­˜å‘½ä¸­ç‡
                hit_diff = redis_metrics['keyspace_hits'] - last_metrics.get('keyspace_hits', 0)
                miss_diff = redis_metrics['keyspace_misses'] - last_metrics.get('keyspace_misses', 0)
                total_requests = hit_diff + miss_diff
                
                if total_requests > 0:
                    derived['cache_hit_rate'] = hit_diff / total_requests
                else:
                    derived['cache_hit_rate'] = 1.0
        
        # å†…å­˜ä½¿ç”¨ç‡
        if redis_metrics['memory_peak'] > 0:
            derived['memory_usage_ratio'] = redis_metrics['memory_used'] / redis_metrics['memory_peak']
        
        # é˜Ÿåˆ—ç§¯å‹ç‡ï¼ˆå¾…å¤„ç†/æ€»æ¶ˆæ¯æ•°çš„æ¯”ä¾‹ï¼‰
        total_messages = (basic_metrics['pending_count'] + 
                         basic_metrics['processing_count'] + 
                         basic_metrics['dead_count'])
        
        if total_messages > 0:
            derived['pending_ratio'] = basic_metrics['pending_count'] / total_messages
        
        return derived
    
    def _save_metrics_history(self, metrics):
        """ä¿å­˜æŒ‡æ ‡å†å²"""
        self.redis.lpush(self.metrics_history, json.dumps(metrics))
        # åªä¿ç•™æœ€è¿‘1000æ¡è®°å½•
        self.redis.ltrim(self.metrics_history, 0, 999)
    
    def _get_recent_metrics(self, count=10):
        """è·å–æœ€è¿‘çš„æŒ‡æ ‡æ•°æ®"""
        raw_data = self.redis.lrange(self.metrics_history, 0, count-1)
        return [json.loads(data) for data in raw_data]
    
    def generate_performance_report(self, hours=24):
        """ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š"""
        cutoff_time = time.time() - (hours * 3600)
        all_metrics = self._get_recent_metrics(1000)
        
        # è¿‡æ»¤æŒ‡å®šæ—¶é—´èŒƒå›´çš„æ•°æ®
        filtered_metrics = [m for m in all_metrics if m['timestamp'] > cutoff_time]
        
        if not filtered_metrics:
            return {'error': 'æ²¡æœ‰è¶³å¤Ÿçš„å†å²æ•°æ®'}
        
        # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
        report = {
            'period': f'æœ€è¿‘{hours}å°æ—¶',
            'data_points': len(filtered_metrics),
            'queue_stats': self._analyze_queue_performance(filtered_metrics),
            'redis_stats': self._analyze_redis_performance(filtered_metrics),
            'alerts': self._generate_alerts(filtered_metrics)
        }
        
        return report
    
    def _analyze_queue_performance(self, metrics_list):
        """åˆ†æé˜Ÿåˆ—æ€§èƒ½"""
        pending_counts = [m['pending_count'] for m in metrics_list]
        processing_counts = [m['processing_count'] for m in metrics_list]
        
        return {
            'avg_pending': sum(pending_counts) / len(pending_counts),
            'max_pending': max(pending_counts),
            'min_pending': min(pending_counts),
            'avg_processing': sum(processing_counts) / len(processing_counts),
            'max_processing': max(processing_counts)
        }
    
    def _analyze_redis_performance(self, metrics_list):
        """åˆ†æRedisæ€§èƒ½"""
        memory_usage = [m.get('memory_used', 0) for m in metrics_list]
        cmd_rates = [m.get('commands_per_second', 0) for m in metrics_list]
        hit_rates = [m.get('cache_hit_rate', 1.0) for m in metrics_list]
        
        return {
            'avg_memory_mb': sum(memory_usage) / len(memory_usage) / 1024 / 1024,
            'peak_memory_mb': max(memory_usage) / 1024 / 1024,
            'avg_commands_per_sec': sum(cmd_rates) / len(cmd_rates),
            'avg_cache_hit_rate': sum(hit_rates) / len(hit_rates)
        }
    
    def _generate_alerts(self, metrics_list):
        """ç”Ÿæˆå‘Šè­¦"""
        alerts = []
        latest_metrics = metrics_list[0] if metrics_list else {}
        
        # é˜Ÿåˆ—ç§¯å‹å‘Šè­¦
        pending_count = latest_metrics.get('pending_count', 0)
        if pending_count > 1000:
            alerts.append({
                'level': 'warning',
                'message': f'é˜Ÿåˆ—ç§¯å‹ä¸¥é‡: {pending_count}æ¡å¾…å¤„ç†æ¶ˆæ¯'
            })
        
        # å†…å­˜ä½¿ç”¨å‘Šè­¦
        memory_usage_ratio = latest_metrics.get('memory_usage_ratio', 0)
        if memory_usage_ratio > 0.85:
            alerts.append({
                'level': 'error',
                'message': f'å†…å­˜ä½¿ç”¨è¿‡é«˜: {memory_usage_ratio*100:.1f}%'
            })
        
        # ç¼“å­˜å‘½ä¸­ç‡å‘Šè­¦
        cache_hit_rate = latest_metrics.get('cache_hit_rate', 1.0)
        if cache_hit_rate < 0.8:
            alerts.append({
                'level': 'warning',
                'message': f'ç¼“å­˜å‘½ä¸­ç‡è¿‡ä½: {cache_hit_rate*100:.1f}%'
            })
        
        # æ­»ä¿¡é˜Ÿåˆ—å‘Šè­¦
        dead_count = latest_metrics.get('dead_count', 0)
        if dead_count > 100:
            alerts.append({
                'level': 'error',
                'message': f'æ­»ä¿¡æ¶ˆæ¯è¿‡å¤š: {dead_count}æ¡'
            })
        
        return alerts

# æ€§èƒ½ç›‘æ§å®ˆæŠ¤è¿›ç¨‹
class PerformanceMonitorDaemon:
    def __init__(self, redis_client, queue_name):
        self.monitor = QueuePerformanceMonitor(redis_client, queue_name)
        self.running = False
        
    def start_monitoring(self, interval=60):
        """å¯åŠ¨æ€§èƒ½ç›‘æ§"""
        self.running = True
        
        def monitoring_loop():
            while self.running:
                try:
                    # æ”¶é›†æŒ‡æ ‡
                    metrics = self.monitor.collect_performance_metrics()
                    
                    # ç”ŸæˆæŠ¥å‘Š
                    report = self.monitor.generate_performance_report(hours=1)
                    
                    # æ£€æŸ¥å‘Šè­¦
                    if 'alerts' in report and report['alerts']:
                        for alert in report['alerts']:
                            self._handle_alert(alert)
                    
                    # è®°å½•å…³é”®æŒ‡æ ‡
                    self._log_key_metrics(metrics)
                    
                    time.sleep(interval)
                    
                except Exception as e:
                    print(f"æ€§èƒ½ç›‘æ§å¼‚å¸¸: {e}")
                    time.sleep(30)
        
        # å¯åŠ¨ç›‘æ§çº¿ç¨‹
        monitor_thread = threading.Thread(target=monitoring_loop, daemon=True)
        monitor_thread.start()
        
        return monitor_thread
    
    def stop_monitoring(self):
        """åœæ­¢æ€§èƒ½ç›‘æ§"""
        self.running = False
    
    def _handle_alert(self, alert):
        """å¤„ç†å‘Šè­¦"""
        level_emoji = {'warning': 'âš ï¸', 'error': 'ğŸš¨', 'info': 'â„¹ï¸'}
        emoji = level_emoji.get(alert['level'], 'ğŸ“¢')
        
        print(f"{emoji} [{alert['level'].upper()}] {alert['message']}")
        
        # è¿™é‡Œå¯ä»¥é›†æˆå„ç§å‘Šè­¦æ¸ é“
        # send_to_slack(alert)
        # send_email_alert(alert) 
        # send_to_webhook(alert)
    
    def _log_key_metrics(self, metrics):
        """è®°å½•å…³é”®æŒ‡æ ‡"""
        key_metrics = {
            'å¾…å¤„ç†': metrics.get('pending_count', 0),
            'å¤„ç†ä¸­': metrics.get('processing_count', 0),
            'æ­»ä¿¡': metrics.get('dead_count', 0),
            'å†…å­˜MB': metrics.get('memory_used', 0) / 1024 / 1024,
            'å‘½ä»¤/ç§’': metrics.get('commands_per_second', 0)
        }
        
        metrics_str = ' | '.join([f'{k}:{v}' for k, v in key_metrics.items()])
        print(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] é˜Ÿåˆ—æŒ‡æ ‡: {metrics_str}")

# ä½¿ç”¨ç¤ºä¾‹ï¼šå®Œæ•´çš„ç”Ÿäº§çº§é˜Ÿåˆ—ç³»ç»Ÿ
def setup_production_queue_system():
    """æ­å»ºå®Œæ•´çš„ç”Ÿäº§çº§é˜Ÿåˆ—ç³»ç»Ÿ"""
    
    # 1. åˆå§‹åŒ–é«˜å¯ç”¨é˜Ÿåˆ—
    sentinel_hosts = [('sentinel1', 26379), ('sentinel2', 26379)]
    ha_queue = HighAvailabilityQueue(sentinel_hosts, 'mymaster', 'prod_queue')
    
    # 2. å¯åŠ¨æ€§èƒ½ç›‘æ§
    redis_client = redis.Redis()
    monitor_daemon = PerformanceMonitorDaemon(redis_client, 'prod_queue')
    monitor_daemon.start_monitoring(interval=30)
    
    # 3. å¯åŠ¨æ™ºèƒ½æ¶ˆè´¹è€…
    consumers = []
    for i in range(3):
        consumer = SmartConsumer(
            redis_client,
            'prod_queue',
            f'consumer_{i}',
            capabilities={
                'max_processing_time': 120,
                'memory_limit': 1024,
                'cpu_cores': 2
            }
        )
        
        # åœ¨ç‹¬ç«‹çº¿ç¨‹ä¸­å¯åŠ¨æ¶ˆè´¹è€…
        consumer_thread = threading.Thread(target=consumer.start_consuming, daemon=True)
        consumer_thread.start()
        consumers.append(consumer)
    
    # 4. å¯åŠ¨é‡è¯•å¤„ç†å™¨
    retry_thread = threading.Thread(target=retry_processor_daemon, daemon=True)
    retry_thread.start()
    
    # 5. å¯åŠ¨æŒä¹…åŒ–ç›‘æ§
    persistence_thread = threading.Thread(target=persistence_monitoring_task, daemon=True)
    persistence_thread.start()
    
    print("ç”Ÿäº§çº§é˜Ÿåˆ—ç³»ç»Ÿå·²å¯åŠ¨:")
    print(f"- é«˜å¯ç”¨é˜Ÿåˆ—: âœ“")
    print(f"- æ€§èƒ½ç›‘æ§: âœ“")
    print(f"- æ™ºèƒ½æ¶ˆè´¹è€…: {len(consumers)}ä¸ª")
    print(f"- é‡è¯•å¤„ç†å™¨: âœ“")
    print(f"- æŒä¹…åŒ–ç›‘æ§: âœ“")
    
    return {
        'queue': ha_queue,
        'monitor': monitor_daemon,
        'consumers': consumers
    }

---

## 5. ğŸ“‹ æ ¸å¿ƒè¦ç‚¹æ€»ç»“


### 5.1 å¿…é¡»æŒæ¡çš„æ ¸å¿ƒæ¦‚å¿µ


```
ğŸ”¸ quicklistç»“æ„ï¼šRedis Listçš„åº•å±‚å®ç°ï¼Œç»“åˆäº†åŒå‘é“¾è¡¨å’Œå‹ç¼©åˆ—è¡¨
ğŸ”¸ æ¶ˆæ¯ç¡®è®¤æœºåˆ¶ï¼šé€šè¿‡BRPOPLPUSHå®ç°å¯é æ¶ˆæ¯é˜Ÿåˆ—ï¼Œé˜²æ­¢æ¶ˆæ¯ä¸¢å¤±
ğŸ”¸ æ­»ä¿¡é˜Ÿåˆ—ï¼šå¤„ç†å¤±è´¥æ¶ˆæ¯çš„å…œåº•æ–¹æ¡ˆï¼Œæ”¯æŒåˆ†æå’Œå¤æ´»
ğŸ”¸ é‡è¯•æœºåˆ¶ï¼šæŒ‡æ•°é€€é¿ã€çº¿æ€§å¢é•¿ç­‰ç­–ç•¥ï¼Œé¿å…ç³»ç»Ÿè¿‡è½½
ğŸ”¸ ä¸‰ç§æ¶ˆæ¯æ¨¡å¼ï¼šListé˜Ÿåˆ—ã€Pub/Subã€Streamå„æœ‰é€‚ç”¨åœºæ™¯
ğŸ”¸ é«˜å¯ç”¨è®¾è®¡ï¼šSentinelã€åˆ†ç‰‡ã€è´Ÿè½½å‡è¡¡ä¿è¯ç”Ÿäº§å¯ç”¨æ€§
```

### 5.2 å…³é”®ç†è§£è¦ç‚¹


**ğŸ”¹ quicklistçš„è®¾è®¡æ™ºæ…§**
```
å†…å­˜ä¼˜åŒ–ï¼š
â€¢ å°†å¤šä¸ªå°å…ƒç´ æ‰“åŒ…åˆ°ziplistä¸­ï¼Œå‡å°‘æŒ‡é’ˆå¼€é”€
â€¢ LZFå‹ç¼©ä¸­é—´èŠ‚ç‚¹ï¼Œè¿›ä¸€æ­¥èŠ‚çœå†…å­˜
â€¢ å¤´å°¾èŠ‚ç‚¹ä¸å‹ç¼©ï¼Œä¿æŒè®¿é—®æ€§èƒ½

æ€§èƒ½å¹³è¡¡ï¼š
â€¢ é…ç½®å‚æ•°å¯è°ƒï¼šlist-max-ziplist-sizeæ§åˆ¶èŠ‚ç‚¹å¤§å°
â€¢ å‹ç¼©æ·±åº¦å¯æ§ï¼šlist-compress-depthå¹³è¡¡å†…å­˜å’Œæ€§èƒ½
â€¢ æ“ä½œå¤æ‚åº¦ï¼šå¤´å°¾O(1)ï¼Œä¸­é—´O(N)
```

**ğŸ”¹ å¯é æ¶ˆæ¯é˜Ÿåˆ—çš„æ ¸å¿ƒè¦ç´ **
```
åŸå­æ“ä½œï¼š
â€¢ BRPOPLPUSHç¡®ä¿æ¶ˆæ¯ä¸ä¸¢å¤±
â€¢ ä»pendingé˜Ÿåˆ—åŸå­ç§»åŠ¨åˆ°processingé˜Ÿåˆ—
â€¢ é¿å…æ¶ˆè´¹è€…å´©æºƒå¯¼è‡´çš„æ¶ˆæ¯ä¸¢å¤±

ç¡®è®¤æœºåˆ¶ï¼š
â€¢ ACKï¼šæ¶ˆæ¯å¤„ç†æˆåŠŸåç¡®è®¤åˆ é™¤
â€¢ NACKï¼šæ¶ˆæ¯å¤„ç†å¤±è´¥åé‡æ–°æ’é˜Ÿ
â€¢ è¶…æ—¶æ£€æµ‹ï¼šé•¿æ—¶é—´æœªç¡®è®¤çš„æ¶ˆæ¯é‡æ–°å…¥é˜Ÿ

é‡è¯•ç­–ç•¥ï¼š
â€¢ æŒ‡æ•°é€€é¿ï¼šé¿å…ç³»ç»Ÿè¿‡è½½
â€¢ æœ€å¤§é‡è¯•æ¬¡æ•°ï¼šé˜²æ­¢æ— é™é‡è¯•
â€¢ æ­»ä¿¡é˜Ÿåˆ—ï¼šå¤„ç†æœ€ç»ˆå¤±è´¥çš„æ¶ˆæ¯
```

**ğŸ”¹ ä¸‰ç§æ¶ˆæ¯æœºåˆ¶çš„æœ¬è´¨åŒºåˆ«**
```
æ•°æ®æŒä¹…åŒ–ï¼š
â€¢ Listï¼šæŒä¹…åŒ–ï¼Œæ¶ˆæ¯ä¸ä¼šä¸¢å¤±
â€¢ Pub/Subï¼šå†…å­˜ä¸­ï¼Œè®¢é˜…è€…ç¦»çº¿æ¶ˆæ¯ä¸¢å¤±
â€¢ Streamï¼šæŒä¹…åŒ–ï¼Œæ”¯æŒå†å²æ¶ˆæ¯æŸ¥è¯¢

æ¶ˆè´¹æ¨¡å¼ï¼š
â€¢ Listï¼šç«äº‰æ¶ˆè´¹ï¼Œå¤šæ¶ˆè´¹è€…æŠ¢å å¼å¤„ç†
â€¢ Pub/Subï¼šå¹¿æ’­æ¶ˆè´¹ï¼Œæ‰€æœ‰è®¢é˜…è€…éƒ½æ”¶åˆ°
â€¢ Streamï¼šæ¶ˆè´¹ç»„ï¼Œæ”¯æŒè´Ÿè½½å‡è¡¡å’Œç¡®è®¤

é€‚ç”¨åœºæ™¯ï¼š
â€¢ Listï¼šä»»åŠ¡é˜Ÿåˆ—ï¼Œéœ€è¦å¯é å¤„ç†
â€¢ Pub/Subï¼šå®æ—¶é€šçŸ¥ï¼Œå…è®¸æ¶ˆæ¯ä¸¢å¤±
â€¢ Streamï¼šäº‹ä»¶æµï¼Œéœ€è¦å†å²è¿½æº¯
```

### 5.3 ç”Ÿäº§çº§å®ç°è¦ç‚¹


**ğŸ­ é«˜å¯ç”¨æ¶æ„è®¾è®¡**
```
å¤šå±‚ä¿éšœï¼š
â€¢ Redis Sentinelï¼šè‡ªåŠ¨æ•…éšœè½¬ç§»
â€¢ æ¶ˆæ¯åˆ†ç‰‡ï¼šé¿å…å•ç‚¹çƒ­ç‚¹
â€¢ è¿æ¥æ± ï¼šæé«˜è¿æ¥å¤ç”¨ç‡
â€¢ ç›‘æ§å‘Šè­¦ï¼šåŠæ—¶å‘ç°é—®é¢˜

è´Ÿè½½å‡è¡¡ï¼š
â€¢ æ¶ˆè´¹è€…æ³¨å†Œï¼šåŠ¨æ€ç®¡ç†æ¶ˆè´¹è€…
â€¢ èƒ½åŠ›åŒ¹é…ï¼šæ ¹æ®æ¶ˆæ¯è¦æ±‚é€‰æ‹©æ¶ˆè´¹è€…
â€¢ å¿ƒè·³æ£€æµ‹ï¼šæ¸…ç†å¤±æ•ˆæ¶ˆè´¹è€…
â€¢ æ™ºèƒ½åˆ†å‘ï¼šåŸºäºè´Ÿè½½å’Œèƒ½åŠ›åˆ†é…

æŒä¹…åŒ–ä¿éšœï¼š
â€¢ RDB+AOFï¼šåŒé‡æŒä¹…åŒ–æœºåˆ¶
â€¢ å®šæœŸæ£€æŸ¥ï¼šç›‘æ§æŒä¹…åŒ–çŠ¶æ€
â€¢ è‡ªåŠ¨ä¼˜åŒ–ï¼šAOFé‡å†™ã€RDBä¿å­˜
â€¢ å¤‡ä»½ç­–ç•¥ï¼šå®šæœŸå¤‡ä»½å…³é”®æ•°æ®
```

**ğŸ“Š ç›‘æ§ä½“ç³»å»ºè®¾**
```
æ ¸å¿ƒæŒ‡æ ‡ï¼š
â€¢ é˜Ÿåˆ—é•¿åº¦ï¼špendingã€processingã€deadã€retry
â€¢ å¤„ç†é€Ÿç‡ï¼šæ¶ˆæ¯å¤„ç†ååé‡
â€¢ é”™è¯¯ç‡ï¼šå¤±è´¥æ¶ˆæ¯å æ¯”
â€¢ èµ„æºä½¿ç”¨ï¼šå†…å­˜ã€CPUã€ç½‘ç»œ

å‘Šè­¦æœºåˆ¶ï¼š
â€¢ é˜ˆå€¼å‘Šè­¦ï¼šé˜Ÿåˆ—ç§¯å‹ã€å†…å­˜ä½¿ç”¨
â€¢ è¶‹åŠ¿å‘Šè­¦ï¼šé”™è¯¯ç‡ä¸Šå‡ã€å¤„ç†é€Ÿç‡ä¸‹é™
â€¢ å¼‚å¸¸å‘Šè­¦ï¼šæ¶ˆè´¹è€…ç¦»çº¿ã€Redisè¿æ¥å¤±è´¥

æ€§èƒ½ä¼˜åŒ–ï¼š
â€¢ æ‰¹é‡æ“ä½œï¼šå‡å°‘ç½‘ç»œå¾€è¿”
â€¢ ç®¡é“æŠ€æœ¯ï¼šæ‰¹é‡å‘é€å‘½ä»¤
â€¢ è¿æ¥å¤ç”¨ï¼šé¿å…é¢‘ç¹å»ºè¿
â€¢ åˆ†ç‰‡ç­–ç•¥ï¼šå‡åŒ€åˆ†å¸ƒè´Ÿè½½
```

### 5.4 å®é™…åº”ç”¨æŒ‡å¯¼


**ğŸ¯ æŠ€æœ¯é€‰å‹å»ºè®®**
```
åœºæ™¯1ï¼šç®€å•ä»»åŠ¡é˜Ÿåˆ—
â€¢ é€‰æ‹©ï¼šList + æ‰‹åŠ¨ç¡®è®¤æœºåˆ¶
â€¢ åŸå› ï¼šå®ç°ç®€å•ï¼Œæ»¡è¶³åŸºæœ¬å¯é æ€§è¦æ±‚
â€¢ é€‚ç”¨ï¼šä¸­å°å‹ç³»ç»Ÿï¼Œå¯¹å¯é æ€§è¦æ±‚ä¸æé«˜

åœºæ™¯2ï¼šé«˜å¯é æ¶ˆæ¯é˜Ÿåˆ—
â€¢ é€‰æ‹©ï¼šStream + æ¶ˆè´¹ç»„
â€¢ åŸå› ï¼šå†…ç½®ç¡®è®¤æœºåˆ¶ï¼ŒåŠŸèƒ½å®Œå¤‡
â€¢ é€‚ç”¨ï¼šé‡‘èã€æ”¯ä»˜ç­‰å¯¹å¯é æ€§è¦æ±‚æé«˜çš„åœºæ™¯

åœºæ™¯3ï¼šå®æ—¶é€šçŸ¥ç³»ç»Ÿ
â€¢ é€‰æ‹©ï¼šPub/Sub + ç¼“å­˜è¡¥å¿
â€¢ åŸå› ï¼šå®æ—¶æ€§æœ€å¥½ï¼Œé…åˆç¼“å­˜å¤„ç†ç¦»çº¿æ¶ˆæ¯
â€¢ é€‚ç”¨ï¼šèŠå¤©ã€é€šçŸ¥ã€ç›‘æ§å‘Šè­¦ç­‰åœºæ™¯

åœºæ™¯4ï¼šå¤§è§„æ¨¡åˆ†å¸ƒå¼é˜Ÿåˆ—
â€¢ é€‰æ‹©ï¼šList + åˆ†ç‰‡ + è´Ÿè½½å‡è¡¡
â€¢ åŸå› ï¼šå¯æ‰©å±•æ€§å¥½ï¼Œæ”¯æŒæµ·é‡æ¶ˆæ¯
â€¢ é€‚ç”¨ï¼šç”µå•†ã€ç‰©æµç­‰å¤§è§„æ¨¡ä¸šåŠ¡åœºæ™¯
```

**âš¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®**
```
å¼€å‘é˜¶æ®µï¼š
â€¢ åˆç†è®¾ç½®quicklistå‚æ•°
â€¢ ä½¿ç”¨æ‰¹é‡æ“ä½œå‡å°‘ç½‘ç»œå¼€é”€
â€¢ å®ç°ç®€å•çš„é‡è¯•æœºåˆ¶

æµ‹è¯•é˜¶æ®µï¼š
â€¢ å‹åŠ›æµ‹è¯•ç¡®å®šç³»ç»Ÿå®¹é‡
â€¢ ç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ
â€¢ éªŒè¯æ•…éšœæ¢å¤èƒ½åŠ›

ç”Ÿäº§éƒ¨ç½²ï¼š
â€¢ é…ç½®RedisæŒä¹…åŒ–
â€¢ éƒ¨ç½²ç›‘æ§å’Œå‘Šè­¦ç³»ç»Ÿ
â€¢ å»ºç«‹è¿ç»´å’Œæ•…éšœå¤„ç†æµç¨‹

è¿ç»´ç›‘æ§ï¼š
â€¢ å®šæœŸæ£€æŸ¥é˜Ÿåˆ—ç§¯å‹æƒ…å†µ
â€¢ ç›‘æ§æ­»ä¿¡é˜Ÿåˆ—å¢é•¿
â€¢ åˆ†ææ¶ˆè´¹è€…å¤„ç†æ€§èƒ½
â€¢ ä¼˜åŒ–Redisé…ç½®å‚æ•°
```

### 5.5 å¸¸è§é—®é¢˜è§£å†³


```
â“ æ¶ˆæ¯é‡å¤æ¶ˆè´¹ï¼Ÿ
â€¢ åŸå› ï¼šæ¶ˆè´¹è€…å¤„ç†æˆåŠŸä½†ç¡®è®¤å¤±è´¥
â€¢ è§£å†³ï¼šå®ç°ä¸šåŠ¡å¹‚ç­‰æ€§ï¼Œå»é‡æœºåˆ¶
â€¢ é¢„é˜²ï¼šç¼©çŸ­ç¡®è®¤è¶…æ—¶æ—¶é—´

â“ æ¶ˆæ¯ä¹±åºé—®é¢˜ï¼Ÿ
â€¢ åŸå› ï¼šå¤šæ¶ˆè´¹è€…å¹¶å‘å¤„ç†
â€¢ è§£å†³ï¼šæŒ‰ä¸šåŠ¡é”®åˆ†ç‰‡ï¼Œä¿è¯åŒé”®é¡ºåº
â€¢ é¢„é˜²ï¼šå•æ¶ˆè´¹è€…å¤„ç†æœ‰åºæ¶ˆæ¯

â“ é˜Ÿåˆ—ç§¯å‹ä¸¥é‡ï¼Ÿ
â€¢ åŸå› ï¼šç”Ÿäº§é€Ÿåº¦è¶…è¿‡æ¶ˆè´¹é€Ÿåº¦
â€¢ è§£å†³ï¼šå¢åŠ æ¶ˆè´¹è€…ã€ä¼˜åŒ–å¤„ç†é€»è¾‘
â€¢ é¢„é˜²ï¼šç›‘æ§é˜Ÿåˆ—é•¿åº¦ï¼Œè‡ªåŠ¨æ‰©å®¹

â“ å†…å­˜ä½¿ç”¨è¿‡é«˜ï¼Ÿ
â€¢ åŸå› ï¼šå¤§é‡æ¶ˆæ¯ç§¯å‹æˆ–é…ç½®ä¸å½“
â€¢ è§£å†³ï¼šä¼˜åŒ–quicklistå‚æ•°ã€æ¸…ç†è¿‡æœŸæ¶ˆæ¯
â€¢ é¢„é˜²ï¼šå®šæœŸç›‘æ§å†…å­˜ä½¿ç”¨æƒ…å†µ

â“ æ­»ä¿¡æ¶ˆæ¯è¿‡å¤šï¼Ÿ
â€¢ åŸå› ï¼šæ¶ˆæ¯æ ¼å¼é”™è¯¯æˆ–ä¸šåŠ¡é€»è¾‘é—®é¢˜
â€¢ è§£å†³ï¼šåˆ†ææ­»ä¿¡åŸå› ã€ä¿®å¤ä¸šåŠ¡é€»è¾‘
â€¢ é¢„é˜²ï¼šè¾“å…¥éªŒè¯ã€å¼‚å¸¸å¤„ç†
```

**æ ¸å¿ƒè®°å¿†**ï¼š
- quicklistä¼˜åŒ–å†…å­˜ï¼Œå¤´å°¾å¿«ä¸­é—´æ…¢
- å¯é é˜Ÿåˆ—ä¸‰è¦ç´ ï¼šåŸå­æ“ä½œã€ç¡®è®¤æœºåˆ¶ã€é‡è¯•ç­–ç•¥
- ListæŒä¹…ç«äº‰ï¼ŒPub/Subå®æ—¶å¹¿æ’­ï¼ŒStreamå®Œå¤‡å¼ºå¤§
- ç”Ÿäº§çº§éƒ¨ç½²éœ€è¦é«˜å¯ç”¨ã€è´Ÿè½½å‡è¡¡ã€ç›‘æ§å‘Šè­¦