---
title: 8ã€Redisé«˜çº§æ•°æ®ç»“æ„å’Œæ‰©å±•åŠŸèƒ½
---
## ğŸ“š ç›®å½•

1. [é«˜çº§æ•°æ®ç»“æ„æ¦‚è¿°](#1-é«˜çº§æ•°æ®ç»“æ„æ¦‚è¿°)
2. [ä½å›¾Bitmapè¯¦è§£](#2-ä½å›¾bitmapè¯¦è§£)
3. [HyperLogLogåŸºæ•°ç»Ÿè®¡](#3-hyperloglogåŸºæ•°ç»Ÿè®¡)
4. [GEOåœ°ç†ä½ç½®åŠŸèƒ½](#4-geoåœ°ç†ä½ç½®åŠŸèƒ½)
5. [Streamæµæ•°æ®ç»“æ„](#5-streamæµæ•°æ®ç»“æ„)
6. [å®é™…åº”ç”¨åœºæ™¯](#6-å®é™…åº”ç”¨åœºæ™¯)
7. [æ ¸å¿ƒè¦ç‚¹æ€»ç»“](#7-æ ¸å¿ƒè¦ç‚¹æ€»ç»“)

---

## 1. ğŸ¯ é«˜çº§æ•°æ®ç»“æ„æ¦‚è¿°


Redisé™¤äº†åŸºæœ¬çš„å­—ç¬¦ä¸²ã€åˆ—è¡¨ã€é›†åˆã€å“ˆå¸Œã€æœ‰åºé›†åˆå¤–ï¼Œè¿˜æä¾›äº†å‡ ç§ç‰¹æ®Šçš„æ•°æ®ç»“æ„æ¥è§£å†³ç‰¹å®šåœºæ™¯çš„é—®é¢˜ã€‚

### 1.1 ä»€ä¹ˆæ˜¯é«˜çº§æ•°æ®ç»“æ„


è¿™äº›æ•°æ®ç»“æ„æ˜¯ä¸ºäº†è§£å†³ä¼ ç»Ÿæ•°æ®ç»“æ„ä¸æ“…é•¿å¤„ç†çš„ç‰¹å®šé—®é¢˜è€Œè®¾è®¡çš„ï¼š

```
ä¼ ç»Ÿæ•°æ®ç»“æ„çš„å±€é™ï¼š
- ç»Ÿè®¡å¤§é‡æ•°æ®çš„ç‹¬ç«‹è®¿å®¢ï¼šç”¨Setå­˜å‚¨ä¼šå ç”¨å¤§é‡å†…å­˜
- è®°å½•ç”¨æˆ·æ¯æ—¥ç­¾åˆ°ï¼šç”¨å­—ç¬¦ä¸²å­˜å‚¨ä¸å¤Ÿé«˜æ•ˆ
- è®¡ç®—åœ°ç†ä½ç½®è·ç¦»ï¼šéœ€è¦å¤æ‚çš„æ•°å­¦è¿ç®—
- å¤„ç†æ—¶é—´åºåˆ—æ•°æ®ï¼šç°æœ‰ç»“æ„éš¾ä»¥ä¼˜é›…å¤„ç†
```

**ğŸ”¸ å››ç§é«˜çº§æ•°æ®ç»“æ„**
- **Bitmapä½å›¾**ï¼šç”¨ä½æ¥è¡¨ç¤ºçŠ¶æ€ï¼ŒèŠ‚çœå†…å­˜
- **HyperLogLog**ï¼šä¼°ç®—å¤§æ•°æ®é›†çš„åŸºæ•°ï¼Œå†…å­˜å ç”¨æå°
- **GEOåœ°ç†ä½ç½®**ï¼šå­˜å‚¨å’Œè®¡ç®—åœ°ç†åæ ‡
- **Streamæµ**ï¼šå¤„ç†æ—¶é—´åºåˆ—å’Œæ¶ˆæ¯é˜Ÿåˆ—

### 1.2 ä¸ºä»€ä¹ˆéœ€è¦è¿™äº›ç»“æ„


ç”¨ä¸€ä¸ªç®€å•çš„ä¾‹å­æ¥è¯´æ˜ï¼š

```bash
éœ€æ±‚ï¼šç»Ÿè®¡ç½‘ç«™æ¯æ—¥ç‹¬ç«‹è®¿å®¢æ•°ï¼ˆUVï¼‰

æ–¹æ¡ˆ1ï¼šç”¨Setå­˜å‚¨æ‰€æœ‰ç”¨æˆ·ID
SADD daily_visitors user1 user2 user3 ...
é—®é¢˜ï¼š1åƒä¸‡ç”¨æˆ·éœ€è¦å‡ ç™¾MBå†…å­˜

æ–¹æ¡ˆ2ï¼šç”¨HyperLogLogä¼°ç®—
PFADD daily_visitors user1 user2 user3 ...
PFCOUNT daily_visitors
ä¼˜åŠ¿ï¼šåªéœ€è¦12KBå†…å­˜ï¼Œè¯¯å·®ç‡0.81%
```

è¿™å°±æ˜¯ä¸ºä»€ä¹ˆéœ€è¦ä¸“ç”¨æ•°æ®ç»“æ„çš„åŸå› ï¼š**ç”¨æœ€å°‘çš„èµ„æºè§£å†³ç‰¹å®šé—®é¢˜**ã€‚

---

## 2. ğŸ—‚ï¸ ä½å›¾Bitmapè¯¦è§£


### 2.1 BitmapåŸºæœ¬æ¦‚å¿µ


ä½å›¾æœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ª**å­—ç¬¦ä¸²**ï¼Œä½†å¯ä»¥å¯¹å…¶ä¸­çš„æ¯ä¸€ä½è¿›è¡Œæ“ä½œã€‚æŠŠè¿™äº›ä½æƒ³è±¡æˆå¼€å…³ï¼Œ0è¡¨ç¤ºå…³ï¼Œ1è¡¨ç¤ºå¼€ã€‚

**ğŸ”¸ ä½å›¾çš„å­˜å‚¨æ–¹å¼**
```
å­—ç¬¦ä¸² "A" çš„äºŒè¿›åˆ¶è¡¨ç¤ºï¼š01000001
ä½ç½®ï¼š  7 6 5 4 3 2 1 0
æ•°å€¼ï¼š  0 1 0 0 0 0 0 1

æ¯ä¸€ä½éƒ½æœ‰ä¸€ä¸ªä½ç½®ç¼–å·ï¼Œä»0å¼€å§‹
```

**ğŸ’¡ ä¸ºä»€ä¹ˆå«"ä½å›¾"**
- **ä½**ï¼šæ“ä½œçš„æ˜¯äºŒè¿›åˆ¶ä½ï¼ˆbitï¼‰
- **å›¾**ï¼šå¯ä»¥æƒ³è±¡æˆä¸€å¹…ç”±0å’Œ1ç»„æˆçš„"å›¾åƒ"

### 2.2 BitmapåŸºæœ¬å‘½ä»¤


**ğŸ”§ SETBIT - è®¾ç½®ä½å€¼**
```bash
# æ ¼å¼ï¼šSETBIT key offset value
SETBIT user_sign:123 0 1    # è®¾ç½®ç¬¬0ä½ä¸º1
SETBIT user_sign:123 1 1    # è®¾ç½®ç¬¬1ä½ä¸º1  
SETBIT user_sign:123 2 0    # è®¾ç½®ç¬¬2ä½ä¸º0
```

**ğŸ” GETBIT - è·å–ä½å€¼**
```bash
# æ ¼å¼ï¼šGETBIT key offset
GETBIT user_sign:123 0      # è¿”å›ï¼š1
GETBIT user_sign:123 2      # è¿”å›ï¼š0
GETBIT user_sign:123 10     # è¿”å›ï¼š0ï¼ˆæœªè®¾ç½®çš„ä½é»˜è®¤ä¸º0ï¼‰
```

**ğŸ“Š BITCOUNT - ç»Ÿè®¡ä½æ•°**
```bash
# æ ¼å¼ï¼šBITCOUNT key [start end]
BITCOUNT user_sign:123      # ç»Ÿè®¡æ‰€æœ‰ä¸º1çš„ä½æ•°
BITCOUNT user_sign:123 0 1  # ç»Ÿè®¡å­—èŠ‚0åˆ°1ä¸­ä¸º1çš„ä½æ•°
```

### 2.3 ç”¨æˆ·ç­¾åˆ°ç»Ÿè®¡å®ä¾‹


è¿™æ˜¯Bitmapæœ€ç»å…¸çš„åº”ç”¨åœºæ™¯ä¹‹ä¸€ã€‚

**ğŸ¯ éœ€æ±‚åœºæ™¯**
```
è®°å½•ç”¨æˆ·æ¯æ—¥ç­¾åˆ°çŠ¶æ€ï¼š
- ç”¨æˆ·IDï¼š123
- 1æœˆä»½ç­¾åˆ°è®°å½•ï¼š1å·ç­¾åˆ°ï¼Œ2å·æœªç­¾åˆ°ï¼Œ3å·ç­¾åˆ°...
- éœ€è¦å¿«é€ŸæŸ¥è¯¢æŸå¤©æ˜¯å¦ç­¾åˆ°
- éœ€è¦ç»Ÿè®¡æœ¬æœˆç­¾åˆ°å¤©æ•°
```

**ğŸ’» ç­¾åˆ°ç³»ç»Ÿå®ç°**
```python
import redis
from datetime import datetime, timedelta

class UserSignIn:
    def __init__(self, redis_client):
        self.redis = redis_client
    
    def sign_in(self, user_id, date=None):
        """ç”¨æˆ·ç­¾åˆ°"""
        if date is None:
            date = datetime.now()
        
        # ç”Ÿæˆkeyï¼šuser_sign:ç”¨æˆ·ID:å¹´æœˆ
        key = f"user_sign:{user_id}:{date.strftime('%Y%m')}"
        # offsetæ˜¯å½“æœˆçš„ç¬¬å‡ å¤©-1ï¼ˆå› ä¸ºä»0å¼€å§‹ï¼‰
        offset = date.day - 1
        
        # è®¾ç½®å¯¹åº”ä½ä¸º1
        return self.redis.setbit(key, offset, 1)
    
    def check_sign_in(self, user_id, date=None):
        """æ£€æŸ¥æŸå¤©æ˜¯å¦ç­¾åˆ°"""
        if date is None:
            date = datetime.now()
            
        key = f"user_sign:{user_id}:{date.strftime('%Y%m')}"
        offset = date.day - 1
        
        return bool(self.redis.getbit(key, offset))
    
    def get_month_sign_count(self, user_id, year_month=None):
        """è·å–æŸæœˆç­¾åˆ°å¤©æ•°"""
        if year_month is None:
            year_month = datetime.now().strftime('%Y%m')
            
        key = f"user_sign:{user_id}:{year_month}"
        return self.redis.bitcount(key)
    
    def get_sign_info(self, user_id, year_month=None):
        """è·å–ç­¾åˆ°è¯¦ç»†ä¿¡æ¯"""
        if year_month is None:
            year_month = datetime.now().strftime('%Y%m')
        
        key = f"user_sign:{user_id}:{year_month}"
        
        # è·å–ç­¾åˆ°å¤©æ•°
        sign_count = self.redis.bitcount(key)
        
        # è·å–è¿ç»­ç­¾åˆ°å¤©æ•°ï¼ˆä»å½“å¤©å¾€å‰ç®—ï¼‰
        today = datetime.now().day
        consecutive_days = 0
        
        for i in range(today - 1, -1, -1):  # ä»æ˜¨å¤©å¼€å§‹å¾€å‰æŸ¥
            if self.redis.getbit(key, i):
                consecutive_days += 1
            else:
                break
        
        return {
            'total_days': sign_count,
            'consecutive_days': consecutive_days,
            'today_signed': self.check_sign_in(user_id)
        }

# ä½¿ç”¨ç¤ºä¾‹
redis_client = redis.Redis(host='localhost', port=6379, db=0)
sign_system = UserSignIn(redis_client)

# ç”¨æˆ·123åœ¨1æœˆ1æ—¥ç­¾åˆ°
sign_system.sign_in(123, datetime(2025, 1, 1))
sign_system.sign_in(123, datetime(2025, 1, 2))
sign_system.sign_in(123, datetime(2025, 1, 4))

# æŸ¥è¯¢ç­¾åˆ°ä¿¡æ¯
info = sign_system.get_sign_info(123, "202501")
print(f"æ€»ç­¾åˆ°å¤©æ•°ï¼š{info['total_days']}")
print(f"è¿ç»­ç­¾åˆ°å¤©æ•°ï¼š{info['consecutive_days']}")
```

### 2.4 æ´»è·ƒç”¨æˆ·ç»Ÿè®¡


Bitmapè¿˜å¯ä»¥ç”¨æ¥ç»Ÿè®¡æ´»è·ƒç”¨æˆ·ï¼Œç‰¹åˆ«é€‚åˆæŒ‰æ—¶é—´ç»´åº¦åˆ†æã€‚

**ğŸ¯ æ´»è·ƒç”¨æˆ·ç»Ÿè®¡åœºæ™¯**
```bash
# è®°å½•ç”¨æˆ·1001åœ¨ç¬¬100å¤©æ´»è·ƒ
SETBIT active_users:day100 1001 1

# è®°å½•ç”¨æˆ·1002åœ¨ç¬¬100å¤©æ´»è·ƒ  
SETBIT active_users:day100 1002 1

# ç»Ÿè®¡ç¬¬100å¤©æ´»è·ƒç”¨æˆ·æ•°
BITCOUNT active_users:day100

# æ‰¾å‡ºè¿ç»­7å¤©éƒ½æ´»è·ƒçš„ç”¨æˆ·ï¼ˆä½è¿ç®—ï¼‰
BITOP AND result active_users:day100 active_users:day101 active_users:day102 active_users:day103 active_users:day104 active_users:day105 active_users:day106
BITCOUNT result
```

**ğŸ’» æ´»è·ƒç”¨æˆ·åˆ†æç³»ç»Ÿ**
```python
from datetime import datetime, timedelta

class UserActivityAnalyzer:
    def __init__(self, redis_client):
        self.redis = redis_client
    
    def mark_user_active(self, user_id, date=None):
        """æ ‡è®°ç”¨æˆ·åœ¨æŸå¤©æ´»è·ƒ"""
        if date is None:
            date = datetime.now()
        
        day_key = f"active_users:{date.strftime('%Y%m%d')}"
        return self.redis.setbit(day_key, user_id, 1)
    
    def get_daily_active_users(self, date):
        """è·å–æŸå¤©æ´»è·ƒç”¨æˆ·æ•°"""
        day_key = f"active_users:{date.strftime('%Y%m%d')}"
        return self.redis.bitcount(day_key)
    
    def get_continuous_active_users(self, days=7):
        """è·å–è¿ç»­Nå¤©éƒ½æ´»è·ƒçš„ç”¨æˆ·æ•°"""
        today = datetime.now()
        keys = []
        
        for i in range(days):
            date = today - timedelta(days=i)
            keys.append(f"active_users:{date.strftime('%Y%m%d')}")
        
        # ç”Ÿæˆç»“æœkey
        result_key = f"continuous_active_{days}days"
        
        # ä½è¿ç®—ï¼šæ‰€æœ‰å¤©æ•°çš„äº¤é›†
        if len(keys) > 1:
            self.redis.bitop("AND", result_key, *keys)
        else:
            # åªæœ‰ä¸€å¤©çš„æƒ…å†µ
            return self.get_daily_active_users(today)
        
        count = self.redis.bitcount(result_key)
        # æ¸…ç†ä¸´æ—¶ç»“æœ
        self.redis.delete(result_key)
        
        return count
    
    def get_weekly_report(self):
        """ç”Ÿæˆå‘¨æ´»è·ƒæŠ¥å‘Š"""
        today = datetime.now()
        report = {}
        
        for i in range(7):
            date = today - timedelta(days=i)
            day_name = date.strftime('%Y-%m-%d')
            active_count = self.get_daily_active_users(date)
            report[day_name] = active_count
        
        # è®¡ç®—å‘¨æ´»è·ƒç”¨æˆ·ï¼ˆ7å¤©å†…è‡³å°‘æ´»è·ƒ1å¤©ï¼‰
        keys = [f"active_users:{(today - timedelta(days=i)).strftime('%Y%m%d')}" 
                for i in range(7)]
        
        weekly_key = "weekly_active_users"
        self.redis.bitop("OR", weekly_key, *keys)  # å¹¶é›†
        weekly_active = self.redis.bitcount(weekly_key)
        self.redis.delete(weekly_key)
        
        report['weekly_total'] = weekly_active
        return report

# ä½¿ç”¨ç¤ºä¾‹
analyzer = UserActivityAnalyzer(redis_client)

# æ¨¡æ‹Ÿç”¨æˆ·æ´»è·ƒ
analyzer.mark_user_active(1001)
analyzer.mark_user_active(1002)
analyzer.mark_user_active(1003)

# ç”ŸæˆæŠ¥å‘Š
weekly_report = analyzer.get_weekly_report()
print("å‘¨æ´»è·ƒæŠ¥å‘Šï¼š", weekly_report)
```

### 2.5 Bitmapä¼˜åŠ¿ä¸é™åˆ¶


**âœ… Bitmapä¼˜åŠ¿**
```
å†…å­˜é«˜æ•ˆï¼š1ä¸ªç”¨æˆ·1å¤©åªéœ€1ä½ï¼Œ800ä¸‡ç”¨æˆ·ä¸€å¹´åªéœ€365MB
æ“ä½œå¿«é€Ÿï¼šä½è¿ç®—é€Ÿåº¦æå¿«
æ”¯æŒå¤æ‚åˆ†æï¼šé€šè¿‡ä½è¿ç®—å¯ä»¥åšäº¤é›†ã€å¹¶é›†ã€å·®é›†åˆ†æ
```

**âš ï¸ Bitmapé™åˆ¶**
```
ç¨€ç–æ•°æ®æµªè´¹ï¼šå¦‚æœç”¨æˆ·IDè·¨åº¦å¾ˆå¤§ï¼ˆå¦‚1å’Œ1000000ï¼‰ï¼Œä¸­é—´çš„ä½ä¼šæµªè´¹
åªèƒ½å­˜å‚¨å¸ƒå°”å€¼ï¼šæ¯ä½åªèƒ½æ˜¯0æˆ–1ï¼Œä¸èƒ½å­˜å‚¨å…¶ä»–æ•°å€¼
å†…å­˜é¢„åˆ†é…ï¼šè®¾ç½®æŸä½ä¼šå¯¼è‡´åˆ†é…åˆ°è¯¥ä½çš„æ‰€æœ‰å†…å­˜
```

**ğŸ’¡ ä½¿ç”¨å»ºè®®**
- é€‚åˆï¼šç”¨æˆ·IDè¿ç»­ã€éœ€è¦é«˜æ•ˆç»Ÿè®¡çš„åœºæ™¯
- ä¸é€‚åˆï¼šç”¨æˆ·IDç¨€ç–ã€éœ€è¦å­˜å‚¨å¤šçŠ¶æ€å€¼çš„åœºæ™¯

---

## 3. ğŸ“Š HyperLogLogåŸºæ•°ç»Ÿè®¡


### 3.1 HyperLogLogåŸºæœ¬æ¦‚å¿µ


HyperLogLogæ˜¯ä¸€ç§**æ¦‚ç‡æ€§æ•°æ®ç»“æ„**ï¼Œç”¨æ¥ä¼°ç®—é›†åˆä¸­ä¸é‡å¤å…ƒç´ çš„æ•°é‡ï¼ˆåŸºæ•°ï¼‰ã€‚

**ğŸ”¸ ä»€ä¹ˆæ˜¯åŸºæ•°**
```
åŸºæ•°å°±æ˜¯é›†åˆä¸­ä¸é‡å¤å…ƒç´ çš„ä¸ªæ•°ï¼š
é›†åˆ {1, 2, 2, 3, 3, 3} çš„åŸºæ•°æ˜¯ 3
é›†åˆ {a, b, c, a, b} çš„åŸºæ•°æ˜¯ 3

åœ¨ç½‘ç«™ç»Ÿè®¡ä¸­ï¼š
- é¡µé¢è®¿é—®æ¬¡æ•°ï¼ˆPVï¼‰ï¼šæ€»è®¿é—®æ•°ï¼ŒåŒ…æ‹¬é‡å¤è®¿é—®
- ç‹¬ç«‹è®¿å®¢æ•°ï¼ˆUVï¼‰ï¼šåŸºæ•°ï¼Œä¸é‡å¤çš„è®¿å®¢æ•°
```

**ğŸ’¡ ä¸ºä»€ä¹ˆéœ€è¦HyperLogLog**

ä¼ ç»Ÿæ–¹æ³•ç»Ÿè®¡UVçš„é—®é¢˜ï¼š
```bash
# ç”¨Setå­˜å‚¨æ‰€æœ‰è®¿å®¢ID
SADD daily_visitors user1 user2 user3 ... user10000000

é—®é¢˜ï¼š
- 1åƒä¸‡ç”¨æˆ·IDéœ€è¦å‡ ç™¾MBå†…å­˜
- å†…å­˜éšç”¨æˆ·æ•°çº¿æ€§å¢é•¿
- å¤§å‹ç½‘ç«™æ— æ³•æ‰¿å—
```

HyperLogLogçš„ä¼˜åŠ¿ï¼š
```bash
# ç”¨HyperLogLogä¼°ç®—è®¿å®¢æ•°
PFADD daily_visitors user1 user2 user3 ... user10000000
PFCOUNT daily_visitors

ä¼˜åŠ¿ï¼š
- å›ºå®šåªéœ€è¦12KBå†…å­˜
- æ ‡å‡†è¯¯å·®ç‡0.81%
- æ”¯æŒåˆå¹¶è®¡ç®—
```

### 3.2 HyperLogLogåŸºæœ¬å‘½ä»¤


**ğŸ”§ PFADD - æ·»åŠ å…ƒç´ **
```bash
# æ ¼å¼ï¼šPFADD key element [element ...]
PFADD uv:20250128 user1 user2 user3
PFADD uv:20250128 user2 user4 user5  # user2é‡å¤ï¼Œä¸ä¼šå½±å“åŸºæ•°
```

**ğŸ“Š PFCOUNT - ç»Ÿè®¡åŸºæ•°**
```bash
# æ ¼å¼ï¼šPFCOUNT key [key ...]
PFCOUNT uv:20250128                    # è¿”å›ä¼°ç®—çš„ç‹¬ç«‹å…ƒç´ æ•°
PFCOUNT uv:20250128 uv:20250129       # è¿”å›å¤šä¸ªHLLçš„å¹¶é›†åŸºæ•°
```

**ğŸ”„ PFMERGE - åˆå¹¶HyperLogLog**
```bash
# æ ¼å¼ï¼šPFMERGE destkey sourcekey [sourcekey ...]
PFMERGE uv:week uv:20250128 uv:20250129 uv:20250130
PFCOUNT uv:week  # è·å¾—è¿™3å¤©çš„æ€»UVæ•°ï¼ˆå»é‡åï¼‰
```

### 3.3 ç½‘ç«™UVç»Ÿè®¡å®ä¾‹


**ğŸ’» UVç»Ÿè®¡ç³»ç»Ÿ**
```python
import redis
from datetime import datetime, timedelta

class UVAnalyzer:
    def __init__(self, redis_client):
        self.redis = redis_client
    
    def add_visitor(self, page_id, user_id, date=None):
        """è®°å½•è®¿å®¢"""
        if date is None:
            date = datetime.now()
        
        # æ¯æ—¥UVç»Ÿè®¡
        daily_key = f"uv:daily:{page_id}:{date.strftime('%Y%m%d')}"
        self.redis.pfadd(daily_key, str(user_id))
        
        # æ¯æœˆUVç»Ÿè®¡
        monthly_key = f"uv:monthly:{page_id}:{date.strftime('%Y%m')}"
        self.redis.pfadd(monthly_key, str(user_id))
        
        return True
    
    def get_daily_uv(self, page_id, date=None):
        """è·å–æŸé¡µé¢æŸå¤©çš„UV"""
        if date is None:
            date = datetime.now()
        
        daily_key = f"uv:daily:{page_id}:{date.strftime('%Y%m%d')}"
        return self.redis.pfcount(daily_key)
    
    def get_weekly_uv(self, page_id, end_date=None):
        """è·å–æŸé¡µé¢ä¸€å‘¨çš„UVï¼ˆå»é‡ï¼‰"""
        if end_date is None:
            end_date = datetime.now()
        
        # æ”¶é›†7å¤©çš„key
        keys = []
        for i in range(7):
            date = end_date - timedelta(days=i)
            key = f"uv:daily:{page_id}:{date.strftime('%Y%m%d')}"
            keys.append(key)
        
        # åˆå¹¶åˆ°ä¸´æ—¶key
        temp_key = f"uv:temp:weekly:{page_id}"
        self.redis.pfmerge(temp_key, *keys)
        
        # è·å–åˆå¹¶åçš„UVæ•°
        weekly_uv = self.redis.pfcount(temp_key)
        
        # æ¸…ç†ä¸´æ—¶key
        self.redis.delete(temp_key)
        
        return weekly_uv
    
    def get_monthly_uv(self, page_id, year_month=None):
        """è·å–æŸé¡µé¢æŸæœˆçš„UV"""
        if year_month is None:
            year_month = datetime.now().strftime('%Y%m')
        
        monthly_key = f"uv:monthly:{page_id}:{year_month}"
        return self.redis.pfcount(monthly_key)
    
    def get_multi_page_uv(self, page_ids, date=None):
        """è·å–å¤šä¸ªé¡µé¢çš„æ€»UVï¼ˆå»é‡ï¼‰"""
        if date is None:
            date = datetime.now()
        
        keys = []
        for page_id in page_ids:
            key = f"uv:daily:{page_id}:{date.strftime('%Y%m%d')}"
            keys.append(key)
        
        # ç›´æ¥ç»Ÿè®¡å¤šä¸ªHLLçš„å¹¶é›†
        return self.redis.pfcount(*keys)
    
    def generate_uv_report(self, page_id):
        """ç”ŸæˆUVç»Ÿè®¡æŠ¥å‘Š"""
        today = datetime.now()
        
        report = {
            'page_id': page_id,
            'date': today.strftime('%Y-%m-%d'),
            'today_uv': self.get_daily_uv(page_id, today),
            'yesterday_uv': self.get_daily_uv(page_id, today - timedelta(days=1)),
            'weekly_uv': self.get_weekly_uv(page_id, today),
            'monthly_uv': self.get_monthly_uv(page_id)
        }
        
        # è®¡ç®—å¢é•¿ç‡
        if report['yesterday_uv'] > 0:
            growth_rate = (report['today_uv'] - report['yesterday_uv']) / report['yesterday_uv'] * 100
            report['daily_growth_rate'] = round(growth_rate, 2)
        else:
            report['daily_growth_rate'] = 0
        
        return report

# ä½¿ç”¨ç¤ºä¾‹
redis_client = redis.Redis(host='localhost', port=6379, db=0)
uv_analyzer = UVAnalyzer(redis_client)

# æ¨¡æ‹Ÿç”¨æˆ·è®¿é—®
for user_id in range(1, 10001):  # 1ä¸‡ç”¨æˆ·è®¿é—®
    uv_analyzer.add_visitor('homepage', user_id)
    if user_id % 2 == 0:  # ä¸€åŠç”¨æˆ·è¿˜è®¿é—®äº†äº§å“é¡µ
        uv_analyzer.add_visitor('product', user_id)

# ç”ŸæˆæŠ¥å‘Š
homepage_report = uv_analyzer.generate_uv_report('homepage')
print("é¦–é¡µUVæŠ¥å‘Šï¼š", homepage_report)

# æŸ¥çœ‹å¤šé¡µé¢æ€»UV
total_uv = uv_analyzer.get_multi_page_uv(['homepage', 'product'])
print(f"é¦–é¡µ+äº§å“é¡µæ€»UVï¼š{total_uv}")
```

### 3.4 å»é‡è®¡æ•°åº”ç”¨


é™¤äº†UVç»Ÿè®¡ï¼ŒHyperLogLogè¿˜å¯ä»¥ç”¨äºå„ç§éœ€è¦å»é‡è®¡æ•°çš„åœºæ™¯ã€‚

**ğŸ¯ å®é™…åº”ç”¨åœºæ™¯**
```python
class DeduplicationCounter:
    def __init__(self, redis_client):
        self.redis = redis_client
    
    def count_unique_ips(self, log_entries):
        """ç»Ÿè®¡ç‹¬ç«‹IPæ•°"""
        key = "unique_ips"
        ips = [entry['ip'] for entry in log_entries]
        self.redis.pfadd(key, *ips)
        return self.redis.pfcount(key)
    
    def count_unique_devices(self, device_ids):
        """ç»Ÿè®¡ç‹¬ç«‹è®¾å¤‡æ•°"""
        key = "unique_devices"
        self.redis.pfadd(key, *device_ids)
        return self.redis.pfcount(key)
    
    def count_search_queries(self, queries):
        """ç»Ÿè®¡ç‹¬ç«‹æœç´¢è¯æ•°"""
        key = "unique_queries"
        self.redis.pfadd(key, *queries)
        return self.redis.pfcount(key)
    
    def merge_counters(self, source_keys, dest_key):
        """åˆå¹¶å¤šä¸ªè®¡æ•°å™¨"""
        self.redis.pfmerge(dest_key, *source_keys)
        return self.redis.pfcount(dest_key)

# ä½¿ç”¨ç¤ºä¾‹
counter = DeduplicationCounter(redis_client)

# æ¨¡æ‹Ÿæ—¥å¿—åˆ†æ
log_entries = [
    {'ip': '192.168.1.1', 'user': 'user1'},
    {'ip': '192.168.1.2', 'user': 'user2'},
    {'ip': '192.168.1.1', 'user': 'user1'},  # é‡å¤IP
    {'ip': '192.168.1.3', 'user': 'user3'},
]

unique_ip_count = counter.count_unique_ips(log_entries)
print(f"ç‹¬ç«‹IPæ•°ï¼š{unique_ip_count}")  # è¾“å‡ºï¼š3
```

### 3.5 HyperLogLogç²¾åº¦åˆ†æ


**ğŸ“Š è¯¯å·®ç‡åˆ†æ**

HyperLogLogçš„æ ‡å‡†è¯¯å·®ç‡å…¬å¼ï¼š
```
æ ‡å‡†è¯¯å·®ç‡ = 1.04 / âˆšm

å…¶ä¸­mæ˜¯ç”¨äºè®¡ç®—çš„æ¡¶æ•°ï¼ˆRedisä¸­m=16384ï¼‰
æ‰€ä»¥Redis HyperLogLogçš„æ ‡å‡†è¯¯å·®ç‡ = 1.04 / âˆš16384 â‰ˆ 0.81%
```

**å®é™…æµ‹è¯•å¯¹æ¯”**
```python
def accuracy_test():
    # ç”ŸæˆçœŸå®æ•°æ®
    real_set = set(range(100000))  # 10ä¸‡å”¯ä¸€æ•°å­—
    real_count = len(real_set)
    
    # ç”¨HyperLogLogä¼°ç®—
    key = "accuracy_test"
    redis_client.delete(key)
    redis_client.pfadd(key, *real_set)
    estimated_count = redis_client.pfcount(key)
    
    # è®¡ç®—è¯¯å·®
    error_rate = abs(estimated_count - real_count) / real_count * 100
    
    print(f"çœŸå®åŸºæ•°ï¼š{real_count}")
    print(f"ä¼°ç®—åŸºæ•°ï¼š{estimated_count}")
    print(f"è¯¯å·®ç‡ï¼š{error_rate:.2f}%")

# è¿è¡Œæµ‹è¯•
accuracy_test()
# å…¸å‹è¾“å‡ºï¼š
# çœŸå®åŸºæ•°ï¼š100000
# ä¼°ç®—åŸºæ•°ï¼š99723
# è¯¯å·®ç‡ï¼š0.28%
```

**ğŸ’¡ ä½¿ç”¨å»ºè®®**
- **é€‚ç”¨åœºæ™¯**ï¼šæ•°æ®é‡å¤§ï¼ˆ>10000ï¼‰ï¼Œå…è®¸å°è¯¯å·®ï¼Œå†…å­˜æ•æ„Ÿ
- **ä¸é€‚ç”¨**ï¼šéœ€è¦ç²¾ç¡®è®¡æ•°ï¼Œæ•°æ®é‡å°ï¼Œè¯¯å·®æ•æ„Ÿçš„ä¸šåŠ¡
- **è¯¯å·®å¯æ¥å—çš„ä¸šåŠ¡**ï¼šç½‘ç«™UVç»Ÿè®¡ã€å¹¿å‘Šå»é‡ã€A/Bæµ‹è¯•åˆ†æµç­‰

---

## 4. ğŸŒ GEOåœ°ç†ä½ç½®åŠŸèƒ½


### 4.1 GEOåŸºæœ¬æ¦‚å¿µ


Redisçš„GEOåŠŸèƒ½åŸºäº**æœ‰åºé›†åˆï¼ˆSorted Setï¼‰**å®ç°ï¼Œå¯ä»¥å­˜å‚¨åœ°ç†ä½ç½®åæ ‡å¹¶è¿›è¡Œè·ç¦»è®¡ç®—ã€èŒƒå›´æŸ¥è¯¢ç­‰æ“ä½œã€‚

**ğŸ”¸ åœ°ç†åæ ‡ç³»ç»Ÿ**
```
ç»çº¬åº¦åæ ‡ç³»ï¼š
- ç»åº¦ï¼ˆlongitudeï¼‰ï¼šä¸œè¥¿æ–¹å‘ï¼Œ-180Â° åˆ° 180Â°
- çº¬åº¦ï¼ˆlatitudeï¼‰ï¼šå—åŒ—æ–¹å‘ï¼Œ-90Â° åˆ° 90Â°
- ç¤ºä¾‹ï¼šåŒ—äº¬åæ ‡ (116.404, 39.915)
```

**ğŸ’¡ GEOåº•å±‚å®ç°**

Redisä½¿ç”¨**GeoHash**ç®—æ³•å°†äºŒç»´çš„ç»çº¬åº¦åæ ‡è½¬æ¢ä¸ºä¸€ç»´çš„å­—ç¬¦ä¸²ï¼Œç„¶åå­˜å‚¨åœ¨æœ‰åºé›†åˆä¸­ï¼š
```
åæ ‡ (116.404, 39.915) â†’ GeoHash: wx4fbxxxx â†’ Scoreå­˜å‚¨åœ¨Zsetä¸­
```

### 4.2 GEOåŸºæœ¬å‘½ä»¤


**ğŸ”§ GEOADD - æ·»åŠ åœ°ç†ä½ç½®**
```bash
# æ ¼å¼ï¼šGEOADD key longitude latitude member [longitude latitude member ...]
GEOADD cities 116.404 39.915 "beijing"
GEOADD cities 121.472 31.231 "shanghai" 113.265 23.108 "guangzhou"
```

**ğŸ“ GEODIST - è®¡ç®—è·ç¦»**
```bash
# æ ¼å¼ï¼šGEODIST key member1 member2 [m|km|mi|ft]
GEODIST cities beijing shanghai km    # è¿”å›åŒ—äº¬åˆ°ä¸Šæµ·çš„è·ç¦»ï¼ˆåƒç±³ï¼‰
GEODIST cities beijing guangzhou mi   # è¿”å›åŒ—äº¬åˆ°å¹¿å·çš„è·ç¦»ï¼ˆè‹±é‡Œï¼‰
```

**ğŸ¯ GEORADIUS - èŒƒå›´æŸ¥è¯¢**
```bash
# æ ¼å¼ï¼šGEORADIUS key longitude latitude radius m|km|mi|ft [WITHCOORD] [WITHDIST] [WITHHASH] [COUNT count]
GEORADIUS cities 116.404 39.915 1000 km WITHDIST COUNT 10
# æŸ¥æ‰¾åŒ—äº¬åæ ‡1000å…¬é‡ŒèŒƒå›´å†…çš„åŸå¸‚ï¼Œè¿”å›è·ç¦»ä¿¡æ¯ï¼Œæœ€å¤š10ä¸ª
```

**ğŸ” GEORADIUSBYMEMBER - ä»¥æˆå‘˜ä¸ºä¸­å¿ƒæŸ¥è¯¢**
```bash
# æ ¼å¼ï¼šGEORADIUSBYMEMBER key member radius m|km|mi|ft [WITHCOORD] [WITHDIST] [WITHHASH] [COUNT count]
GEORADIUSBYMEMBER cities beijing 1000 km WITHDIST
# æŸ¥æ‰¾åŒ—äº¬1000å…¬é‡ŒèŒƒå›´å†…çš„åŸå¸‚
```

### 4.3 LBSåº”ç”¨å®ä¾‹


**ğŸ’» é™„è¿‘çš„äººåŠŸèƒ½**
```python
import redis
import math
from datetime import datetime

class LocationService:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.user_locations_key = "user_locations"
    
    def update_user_location(self, user_id, longitude, latitude):
        """æ›´æ–°ç”¨æˆ·ä½ç½®"""
        try:
            # éªŒè¯åæ ‡èŒƒå›´
            if not (-180 <= longitude <= 180) or not (-90 <= latitude <= 90):
                raise ValueError("åæ ‡è¶…å‡ºæœ‰æ•ˆèŒƒå›´")
            
            # æ·»åŠ åˆ°GEOé›†åˆ
            result = self.redis.geoadd(self.user_locations_key, longitude, latitude, user_id)
            
            # è®°å½•æ›´æ–°æ—¶é—´
            timestamp_key = f"location_timestamp:{user_id}"
            self.redis.set(timestamp_key, datetime.now().isoformat())
            self.redis.expire(timestamp_key, 3600 * 24)  # 24å°æ—¶è¿‡æœŸ
            
            return result
        except Exception as e:
            print(f"æ›´æ–°ä½ç½®å¤±è´¥ï¼š{e}")
            return False
    
    def find_nearby_users(self, user_id, radius_km=5, count=20):
        """æŸ¥æ‰¾é™„è¿‘çš„ç”¨æˆ·"""
        try:
            # ä»¥æŒ‡å®šç”¨æˆ·ä¸ºä¸­å¿ƒæŸ¥æ‰¾
            result = self.redis.georadiusbymember(
                self.user_locations_key,
                user_id,
                radius_km,
                unit='km',
                withdist=True,
                withcoord=True,
                count=count + 1  # +1æ˜¯å› ä¸ºä¼šåŒ…å«è‡ªå·±
            )
            
            nearby_users = []
            for item in result:
                nearby_user_id = item[0].decode() if isinstance(item[0], bytes) else item[0]
                distance = float(item[1])
                coordinates = item[2]  # [longitude, latitude]
                
                # æ’é™¤è‡ªå·±
                if nearby_user_id != str(user_id):
                    nearby_users.append({
                        'user_id': nearby_user_id,
                        'distance_km': round(distance, 2),
                        'longitude': coordinates[0],
                        'latitude': coordinates[1]
                    })
            
            return nearby_users[:count]
        except Exception as e:
            print(f"æŸ¥æ‰¾é™„è¿‘ç”¨æˆ·å¤±è´¥ï¼š{e}")
            return []
    
    def find_nearby_by_coordinates(self, longitude, latitude, radius_km=5, count=20):
        """æ ¹æ®åæ ‡æŸ¥æ‰¾é™„è¿‘ç”¨æˆ·"""
        try:
            result = self.redis.georadius(
                self.user_locations_key,
                longitude,
                latitude,
                radius_km,
                unit='km',
                withdist=True,
                withcoord=True,
                count=count
            )
            
            nearby_users = []
            for item in result:
                user_id = item[0].decode() if isinstance(item[0], bytes) else item[0]
                distance = float(item[1])
                coordinates = item[2]
                
                nearby_users.append({
                    'user_id': user_id,
                    'distance_km': round(distance, 2),
                    'longitude': coordinates[0],
                    'latitude': coordinates[1]
                })
            
            return nearby_users
        except Exception as e:
            print(f"åæ ‡æŸ¥æ‰¾å¤±è´¥ï¼š{e}")
            return []
    
    def get_distance_between_users(self, user_id1, user_id2, unit='km'):
        """è·å–ä¸¤ä¸ªç”¨æˆ·ä¹‹é—´çš„è·ç¦»"""
        try:
            distance = self.redis.geodist(self.user_locations_key, user_id1, user_id2, unit)
            return float(distance) if distance else None
        except Exception as e:
            print(f"è·ç¦»è®¡ç®—å¤±è´¥ï¼š{e}")
            return None
    
    def remove_user_location(self, user_id):
        """ç§»é™¤ç”¨æˆ·ä½ç½®ä¿¡æ¯"""
        try:
            # ä»GEOé›†åˆä¸­åˆ é™¤
            result = self.redis.zrem(self.user_locations_key, user_id)
            
            # åˆ é™¤æ—¶é—´æˆ³
            timestamp_key = f"location_timestamp:{user_id}"
            self.redis.delete(timestamp_key)
            
            return result > 0
        except Exception as e:
            print(f"åˆ é™¤ä½ç½®å¤±è´¥ï¼š{e}")
            return False

# ä½¿ç”¨ç¤ºä¾‹
redis_client = redis.Redis(host='localhost', port=6379, db=0)
location_service = LocationService(redis_client)

# æ›´æ–°ç”¨æˆ·ä½ç½®
location_service.update_user_location("user1", 116.404, 39.915)  # åŒ—äº¬
location_service.update_user_location("user2", 116.407, 39.918)  # åŒ—äº¬é™„è¿‘
location_service.update_user_location("user3", 121.472, 31.231)  # ä¸Šæµ·

# æŸ¥æ‰¾user1é™„è¿‘çš„ç”¨æˆ·
nearby = location_service.find_nearby_users("user1", radius_km=10, count=5)
print("user1é™„è¿‘çš„ç”¨æˆ·ï¼š", nearby)

# è®¡ç®—ä¸¤ç”¨æˆ·è·ç¦»
distance = location_service.get_distance_between_users("user1", "user3")
print(f"user1å’Œuser3çš„è·ç¦»ï¼š{distance}å…¬é‡Œ")
```

**ğŸš— æ‰“è½¦åº”ç”¨åœºæ™¯**
```python
class TaxiService:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.drivers_key = "taxi_drivers"
        self.passengers_key = "taxi_passengers"
    
    def driver_online(self, driver_id, longitude, latitude):
        """å¸æœºä¸Šçº¿"""
        self.redis.geoadd(self.drivers_key, longitude, latitude, driver_id)
        driver_info_key = f"driver:{driver_id}"
        self.redis.hset(driver_info_key, mapping={
            'status': 'available',
            'longitude': longitude,
            'latitude': latitude,
            'online_time': datetime.now().isoformat()
        })
        self.redis.expire(driver_info_key, 3600 * 8)  # 8å°æ—¶è¿‡æœŸ
    
    def find_nearby_drivers(self, passenger_longitude, passenger_latitude, radius_km=3, count=10):
        """ä¸ºä¹˜å®¢æ‰¾é™„è¿‘å¸æœº"""
        nearby_drivers = self.redis.georadius(
            self.drivers_key,
            passenger_longitude,
            passenger_latitude,
            radius_km,
            unit='km',
            withdist=True,
            count=count
        )
        
        available_drivers = []
        for item in nearby_drivers:
            driver_id = item[0].decode() if isinstance(item[0], bytes) else item[0]
            distance = float(item[1])
            
            # æ£€æŸ¥å¸æœºçŠ¶æ€
            driver_info = self.redis.hgetall(f"driver:{driver_id}")
            if driver_info and driver_info.get(b'status', b'').decode() == 'available':
                available_drivers.append({
                    'driver_id': driver_id,
                    'distance_km': round(distance, 2),
                    'status': driver_info.get(b'status', b'').decode()
                })
        
        # æŒ‰è·ç¦»æ’åº
        available_drivers.sort(key=lambda x: x['distance_km'])
        return available_drivers
    
    def driver_offline(self, driver_id):
        """å¸æœºä¸‹çº¿"""
        self.redis.zrem(self.drivers_key, driver_id)
        self.redis.delete(f"driver:{driver_id}")

# ä½¿ç”¨ç¤ºä¾‹
taxi_service = TaxiService(redis_client)

# å¸æœºä¸Šçº¿
taxi_service.driver_online("driver001", 116.404, 39.915)
taxi_service.driver_online("driver002", 116.408, 39.920)

# ä¹˜å®¢å«è½¦
passenger_location = (116.405, 39.916)
nearby_drivers = taxi_service.find_nearby_drivers(*passenger_location, radius_km=2)
print("é™„è¿‘å¯ç”¨å¸æœºï¼š", nearby_drivers)
```

### 4.4 å•†åº—ä½ç½®æœåŠ¡


**ğŸª å‘¨è¾¹å•†åº—æŸ¥è¯¢**
```python
class StoreLocator:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.stores_key = "stores"
    
    def add_store(self, store_id, longitude, latitude, store_info):
        """æ·»åŠ å•†åº—"""
        # æ·»åŠ åœ°ç†ä½ç½®
        self.redis.geoadd(self.stores_key, longitude, latitude, store_id)
        
        # å­˜å‚¨å•†åº—è¯¦ç»†ä¿¡æ¯
        store_key = f"store:{store_id}"
        store_info.update({
            'longitude': longitude,
            'latitude': latitude,
            'created_time': datetime.now().isoformat()
        })
        self.redis.hset(store_key, mapping=store_info)
    
    def find_nearby_stores(self, longitude, latitude, radius_km=2, category=None, count=20):
        """æŸ¥æ‰¾é™„è¿‘å•†åº—"""
        nearby_stores = self.redis.georadius(
            self.stores_key,
            longitude,
            latitude,
            radius_km,
            unit='km',
            withdist=True,
            withcoord=True,
            count=count
        )
        
        stores = []
        for item in nearby_stores:
            store_id = item[0].decode() if isinstance(item[0], bytes) else item[0]
            distance = float(item[1])
            coordinates = item[2]
            
            # è·å–å•†åº—è¯¦ç»†ä¿¡æ¯
            store_info = self.redis.hgetall(f"store:{store_id}")
            if store_info:
                store_data = {
                    'store_id': store_id,
                    'distance_km': round(distance, 2),
                    'longitude': coordinates[0],
                    'latitude': coordinates[1]
                }
                
                # è§£æå•†åº—ä¿¡æ¯
                for key, value in store_info.items():
                    if isinstance(key, bytes):
                        key = key.decode()
                    if isinstance(value, bytes):
                        value = value.decode()
                    store_data[key] = value
                
                # æŒ‰ç±»åˆ«è¿‡æ»¤
                if category is None or store_data.get('category') == category:
                    stores.append(store_data)
        
        return sorted(stores, key=lambda x: x['distance_km'])
    
    def get_store_coverage(self, store_id, radius_km=5):
        """è·å–å•†åº—è¦†ç›–èŒƒå›´å†…çš„å…¶ä»–å•†åº—"""
        competitors = self.redis.georadiusbymember(
            self.stores_key,
            store_id,
            radius_km,
            unit='km',
            withdist=True,
            count=50
        )
        
        nearby_competitors = []
        for item in competitors:
            competitor_id = item[0].decode() if isinstance(item[0], bytes) else item[0]
            distance = float(item[1])
            
            # æ’é™¤è‡ªå·±
            if competitor_id != store_id:
                competitor_info = self.redis.hgetall(f"store:{competitor_id}")
                if competitor_info:
                    nearby_competitors.append({
                        'store_id': competitor_id,
                        'distance_km': round(distance, 2),
                        'name': competitor_info.get(b'name', b'').decode(),
                        'category': competitor_info.get(b'category', b'').decode()
                    })
        
        return sorted(nearby_competitors, key=lambda x: x['distance_km'])

# ä½¿ç”¨ç¤ºä¾‹
store_locator = StoreLocator(redis_client)

# æ·»åŠ å•†åº—
store_locator.add_store("store001", 116.404, 39.915, {
    'name': 'åŒ—äº¬è¶…å¸‚',
    'category': 'è¶…å¸‚',
    'phone': '123-456-7890'
})

store_locator.add_store("store002", 116.408, 39.918, {
    'name': 'ä¾¿åˆ©åº—24h',
    'category': 'ä¾¿åˆ©åº—',
    'phone': '098-765-4321'
})

# æŸ¥æ‰¾é™„è¿‘è¶…å¸‚
user_location = (116.405, 39.916)
nearby_supermarkets = store_locator.find_nearby_stores(
    *user_location, 
    radius_km=1, 
    category='è¶…å¸‚'
)
print("é™„è¿‘è¶…å¸‚ï¼š", nearby_supermarkets)
```

### 4.5 GEOæ€§èƒ½ä¼˜åŒ–


**âš¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®**

1. **åˆç†è®¾ç½®æŸ¥è¯¢åŠå¾„**
```python
# é¿å…è¿‡å¤§çš„æŸ¥è¯¢åŠå¾„
def smart_radius_query(longitude, latitude, max_radius_km=10):
    """æ™ºèƒ½åŠå¾„æŸ¥è¯¢ï¼šä»å°åˆ°å¤§é€æ­¥æ‰©å¤§èŒƒå›´"""
    for radius in [1, 3, 5, max_radius_km]:
        results = redis_client.georadius(
            "locations", longitude, latitude, radius, 
            unit='km', count=20
        )
        if len(results) >= 5:  # æ‰¾åˆ°è¶³å¤Ÿçš„ç»“æœå°±åœæ­¢
            return results
    return results
```

2. **ä½¿ç”¨COUNTé™åˆ¶ç»“æœæ•°é‡**
```bash
# é™åˆ¶è¿”å›ç»“æœæ•°é‡ï¼Œé¿å…è¿”å›è¿‡å¤šæ•°æ®
GEORADIUS locations 116.404 39.915 10 km COUNT 20
```

3. **å®šæœŸæ¸…ç†è¿‡æœŸä½ç½®**
```python
def cleanup_old_locations():
    """æ¸…ç†é•¿æ—¶é—´æœªæ›´æ–°çš„ä½ç½®"""
    all_users = redis_client.zrange("user_locations", 0, -1)
    current_time = datetime.now()
    
    for user_id in all_users:
        timestamp_key = f"location_timestamp:{user_id}"
        last_update = redis_client.get(timestamp_key)
        
        if last_update:
            last_update_time = datetime.fromisoformat(last_update.decode())
            if (current_time - last_update_time).hours > 24:
                # 24å°æ—¶æœªæ›´æ–°çš„ç”¨æˆ·ä½ç½®åˆ é™¤
                redis_client.zrem("user_locations", user_id)
                redis_client.delete(timestamp_key)
```

---

## 5. ğŸŒŠ Streamæµæ•°æ®ç»“æ„


### 5.1 StreamåŸºæœ¬æ¦‚å¿µ


Redis Streamæ˜¯Redis 5.0å¼•å…¥çš„æ–°æ•°æ®ç»“æ„ï¼Œä¸“é—¨ç”¨äºå¤„ç†**æ—¶é—´åºåˆ—æ•°æ®**å’Œ**æ¶ˆæ¯é˜Ÿåˆ—**åœºæ™¯ã€‚

**ğŸ”¸ ä»€ä¹ˆæ˜¯Stream**
```
Streamå¯ä»¥ç†è§£ä¸ºä¸€ä¸ªåªèƒ½è¿½åŠ çš„æ—¥å¿—ï¼š
- æ¯æ¡æ¶ˆæ¯éƒ½æœ‰å”¯ä¸€çš„IDï¼ˆæ—¶é—´æˆ³+åºå·ï¼‰
- æ¶ˆæ¯æŒ‰æ—¶é—´é¡ºåºå­˜å‚¨
- æ”¯æŒæ¶ˆè´¹è€…ç»„æ¨¡å¼
- å¯ä»¥ä»ä»»æ„ä½ç½®å¼€å§‹è¯»å–
```

**ğŸ’¡ Stream vs å…¶ä»–æ•°æ®ç»“æ„**
```bash
Listä½œä¸ºé˜Ÿåˆ—çš„é—®é¢˜ï¼š
- LPUSH/RPOPï¼šæ¶ˆæ¯æ¶ˆè´¹åå°±æ¶ˆå¤±äº†
- æ— æ³•æ”¯æŒå¤šä¸ªæ¶ˆè´¹è€…
- æ²¡æœ‰æ¶ˆæ¯ç¡®è®¤æœºåˆ¶

Pub/Subçš„é—®é¢˜ï¼š
- æ¶ˆæ¯ä¸æŒä¹…åŒ–ï¼Œæ–­çº¿å°±ä¸¢å¤±
- æ— æ³•ä»å†å²æ¶ˆæ¯å¼€å§‹æ¶ˆè´¹
- æ²¡æœ‰æ¶ˆè´¹ç¡®è®¤

Streamçš„ä¼˜åŠ¿ï¼š
- æ¶ˆæ¯æŒä¹…åŒ–å­˜å‚¨
- æ”¯æŒæ¶ˆè´¹è€…ç»„
- æœ‰æ¶ˆæ¯ç¡®è®¤æœºåˆ¶
- å¯ä»¥ä»ä»»æ„æ—¶é—´ç‚¹æ¶ˆè´¹
```

### 5.2 StreamåŸºæœ¬å‘½ä»¤


**ğŸ“ XADD - æ·»åŠ æ¶ˆæ¯**
```bash
# æ ¼å¼ï¼šXADD key ID field value [field value ...]
XADD mystream * user_id 123 action login ip 192.168.1.1
# * è¡¨ç¤ºè‡ªåŠ¨ç”ŸæˆIDï¼Œè¿”å›ç±»ä¼¼ï¼š1640995200000-0
```

**ğŸ‘ï¸ XREAD - è¯»å–æ¶ˆæ¯**
```bash
# æ ¼å¼ï¼šXREAD [COUNT count] [BLOCK milliseconds] STREAMS key [key ...] id [id ...]
XREAD COUNT 10 STREAMS mystream 0    # ä»å¤´å¼€å§‹è¯»å–10æ¡æ¶ˆæ¯
XREAD BLOCK 5000 STREAMS mystream $  # é˜»å¡5ç§’ç­‰å¾…æ–°æ¶ˆæ¯
```

**ğŸ‘¥ XGROUP - æ¶ˆè´¹è€…ç»„ç®¡ç†**
```bash
# åˆ›å»ºæ¶ˆè´¹è€…ç»„
XGROUP CREATE mystream mygroup 0 MKSTREAM

# æ¶ˆè´¹è€…ç»„è¯»å–æ¶ˆæ¯
XREADGROUP GROUP mygroup consumer1 COUNT 1 STREAMS mystream >

# ç¡®è®¤æ¶ˆæ¯å¤„ç†å®Œæˆ
XACK mystream mygroup message_id
```

### 5.3 æ—¥å¿—ç³»ç»Ÿå®ä¾‹


**ğŸ’» åº”ç”¨æ—¥å¿—æ”¶é›†ç³»ç»Ÿ**
```python
import redis
import json
from datetime import datetime
import threading
import time

class LogCollector:
    def __init__(self, redis_client, stream_name="app_logs"):
        self.redis = redis_client
        self.stream_name = stream_name
        
    def add_log(self, level, message, user_id=None, ip=None, **extra_fields):
        """æ·»åŠ æ—¥å¿—è®°å½•"""
        log_data = {
            'timestamp': datetime.now().isoformat(),
            'level': level,
            'message': message,
            'thread_id': threading.get_ident()
        }
        
        # æ·»åŠ å¯é€‰å­—æ®µ
        if user_id:
            log_data['user_id'] = str(user_id)
        if ip:
            log_data['ip'] = ip
        
        # æ·»åŠ é¢å¤–å­—æ®µ
        log_data.update(extra_fields)
        
        # æ·»åŠ åˆ°Stream
        message_id = self.redis.xadd(self.stream_name, log_data)
        return message_id
    
    def get_recent_logs(self, count=100):
        """è·å–æœ€è¿‘çš„æ—¥å¿—"""
        logs = self.redis.xrevrange(self.stream_name, count=count)
        
        formatted_logs = []
        for log_id, fields in logs:
            log_entry = {
                'id': log_id.decode(),
                'timestamp': fields[b'timestamp'].decode(),
                'level': fields[b'level'].decode(),
                'message': fields[b'message'].decode()
            }
            
            # æ·»åŠ å…¶ä»–å­—æ®µ
            for key, value in fields.items():
                key_str = key.decode()
                if key_str not in ['timestamp', 'level', 'message']:
                    log_entry[key_str] = value.decode()
            
            formatted_logs.append(log_entry)
        
        return formatted_logs
    
    def get_logs_by_level(self, level, start_time=None, count=100):
        """æ ¹æ®æ—¥å¿—çº§åˆ«ç­›é€‰"""
        if start_time:
            start_id = f"{int(start_time.timestamp() * 1000)}-0"
        else:
            start_id = "-"  # ä»æœ€å¼€å§‹
        
        logs = self.redis.xrange(self.stream_name, start_id, count=count)
        
        filtered_logs = []
        for log_id, fields in logs:
            if fields.get(b'level', b'').decode() == level:
                log_entry = {
                    'id': log_id.decode(),
                    'timestamp': fields[b'timestamp'].decode(),
                    'level': fields[b'level'].decode(),
                    'message': fields[b'message'].decode()
                }
                filtered_logs.append(log_entry)
        
        return filtered_logs
    
    def create_log_consumer_group(self, group_name):
        """åˆ›å»ºæ—¥å¿—æ¶ˆè´¹è€…ç»„"""
        try:
            self.redis.xgroup_create(self.stream_name, group_name, id='0', mkstream=True)
            return True
        except redis.exceptions.ResponseError as e:
            if "BUSYGROUP" in str(e):
                print(f"æ¶ˆè´¹è€…ç»„ {group_name} å·²å­˜åœ¨")
                return True
            raise e
    
    def consume_logs(self, group_name, consumer_name, count=10, block_time=1000):
        """æ¶ˆè´¹æ—¥å¿—æ¶ˆæ¯"""
        try:
            messages = self.redis.xreadgroup(
                group_name, consumer_name, 
                {self.stream_name: '>'}, 
                count=count, block=block_time
            )
            
            processed_messages = []
            for stream, msgs in messages:
                for msg_id, fields in msgs:
                    log_entry = {
                        'id': msg_id.decode(),
                        'stream': stream.decode()
                    }
                    
                    # è§£æå­—æ®µ
                    for key, value in fields.items():
                        log_entry[key.decode()] = value.decode()
                    
                    processed_messages.append(log_entry)
                    
                    # ç¡®è®¤æ¶ˆæ¯å¤„ç†å®Œæˆ
                    self.redis.xack(self.stream_name, group_name, msg_id)
            
            return processed_messages
        except Exception as e:
            print(f"æ¶ˆè´¹æ—¥å¿—å¤±è´¥ï¼š{e}")
            return []

# ä½¿ç”¨ç¤ºä¾‹
redis_client = redis.Redis(host='localhost', port=6379, db=0)
log_collector = LogCollector(redis_client)

# æ·»åŠ ä¸åŒçº§åˆ«çš„æ—¥å¿—
log_collector.add_log('INFO', 'ç”¨æˆ·ç™»å½•æˆåŠŸ', user_id=123, ip='192.168.1.1')
log_collector.add_log('ERROR', 'æ•°æ®åº“è¿æ¥å¤±è´¥', error_code='DB001')
log_collector.add_log('WARNING', 'APIè°ƒç”¨è¶…æ—¶', api='/users', response_time=5.2)

# è·å–æœ€è¿‘æ—¥å¿—
recent_logs = log_collector.get_recent_logs(count=5)
print("æœ€è¿‘æ—¥å¿—ï¼š")
for log in recent_logs:
    print(f"[{log['level']}] {log['timestamp']}: {log['message']}")

# åˆ›å»ºæ¶ˆè´¹è€…ç»„è¿›è¡Œæ—¥å¿—å¤„ç†
log_collector.create_log_consumer_group('log_processors')

# æ¨¡æ‹Ÿæ—¥å¿—æ¶ˆè´¹è€…
def log_processor():
    while True:
        messages = log_collector.consume_logs('log_processors', 'worker1', count=5)
        for msg in messages:
            print(f"å¤„ç†æ—¥å¿—ï¼š{msg['level']} - {msg['message']}")
        time.sleep(1)

# åœ¨å®é™…åº”ç”¨ä¸­ï¼Œå¯ä»¥å¯åŠ¨å¤šä¸ªçº¿ç¨‹æ¥å¤„ç†æ—¥å¿—
# threading.Thread(target=log_processor, daemon=True).start()
```

### 5.4 æ¶ˆæ¯ä¸­é—´ä»¶æ›¿ä»£æ–¹æ¡ˆ


**ğŸ”„ ç®€å•æ¶ˆæ¯é˜Ÿåˆ—å®ç°**
```python
class StreamMessageQueue:
    def __init__(self, redis_client, queue_name):
        self.redis = redis_client
        self.queue_name = queue_name
    
    def send_message(self, message_type, data, priority=0):
        """å‘é€æ¶ˆæ¯"""
        message_data = {
            'type': message_type,
            'data': json.dumps(data) if isinstance(data, dict) else str(data),
            'priority': str(priority),
            'sent_at': datetime.now().isoformat()
        }
        
        return self.redis.xadd(self.queue_name, message_data)
    
    def create_consumer_group(self, group_name, start_id='0'):
        """åˆ›å»ºæ¶ˆè´¹è€…ç»„"""
        try:
            return self.redis.xgroup_create(self.queue_name, group_name, id=start_id, mkstream=True)
        except redis.exceptions.ResponseError as e:
            if "BUSYGROUP" not in str(e):
                raise e
    
    def consume_messages(self, group_name, consumer_name, count=10, block_time=5000):
        """æ¶ˆè´¹æ¶ˆæ¯"""
        messages = self.redis.xreadgroup(
            group_name, consumer_name,
            {self.queue_name: '>'},
            count=count, block=block_time
        )
        
        processed = []
        for stream, msgs in messages:
            for msg_id, fields in msgs:
                message = {
                    'id': msg_id.decode(),
                    'type': fields[b'type'].decode(),
                    'priority': int(fields[b'priority'].decode()),
                    'sent_at': fields[b'sent_at'].decode()
                }
                
                # è§£ææ•°æ®
                data_str = fields[b'data'].decode()
                try:
                    message['data'] = json.loads(data_str)
                except json.JSONDecodeError:
                    message['data'] = data_str
                
                processed.append(message)
        
        return processed
    
    def ack_message(self, group_name, message_id):
        """ç¡®è®¤æ¶ˆæ¯å¤„ç†å®Œæˆ"""
        return self.redis.xack(self.queue_name, group_name, message_id)
    
    def get_pending_messages(self, group_name, consumer_name=None):
        """è·å–æœªç¡®è®¤çš„æ¶ˆæ¯"""
        if consumer_name:
            # è·å–ç‰¹å®šæ¶ˆè´¹è€…çš„æœªç¡®è®¤æ¶ˆæ¯
            result = self.redis.xpending_range(
                self.queue_name, group_name, 
                min='-', max='+', count=100, consumer=consumer_name
            )
        else:
            # è·å–æ¶ˆè´¹è€…ç»„çš„æœªç¡®è®¤æ¶ˆæ¯æ¦‚è§ˆ
            result = self.redis.xpending(self.queue_name, group_name)
        
        return result

# ä½¿ç”¨ç¤ºä¾‹
message_queue = StreamMessageQueue(redis_client, 'task_queue')

# åˆ›å»ºæ¶ˆè´¹è€…ç»„
message_queue.create_consumer_group('workers')

# å‘é€ä»»åŠ¡æ¶ˆæ¯
task_data = {
    'user_id': 123,
    'action': 'send_email',
    'email': 'user@example.com',
    'template': 'welcome'
}
message_id = message_queue.send_message('email_task', task_data, priority=1)
print(f"å‘é€æ¶ˆæ¯ï¼š{message_id}")

# æ¶ˆè´¹æ¶ˆæ¯
messages = message_queue.consume_messages('workers', 'worker1', count=5)
for msg in messages:
    print(f"æ”¶åˆ°ä»»åŠ¡ï¼š{msg['type']} - {msg['data']}")
    
    # æ¨¡æ‹Ÿä»»åŠ¡å¤„ç†
    if msg['type'] == 'email_task':
        print(f"å¤„ç†é‚®ä»¶ä»»åŠ¡ï¼š{msg['data']['email']}")
        
        # ç¡®è®¤ä»»åŠ¡å®Œæˆ
        message_queue.ack_message('workers', msg['id'])
        print(f"ä»»åŠ¡ç¡®è®¤ï¼š{msg['id']}")
```

### 5.5 Streamæ€§èƒ½ä¼˜åŒ–


**âš¡ æ€§èƒ½ä¼˜åŒ–ç­–ç•¥**

1. **è®¾ç½®åˆç†çš„æœ€å¤§é•¿åº¦**
```python
def add_log_with_trimming(redis_client, stream_name, data, max_len=100000):
    """æ·»åŠ æ—¥å¿—å¹¶é™åˆ¶Streamé•¿åº¦"""
    return redis_client.xadd(
        stream_name, 
        data, 
        maxlen=max_len,
        approximate=True  # ä½¿ç”¨è¿‘ä¼¼ä¿®å‰ªï¼Œæ€§èƒ½æ›´å¥½
    )
```

2. **æ‰¹é‡æ¶ˆè´¹æ¶ˆæ¯**
```python
def batch_consume(redis_client, stream_name, group_name, consumer_name, batch_size=100):
    """æ‰¹é‡æ¶ˆè´¹æ¶ˆæ¯æé«˜æ•ˆç‡"""
    messages = redis_client.xreadgroup(
        group_name, consumer_name,
        {stream_name: '>'},
        count=batch_size,
        block=1000
    )
    
    # æ‰¹é‡å¤„ç†
    message_ids = []
    for stream, msgs in messages:
        for msg_id, fields in msgs:
            # å¤„ç†æ¶ˆæ¯é€»è¾‘...
            message_ids.append(msg_id)
    
    # æ‰¹é‡ç¡®è®¤
    if message_ids:
        redis_client.xack(stream_name, group_name, *message_ids)
    
    return len(message_ids)
```

3. **æ¸…ç†è¿‡æœŸæ¶ˆæ¯**
```python
def cleanup_old_messages(redis_client, stream_name, keep_days=30):
    """æ¸…ç†æ—§æ¶ˆæ¯"""
    cutoff_timestamp = int((datetime.now() - timedelta(days=keep_days)).timestamp() * 1000)
    cutoff_id = f"{cutoff_timestamp}-0"
    
    # åˆ é™¤æŒ‡å®šæ—¶é—´ä¹‹å‰çš„æ¶ˆæ¯
    return redis_client.xtrim(stream_name, minid=cutoff_id, approximate=True)
```

---

## 6. ğŸ¯ å®é™…åº”ç”¨åœºæ™¯


### 6.1 ç”¨æˆ·è¡Œä¸ºç»Ÿè®¡ç»¼åˆæ–¹æ¡ˆ


ç»“åˆå¤šç§æ•°æ®ç»“æ„å®ç°å®Œæ•´çš„ç”¨æˆ·è¡Œä¸ºç»Ÿè®¡ç³»ç»Ÿï¼š

```python
class UserBehaviorAnalytics:
    def __init__(self, redis_client):
        self.redis = redis_client
        
    def track_user_action(self, user_id, action, timestamp=None, **extra_data):
        """è·Ÿè¸ªç”¨æˆ·è¡Œä¸º"""
        if timestamp is None:
            timestamp = datetime.now()
        
        # 1. ç”¨Bitmapè®°å½•æ¯æ—¥æ´»è·ƒ
        date_str = timestamp.strftime('%Y%m%d')
        self.redis.setbit(f"active_users:{date_str}", user_id, 1)
        
        # 2. ç”¨HyperLogLogè®°å½•ç‹¬ç«‹è®¿å®¢
        self.redis.pfadd(f"uv:{date_str}", str(user_id))
        
        # 3. ç”¨Streamè®°å½•è¯¦ç»†è¡Œä¸ºæ—¥å¿—
        log_data = {
            'user_id': str(user_id),
            'action': action,
            'timestamp': timestamp.isoformat()
        }
        log_data.update(extra_data)
        self.redis.xadd('user_behaviors', log_data)
        
        # 4. å¦‚æœæœ‰åœ°ç†ä½ç½®ï¼Œè®°å½•åˆ°GEO
        if 'longitude' in extra_data and 'latitude' in extra_data:
            self.redis.geoadd(
                f"user_locations:{date_str}", 
                extra_data['longitude'], 
                extra_data['latitude'], 
                user_id
            )
    
    def get_daily_analytics(self, date_str):
        """è·å–æ¯æ—¥åˆ†ææŠ¥å‘Š"""
        return {
            'active_users': self.redis.bitcount(f"active_users:{date_str}"),
            'unique_visitors': self.redis.pfcount(f"uv:{date_str}"),
            'user_locations_count': self.redis.zcard(f"user_locations:{date_str}")
        }
    
    def get_user_nearby_actions(self, user_id, longitude, latitude, radius_km=1):
        """è·å–ç”¨æˆ·é™„è¿‘çš„å…¶ä»–ç”¨æˆ·è¡Œä¸º"""
        today = datetime.now().strftime('%Y%m%d')
        nearby_users = self.redis.georadius(
            f"user_locations:{today}",
            longitude, latitude, radius_km,
            unit='km', withdist=True
        )
        return nearby_users

# ä½¿ç”¨ç¤ºä¾‹
analytics = UserBehaviorAnalytics(redis_client)

# è·Ÿè¸ªç”¨æˆ·è¡Œä¸º
analytics.track_user_action(
    user_id=123,
    action='page_view',
    page='/products',
    longitude=116.404,
    latitude=39.915
)

# è·å–åˆ†ææŠ¥å‘Š
today = datetime.now().strftime('%Y%m%d')
report = analytics.get_daily_analytics(today)
print("ä»Šæ—¥åˆ†æï¼š", report)
```

### 6.2 åœ°ç†ä½ç½®æœåŠ¡æ•´åˆ


å°†GEOåŠŸèƒ½ä¸å…¶ä»–æ•°æ®ç»“æ„ç»“åˆï¼Œå®ç°å®Œæ•´çš„LBSæœåŠ¡ï¼š

```python
class IntegratedLBSService:
    def __init__(self, redis_client):
        self.redis = redis_client
    
    def checkin(self, user_id, location_name, longitude, latitude):
        """ç”¨æˆ·ç­¾åˆ°"""
        checkin_time = datetime.now()
        
        # 1. æ›´æ–°ç”¨æˆ·åœ°ç†ä½ç½®
        self.redis.geoadd('user_locations', longitude, latitude, user_id)
        
        # 2. è®°å½•ç­¾åˆ°å†å²ï¼ˆStreamï¼‰
        checkin_data = {
            'user_id': str(user_id),
            'location_name': location_name,
            'longitude': str(longitude),
            'latitude': str(latitude),
            'checkin_time': checkin_time.isoformat()
        }
        self.redis.xadd('checkin_history', checkin_data)
        
        # 3. ç»Ÿè®¡åœ°ç‚¹çƒ­åº¦ï¼ˆHyperLogLogï¼‰
        location_key = f"location_visitors:{location_name}"
        self.redis.pfadd(location_key, str(user_id))
        
        # 4. è®°å½•ç”¨æˆ·ç­¾åˆ°ä½å›¾
        date_str = checkin_time.strftime('%Y%m%d')
        self.redis.setbit(f"user_checkins:{user_id}:{date_str[:6]}", checkin_time.day - 1, 1)
        
        return True
    
    def get_location_stats(self, location_name):
        """è·å–åœ°ç‚¹ç»Ÿè®¡ä¿¡æ¯"""
        visitor_count = self.redis.pfcount(f"location_visitors:{location_name}")
        
        # è·å–æœ€è¿‘ç­¾åˆ°è®°å½•
        recent_checkins = self.redis.xrevrange(
            'checkin_history', 
            count=10
        )
        
        location_checkins = []
        for checkin_id, fields in recent_checkins:
            if fields[b'location_name'].decode() == location_name:
                location_checkins.append({
                    'user_id': fields[b'user_id'].decode(),
                    'checkin_time': fields[b'checkin_time'].decode()
                })
        
        return {
            'location_name': location_name,
            'total_visitors': visitor_count,
            'recent_checkins': location_checkins[:5]
        }
    
    def find_popular_locations(self, user_longitude, user_latitude, radius_km=10):
        """æ‰¾å‡ºç”¨æˆ·é™„è¿‘çš„çƒ­é—¨åœ°ç‚¹"""
        # è¿™é‡Œç®€åŒ–å®ç°ï¼Œå®é™…ä¸­éœ€è¦ç»´æŠ¤åœ°ç‚¹çš„åœ°ç†ä½ç½®æ•°æ®
        # å¯ä»¥ç»“åˆå¤šä¸ªæ•°æ®ç»“æ„å®ç°å¤æ‚æŸ¥è¯¢
        pass

# ä½¿ç”¨ç¤ºä¾‹
lbs_service = IntegratedLBSService(redis_client)

# ç”¨æˆ·ç­¾åˆ°
lbs_service.checkin(123, 'æ˜Ÿå·´å…‹å’–å•¡', 116.404, 39.915)
lbs_service.checkin(456, 'æ˜Ÿå·´å…‹å’–å•¡', 116.404, 39.915)

# æŸ¥çœ‹åœ°ç‚¹ç»Ÿè®¡
stats = lbs_service.get_location_stats('æ˜Ÿå·´å…‹å’–å•¡')
print("åœ°ç‚¹ç»Ÿè®¡ï¼š", stats)
```

### 6.3 æ•°æ®ç»“æ„é€‰æ‹©æŒ‡å—


æ ¹æ®ä¸åŒçš„åº”ç”¨éœ€æ±‚ï¼Œé€‰æ‹©æœ€åˆé€‚çš„æ•°æ®ç»“æ„ï¼š

| éœ€æ±‚åœºæ™¯ | æ¨èæ•°æ®ç»“æ„ | åŸå› è¯´æ˜ | ç¤ºä¾‹ |
|---------|-------------|---------|------|
| **æ¯æ—¥ç­¾åˆ°è®°å½•** | `Bitmap` | èŠ‚çœå†…å­˜ï¼Œå¿«é€Ÿç»Ÿè®¡ | `SETBIT user:123:202501 5 1` |
| **ç½‘ç«™UVç»Ÿè®¡** | `HyperLogLog` | å†…å­˜å ç”¨å›ºå®šï¼Œå…è®¸è¯¯å·® | `PFADD daily_uv user123` |
| **é™„è¿‘çš„äºº** | `GEO` | æ”¯æŒè·ç¦»è®¡ç®—å’ŒèŒƒå›´æŸ¥è¯¢ | `GEORADIUS users 116.4 39.9 5 km` |
| **ç”¨æˆ·è¡Œä¸ºæ—¥å¿—** | `Stream` | æ—¶åºæ•°æ®ï¼Œæ”¯æŒå›æ”¾ | `XADD behaviors * user 123 action click` |
| **åœ¨çº¿çŠ¶æ€** | `String + TTL` | ç®€å•çŠ¶æ€ï¼Œè‡ªåŠ¨è¿‡æœŸ | `SET online:123 1 EX 300` |
| **è®¿é—®è®¡æ•°** | `String` | ç®€å•é€’å¢ | `INCR page_views:homepage` |

### 6.4 æ··åˆä½¿ç”¨æœ€ä½³å®è·µ


**å¤æ‚ä¸šåŠ¡åœºæ™¯çš„æ•°æ®ç»“æ„ç»„åˆ**ï¼š

```python
class ComprehensiveAnalytics:
    def __init__(self, redis_client):
        self.redis = redis_client
    
    def record_user_visit(self, user_id, page_id, longitude=None, latitude=None):
        """è®°å½•ç”¨æˆ·è®¿é—®"""
        now = datetime.now()
        date_str = now.strftime('%Y%m%d')
        
        # 1. Bitmapï¼šè®°å½•ç”¨æˆ·æ¯æ—¥æ´»è·ƒçŠ¶æ€
        self.redis.setbit(f"daily_active:{date_str}", user_id, 1)
        
        # 2. HyperLogLogï¼šé¡µé¢UVç»Ÿè®¡
        self.redis.pfadd(f"page_uv:{page_id}:{date_str}", str(user_id))
        
        # 3. Stringï¼šé¡µé¢PVè®¡æ•°
        self.redis.incr(f"page_pv:{page_id}:{date_str}")
        
        # 4. Streamï¼šè¯¦ç»†è®¿é—®æ—¥å¿—
        visit_data = {
            'user_id': str(user_id),
            'page_id': page_id,
            'timestamp': now.isoformat(),
            'user_agent': 'mobile_app'  # ç¤ºä¾‹
        }
        
        if longitude and latitude:
            visit_data.update({
                'longitude': str(longitude),
                'latitude': str(latitude)
            })
            # 5. GEOï¼šç”¨æˆ·ä½ç½®ä¿¡æ¯
            self.redis.geoadd(f"user_locations:{date_str}", longitude, latitude, user_id)
        
        self.redis.xadd('visit_logs', visit_data)
        
        # 6. Hashï¼šç”¨æˆ·ä¼šè¯ä¿¡æ¯
        session_key = f"session:{user_id}"
        self.redis.hset(session_key, mapping={
            'last_visit': now.isoformat(),
            'page_id': page_id
        })
        self.redis.expire(session_key, 3600)  # 1å°æ—¶ä¼šè¯è¿‡æœŸ
    
    def get_comprehensive_stats(self, date_str):
        """è·å–ç»¼åˆç»Ÿè®¡æŠ¥å‘Š"""
        stats = {
            'date': date_str,
            'daily_active_users': self.redis.bitcount(f"daily_active:{date_str}"),
            'page_stats': {}
        }
        
        # è·å–æ‰€æœ‰é¡µé¢çš„ç»Ÿè®¡
        page_keys = list(self.redis.scan_iter(match=f"page_uv:*:{date_str}"))
        for key in page_keys:
            key_str = key.decode() if isinstance(key, bytes) else key
            page_id = key_str.split(':')[1]  # æå–é¡µé¢ID
            
            uv = self.redis.pfcount(key)
            pv = int(self.redis.get(f"page_pv:{page_id}:{date_str}") or 0)
            
            stats['page_stats'][page_id] = {
                'uv': uv,
                'pv': pv,
                'avg_pv_per_user': round(pv / uv, 2) if uv > 0 else 0
            }
        
        return stats

# ä½¿ç”¨ç¤ºä¾‹
analytics = ComprehensiveAnalytics(redis_client)

# æ¨¡æ‹Ÿç”¨æˆ·è®¿é—®
for i in range(1000):
    user_id = i % 100 + 1  # 100ä¸ªç”¨æˆ·ï¼Œæœ‰é‡å¤è®¿é—®
    page_id = 'homepage' if i % 3 == 0 else 'product'
    
    # éƒ¨åˆ†ç”¨æˆ·æœ‰ä½ç½®ä¿¡æ¯
    if i % 5 == 0:
        lng = 116.404 + (i % 10) * 0.001  # æ¨¡æ‹Ÿä¸åŒä½ç½®
        lat = 39.915 + (i % 10) * 0.001
        analytics.record_user_visit(user_id, page_id, lng, lat)
    else:
        analytics.record_user_visit(user_id, page_id)

# ç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
today = datetime.now().strftime('%Y%m%d')
comprehensive_stats = analytics.get_comprehensive_stats(today)
print("ç»¼åˆç»Ÿè®¡æŠ¥å‘Šï¼š")
print(json.dumps(comprehensive_stats, indent=2, ensure_ascii=False))
```

### 6.5 æ¶ˆæ¯ç³»ç»Ÿä¸å®æ—¶é€šçŸ¥


**å®æ—¶é€šçŸ¥ç³»ç»Ÿç»“åˆStreamå’Œå…¶ä»–ç»“æ„**ï¼š

```python
class NotificationSystem:
    def __init__(self, redis_client):
        self.redis = redis_client
        self.notifications_stream = 'notifications'
    
    def send_notification(self, user_ids, notification_type, title, content, extra_data=None):
        """å‘é€é€šçŸ¥"""
        notification_id = str(int(time.time() * 1000))
        
        for user_id in user_ids:
            # 1. Streamï¼šå­˜å‚¨é€šçŸ¥è¯¦æƒ…
            notification_data = {
                'notification_id': notification_id,
                'user_id': str(user_id),
                'type': notification_type,
                'title': title,
                'content': content,
                'created_at': datetime.now().isoformat(),
                'status': 'unread'
            }
            
            if extra_data:
                notification_data.update(extra_data)
            
            self.redis.xadd(f'user_notifications:{user_id}', notification_data)
            
            # 2. Bitmapï¼šè®°å½•ç”¨æˆ·æœ‰æœªè¯»æ¶ˆæ¯
            date_str = datetime.now().strftime('%Y%m%d')
            self.redis.setbit(f'unread_notifications:{date_str}', user_id, 1)
            
            # 3. Stringï¼šæœªè¯»æ¶ˆæ¯è®¡æ•°
            self.redis.incr(f'unread_count:{user_id}')
            self.redis.expire(f'unread_count:{user_id}', 3600 * 24 * 30)
        
        return notification_id
    
    def mark_as_read(self, user_id, notification_id):
        """æ ‡è®°é€šçŸ¥ä¸ºå·²è¯»"""
        # å‡å°‘æœªè¯»è®¡æ•°
        current_count = self.redis.decr(f'unread_count:{user_id}')
        
        # å¦‚æœè®¡æ•°ä¸º0ï¼Œæ¸…é™¤æœªè¯»æ ‡è®°
        if current_count <= 0:
            date_str = datetime.now().strftime('%Y%m%d')
            self.redis.setbit(f'unread_notifications:{date_str}', user_id, 0)
            self.redis.delete(f'unread_count:{user_id}')
    
    def get_users_with_unread(self, date_str=None):
        """è·å–æœ‰æœªè¯»æ¶ˆæ¯çš„ç”¨æˆ·åˆ—è¡¨"""
        if date_str is None:
            date_str = datetime.now().strftime('%Y%m%d')
        
        # ä½¿ç”¨Bitmapå¿«é€Ÿç­›é€‰
        unread_bitmap_key = f'unread_notifications:{date_str}'
        
        # è¿™é‡Œç®€åŒ–å¤„ç†ï¼Œå®é™…ä¸­éœ€è¦éå†Bitmap
        # æˆ–è€…ä½¿ç”¨å…¶ä»–æ–¹å¼è·å–æœ‰æœªè¯»æ¶ˆæ¯çš„ç”¨æˆ·
        unread_users = []
        for user_id in range(1, 10000):  # å‡è®¾ç”¨æˆ·IDèŒƒå›´
            if self.redis.getbit(unread_bitmap_key, user_id):
                unread_count = self.redis.get(f'unread_count:{user_id}')
                if unread_count:
                    unread_users.append({
                        'user_id': user_id,
                        'unread_count': int(unread_count)
                    })
        
        return unread_users

# ä½¿ç”¨ç¤ºä¾‹
notification_system = NotificationSystem(redis_client)

# ç¾¤å‘é€šçŸ¥
user_list = [123, 456, 789]
notification_system.send_notification(
    user_list,
    'promotion',
    'é™æ—¶ä¼˜æƒ ',
    'å…¨åœºå•†å“8æŠ˜ï¼Œä»…é™ä»Šæ—¥ï¼',
    {'promotion_code': 'SALE80', 'expire_time': '2025-01-29'}
)

# æŸ¥çœ‹æœ‰æœªè¯»æ¶ˆæ¯çš„ç”¨æˆ·
users_with_unread = notification_system.get_users_with_unread()
print("æœ‰æœªè¯»æ¶ˆæ¯çš„ç”¨æˆ·ï¼š", users_with_unread[:5])
```

---

## 7. ğŸ“‹ æ ¸å¿ƒè¦ç‚¹æ€»ç»“


### 7.1 å¿…é¡»æŒæ¡çš„æ ¸å¿ƒæ¦‚å¿µ


```
ğŸ”¸ Bitmapä½å›¾ï¼šç”¨ä½è¡¨ç¤ºçŠ¶æ€ï¼Œæåº¦èŠ‚çœå†…å­˜ï¼Œé€‚åˆç”¨æˆ·ç­¾åˆ°ã€æ´»è·ƒç»Ÿè®¡
ğŸ”¸ HyperLogLogï¼šæ¦‚ç‡æ€§åŸºæ•°ç»Ÿè®¡ï¼Œå›ºå®š12KBå†…å­˜ï¼Œ0.81%è¯¯å·®ï¼Œé€‚åˆUVç»Ÿè®¡
ğŸ”¸ GEOåœ°ç†ä½ç½®ï¼šåŸºäºæœ‰åºé›†åˆï¼Œæ”¯æŒè·ç¦»è®¡ç®—å’ŒèŒƒå›´æŸ¥è¯¢ï¼Œé€‚åˆLBSåº”ç”¨
ğŸ”¸ Streamæµï¼šæ—¶åºæ•°æ®ç»“æ„ï¼Œæ”¯æŒæ¶ˆè´¹è€…ç»„ï¼Œé€‚åˆæ—¥å¿—ç³»ç»Ÿå’Œæ¶ˆæ¯é˜Ÿåˆ—
```

### 7.2 å…³é”®ä½¿ç”¨åœºæ™¯å¯¹æ¯”


| æ•°æ®ç»“æ„ | **æœ€é€‚åˆçš„åœºæ™¯** | **å†…å­˜ç‰¹ç‚¹** | **æ€§èƒ½ç‰¹ç‚¹** | **é™åˆ¶** |
|---------|---------------|-------------|-------------|---------|
| **Bitmap** | `ç”¨æˆ·ç­¾åˆ°ã€æ´»è·ƒç»Ÿè®¡` | `1ç”¨æˆ·1ä½ï¼Œæçœå†…å­˜` | `ä½è¿ç®—æå¿«` | `åªèƒ½å­˜å¸ƒå°”å€¼ï¼Œç¨€ç–æµªè´¹` |
| **HyperLogLog** | `UVç»Ÿè®¡ã€å»é‡è®¡æ•°` | `å›ºå®š12KBå†…å­˜` | `å¸¸æ•°æ—¶é—´å¤æ‚åº¦` | `æœ‰0.81%è¯¯å·®ï¼Œä¸èƒ½è·å–å…ƒç´ ` |
| **GEO** | `LBSæœåŠ¡ã€ä½ç½®æŸ¥è¯¢` | `åæ ‡+æˆå‘˜ä¿¡æ¯` | `åœ°ç†è®¡ç®—ä¼˜åŒ–` | `åŸºäºZsetï¼Œä¸æ”¯æŒå¤æ‚å‡ ä½•` |
| **Stream** | `æ—¥å¿—ã€æ¶ˆæ¯é˜Ÿåˆ—` | `æ¶ˆæ¯ç´¯ç§¯å¢é•¿` | `é¡ºåºè¯»å†™ä¼˜åŒ–` | `åªèƒ½è¿½åŠ ï¼Œéœ€è¦å®šæœŸæ¸…ç†` |

### 7.3 å®é™…åº”ç”¨é€‰æ‹©åŸåˆ™


**ğŸ¯ é€‰æ‹©Bitmapçš„æ—¶æœº**
```
âœ… é€‚ç”¨ï¼š
- éœ€è¦è®°å½•ç”¨æˆ·çš„æ¯æ—¥çŠ¶æ€ï¼ˆç­¾åˆ°ã€æ´»è·ƒç­‰ï¼‰
- ç”¨æˆ·IDç›¸å¯¹è¿ç»­ï¼ˆå¦‚1åˆ°1000000ï¼‰
- éœ€è¦å¿«é€Ÿç»Ÿè®¡ç¬¦åˆæ¡ä»¶çš„ç”¨æˆ·æ•°
- å†…å­˜ä½¿ç”¨è¦æ±‚ä¸¥æ ¼

âŒ ä¸é€‚ç”¨ï¼š
- ç”¨æˆ·IDç¨€ç–ï¼ˆå¦‚åªæœ‰1ã€100000ã€900000å‡ ä¸ªç”¨æˆ·ï¼‰
- éœ€è¦å­˜å‚¨å¤šç§çŠ¶æ€ï¼ˆä¸åªæ˜¯æœ‰/æ— ï¼‰
- éœ€è¦æŸ¥è¯¢å…·ä½“å“ªäº›ç”¨æˆ·ï¼ˆBitmapä¸èƒ½åå‘æŸ¥è¯¢ï¼‰
```

**ğŸ¯ é€‰æ‹©HyperLogLogçš„æ—¶æœº**
```
âœ… é€‚ç”¨ï¼š
- åªéœ€è¦çŸ¥é“æ•°é‡ï¼Œä¸éœ€è¦çŸ¥é“å…·ä½“å…ƒç´ 
- æ•°æ®é‡å¾ˆå¤§ï¼ˆ>10ä¸‡ï¼‰ï¼Œå†…å­˜æœ‰é™
- å¯ä»¥æ¥å—å°å¹…åº¦è¯¯å·®ï¼ˆå¦‚ç»Ÿè®¡ç±»ä¸šåŠ¡ï¼‰
- éœ€è¦åˆå¹¶å¤šä¸ªæ—¶é—´æ®µçš„ç»Ÿè®¡

âŒ ä¸é€‚ç”¨ï¼š
- éœ€è¦ç²¾ç¡®è®¡æ•°ï¼ˆå¦‚é‡‘èäº¤æ˜“ï¼‰
- æ•°æ®é‡å¾ˆå°ï¼ˆ<1ä¸‡ï¼Œç”¨Setæ›´ç®€å•ï¼‰
- éœ€è¦è·å–å…·ä½“çš„å…ƒç´ åˆ—è¡¨
```

**ğŸ¯ é€‰æ‹©GEOçš„æ—¶æœº**
```
âœ… é€‚ç”¨ï¼š
- éœ€è¦åœ°ç†è·ç¦»è®¡ç®—
- éœ€è¦èŒƒå›´æŸ¥è¯¢ï¼ˆé™„è¿‘çš„XXXï¼‰
- ä½ç½®æ•°æ®éœ€è¦æŒä¹…åŒ–
- LBSç›¸å…³çš„ä¸šåŠ¡åœºæ™¯

âŒ ä¸é€‚ç”¨ï¼š
- åªæ˜¯ç®€å•å­˜å‚¨åæ ‡ï¼Œä¸éœ€è¦è®¡ç®—
- éœ€è¦å¤æ‚çš„åœ°ç†è®¡ç®—ï¼ˆå¤šè¾¹å½¢ã€è·¯å¾„è§„åˆ’ç­‰ï¼‰
- å¯¹ç²¾åº¦è¦æ±‚æé«˜çš„åº”ç”¨
```

**ğŸ¯ é€‰æ‹©Streamçš„æ—¶æœº**
```
âœ… é€‚ç”¨ï¼š
- æ—¶é—´åºåˆ—æ•°æ®ï¼ˆæ—¥å¿—ã€äº‹ä»¶ï¼‰
- éœ€è¦æ¶ˆæ¯ç¡®è®¤æœºåˆ¶
- æ”¯æŒå¤šæ¶ˆè´¹è€…ç»„
- éœ€è¦æ¶ˆæ¯å›æ”¾åŠŸèƒ½

âŒ ä¸é€‚ç”¨ï¼š
- ç®€å•çš„é˜Ÿåˆ—éœ€æ±‚ï¼ˆListå¤Ÿç”¨ï¼‰
- æ¶ˆæ¯ä¸éœ€è¦æŒä¹…åŒ–ï¼ˆPub/Subå¤Ÿç”¨ï¼‰
- å†…å­˜æåº¦å—é™ï¼ˆStreamä¼šç´¯ç§¯æ¶ˆæ¯ï¼‰
```

### 7.4 æ€§èƒ½ä¼˜åŒ–æ ¸å¿ƒè¦ç‚¹


**ğŸ’¡ é€šç”¨ä¼˜åŒ–åŸåˆ™**
```
å†…å­˜ä¼˜åŒ–ï¼š
- Bitmapï¼šé¿å…ç¨€ç–æ•°æ®ï¼Œåˆç†è®¾ç½®ç”¨æˆ·IDèŒƒå›´
- HyperLogLogï¼šå®šæœŸåˆå¹¶ç»Ÿè®¡ï¼Œé¿å…åˆ›å»ºè¿‡å¤škey
- GEOï¼šå®šæœŸæ¸…ç†è¿‡æœŸä½ç½®ä¿¡æ¯
- Streamï¼šè®¾ç½®maxlené™åˆ¶æ¶ˆæ¯æ•°é‡

æŸ¥è¯¢ä¼˜åŒ–ï¼š
- ä½¿ç”¨COUNTé™åˆ¶è¿”å›ç»“æœæ•°é‡
- åˆç†è®¾ç½®æŸ¥è¯¢åŠå¾„å’Œæ—¶é—´èŒƒå›´
- æ‰¹é‡æ“ä½œä»£æ›¿å•ä¸ªæ“ä½œ
- ç»“åˆæœ¬åœ°ç¼“å­˜å‡å°‘Redisè®¿é—®

ä¸šåŠ¡è®¾è®¡ï¼š
- æ ¹æ®æŸ¥è¯¢æ¨¡å¼è®¾è®¡æ•°æ®ç»“æ„
- é¢„ä¼°æ•°æ®é‡ï¼Œæå‰è§„åˆ’å®¹é‡
- è®¾ç½®åˆç†çš„è¿‡æœŸæ—¶é—´
- å»ºç«‹ç›‘æ§å’Œå‘Šè­¦æœºåˆ¶
```

### 7.5 å¸¸è§ä½¿ç”¨é™·é˜±


**âš ï¸ Bitmapé™·é˜±**
```bash
# å±é™©ï¼šç¨€ç–ç”¨æˆ·ID
SETBIT users 1 1          # æ­£å¸¸
SETBIT users 10000000 1   # æµªè´¹äº†1åƒä¸‡ä½çš„å†…å­˜

# å®‰å…¨ï¼šè¿ç»­ç”¨æˆ·IDæˆ–ä½¿ç”¨å“ˆå¸Œæ˜ å°„
SETBIT users 1 1
SETBIT users 2 1
```

**âš ï¸ HyperLogLogé™·é˜±**
```bash
# å±é™©ï¼šæŠŠHyperLogLogå½“ç²¾ç¡®è®¡æ•°å™¨
PFCOUNT user_orders  # åªèƒ½ä¼°ç®—ï¼Œä¸æ˜¯ç²¾ç¡®å€¼

# å®‰å…¨ï¼šæ˜ç¡®è¿™æ˜¯ä¼°ç®—å€¼
estimated_count = PFCOUNT user_orders
print(f"ä¼°ç®—è®¢å•æ•°ï¼šçº¦{estimated_count}ä¸ª")
```

**âš ï¸ GEOé™·é˜±**
```bash
# å±é™©ï¼šæŸ¥è¯¢èŒƒå›´è¿‡å¤§
GEORADIUS locations 116.404 39.915 1000000 km  # å…¨çƒèŒƒå›´æŸ¥è¯¢

# å®‰å…¨ï¼šåˆç†é™åˆ¶æŸ¥è¯¢èŒƒå›´
GEORADIUS locations 116.404 39.915 10 km COUNT 50
```

**âš ï¸ Streamé™·é˜±**
```bash
# å±é™©ï¼šStreamæ— é™å¢é•¿
XADD logs * level INFO message "test"  # æ²¡æœ‰é•¿åº¦é™åˆ¶

# å®‰å…¨ï¼šè®¾ç½®æœ€å¤§é•¿åº¦
XADD logs MAXLEN ~ 100000 * level INFO message "test"
```

### 7.6 ç›‘æ§å’Œç»´æŠ¤


**ğŸ“Š å…³é”®ç›‘æ§æŒ‡æ ‡**
```python
def monitor_advanced_structures(redis_client):
    """ç›‘æ§é«˜çº§æ•°æ®ç»“æ„ä½¿ç”¨æƒ…å†µ"""
    report = {
        'timestamp': datetime.now().isoformat(),
        'bitmap_stats': {},
        'hyperloglog_stats': {},
        'geo_stats': {},
        'stream_stats': {}
    }
    
    # ç›‘æ§æ‰€æœ‰key
    for key in redis_client.scan_iter():
        key_str = key.decode() if isinstance(key, bytes) else key
        key_type = redis_client.type(key).decode()
        
        if key_type == 'string' and 'active_users:' in key_str:
            # Bitmapç›‘æ§
            bit_count = redis_client.bitcount(key)
            memory = redis_client.memory_usage(key)
            report['bitmap_stats'][key_str] = {
                'active_bits': bit_count,
                'memory_bytes': memory
            }
        
        elif key_str.startswith('uv:'):
            # HyperLogLogç›‘æ§
            count = redis_client.pfcount(key)
            memory = redis_client.memory_usage(key)
            report['hyperloglog_stats'][key_str] = {
                'estimated_count': count,
                'memory_bytes': memory
            }
        
        elif key_type == 'zset' and 'locations' in key_str:
            # GEOç›‘æ§
            member_count = redis_client.zcard(key)
            memory = redis_client.memory_usage(key)
            report['geo_stats'][key_str] = {
                'location_count': member_count,
                'memory_bytes': memory
            }
        
        elif key_type == 'stream':
            # Streamç›‘æ§
            stream_info = redis_client.xinfo_stream(key)
            report['stream_stats'][key_str] = {
                'length': stream_info['length'],
                'groups': stream_info['groups'],
                'last_entry_id': stream_info['last-generated-id'].decode()
            }
    
    return report

# å®šæœŸæ‰§è¡Œç›‘æ§
monitoring_report = monitor_advanced_structures(redis_client)
print("é«˜çº§æ•°æ®ç»“æ„ç›‘æ§æŠ¥å‘Šï¼š")
print(json.dumps(monitoring_report, indent=2, ensure_ascii=False))
```

**æ ¸å¿ƒè®°å¿†**ï¼š
- é«˜çº§æ•°æ®ç»“æ„è§£å†³ç‰¹å®šåœºæ™¯é—®é¢˜ï¼Œé€‰å¯¹ç»“æ„äº‹åŠåŠŸå€
- Bitmapçœå†…å­˜è®°çŠ¶æ€ï¼ŒHyperLogLogä¼°ç®—å¤§æ•°æ®  
- GEOç®—è·ç¦»æŸ¥é™„è¿‘ï¼ŒStreamå­˜æ—¥å¿—åšé˜Ÿåˆ—
- ç»„åˆä½¿ç”¨æ•ˆæœå¥½ï¼Œç›‘æ§ç»´æŠ¤ä¸å¯å°‘